W&B disabled.
2023-08-25 13:59:39,432 MainThread INFO: Experiment Name:testing_must_mtsac
2023-08-25 13:59:39,433 MainThread INFO: {
  "env_name": "mt10",
  "env": {
    "reward_scale": 1,
    "obs_norm": false
  },
  "meta_env": {
    "obs_type": "with_goal_and_id"
  },
  "replay_buffer": {
    "size": 1000000.0
  },
  "net": {
    "hidden_shapes": [
      50,
      50
    ]
  },
  "task_embedding": {
    "em_hidden_shapes": [
      50,
      50
    ]
  },
  "traj_encoder": {
    "hidden_shapes": [
      50,
      50
    ],
    "latent_size": 64
  },
  "sparse_training": {
    "pruning_ratio": 0.9
  },
  "general_setting": {
    "discount": 0.99,
    "pretrain_epochs": 0,
    "num_epochs": 200,
    "epoch_frames": 150,
    "max_episode_frames": 150,
    "batch_size": 1280,
    "min_pool": 10000,
    "target_hard_update_period": 1000,
    "use_soft_update": true,
    "tau": 0.005,
    "opt_times": 2,
    "mask_update_interval": 22,
    "update_end_epoch": 50,
    "eval_episodes": 1
  },
  "sac": {
    "plr": 0.0003,
    "qlr": 0.0003,
    "reparameterization": true,
    "automatic_entropy_tuning": true,
    "policy_std_reg_weight": 0,
    "policy_mean_reg_weight": 0
  }
}
finish policy net init
mask generator finish initialization
/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Could not seed environment <MTEnv instance>[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
2023-08-25 14:03:35,119 MainThread INFO: Finished Pretrain
  0%|          | 0/200 [00:00<?, ?it/s]epoch, update_end_epoch 0 50
freq 22
sample: [6, 5, 0, 3, 9, 8, 4, 2, 1, 7]
replay_buffer._size: [300 300 300 300 300 300 300 300 300 300]
train_time 0.35111093521118164
eval time 7.7695605754852295
snapshot at best
2023-08-25 14:03:44,131 MainThread INFO: EPOCH:0
2023-08-25 14:03:44,131 MainThread INFO: Time Consumed:8.771560668945312s
2023-08-25 14:03:44,131 MainThread INFO: Total Frames:1500s
/scratch/qianxi/t3s/t3s_code/./torchrl/algo/rl_algo.py:341: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  value = torch.sum((self.mask_buffer["Policy"][each_task][0] == 0).nonzero().squeeze()).item()
  0%|          | 1/200 [00:09<32:36,  9.83s/it]------------------------------------  -----------  --------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1150.65769
Train_Epoch_Reward                    19064.02106
Running_Training_Average_Rewards      1906.40211
Explore_Time                          0.00546
Train___Time                          0.35111
Eval____Time                          7.76956
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -44.03903
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.58947
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.67230
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.90012
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.26688
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.96329
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.10439
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11750.63279
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.72102
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.79937
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           6.64085      0.37975   7.02060     6.26110
alpha_0                               0.99955      0.00015   0.99970     0.99940
alpha_1                               0.99955      0.00015   0.99970     0.99940
alpha_2                               0.99955      0.00015   0.99970     0.99940
alpha_3                               0.99955      0.00015   0.99970     0.99940
alpha_4                               0.99955      0.00015   0.99970     0.99940
alpha_5                               0.99955      0.00015   0.99970     0.99940
alpha_6                               0.99955      0.00015   0.99970     0.99940
alpha_7                               0.99955      0.00015   0.99970     0.99940
alpha_8                               0.99955      0.00015   0.99970     0.99940
alpha_9                               0.99955      0.00015   0.99970     0.99940
Alpha_loss                            -0.00100     0.00100   -0.00000    -0.00200
Training/policy_loss                  -2.67548     0.00486   -2.67062    -2.68035
Training/qf1_loss                     959.89398    54.99799  1014.89197  904.89600
Training/qf2_loss                     959.92010    54.99902  1014.91913  904.92108
Training/pf_norm                      0.44084      0.01099   0.45182     0.42985
Training/qf1_norm                     19.01170     0.75876   19.77047    18.25294
Training/qf2_norm                     18.83871     0.74546   19.58417    18.09326
log_std/mean                          -0.00127     0.00022   -0.00105    -0.00149
log_std/std                           0.00116      0.00000   0.00116     0.00115
log_std/max                           0.00052      0.00023   0.00075     0.00029
log_std/min                           -0.00374     0.00024   -0.00350    -0.00398
log_probs/mean                        -2.67898     0.00469   -2.67429    -2.68367
log_probs/std                         0.43973      0.01963   0.45936     0.42010
log_probs/max                         -1.39780     0.05315   -1.34465    -1.45096
log_probs/min                         -5.47410     1.82363   -3.65047    -7.29773
mean/mean                             0.00005      0.00009   0.00014     -0.00004
mean/std                              0.00127      0.00013   0.00139     0.00114
mean/max                              0.00187      0.00020   0.00207     0.00168
mean/min                              -0.00282     0.00007   -0.00275    -0.00289
------------------------------------  -----------  --------  ----------  ---------
snapshot at 0
history save at ./log/testing_must_mtsac/mt10/17/model
epoch, update_end_epoch 1 50
freq 22
sample: [5, 7, 1, 3, 4, 2, 6, 0, 8, 9]
replay_buffer._size: [450 450 450 450 450 450 450 450 450 450]
train_time 0.23598027229309082
eval time 0.003098011016845703
2023-08-25 14:03:45,361 MainThread INFO: EPOCH:1
2023-08-25 14:03:45,362 MainThread INFO: Time Consumed:0.24945783615112305s
2023-08-25 14:03:45,362 MainThread INFO: Total Frames:3000s
  1%|          | 2/200 [00:10<14:10,  4.30s/it]------------------------------------  -----------  -------  ---------  ---------
Name                                  Value
Running_Average_Rewards               1150.01674
Train_Epoch_Reward                    3060.12110
Running_Training_Average_Rewards      1106.20711
Explore_Time                          0.00493
Train___Time                          0.23598
Eval____Time                          0.00310
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -56.85807
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.58947
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.67230
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.90012
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.26688
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.96329
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.10439
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11750.63279
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.72102
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.79937
mean_success_rate                     0.00000

Name                                  Mean         Std      Max        Min
Reward_Mean                           6.50356      0.21834  6.72190    6.28522
alpha_0                               0.99895      0.00015  0.99910    0.99880
alpha_1                               0.99895      0.00015  0.99910    0.99880
alpha_2                               0.99895      0.00015  0.99910    0.99880
alpha_3                               0.99895      0.00015  0.99910    0.99880
alpha_4                               0.99895      0.00015  0.99910    0.99880
alpha_5                               0.99895      0.00015  0.99910    0.99880
alpha_6                               0.99895      0.00015  0.99910    0.99880
alpha_7                               0.99895      0.00015  0.99910    0.99880
alpha_8                               0.99895      0.00015  0.99910    0.99880
alpha_9                               0.99895      0.00015  0.99910    0.99880
Alpha_loss                            -0.00501     0.00100  -0.00401   -0.00602
Training/policy_loss                  -2.68147     0.00047  -2.68099   -2.68194
Training/qf1_loss                     927.07373    1.77197  928.84570  925.30176
Training/qf2_loss                     927.10059    1.77277  928.87335  925.32782
Training/pf_norm                      0.39928      0.00459  0.40387    0.39469
Training/qf1_norm                     18.72187     0.45529  19.17716   18.26658
Training/qf2_norm                     18.55501     0.44743  19.00245   18.10758
log_std/mean                          -0.00214     0.00022  -0.00193   -0.00236
log_std/std                           0.00116      0.00000  0.00116    0.00116
log_std/max                           -0.00041     0.00023  -0.00018   -0.00064
log_std/min                           -0.00468     0.00023  -0.00445   -0.00491
log_probs/mean                        -2.68569     0.00029  -2.68539   -2.68598
log_probs/std                         0.43884      0.00515  0.44399    0.43368
log_probs/max                         -1.28658     0.06256  -1.22402   -1.34914
log_probs/min                         -4.19805     0.34769  -3.85036   -4.54574
mean/mean                             -0.00007     0.00002  -0.00005   -0.00010
mean/std                              0.00093      0.00003  0.00096    0.00090
mean/max                              0.00114      0.00015  0.00129    0.00098
mean/min                              -0.00274     0.00003  -0.00271   -0.00276
------------------------------------  -----------  -------  ---------  ---------
epoch, update_end_epoch 2 50
freq 22
sample: [2, 8, 0, 1, 5, 9, 4, 7, 3, 6]
replay_buffer._size: [600 600 600 600 600 600 579 600 600 599]
train_time 0.14472746849060059
eval time 0.0032274723052978516
2023-08-25 14:03:45,633 MainThread INFO: EPOCH:2
2023-08-25 14:03:45,633 MainThread INFO: Time Consumed:0.15250897407531738s
2023-08-25 14:03:45,633 MainThread INFO: Total Frames:4500s
  2%|▏         | 3/200 [00:10<08:03,  2.45s/it]------------------------------------  -----------  --------  ---------  ---------
Name                                  Value
Running_Average_Rewards               1149.93408
Train_Epoch_Reward                    6144.96616
Running_Training_Average_Rewards      942.30361
Explore_Time                          0.00397
Train___Time                          0.14473
Eval____Time                          0.00323
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.92816
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.58947
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.67230
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.90012
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.26688
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.96329
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.10439
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11750.63279
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.72102
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.79937
mean_success_rate                     0.00000

Name                                  Mean         Std       Max        Min
Reward_Mean                           5.18328      0.10544   5.28871    5.07784
alpha_0                               0.99835      0.00015   0.99850    0.99820
alpha_1                               0.99835      0.00015   0.99850    0.99820
alpha_2                               0.99835      0.00015   0.99850    0.99820
alpha_3                               0.99835      0.00015   0.99850    0.99820
alpha_4                               0.99835      0.00015   0.99850    0.99820
alpha_5                               0.99835      0.00015   0.99850    0.99820
alpha_6                               0.99835      0.00015   0.99850    0.99820
alpha_7                               0.99835      0.00015   0.99850    0.99820
alpha_8                               0.99835      0.00015   0.99850    0.99820
alpha_9                               0.99835      0.00015   0.99850    0.99820
Alpha_loss                            -0.00903     0.00102   -0.00801   -0.01005
Training/policy_loss                  -2.68107     0.01310   -2.66797   -2.69417
Training/qf1_loss                     658.42599    46.94424  705.37024  611.48175
Training/qf2_loss                     658.44952    46.94409  705.39362  611.50543
Training/pf_norm                      0.40711      0.02503   0.43215    0.38208
Training/qf1_norm                     15.99460     0.20918   16.20378   15.78542
Training/qf2_norm                     15.85580     0.20649   16.06228   15.64931
log_std/mean                          -0.00301     0.00022   -0.00279   -0.00323
log_std/std                           0.00116      0.00000   0.00117    0.00116
log_std/max                           -0.00130     0.00021   -0.00108   -0.00151
log_std/min                           -0.00560     0.00023   -0.00537   -0.00584
log_probs/mean                        -2.68600     0.01330   -2.67270   -2.69930
log_probs/std                         0.43075      0.00166   0.43240    0.42909
log_probs/max                         -1.30636     0.05747   -1.24889   -1.36383
log_probs/min                         -4.79271     0.22693   -4.56578   -5.01964
mean/mean                             -0.00011     0.00002   -0.00009   -0.00014
mean/std                              0.00088      0.00001   0.00089    0.00087
mean/max                              0.00122      0.00009   0.00131    0.00114
mean/min                              -0.00284     0.00003   -0.00281   -0.00287
------------------------------------  -----------  --------  ---------  ---------
epoch, update_end_epoch 3 50
freq 22
sample: [4, 3, 7, 8, 2, 6, 1, 5, 0, 9]
replay_buffer._size: [750 750 725 750 747 734 750 723 750 736]
train_time 0.13714814186096191
eval time 0.0023496150970458984
2023-08-25 14:03:45,902 MainThread INFO: EPOCH:3
2023-08-25 14:03:45,902 MainThread INFO: Time Consumed:0.1423022747039795s
2023-08-25 14:03:45,902 MainThread INFO: Total Frames:6000s
  2%|▏         | 4/200 [00:10<05:11,  1.59s/it]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1149.59296
Train_Epoch_Reward                    2825.88361
Running_Training_Average_Rewards      401.03236
Explore_Time                          0.00228
Train___Time                          0.13715
Eval____Time                          0.00235
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.11346
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.62132
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.66353
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.87275
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.25531
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.94126
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.08889
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11745.40374
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.74381
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.76027
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           8.70302      0.03461   8.73763     8.66841
alpha_0                               0.99775      0.00015   0.99790     0.99760
alpha_1                               0.99775      0.00015   0.99790     0.99760
alpha_2                               0.99775      0.00015   0.99790     0.99760
alpha_3                               0.99775      0.00015   0.99790     0.99760
alpha_4                               0.99775      0.00015   0.99790     0.99760
alpha_5                               0.99775      0.00015   0.99790     0.99760
alpha_6                               0.99775      0.00015   0.99790     0.99760
alpha_7                               0.99775      0.00015   0.99790     0.99760
alpha_8                               0.99775      0.00015   0.99790     0.99760
alpha_9                               0.99775      0.00015   0.99790     0.99760
Alpha_loss                            -0.01303     0.00100   -0.01203    -0.01403
Training/policy_loss                  -2.67651     0.00318   -2.67333    -2.67969
Training/qf1_loss                     1628.62579   29.28595  1657.91174  1599.33984
Training/qf2_loss                     1628.66260   29.28638  1657.94897  1599.37622
Training/pf_norm                      0.43197      0.00943   0.44140     0.42254
Training/qf1_norm                     23.28365     0.06844   23.35208    23.21521
Training/qf2_norm                     23.03896     0.06687   23.10583    22.97209
log_std/mean                          -0.00388     0.00022   -0.00366    -0.00410
log_std/std                           0.00117      0.00000   0.00117     0.00117
log_std/max                           -0.00214     0.00021   -0.00193    -0.00236
log_std/min                           -0.00654     0.00023   -0.00631    -0.00678
log_probs/mean                        -2.68214     0.00301   -2.67913    -2.68515
log_probs/std                         0.42434      0.00434   0.42868     0.42000
log_probs/max                         -1.24456     0.11526   -1.12930    -1.35982
log_probs/min                         -4.55554     0.54729   -4.00825    -5.10283
mean/mean                             -0.00010     0.00000   -0.00009    -0.00010
mean/std                              0.00094      0.00003   0.00098     0.00091
mean/max                              0.00154      0.00008   0.00162     0.00146
mean/min                              -0.00298     0.00006   -0.00292    -0.00305
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 4 50
freq 22
sample: [8, 5, 3, 4, 7, 9, 6, 1, 2, 0]
replay_buffer._size: [881 889 852 899 892 889 879 848 874 875]
train_time 0.18079519271850586
eval time 0.002651214599609375
2023-08-25 14:03:46,189 MainThread INFO: EPOCH:4
2023-08-25 14:03:46,189 MainThread INFO: Time Consumed:0.18618535995483398s
2023-08-25 14:03:46,189 MainThread INFO: Total Frames:7500s
  2%|▎         | 5/200 [00:11<03:38,  1.12s/it]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1149.37273
Train_Epoch_Reward                    32211.20963
Running_Training_Average_Rewards      1372.73531
Explore_Time                          0.00222
Train___Time                          0.18080
Eval____Time                          0.00265
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.41434
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.61049
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.67785
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.84062
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.24790
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.92490
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.08602
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11737.41720
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.73428
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.72988
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           10.33747     0.33328   10.67075    10.00419
alpha_0                               0.99715      0.00015   0.99730     0.99700
alpha_1                               0.99715      0.00015   0.99730     0.99700
alpha_2                               0.99715      0.00015   0.99730     0.99700
alpha_3                               0.99715      0.00015   0.99730     0.99700
alpha_4                               0.99716      0.00015   0.99730     0.99701
alpha_5                               0.99715      0.00015   0.99730     0.99700
alpha_6                               0.99715      0.00015   0.99730     0.99701
alpha_7                               0.99715      0.00015   0.99730     0.99700
alpha_8                               0.99715      0.00015   0.99730     0.99700
alpha_9                               0.99715      0.00015   0.99730     0.99701
Alpha_loss                            -0.01707     0.00097   -0.01610    -0.01804
Training/policy_loss                  -2.68918     0.01351   -2.67566    -2.70269
Training/qf1_loss                     2107.60266   71.44714  2179.04980  2036.15552
Training/qf2_loss                     2107.64752   71.44891  2179.09644  2036.19861
Training/pf_norm                      0.39354      0.05306   0.44661     0.34048
Training/qf1_norm                     26.66645     0.70408   27.37053    25.96237
Training/qf2_norm                     26.36271     0.69351   27.05622    25.66920
log_std/mean                          -0.00475     0.00022   -0.00453    -0.00497
log_std/std                           0.00118      0.00000   0.00118     0.00118
log_std/max                           -0.00300     0.00021   -0.00279    -0.00320
log_std/min                           -0.00749     0.00024   -0.00725    -0.00772
log_probs/mean                        -2.69555     0.01338   -2.68218    -2.70893
log_probs/std                         0.43274      0.00114   0.43388     0.43160
log_probs/max                         -1.31590     0.03313   -1.28278    -1.34903
log_probs/min                         -4.44989     0.45530   -3.99459    -4.90520
mean/mean                             -0.00010     0.00002   -0.00008    -0.00012
mean/std                              0.00109      0.00004   0.00113     0.00104
mean/max                              0.00186      0.00006   0.00191     0.00180
mean/min                              -0.00318     0.00008   -0.00310    -0.00325
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 5 50
freq 22
sample: [5, 3, 1, 0, 4, 8, 2, 7, 6, 9]
replay_buffer._size: [1031 1030 1030 1038 1034 1033 1034 1030 1015 1011]
train_time 0.20615577697753906
eval time 0.0034339427947998047
2023-08-25 14:03:46,498 MainThread INFO: EPOCH:5
2023-08-25 14:03:46,498 MainThread INFO: Time Consumed:0.21196675300598145s
2023-08-25 14:03:46,498 MainThread INFO: Total Frames:9000s
  3%|▎         | 6/200 [00:11<02:44,  1.18it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1147.84329
Train_Epoch_Reward                    26172.13208
Running_Training_Average_Rewards      2040.30751
Explore_Time                          0.00194
Train___Time                          0.20616
Eval____Time                          0.00343
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -54.03731
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.59847
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.66647
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.81600
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.22686
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.88956
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.05494
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11705.54294
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.72479
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.72384
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.11794     0.83976    10.95770    9.27818
alpha_0                               0.99656      0.00015    0.99670     0.99641
alpha_1                               0.99655      0.00015    0.99670     0.99640
alpha_2                               0.99656      0.00015    0.99670     0.99641
alpha_3                               0.99656      0.00015    0.99671     0.99641
alpha_4                               0.99656      0.00015    0.99671     0.99641
alpha_5                               0.99656      0.00015    0.99671     0.99641
alpha_6                               0.99656      0.00015    0.99671     0.99641
alpha_7                               0.99655      0.00015    0.99670     0.99641
alpha_8                               0.99656      0.00015    0.99671     0.99641
alpha_9                               0.99656      0.00015    0.99671     0.99641
Alpha_loss                            -0.02103     0.00101    -0.02002    -0.02204
Training/policy_loss                  -2.66810     0.00348    -2.66462    -2.67157
Training/qf1_loss                     1960.17523   332.57745  2292.75269  1627.59778
Training/qf2_loss                     1960.22095   332.58032  2292.80127  1627.64062
Training/pf_norm                      0.42180      0.00836    0.43016     0.41344
Training/qf1_norm                     26.20421     1.75835    27.96256    24.44586
Training/qf2_norm                     25.89971     1.73273    27.63244    24.16699
log_std/mean                          -0.00562     0.00022    -0.00540    -0.00584
log_std/std                           0.00120      0.00000    0.00120     0.00119
log_std/max                           -0.00382     0.00021    -0.00361    -0.00403
log_std/min                           -0.00844     0.00023    -0.00820    -0.00867
log_probs/mean                        -2.67511     0.00366    -2.67145    -2.67878
log_probs/std                         0.42091      0.01009    0.43100     0.41083
log_probs/max                         -1.37030     0.00270    -1.36760    -1.37300
log_probs/min                         -4.05887     0.20306    -3.85581    -4.26192
mean/mean                             -0.00012     0.00000    -0.00012    -0.00012
mean/std                              0.00122      0.00004    0.00126     0.00118
mean/max                              0.00208      0.00005    0.00213     0.00202
mean/min                              -0.00324     0.00002    -0.00321    -0.00326
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 6 50
freq 22
sample: [8, 5, 3, 7, 4, 9, 6, 1, 0, 2]
replay_buffer._size: [1200 1200 1200 1200 1200 1200 1200 1200 1200 1200]
train_time 0.13724875450134277
eval time 0.0026133060455322266
2023-08-25 14:03:46,783 MainThread INFO: EPOCH:6
2023-08-25 14:03:46,783 MainThread INFO: Time Consumed:0.14358234405517578s
2023-08-25 14:03:46,783 MainThread INFO: Total Frames:10500s
  4%|▎         | 7/200 [00:11<02:08,  1.50it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1145.69278
Train_Epoch_Reward                    15473.08212
Running_Training_Average_Rewards      2461.88079
Explore_Time                          0.00289
Train___Time                          0.13725
Eval____Time                          0.00261
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -43.96841
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.59639
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.64804
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.80284
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.18386
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.84655
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.00467
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11675.32076
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.71998
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.72212
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           9.89911      0.11076   10.00987    9.78835
alpha_0                               0.99596      0.00015   0.99611     0.99581
alpha_1                               0.99596      0.00015   0.99611     0.99581
alpha_2                               0.99596      0.00015   0.99611     0.99581
alpha_3                               0.99596      0.00015   0.99611     0.99581
alpha_4                               0.99596      0.00015   0.99611     0.99581
alpha_5                               0.99596      0.00015   0.99611     0.99581
alpha_6                               0.99596      0.00015   0.99611     0.99581
alpha_7                               0.99596      0.00015   0.99611     0.99581
alpha_8                               0.99596      0.00015   0.99611     0.99581
alpha_9                               0.99596      0.00015   0.99611     0.99581
Alpha_loss                            -0.02512     0.00098   -0.02413    -0.02610
Training/policy_loss                  -2.69030     0.00582   -2.68448    -2.69612
Training/qf1_loss                     1904.85217   34.54395  1939.39612  1870.30823
Training/qf2_loss                     1904.89862   34.54352  1939.44214  1870.35510
Training/pf_norm                      0.38694      0.00310   0.39003     0.38384
Training/qf1_norm                     25.74403     0.23198   25.97602    25.51205
Training/qf2_norm                     25.44407     0.23695   25.68102    25.20712
log_std/mean                          -0.00651     0.00022   -0.00628    -0.00673
log_std/std                           0.00121      0.00000   0.00121     0.00121
log_std/max                           -0.00468     0.00022   -0.00446    -0.00490
log_std/min                           -0.00939     0.00024   -0.00915    -0.00963
log_probs/mean                        -2.69810     0.00567   -2.69243    -2.70376
log_probs/std                         0.42310      0.00462   0.42772     0.41848
log_probs/max                         -1.31537     0.04332   -1.27205    -1.35869
log_probs/min                         -4.10970     0.32754   -3.78216    -4.43724
mean/mean                             -0.00013     0.00000   -0.00013    -0.00014
mean/std                              0.00136      0.00002   0.00138     0.00134
mean/max                              0.00228      0.00003   0.00231     0.00225
mean/min                              -0.00332     0.00001   -0.00332    -0.00333
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 7 50
freq 22
sample: [6, 0, 5, 3, 7, 1, 2, 9, 8, 4]
replay_buffer._size: [1350 1350 1350 1350 1350 1339 1350 1350 1318 1350]
train_time 0.13860821723937988
eval time 0.0028824806213378906
2023-08-25 14:03:47,055 MainThread INFO: EPOCH:7
2023-08-25 14:03:47,055 MainThread INFO: Time Consumed:0.14493799209594727s
2023-08-25 14:03:47,055 MainThread INFO: Total Frames:12000s
  4%|▍         | 8/200 [00:11<01:43,  1.86it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1142.88798
Train_Epoch_Reward                    10481.47494
Running_Training_Average_Rewards      1737.55630
Explore_Time                          0.00286
Train___Time                          0.13861
Eval____Time                          0.00288
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -44.68582
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.59063
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.63885
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.79830
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.15106
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.81402
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.95638
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11647.09514
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.71706
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.73619
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           8.51232      0.06674   8.57907     8.44558
alpha_0                               0.99536      0.00015   0.99551     0.99521
alpha_1                               0.99536      0.00015   0.99551     0.99521
alpha_2                               0.99536      0.00015   0.99551     0.99521
alpha_3                               0.99536      0.00015   0.99551     0.99521
alpha_4                               0.99536      0.00015   0.99551     0.99521
alpha_5                               0.99536      0.00015   0.99551     0.99521
alpha_6                               0.99536      0.00015   0.99551     0.99521
alpha_7                               0.99536      0.00015   0.99551     0.99521
alpha_8                               0.99536      0.00015   0.99551     0.99521
alpha_9                               0.99536      0.00015   0.99551     0.99521
Alpha_loss                            -0.02909     0.00099   -0.02810    -0.03009
Training/policy_loss                  -2.67952     0.00275   -2.67677    -2.68227
Training/qf1_loss                     1429.06494   52.74390  1481.80884  1376.32104
Training/qf2_loss                     1429.10767   52.74353  1481.85120  1376.36414
Training/pf_norm                      0.42013      0.01563   0.43576     0.40450
Training/qf1_norm                     22.90173     0.14866   23.05038    22.75307
Training/qf2_norm                     22.64010     0.14821   22.78831    22.49189
log_std/mean                          -0.00739     0.00022   -0.00717    -0.00761
log_std/std                           0.00122      0.00000   0.00122     0.00122
log_std/max                           -0.00556     0.00022   -0.00534    -0.00578
log_std/min                           -0.01034     0.00023   -0.01010    -0.01057
log_probs/mean                        -2.68797     0.00259   -2.68538    -2.69057
log_probs/std                         0.42099      0.00014   0.42112     0.42085
log_probs/max                         -1.31886     0.07362   -1.24524    -1.39248
log_probs/min                         -4.03474     0.06987   -3.96487    -4.10461
mean/mean                             -0.00015     0.00000   -0.00015    -0.00015
mean/std                              0.00136      0.00002   0.00138     0.00134
mean/max                              0.00227      0.00004   0.00232     0.00223
mean/min                              -0.00341     0.00000   -0.00340    -0.00341
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 8 50
freq 22
sample: [5, 7, 6, 1, 2, 0, 4, 3, 8, 9]
replay_buffer._size: [1496 1442 1490 1487 1499 1492 1467 1493 1478 1427]
train_time 0.2310502529144287
eval time 0.0029306411743164062
2023-08-25 14:03:47,393 MainThread INFO: EPOCH:8
2023-08-25 14:03:47,394 MainThread INFO: Time Consumed:0.2367570400238037s
2023-08-25 14:03:47,394 MainThread INFO: Total Frames:13500s
  4%|▍         | 9/200 [00:12<01:30,  2.11it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1140.85640
Train_Epoch_Reward                    5236.43646
Running_Training_Average_Rewards      1039.69978
Explore_Time                          0.00227
Train___Time                          0.23105
Eval____Time                          0.00293
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.76573
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.59559
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.64494
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.80195
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.14025
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.81223
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.94143
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11640.02077
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.72430
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.73715
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.83580      0.92733    9.76313     7.90848
alpha_0                               0.99476      0.00015    0.99491     0.99461
alpha_1                               0.99476      0.00015    0.99491     0.99461
alpha_2                               0.99476      0.00015    0.99491     0.99461
alpha_3                               0.99476      0.00015    0.99491     0.99461
alpha_4                               0.99477      0.00015    0.99492     0.99462
alpha_5                               0.99477      0.00015    0.99491     0.99462
alpha_6                               0.99476      0.00015    0.99491     0.99461
alpha_7                               0.99476      0.00015    0.99491     0.99461
alpha_8                               0.99476      0.00015    0.99491     0.99461
alpha_9                               0.99476      0.00015    0.99491     0.99461
Alpha_loss                            -0.03309     0.00101    -0.03208    -0.03410
Training/policy_loss                  -2.67474     0.00095    -2.67378    -2.67569
Training/qf1_loss                     1467.65399   262.02374  1729.67773  1205.63025
Training/qf2_loss                     1467.69958   262.02771  1729.72729  1205.67188
Training/pf_norm                      0.41810      0.00770    0.42580     0.41040
Training/qf1_norm                     23.61566     1.91022    25.52588    21.70544
Training/qf2_norm                     23.34069     1.87543    25.21612    21.46526
log_std/mean                          -0.00829     0.00023    -0.00806    -0.00851
log_std/std                           0.00122      0.00000    0.00122     0.00122
log_std/max                           -0.00644     0.00022    -0.00622    -0.00666
log_std/min                           -0.01127     0.00024    -0.01103    -0.01151
log_probs/mean                        -2.68385     0.00112    -2.68273    -2.68498
log_probs/std                         0.41990      0.01056    0.43046     0.40934
log_probs/max                         -1.30588     0.03511    -1.27077    -1.34099
log_probs/min                         -4.61930     0.54393    -4.07538    -5.16323
mean/mean                             -0.00024     0.00003    -0.00021    -0.00027
mean/std                              0.00129      0.00001    0.00129     0.00128
mean/max                              0.00200      0.00007    0.00207     0.00192
mean/min                              -0.00338     0.00000    -0.00338    -0.00338
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 9 50
freq 22
sample: [9, 7, 2, 0, 4, 3, 6, 8, 5, 1]
replay_buffer._size: [1629 1616 1630 1604 1638 1616 1620 1635 1577 1614]
train_time 0.24607348442077637
eval time 0.0025382041931152344
2023-08-25 14:03:47,742 MainThread INFO: EPOCH:9
2023-08-25 14:03:47,742 MainThread INFO: Time Consumed:0.25102734565734863s
2023-08-25 14:03:47,742 MainThread INFO: Total Frames:15000s
  5%|▌         | 10/200 [00:12<01:22,  2.30it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1139.36231
Train_Epoch_Reward                    20959.04770
Running_Training_Average_Rewards      1222.56530
Explore_Time                          0.00200
Train___Time                          0.24607
Eval____Time                          0.00254
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -57.42769
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.58196
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.66001
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.78553
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.12911
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.80429
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.92394
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11643.76245
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.71570
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.72907
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           8.16393      0.15689   8.32082     8.00704
alpha_0                               0.99417      0.00015   0.99432     0.99402
alpha_1                               0.99416      0.00015   0.99431     0.99401
alpha_2                               0.99417      0.00015   0.99431     0.99402
alpha_3                               0.99417      0.00015   0.99432     0.99402
alpha_4                               0.99417      0.00015   0.99432     0.99402
alpha_5                               0.99417      0.00015   0.99432     0.99402
alpha_6                               0.99417      0.00015   0.99432     0.99402
alpha_7                               0.99416      0.00015   0.99431     0.99402
alpha_8                               0.99417      0.00015   0.99432     0.99402
alpha_9                               0.99417      0.00015   0.99431     0.99402
Alpha_loss                            -0.03714     0.00094   -0.03620    -0.03809
Training/policy_loss                  -2.68233     0.01106   -2.67126    -2.69339
Training/qf1_loss                     1328.82465   33.70270  1362.52734  1295.12195
Training/qf2_loss                     1328.86920   33.70282  1362.57202  1295.16638
Training/pf_norm                      0.38532      0.01702   0.40234     0.36830
Training/qf1_norm                     22.20770     0.31051   22.51821    21.89719
Training/qf2_norm                     21.95178     0.30617   22.25795    21.64561
log_std/mean                          -0.00919     0.00023   -0.00896    -0.00941
log_std/std                           0.00122      0.00000   0.00122     0.00121
log_std/max                           -0.00734     0.00022   -0.00712    -0.00757
log_std/min                           -0.01223     0.00024   -0.01198    -0.01247
log_probs/mean                        -2.69218     0.01096   -2.68122    -2.70314
log_probs/std                         0.41880      0.00338   0.42218     0.41542
log_probs/max                         -1.35540     0.01389   -1.34150    -1.36929
log_probs/min                         -3.88629     0.08629   -3.80001    -3.97258
mean/mean                             -0.00039     0.00005   -0.00035    -0.00044
mean/std                              0.00128      0.00000   0.00129     0.00128
mean/max                              0.00167      0.00009   0.00176     0.00159
mean/min                              -0.00340     0.00001   -0.00339    -0.00341
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 10 50
freq 22
sample: [2, 8, 9, 5, 1, 3, 7, 4, 6, 0]
replay_buffer._size: [1800 1800 1800 1800 1800 1800 1800 1800 1800 1800]
train_time 0.202117919921875
eval time 0.0034132003784179688
2023-08-25 14:03:48,147 MainThread INFO: EPOCH:10
2023-08-25 14:03:48,147 MainThread INFO: Time Consumed:0.20862793922424316s
2023-08-25 14:03:48,147 MainThread INFO: Total Frames:16500s
  6%|▌         | 11/200 [00:13<01:20,  2.35it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1138.36354
Train_Epoch_Reward                    1916.32350
Running_Training_Average_Rewards      937.06026
Explore_Time                          0.00258
Train___Time                          0.20212
Eval____Time                          0.00341
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -45.51850
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.57194
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.64841
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.77710
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.09402
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.76922
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.88316
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11617.75203
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.70776
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.73820
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           7.70959      0.62344    8.33303     7.08615
alpha_0                               0.99357      0.00015    0.99372     0.99342
alpha_1                               0.99357      0.00015    0.99372     0.99342
alpha_2                               0.99357      0.00015    0.99372     0.99342
alpha_3                               0.99357      0.00015    0.99372     0.99342
alpha_4                               0.99357      0.00015    0.99372     0.99342
alpha_5                               0.99357      0.00015    0.99372     0.99342
alpha_6                               0.99357      0.00015    0.99372     0.99342
alpha_7                               0.99357      0.00015    0.99372     0.99342
alpha_8                               0.99357      0.00015    0.99372     0.99342
alpha_9                               0.99357      0.00015    0.99372     0.99342
Alpha_loss                            -0.04115     0.00102    -0.04013    -0.04217
Training/policy_loss                  -2.68068     0.00240    -2.67828    -2.68308
Training/qf1_loss                     1316.24817   179.88489  1496.13306  1136.36328
Training/qf2_loss                     1316.29150   179.88843  1496.17993  1136.40308
Training/pf_norm                      0.39542      0.01150    0.40692     0.38391
Training/qf1_norm                     21.26852     1.29910    22.56763    19.96942
Training/qf2_norm                     21.02724     1.27545    22.30269    19.75179
log_std/mean                          -0.01010     0.00023    -0.00987    -0.01032
log_std/std                           0.00122      0.00000    0.00122     0.00121
log_std/max                           -0.00824     0.00023    -0.00802    -0.00847
log_std/min                           -0.01320     0.00024    -0.01296    -0.01344
log_probs/mean                        -2.69119     0.00258    -2.68861    -2.69377
log_probs/std                         0.40648      0.00301    0.40949     0.40347
log_probs/max                         -1.37545     0.10229    -1.27316    -1.47774
log_probs/min                         -4.60356     0.48763    -4.11593    -5.09119
mean/mean                             -0.00062     0.00006    -0.00056    -0.00068
mean/std                              0.00128      0.00001    0.00129     0.00127
mean/max                              0.00128      0.00010    0.00138     0.00119
mean/min                              -0.00342     0.00000    -0.00341    -0.00342
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 11 50
freq 22
sample: [1, 4, 7, 3, 6, 9, 5, 0, 8, 2]
replay_buffer._size: [1950 1950 1950 1950 1950 1950 1950 1950 1950 1950]
train_time 0.16842913627624512
eval time 0.002347230911254883
2023-08-25 14:03:48,460 MainThread INFO: EPOCH:11
2023-08-25 14:03:48,460 MainThread INFO: Time Consumed:0.17457103729248047s
2023-08-25 14:03:48,460 MainThread INFO: Total Frames:18000s
  6%|▌         | 12/200 [00:13<01:13,  2.56it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1136.96216
Train_Epoch_Reward                    11366.63077
Running_Training_Average_Rewards      1141.40007
Explore_Time                          0.00301
Train___Time                          0.16843
Eval____Time                          0.00235
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.81825
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.57896
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.62936
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.77905
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.05534
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.73159
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.84607
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11595.71580
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.71762
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.74356
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           8.76740      0.39919   9.16659     8.36821
alpha_0                               0.99298      0.00015   0.99312     0.99283
alpha_1                               0.99297      0.00015   0.99312     0.99282
alpha_2                               0.99297      0.00015   0.99312     0.99282
alpha_3                               0.99297      0.00015   0.99312     0.99282
alpha_4                               0.99298      0.00015   0.99313     0.99283
alpha_5                               0.99298      0.00015   0.99313     0.99283
alpha_6                               0.99297      0.00015   0.99312     0.99283
alpha_7                               0.99297      0.00015   0.99312     0.99282
alpha_8                               0.99297      0.00015   0.99312     0.99283
alpha_9                               0.99297      0.00015   0.99312     0.99282
Alpha_loss                            -0.04518     0.00106   -0.04412    -0.04624
Training/policy_loss                  -2.68113     0.00838   -2.67275    -2.68951
Training/qf1_loss                     1500.62305   23.30151  1523.92456  1477.32153
Training/qf2_loss                     1500.67407   23.29944  1523.97351  1477.37463
Training/pf_norm                      0.38934      0.02888   0.41822     0.36045
Training/qf1_norm                     23.46848     0.84505   24.31353    22.62342
Training/qf2_norm                     23.17885     0.82969   24.00854    22.34917
log_std/mean                          -0.01101     0.00023   -0.01078    -0.01124
log_std/std                           0.00122      0.00000   0.00123     0.00122
log_std/max                           -0.00914     0.00022   -0.00892    -0.00936
log_std/min                           -0.01417     0.00025   -0.01392    -0.01442
log_probs/mean                        -2.69230     0.00860   -2.68370    -2.70090
log_probs/std                         0.41470      0.00680   0.42150     0.40789
log_probs/max                         -1.42920     0.00465   -1.42455    -1.43385
log_probs/min                         -4.29503     0.37619   -3.91884    -4.67122
mean/mean                             -0.00077     0.00003   -0.00074    -0.00080
mean/std                              0.00132      0.00000   0.00132     0.00132
mean/max                              0.00108      0.00003   0.00110     0.00105
mean/min                              -0.00342     0.00001   -0.00341    -0.00343
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 12 50
freq 22
sample: [6, 2, 7, 0, 8, 3, 1, 9, 4, 5]
replay_buffer._size: [2100 2100 2100 2100 2100 2100 2100 2100 2100 2100]
train_time 0.13662266731262207
eval time 0.0026047229766845703
2023-08-25 14:03:48,769 MainThread INFO: EPOCH:12
2023-08-25 14:03:48,770 MainThread INFO: Time Consumed:0.14255499839782715s
2023-08-25 14:03:48,770 MainThread INFO: Total Frames:19500s
  6%|▋         | 13/200 [00:13<01:08,  2.71it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1135.06590
Train_Epoch_Reward                    18170.58854
Running_Training_Average_Rewards      1048.45143
Explore_Time                          0.00272
Train___Time                          0.13662
Eval____Time                          0.00260
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -46.85477
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.57645
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.60874
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.77289
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.01052
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.68766
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.80428
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11575.89465
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.71735
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.74468
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           9.42432      0.07653   9.50085     9.34779
alpha_0                               0.99238      0.00015   0.99253     0.99223
alpha_1                               0.99238      0.00015   0.99252     0.99223
alpha_2                               0.99238      0.00015   0.99253     0.99223
alpha_3                               0.99238      0.00015   0.99253     0.99223
alpha_4                               0.99238      0.00015   0.99253     0.99223
alpha_5                               0.99238      0.00015   0.99253     0.99224
alpha_6                               0.99238      0.00015   0.99253     0.99223
alpha_7                               0.99238      0.00015   0.99252     0.99223
alpha_8                               0.99238      0.00015   0.99253     0.99223
alpha_9                               0.99237      0.00015   0.99252     0.99223
Alpha_loss                            -0.04923     0.00095   -0.04828    -0.05018
Training/policy_loss                  -2.68563     0.00723   -2.67840    -2.69285
Training/qf1_loss                     1620.69128   84.65381  1705.34509  1536.03748
Training/qf2_loss                     1620.74731   84.65405  1705.40137  1536.09326
Training/pf_norm                      0.38416      0.03058   0.41474     0.35358
Training/qf1_norm                     24.91220     0.18374   25.09594    24.72846
Training/qf2_norm                     24.59205     0.18845   24.78051    24.40360
log_std/mean                          -0.01193     0.00023   -0.01170    -0.01216
log_std/std                           0.00124      0.00000   0.00124     0.00123
log_std/max                           -0.01001     0.00022   -0.00980    -0.01023
log_std/min                           -0.01519     0.00025   -0.01494    -0.01544
log_probs/mean                        -2.69747     0.00711   -2.69036    -2.70459
log_probs/std                         0.40947      0.00245   0.41192     0.40702
log_probs/max                         -1.45915     0.03710   -1.42205    -1.49626
log_probs/min                         -4.32776     0.00503   -4.32272    -4.33279
mean/mean                             -0.00082     0.00000   -0.00082    -0.00082
mean/std                              0.00133      0.00001   0.00134     0.00133
mean/max                              0.00106      0.00000   0.00107     0.00106
mean/min                              -0.00330     0.00000   -0.00330    -0.00331
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 13 50
freq 22
sample: [8, 2, 3, 6, 5, 1, 4, 0, 9, 7]
replay_buffer._size: [2250 2250 2250 2250 2250 2250 2250 2250 2250 2250]
train_time 0.15630030632019043
eval time 0.0031425952911376953
2023-08-25 14:03:49,136 MainThread INFO: EPOCH:13
2023-08-25 14:03:49,136 MainThread INFO: Time Consumed:0.16296982765197754s
2023-08-25 14:03:49,136 MainThread INFO: Total Frames:21000s
  7%|▋         | 14/200 [00:14<01:08,  2.72it/s]------------------------------------  -----------  ---------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1134.19261
Train_Epoch_Reward                    10401.75667
Running_Training_Average_Rewards      1331.29920
Explore_Time                          0.00293
Train___Time                          0.15630
Eval____Time                          0.00314
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -46.12270
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.57000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.61346
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.74516
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.98358
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.66416
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.77926
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11591.75449
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.71099
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.72014
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           7.39099      0.68207    8.07306     6.70891
alpha_0                               0.99178      0.00015    0.99193     0.99164
alpha_1                               0.99178      0.00015    0.99193     0.99163
alpha_2                               0.99178      0.00015    0.99193     0.99163
alpha_3                               0.99178      0.00015    0.99193     0.99163
alpha_4                               0.99179      0.00015    0.99193     0.99164
alpha_5                               0.99179      0.00015    0.99194     0.99164
alpha_6                               0.99178      0.00015    0.99193     0.99163
alpha_7                               0.99178      0.00015    0.99193     0.99163
alpha_8                               0.99178      0.00015    0.99193     0.99163
alpha_9                               0.99178      0.00015    0.99193     0.99163
Alpha_loss                            -0.05328     0.00110    -0.05218    -0.05438
Training/policy_loss                  -2.68848     0.01127    -2.67721    -2.69975
Training/qf1_loss                     1145.41898   162.59006  1308.00903  982.82892
Training/qf2_loss                     1145.46527   162.59406  1308.05933  982.87122
Training/pf_norm                      0.36294      0.02379    0.38673     0.33914
Training/qf1_norm                     20.68219     1.44286    22.12505    19.23932
Training/qf2_norm                     20.43244     1.42179    21.85423    19.01064
log_std/mean                          -0.01286     0.00023    -0.01263    -0.01309
log_std/std                           0.00125      0.00001    0.00126     0.00125
log_std/max                           -0.01088     0.00022    -0.01067    -0.01110
log_std/min                           -0.01618     0.00026    -0.01592    -0.01644
log_probs/mean                        -2.70099     0.01151    -2.68948    -2.71250
log_probs/std                         0.40503      0.00557    0.41060     0.39946
log_probs/max                         -1.41555     0.04524    -1.37032    -1.46079
log_probs/min                         -3.90978     0.02858    -3.88120    -3.93836
mean/mean                             -0.00078     0.00002    -0.00077    -0.00080
mean/std                              0.00133      0.00000    0.00133     0.00133
mean/max                              0.00109      0.00003    0.00112     0.00107
mean/min                              -0.00337     0.00001    -0.00336    -0.00337
------------------------------------  -----------  ---------  ----------  ---------
epoch, update_end_epoch 14 50
freq 22
sample: [3, 5, 1, 9, 8, 0, 4, 2, 6, 7]
replay_buffer._size: [2400 2400 2400 2400 2400 2400 2400 2400 2400 2400]
train_time 0.15116453170776367
eval time 0.002929210662841797
2023-08-25 14:03:49,500 MainThread INFO: EPOCH:14
2023-08-25 14:03:49,501 MainThread INFO: Time Consumed:0.1578364372253418s
2023-08-25 14:03:49,501 MainThread INFO: Total Frames:22500s
  8%|▊         | 15/200 [00:14<01:08,  2.70it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1135.12267
Train_Epoch_Reward                    9315.03307
Running_Training_Average_Rewards      1262.91261
Explore_Time                          0.00304
Train___Time                          0.15116
Eval____Time                          0.00293
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -53.32365
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.53869
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.64374
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.69546
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.97411
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.65350
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.76522
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11628.67275
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.67910
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.68148
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.27301      0.71877    9.99178     8.55424
alpha_0                               0.99119      0.00015    0.99134     0.99104
alpha_1                               0.99119      0.00015    0.99133     0.99104
alpha_2                               0.99119      0.00015    0.99134     0.99104
alpha_3                               0.99119      0.00015    0.99134     0.99104
alpha_4                               0.99119      0.00015    0.99134     0.99104
alpha_5                               0.99120      0.00015    0.99134     0.99105
alpha_6                               0.99119      0.00015    0.99134     0.99104
alpha_7                               0.99118      0.00015    0.99133     0.99104
alpha_8                               0.99119      0.00015    0.99134     0.99104
alpha_9                               0.99118      0.00015    0.99133     0.99103
Alpha_loss                            -0.05724     0.00102    -0.05622    -0.05826
Training/policy_loss                  -2.68085     0.00138    -2.67947    -2.68223
Training/qf1_loss                     1643.62616   276.55817  1920.18433  1367.06799
Training/qf2_loss                     1643.68420   276.56287  1920.24707  1367.12134
Training/pf_norm                      0.37867      0.01477    0.39344     0.36390
Training/qf1_norm                     24.60239     1.52514    26.12754    23.07725
Training/qf2_norm                     24.28130     1.49260    25.77390    22.78869
log_std/mean                          -0.01379     0.00023    -0.01356    -0.01402
log_std/std                           0.00127      0.00001    0.00128     0.00127
log_std/max                           -0.01177     0.00022    -0.01154    -0.01199
log_std/min                           -0.01718     0.00026    -0.01692    -0.01744
log_probs/mean                        -2.69392     0.00155    -2.69237    -2.69547
log_probs/std                         0.40913      0.00017    0.40930     0.40896
log_probs/max                         -1.35921     0.00116    -1.35805    -1.36037
log_probs/min                         -4.45718     0.30234    -4.15485    -4.75952
mean/mean                             -0.00071     0.00001    -0.00070    -0.00072
mean/std                              0.00131      0.00000    0.00131     0.00131
mean/max                              0.00122      0.00006    0.00129     0.00116
mean/min                              -0.00336     0.00000    -0.00336    -0.00337
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 15 50
freq 22
sample: [8, 9, 6, 7, 1, 3, 2, 5, 0, 4]
replay_buffer._size: [2550 2550 2550 2550 2550 2550 2550 2550 2550 2550]
train_time 0.1493992805480957
eval time 0.0025260448455810547
2023-08-25 14:03:50,093 MainThread INFO: EPOCH:15
2023-08-25 14:03:50,094 MainThread INFO: Time Consumed:0.15763497352600098s
2023-08-25 14:03:50,094 MainThread INFO: Total Frames:24000s
  8%|▊         | 16/200 [00:14<01:19,  2.31it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1138.30021
Train_Epoch_Reward                    1211.86517
Running_Training_Average_Rewards      697.62183
Explore_Time                          0.00488
Train___Time                          0.14940
Eval____Time                          0.00253
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.34597
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.51390
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.68393
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.65023
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.98983
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.66844
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.77107
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11676.36151
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.65377
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.64095
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.97892      0.58314    9.56206     8.39577
alpha_0                               0.99059      0.00015    0.99074     0.99045
alpha_1                               0.99059      0.00015    0.99074     0.99044
alpha_2                               0.99059      0.00015    0.99074     0.99044
alpha_3                               0.99059      0.00015    0.99074     0.99044
alpha_4                               0.99059      0.00015    0.99074     0.99045
alpha_5                               0.99060      0.00015    0.99075     0.99045
alpha_6                               0.99059      0.00015    0.99074     0.99044
alpha_7                               0.99059      0.00015    0.99074     0.99044
alpha_8                               0.99059      0.00015    0.99074     0.99044
alpha_9                               0.99059      0.00015    0.99074     0.99044
Alpha_loss                            -0.06131     0.00099    -0.06032    -0.06230
Training/policy_loss                  -2.68616     0.00183    -2.68433    -2.68798
Training/qf1_loss                     1490.17780   174.97003  1665.14783  1315.20776
Training/qf2_loss                     1490.23523   174.97278  1665.20801  1315.26245
Training/pf_norm                      0.36020      0.00219    0.36239     0.35801
Training/qf1_norm                     24.01153     1.24879    25.26032    22.76274
Training/qf2_norm                     23.69860     1.22996    24.92856    22.46864
log_std/mean                          -0.01472     0.00023    -0.01449    -0.01496
log_std/std                           0.00130      0.00001    0.00130     0.00129
log_std/max                           -0.01265     0.00023    -0.01243    -0.01288
log_std/min                           -0.01818     0.00025    -0.01793    -0.01843
log_probs/mean                        -2.69987     0.00169    -2.69818    -2.70156
log_probs/std                         0.40658      0.00773    0.41431     0.39885
log_probs/max                         -1.40020     0.12772    -1.27248    -1.52792
log_probs/min                         -4.52119     0.93093    -3.59026    -5.45212
mean/mean                             -0.00073     0.00000    -0.00073    -0.00074
mean/std                              0.00128      0.00000    0.00128     0.00128
mean/max                              0.00153      0.00009    0.00162     0.00145
mean/min                              -0.00338     0.00001    -0.00337    -0.00339
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 16 50
freq 22
sample: [1, 2, 7, 8, 3, 0, 9, 6, 4, 5]
replay_buffer._size: [2700 2700 2700 2700 2700 2700 2700 2700 2700 2700]
train_time 0.14662575721740723
eval time 0.002307415008544922
2023-08-25 14:03:50,484 MainThread INFO: EPOCH:16
2023-08-25 14:03:50,484 MainThread INFO: Time Consumed:0.1524038314819336s
2023-08-25 14:03:50,484 MainThread INFO: Total Frames:25500s
  8%|▊         | 17/200 [00:15<01:17,  2.36it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1142.60798
Train_Epoch_Reward                    18216.57349
Running_Training_Average_Rewards      958.11572
Explore_Time                          0.00292
Train___Time                          0.14663
Eval____Time                          0.00231
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -45.32241
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.51359
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.72063
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.62022
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.99666
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.68278
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.77974
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11719.96489
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.65299
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.59768
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.55040      0.43830   7.98870     7.11211
alpha_0                               0.99000      0.00015   0.99015     0.98985
alpha_1                               0.99000      0.00015   0.99015     0.98985
alpha_2                               0.99000      0.00015   0.99015     0.98985
alpha_3                               0.99000      0.00015   0.99015     0.98985
alpha_4                               0.99000      0.00015   0.99015     0.98985
alpha_5                               0.99001      0.00015   0.99016     0.98986
alpha_6                               0.99000      0.00015   0.99014     0.98985
alpha_7                               0.99000      0.00015   0.99014     0.98985
alpha_8                               0.99000      0.00015   0.99015     0.98985
alpha_9                               0.98999      0.00015   0.99014     0.98985
Alpha_loss                            -0.06530     0.00096   -0.06434    -0.06626
Training/policy_loss                  -2.68163     0.00444   -2.67719    -2.68607
Training/qf1_loss                     1167.23871   84.30060  1251.53931  1082.93811
Training/qf2_loss                     1167.29059   84.30353  1251.59412  1082.98706
Training/pf_norm                      0.36501      0.00815   0.37317     0.35686
Training/qf1_norm                     20.98469     0.91097   21.89566    20.07371
Training/qf2_norm                     20.71264     0.88962   21.60226    19.82302
log_std/mean                          -0.01566     0.00024   -0.01543    -0.01590
log_std/std                           0.00132      0.00001   0.00133     0.00132
log_std/max                           -0.01354     0.00022   -0.01332    -0.01377
log_std/min                           -0.01920     0.00025   -0.01895    -0.01946
log_probs/mean                        -2.69591     0.00433   -2.69158    -2.70024
log_probs/std                         0.40285      0.00901   0.41186     0.39385
log_probs/max                         -1.38682     0.04249   -1.34433    -1.42930
log_probs/min                         -4.48260     0.38391   -4.09869    -4.86650
mean/mean                             -0.00078     0.00002   -0.00076    -0.00079
mean/std                              0.00128      0.00000   0.00128     0.00128
mean/max                              0.00187      0.00008   0.00195     0.00179
mean/min                              -0.00337     0.00002   -0.00335    -0.00339
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 17 50
freq 22
sample: [3, 2, 1, 8, 7, 5, 6, 0, 9, 4]
replay_buffer._size: [2850 2850 2850 2850 2850 2850 2850 2850 2850 2850]
train_time 0.13296008110046387
eval time 0.0031821727752685547
snapshot at best
2023-08-25 14:03:51,433 MainThread INFO: EPOCH:17
2023-08-25 14:03:51,433 MainThread INFO: Time Consumed:0.7091450691223145s
2023-08-25 14:03:51,433 MainThread INFO: Total Frames:27000s
  9%|▉         | 18/200 [00:16<01:45,  1.73it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1147.23530
Train_Epoch_Reward                    4729.13107
Running_Training_Average_Rewards      805.25232
Explore_Time                          0.00274
Train___Time                          0.13296
Eval____Time                          0.00318
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -56.83272
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.52008
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.75557
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.58760
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.99928
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.69500
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.78355
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11770.92054
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.65767
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.55156
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.67976      0.01044   7.69019     7.66932
alpha_0                               0.98941      0.00015   0.98955     0.98926
alpha_1                               0.98940      0.00015   0.98955     0.98925
alpha_2                               0.98940      0.00015   0.98955     0.98925
alpha_3                               0.98940      0.00015   0.98955     0.98925
alpha_4                               0.98940      0.00015   0.98955     0.98926
alpha_5                               0.98941      0.00015   0.98956     0.98927
alpha_6                               0.98940      0.00015   0.98955     0.98925
alpha_7                               0.98940      0.00015   0.98955     0.98925
alpha_8                               0.98940      0.00015   0.98955     0.98925
alpha_9                               0.98940      0.00015   0.98955     0.98925
Alpha_loss                            -0.06929     0.00115   -0.06814    -0.07044
Training/policy_loss                  -2.67806     0.01396   -2.66409    -2.69202
Training/qf1_loss                     1165.80505   18.81140  1184.61646  1146.99365
Training/qf2_loss                     1165.85992   18.80988  1184.66980  1147.05005
Training/pf_norm                      0.35602      0.05183   0.40784     0.30419
Training/qf1_norm                     21.30768     0.02560   21.33328    21.28207
Training/qf2_norm                     21.02681     0.01606   21.04287    21.01075
log_std/mean                          -0.01662     0.00024   -0.01638    -0.01686
log_std/std                           0.00135      0.00000   0.00135     0.00134
log_std/max                           -0.01442     0.00022   -0.01420    -0.01464
log_std/min                           -0.02022     0.00025   -0.01997    -0.02047
log_probs/mean                        -2.69286     0.01426   -2.67860    -2.70711
log_probs/std                         0.40110      0.00178   0.40288     0.39931
log_probs/max                         -1.35050     0.03431   -1.31618    -1.38481
log_probs/min                         -4.33313     0.74168   -3.59145    -5.07481
mean/mean                             -0.00082     0.00001   -0.00082    -0.00083
mean/std                              0.00131      0.00002   0.00133     0.00130
mean/max                              0.00217      0.00006   0.00223     0.00210
mean/min                              -0.00333     0.00000   -0.00332    -0.00333
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 18 50
freq 22
sample: [9, 6, 4, 0, 8, 3, 2, 1, 5, 7]
replay_buffer._size: [3000 3000 3000 3000 3000 3000 3000 3000 3000 3000]
train_time 0.42818140983581543
eval time 0.002945423126220703
snapshot at best
2023-08-25 14:03:52,474 MainThread INFO: EPOCH:18
2023-08-25 14:03:52,474 MainThread INFO: Time Consumed:0.9269161224365234s
2023-08-25 14:03:52,474 MainThread INFO: Total Frames:28500s
 10%|▉         | 19/200 [00:17<02:09,  1.40it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1152.00044
Train_Epoch_Reward                    3592.64391
Running_Training_Average_Rewards      884.61162
Explore_Time                          0.00329
Train___Time                          0.42818
Eval____Time                          0.00295
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.67564
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.52051
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.78959
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.55704
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.00032
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.70832
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.78230
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11815.59459
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.65780
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.50565
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.26467      0.57832    8.84299     7.68635
alpha_0                               0.98881      0.00015    0.98896     0.98866
alpha_1                               0.98881      0.00015    0.98896     0.98866
alpha_2                               0.98881      0.00015    0.98896     0.98866
alpha_3                               0.98881      0.00015    0.98896     0.98866
alpha_4                               0.98881      0.00015    0.98896     0.98866
alpha_5                               0.98882      0.00015    0.98897     0.98867
alpha_6                               0.98881      0.00015    0.98896     0.98866
alpha_7                               0.98881      0.00015    0.98896     0.98866
alpha_8                               0.98881      0.00015    0.98896     0.98866
alpha_9                               0.98881      0.00015    0.98895     0.98866
Alpha_loss                            -0.07335     0.00105    -0.07230    -0.07441
Training/policy_loss                  -2.68194     0.00423    -2.67771    -2.68617
Training/qf1_loss                     1370.11249   142.33246  1512.44495  1227.78003
Training/qf2_loss                     1370.16937   142.33612  1512.50549  1227.83325
Training/pf_norm                      0.34946      0.00397    0.35343     0.34550
Training/qf1_norm                     22.54973     1.21944    23.76917    21.33029
Training/qf2_norm                     22.25844     1.19464    23.45308    21.06380
log_std/mean                          -0.01757     0.00024    -0.01734    -0.01781
log_std/std                           0.00137      0.00001    0.00138     0.00137
log_std/max                           -0.01526     0.00022    -0.01505    -0.01548
log_std/min                           -0.02123     0.00025    -0.02098    -0.02148
log_probs/mean                        -2.69734     0.00440    -2.69293    -2.70174
log_probs/std                         0.40217      0.00192    0.40409     0.40025
log_probs/max                         -1.49433     0.04382    -1.45051    -1.53815
log_probs/min                         -4.19599     0.44165    -3.75434    -4.63764
mean/mean                             -0.00083     0.00001    -0.00082    -0.00084
mean/std                              0.00141      0.00002    0.00143     0.00139
mean/max                              0.00240      0.00004    0.00244     0.00236
mean/min                              -0.00338     0.00002    -0.00337    -0.00340
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 19 50
freq 22
sample: [6, 8, 1, 2, 3, 0, 5, 9, 4, 7]
replay_buffer._size: [3150 3150 3150 3150 3150 3150 3150 3150 3150 3150]
train_time 0.14142060279846191
eval time 0.003216981887817383
snapshot at best
2023-08-25 14:03:53,221 MainThread INFO: EPOCH:19
2023-08-25 14:03:53,222 MainThread INFO: Time Consumed:0.647697925567627s
2023-08-25 14:03:53,222 MainThread INFO: Total Frames:30000s
 10%|█         | 20/200 [00:18<02:10,  1.38it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1156.08379
Train_Epoch_Reward                    8319.31899
Running_Training_Average_Rewards      554.70313
Explore_Time                          0.00247
Train___Time                          0.14142
Eval____Time                          0.00322
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -56.47002
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.51286
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.81790
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.52231
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.99840
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.71175
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.77454
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11853.48956
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.64392
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.45913
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.44601      0.00287   7.44888     7.44314
alpha_0                               0.98822      0.00015   0.98837     0.98807
alpha_1                               0.98822      0.00015   0.98836     0.98807
alpha_2                               0.98821      0.00015   0.98836     0.98807
alpha_3                               0.98822      0.00015   0.98836     0.98807
alpha_4                               0.98822      0.00015   0.98837     0.98807
alpha_5                               0.98823      0.00015   0.98838     0.98808
alpha_6                               0.98821      0.00015   0.98836     0.98807
alpha_7                               0.98821      0.00015   0.98836     0.98807
alpha_8                               0.98821      0.00015   0.98836     0.98807
alpha_9                               0.98821      0.00015   0.98836     0.98807
Alpha_loss                            -0.07725     0.00110   -0.07615    -0.07835
Training/policy_loss                  -2.67070     0.00783   -2.66287    -2.67853
Training/qf1_loss                     1174.30237   72.35181  1246.65417  1101.95056
Training/qf2_loss                     1174.35638   72.35187  1246.70825  1102.00452
Training/pf_norm                      0.42034      0.03896   0.45930     0.38138
Training/qf1_norm                     20.84517     0.02548   20.87065    20.81970
Training/qf2_norm                     20.57495     0.02563   20.60059    20.54932
log_std/mean                          -0.01854     0.00025   -0.01829    -0.01878
log_std/std                           0.00140      0.00001   0.00141     0.00140
log_std/max                           -0.01614     0.00022   -0.01592    -0.01636
log_std/min                           -0.02224     0.00026   -0.02198    -0.02250
log_probs/mean                        -2.68651     0.00804   -2.67847    -2.69455
log_probs/std                         0.40848      0.01073   0.41921     0.39776
log_probs/max                         -1.43795     0.02271   -1.41523    -1.46066
log_probs/min                         -4.30736     0.14353   -4.16383    -4.45089
mean/mean                             -0.00079     0.00002   -0.00077    -0.00081
mean/std                              0.00148      0.00001   0.00149     0.00146
mean/max                              0.00249      0.00000   0.00249     0.00248
mean/min                              -0.00335     0.00002   -0.00332    -0.00337
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 20 50
freq 22
sample: [1, 9, 7, 4, 8, 3, 0, 5, 2, 6]
replay_buffer._size: [3300 3300 3300 3300 3300 3300 3300 3300 3300 3300]
train_time 0.13218140602111816
eval time 0.002551555633544922
snapshot at best
2023-08-25 14:03:53,945 MainThread INFO: EPOCH:20
2023-08-25 14:03:53,945 MainThread INFO: Time Consumed:0.6306934356689453s
2023-08-25 14:03:53,945 MainThread INFO: Total Frames:31500s
 10%|█         | 21/200 [00:18<02:09,  1.38it/s]------------------------------------  -----------  --------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1160.25357
Train_Epoch_Reward                    2893.09509
Running_Training_Average_Rewards      493.50193
Explore_Time                          0.00293
Train___Time                          0.13218
Eval____Time                          0.00255
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -44.37036
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.51038
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.85082
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.50100
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.99307
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.71890
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.76753
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11883.40283
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.63540
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.42455
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.20489      0.42211   7.62700     6.78279
alpha_0                               0.98763      0.00015   0.98777     0.98748
alpha_1                               0.98762      0.00015   0.98777     0.98747
alpha_2                               0.98762      0.00015   0.98777     0.98747
alpha_3                               0.98762      0.00015   0.98777     0.98748
alpha_4                               0.98763      0.00015   0.98777     0.98748
alpha_5                               0.98764      0.00015   0.98779     0.98749
alpha_6                               0.98762      0.00015   0.98777     0.98747
alpha_7                               0.98762      0.00015   0.98777     0.98747
alpha_8                               0.98762      0.00015   0.98777     0.98747
alpha_9                               0.98762      0.00015   0.98777     0.98747
Alpha_loss                            -0.08145     0.00107   -0.08037    -0.08252
Training/policy_loss                  -2.68491     0.00541   -2.67950    -2.69032
Training/qf1_loss                     1027.40002   31.10706  1058.50708  996.29297
Training/qf2_loss                     1027.45367   31.10907  1058.56274  996.34460
Training/pf_norm                      0.36023      0.01856   0.37879     0.34167
Training/qf1_norm                     20.37104     0.90487   21.27591    19.46617
Training/qf2_norm                     20.11033     0.89122   21.00156    19.21911
log_std/mean                          -0.01952     0.00025   -0.01928    -0.01977
log_std/std                           0.00144      0.00001   0.00144     0.00143
log_std/max                           -0.01704     0.00023   -0.01681    -0.01727
log_std/min                           -0.02324     0.00026   -0.02299    -0.02350
log_probs/mean                        -2.70143     0.00562   -2.69582    -2.70705
log_probs/std                         0.39650      0.00199   0.39849     0.39450
log_probs/max                         -1.52603     0.06492   -1.46111    -1.59095
log_probs/min                         -4.44432     0.41071   -4.03361    -4.85503
mean/mean                             -0.00071     0.00003   -0.00068    -0.00074
mean/std                              0.00154      0.00002   0.00156     0.00153
mean/max                              0.00255      0.00002   0.00257     0.00253
mean/min                              -0.00334     0.00002   -0.00332    -0.00335
------------------------------------  -----------  --------  ----------  ---------
epoch, update_end_epoch 21 50
freq 22
sample: [6, 0, 2, 9, 4, 1, 8, 7, 5, 3]
replay_buffer._size: [3450 3450 3450 3450 3450 3450 3450 3450 3450 3450]
train_time 0.1448056697845459
eval time 0.003481626510620117
2023-08-25 14:03:54,190 MainThread INFO: EPOCH:21
2023-08-25 14:03:54,194 MainThread INFO: Time Consumed:0.1518688201904297s
2023-08-25 14:03:54,194 MainThread INFO: Total Frames:33000s
 11%|█         | 22/200 [00:19<01:44,  1.70it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1162.64426
Train_Epoch_Reward                    19988.69070
Running_Training_Average_Rewards      1040.03683
Explore_Time                          0.00289
Train___Time                          0.14481
Eval____Time                          0.00348
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -58.16330
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.50457
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.88319
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.48739
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.98543
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.72838
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.75616
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11896.65625
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.62612
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.40347
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.09482      0.19103   7.28584     6.90379
alpha_0                               0.98703      0.00015   0.98718     0.98688
alpha_1                               0.98703      0.00015   0.98718     0.98688
alpha_2                               0.98703      0.00015   0.98718     0.98688
alpha_3                               0.98703      0.00015   0.98718     0.98688
alpha_4                               0.98703      0.00015   0.98718     0.98688
alpha_5                               0.98705      0.00015   0.98719     0.98690
alpha_6                               0.98703      0.00015   0.98717     0.98688
alpha_7                               0.98703      0.00015   0.98718     0.98688
alpha_8                               0.98703      0.00015   0.98718     0.98688
alpha_9                               0.98703      0.00015   0.98718     0.98688
Alpha_loss                            -0.08550     0.00093   -0.08456    -0.08643
Training/policy_loss                  -2.68666     0.00571   -2.68095    -2.69237
Training/qf1_loss                     1052.73999   14.10400  1066.84399  1038.63599
Training/qf2_loss                     1052.79376   14.10468  1066.89844  1038.68909
Training/pf_norm                      0.33587      0.00145   0.33732     0.33442
Training/qf1_norm                     20.14900     0.42217   20.57117    19.72684
Training/qf2_norm                     19.88703     0.41318   20.30021    19.47385
log_std/mean                          -0.02051     0.00025   -0.02027    -0.02076
log_std/std                           0.00146      0.00001   0.00147     0.00145
log_std/max                           -0.01794     0.00022   -0.01771    -0.01816
log_std/min                           -0.02426     0.00026   -0.02400    -0.02452
log_probs/mean                        -2.70370     0.00567   -2.69803    -2.70937
log_probs/std                         0.38832      0.00378   0.39210     0.38454
log_probs/max                         -1.45252     0.02243   -1.43009    -1.47494
log_probs/min                         -3.86912     0.02698   -3.84214    -3.89609
mean/mean                             -0.00057     0.00004   -0.00054    -0.00061
mean/std                              0.00167      0.00003   0.00170     0.00163
mean/max                              0.00270      0.00004   0.00274     0.00267
mean/min                              -0.00338     0.00001   -0.00337    -0.00340
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 22 50
freq 22
start to update mask
sample: [3, 2, 9, 6, 8, 4, 7, 5, 0, 1]
replay_buffer._size: [3600 3600 3600 3600 3600 3600 3600 3600 3600 3600]
train_time 0.21309661865234375
eval time 0.004197597503662109
snapshot at best
2023-08-25 14:03:59,371 MainThread INFO: EPOCH:22
2023-08-25 14:03:59,372 MainThread INFO: Time Consumed:0.8204467296600342s
2023-08-25 14:03:59,372 MainThread INFO: Total Frames:34500s
 12%|█▏        | 23/200 [00:24<05:46,  1.96s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1164.75005
Train_Epoch_Reward                    10906.48188
Running_Training_Average_Rewards      1126.27559
Explore_Time                          0.00472
Train___Time                          0.21310
Eval____Time                          0.00420
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.85787
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.49615
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.91778
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.46174
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.97602
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.73241
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.74906
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11909.93900
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.61680
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.37869
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.56153      0.49373    9.05526     8.06780
alpha_0                               0.98644      0.00015    0.98659     0.98629
alpha_1                               0.98644      0.00015    0.98659     0.98629
alpha_2                               0.98643      0.00015    0.98658     0.98629
alpha_3                               0.98644      0.00015    0.98659     0.98629
alpha_4                               0.98644      0.00015    0.98659     0.98629
alpha_5                               0.98645      0.00015    0.98660     0.98631
alpha_6                               0.98643      0.00015    0.98658     0.98629
alpha_7                               0.98643      0.00015    0.98658     0.98629
alpha_8                               0.98643      0.00015    0.98658     0.98629
alpha_9                               0.98644      0.00015    0.98658     0.98629
Alpha_loss                            -0.08939     0.00099    -0.08840    -0.09038
Training/policy_loss                  -2.67349     0.00127    -2.67222    -2.67476
Training/qf1_loss                     1487.46820   177.99957  1665.46777  1309.46863
Training/qf2_loss                     1487.47266   178.00024  1665.47290  1309.47241
Training/pf_norm                      0.38189      0.00498    0.38688     0.37691
Training/qf1_norm                     23.27920     1.01414    24.29335    22.26506
Training/qf2_norm                     23.05012     0.99725    24.04737    22.05287
log_std/mean                          -0.01940     0.00023    -0.01917    -0.01964
log_std/std                           0.00128      0.00000    0.00128     0.00127
log_std/max                           -0.01713     0.00022    -0.01692    -0.01735
log_std/min                           -0.02194     0.00024    -0.02170    -0.02218
log_probs/mean                        -2.69361     0.00111    -2.69249    -2.69472
log_probs/std                         0.39184      0.00129    0.39314     0.39055
log_probs/max                         -1.45651     0.06863    -1.38788    -1.52514
log_probs/min                         -3.90474     0.11675    -3.78798    -4.02149
mean/mean                             -0.00033     0.00004    -0.00028    -0.00037
mean/std                              0.00148      0.00004    0.00152     0.00145
mean/max                              0.00172      0.00006    0.00178     0.00167
mean/min                              -0.00246     0.00004    -0.00243    -0.00250
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 23 50
freq 22
sample: [1, 7, 6, 3, 8, 0, 9, 4, 2, 5]
replay_buffer._size: [3750 3750 3750 3750 3750 3750 3750 3750 3750 3750]
train_time 0.19458937644958496
eval time 0.0060272216796875
snapshot at best
2023-08-25 14:04:00,219 MainThread INFO: EPOCH:23
2023-08-25 14:04:00,219 MainThread INFO: Time Consumed:0.7505331039428711s
2023-08-25 14:04:00,219 MainThread INFO: Total Frames:36000s
 12%|█▏        | 24/200 [00:25<04:45,  1.62s/it]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1165.53006
Train_Epoch_Reward                    10662.21554
Running_Training_Average_Rewards      1385.24627
Explore_Time                          0.00283
Train___Time                          0.19459
Eval____Time                          0.00603
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.03451
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.49135
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.94240
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.43997
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.96904
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.73003
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.74249
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11917.34756
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.60938
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.35742
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.18010      0.01813   7.19823     7.16197
alpha_0                               0.98585      0.00015   0.98600     0.98570
alpha_1                               0.98584      0.00015   0.98599     0.98570
alpha_2                               0.98584      0.00015   0.98599     0.98569
alpha_3                               0.98585      0.00015   0.98600     0.98570
alpha_4                               0.98585      0.00015   0.98600     0.98570
alpha_5                               0.98586      0.00015   0.98601     0.98572
alpha_6                               0.98584      0.00015   0.98599     0.98569
alpha_7                               0.98584      0.00015   0.98599     0.98569
alpha_8                               0.98584      0.00015   0.98599     0.98569
alpha_9                               0.98584      0.00015   0.98599     0.98569
Alpha_loss                            -0.09347     0.00102   -0.09246    -0.09449
Training/policy_loss                  -2.67768     0.00067   -2.67701    -2.67835
Training/qf1_loss                     1069.36993   13.40961  1082.77954  1055.96033
Training/qf2_loss                     1069.37482   13.40961  1082.78442  1055.96521
Training/pf_norm                      0.36742      0.01280   0.38022     0.35462
Training/qf1_norm                     20.38471     0.03409   20.41879    20.35062
Training/qf2_norm                     20.19003     0.03292   20.22295    20.15711
log_std/mean                          -0.02041     0.00027   -0.02014    -0.02067
log_std/std                           0.00129      0.00001   0.00130     0.00129
log_std/max                           -0.01807     0.00025   -0.01782    -0.01832
log_std/min                           -0.02297     0.00028   -0.02269    -0.02325
log_probs/mean                        -2.69848     0.00082   -2.69766    -2.69930
log_probs/std                         0.39043      0.00149   0.39192     0.38894
log_probs/max                         -1.42479     0.05421   -1.37058    -1.47900
log_probs/min                         -3.87757     0.09595   -3.78162    -3.97351
mean/mean                             -0.00012     0.00005   -0.00007    -0.00017
mean/std                              0.00156      0.00002   0.00158     0.00154
mean/max                              0.00186      0.00004   0.00190     0.00182
mean/min                              -0.00224     0.00008   -0.00217    -0.00232
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 24 50
freq 22
sample: [2, 9, 5, 4, 0, 3, 7, 6, 8, 1]
replay_buffer._size: [3900 3900 3900 3900 3900 3900 3900 3900 3900 3900]
train_time 0.17009353637695312
eval time 0.0028793811798095703
snapshot at best
2023-08-25 14:04:01,013 MainThread INFO: EPOCH:24
2023-08-25 14:04:01,013 MainThread INFO: Time Consumed:0.7039508819580078s
2023-08-25 14:04:01,013 MainThread INFO: Total Frames:37500s
 12%|█▎        | 25/200 [00:25<04:00,  1.38s/it]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1174.82321
Train_Epoch_Reward                    5510.45251
Running_Training_Average_Rewards      902.63833
Explore_Time                          0.00255
Train___Time                          0.17009
Eval____Time                          0.00288
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.27693
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.54220
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.81916
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.62036
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.00545
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.71022
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.79814
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12166.73796
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.64719
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.40532
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.27208      0.38868   7.66076     6.88339
alpha_0                               0.98526      0.00015   0.98540     0.98511
alpha_1                               0.98525      0.00015   0.98540     0.98510
alpha_2                               0.98525      0.00015   0.98540     0.98510
alpha_3                               0.98526      0.00015   0.98541     0.98511
alpha_4                               0.98526      0.00015   0.98541     0.98511
alpha_5                               0.98527      0.00015   0.98542     0.98512
alpha_6                               0.98525      0.00015   0.98540     0.98510
alpha_7                               0.98525      0.00015   0.98540     0.98510
alpha_8                               0.98525      0.00015   0.98540     0.98510
alpha_9                               0.98525      0.00015   0.98540     0.98510
Alpha_loss                            -0.09753     0.00101   -0.09651    -0.09854
Training/policy_loss                  -2.67941     0.00046   -2.67895    -2.67986
Training/qf1_loss                     1262.47943   25.64130  1288.12073  1236.83813
Training/qf2_loss                     1262.48486   25.64185  1288.12671  1236.84302
Training/pf_norm                      0.36804      0.00730   0.37534     0.36074
Training/qf1_norm                     20.61797     0.81849   21.43646    19.79948
Training/qf2_norm                     20.41931     0.80920   21.22851    19.61011
log_std/mean                          -0.02151     0.00028   -0.02123    -0.02180
log_std/std                           0.00132      0.00001   0.00132     0.00131
log_std/max                           -0.01910     0.00026   -0.01884    -0.01936
log_std/min                           -0.02414     0.00031   -0.02383    -0.02446
log_probs/mean                        -2.70076     0.00058   -2.70017    -2.70134
log_probs/std                         0.39466      0.00105   0.39571     0.39361
log_probs/max                         -1.41435     0.05541   -1.35894    -1.46976
log_probs/min                         -4.14816     0.01401   -4.13415    -4.16216
mean/mean                             0.00004      0.00003   0.00007     0.00001
mean/std                              0.00169      0.00005   0.00174     0.00165
mean/max                              0.00238      0.00017   0.00255     0.00221
mean/min                              -0.00207     0.00002   -0.00204    -0.00209
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 25 50
freq 22
sample: [2, 1, 3, 5, 8, 9, 7, 4, 6, 0]
replay_buffer._size: [4050 4050 4050 4050 4050 4050 4050 4050 4050 4050]
train_time 0.1523425579071045
eval time 0.002713441848754883
snapshot at best
2023-08-25 14:04:01,867 MainThread INFO: EPOCH:25
2023-08-25 14:04:01,867 MainThread INFO: Time Consumed:0.7647144794464111s
2023-08-25 14:04:01,868 MainThread INFO: Total Frames:39000s
 13%|█▎        | 26/200 [00:26<03:32,  1.22s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1183.59544
Train_Epoch_Reward                    803.21131
Running_Training_Average_Rewards      565.86265
Explore_Time                          0.00260
Train___Time                          0.15234
Eval____Time                          0.00271
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.53714
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.54467
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.82730
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.62425
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.03385
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.73592
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.82690
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12176.09947
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.65006
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.40009
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           7.86401      1.02837    8.89238     6.83564
alpha_0                               0.98466      0.00015    0.98481     0.98452
alpha_1                               0.98466      0.00015    0.98481     0.98451
alpha_2                               0.98466      0.00015    0.98481     0.98451
alpha_3                               0.98467      0.00015    0.98481     0.98452
alpha_4                               0.98467      0.00015    0.98481     0.98452
alpha_5                               0.98468      0.00015    0.98483     0.98453
alpha_6                               0.98466      0.00015    0.98481     0.98451
alpha_7                               0.98466      0.00015    0.98481     0.98451
alpha_8                               0.98466      0.00015    0.98481     0.98451
alpha_9                               0.98466      0.00015    0.98481     0.98451
Alpha_loss                            -0.10149     0.00101    -0.10049    -0.10250
Training/policy_loss                  -2.67514     0.00011    -2.67503    -2.67525
Training/qf1_loss                     1331.73743   278.36926  1610.10669  1053.36816
Training/qf2_loss                     1331.74512   278.37048  1610.11560  1053.37463
Training/pf_norm                      0.36705      0.02341    0.39046     0.34364
Training/qf1_norm                     21.89032     2.14248    24.03279    19.74784
Training/qf2_norm                     21.66805     2.11442    23.78246    19.55363
log_std/mean                          -0.02268     0.00030    -0.02238    -0.02298
log_std/std                           0.00136      0.00002    0.00138     0.00135
log_std/max                           -0.02016     0.00026    -0.01990    -0.02043
log_std/min                           -0.02548     0.00035    -0.02513    -0.02583
log_probs/mean                        -2.69689     0.00000    -2.69689    -2.69689
log_probs/std                         0.39264      0.00078    0.39342     0.39186
log_probs/max                         -1.42873     0.00780    -1.42092    -1.43653
log_probs/min                         -4.26795     0.56955    -3.69840    -4.83751
mean/mean                             0.00017      0.00003    0.00020     0.00014
mean/std                              0.00185      0.00004    0.00189     0.00180
mean/max                              0.00305      0.00018    0.00323     0.00287
mean/min                              -0.00221     0.00009    -0.00212    -0.00229
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 26 50
freq 22
sample: [9, 0, 5, 6, 4, 8, 3, 2, 1, 7]
replay_buffer._size: [4200 4200 4200 4200 4200 4200 4200 4200 4200 4200]
train_time 0.17586660385131836
eval time 0.002941131591796875
snapshot at best
2023-08-25 14:04:02,815 MainThread INFO: EPOCH:26
2023-08-25 14:04:02,815 MainThread INFO: Time Consumed:0.8559150695800781s
2023-08-25 14:04:02,815 MainThread INFO: Total Frames:40500s
 14%|█▎        | 27/200 [00:27<03:16,  1.14s/it]------------------------------------  -----------  --------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1192.58133
Train_Epoch_Reward                    21302.82319
Running_Training_Average_Rewards      920.54957
Explore_Time                          0.00266
Train___Time                          0.17587
Eval____Time                          0.00294
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.04715
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.55919
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.82771
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.63945
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.05478
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.75599
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.84936
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12181.40206
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.66575
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.39504
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.61596      0.45742   8.07339     7.15854
alpha_0                               0.98407      0.00015   0.98422     0.98393
alpha_1                               0.98407      0.00015   0.98422     0.98392
alpha_2                               0.98407      0.00015   0.98422     0.98392
alpha_3                               0.98408      0.00015   0.98422     0.98393
alpha_4                               0.98407      0.00015   0.98422     0.98393
alpha_5                               0.98409      0.00015   0.98424     0.98394
alpha_6                               0.98407      0.00015   0.98422     0.98392
alpha_7                               0.98407      0.00015   0.98422     0.98392
alpha_8                               0.98407      0.00015   0.98421     0.98392
alpha_9                               0.98407      0.00015   0.98421     0.98392
Alpha_loss                            -0.10549     0.00107   -0.10442    -0.10657
Training/policy_loss                  -2.67348     0.00424   -2.66924    -2.67772
Training/qf1_loss                     1072.89120   84.77872  1157.66992  988.11249
Training/qf2_loss                     1072.90155   84.77765  1157.67920  988.12390
Training/pf_norm                      0.37016      0.00191   0.37207     0.36825
Training/qf1_norm                     21.38007     0.93163   22.31170    20.44844
Training/qf2_norm                     21.14824     0.93381   22.08204    20.21443
log_std/mean                          -0.02390     0.00031   -0.02359    -0.02422
log_std/std                           0.00143      0.00002   0.00145     0.00141
log_std/max                           -0.02126     0.00025   -0.02101    -0.02151
log_std/min                           -0.02692     0.00036   -0.02655    -0.02728
log_probs/mean                        -2.69563     0.00441   -2.69122    -2.70004
log_probs/std                         0.39096      0.00985   0.40081     0.38111
log_probs/max                         -1.50187     0.03326   -1.46861    -1.53514
log_probs/min                         -4.50159     0.53187   -3.96972    -5.03346
mean/mean                             0.00026      0.00002   0.00028     0.00024
mean/std                              0.00205      0.00006   0.00212     0.00199
mean/max                              0.00378      0.00017   0.00395     0.00361
mean/min                              -0.00262     0.00012   -0.00250    -0.00274
------------------------------------  -----------  --------  ----------  ---------
epoch, update_end_epoch 27 50
freq 22
sample: [8, 3, 6, 7, 2, 4, 9, 5, 0, 1]
replay_buffer._size: [4350 4350 4350 4350 4350 4350 4350 4350 4350 4350]
train_time 0.13236188888549805
eval time 0.0029070377349853516
2023-08-25 14:04:03,043 MainThread INFO: EPOCH:27
2023-08-25 14:04:03,044 MainThread INFO: Time Consumed:0.13844037055969238s
2023-08-25 14:04:03,044 MainThread INFO: Total Frames:42000s
 14%|█▍        | 28/200 [00:27<02:29,  1.15it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1192.89822
Train_Epoch_Reward                    15654.01565
Running_Training_Average_Rewards      1258.66834
Explore_Time                          0.00255
Train___Time                          0.13236
Eval____Time                          0.00291
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.45794
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.59692
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.82312
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.68276
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.10263
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.79324
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.89097
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12175.86954
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.70615
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.39602
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.32734      0.01376   7.34110     7.31358
alpha_0                               0.98348      0.00015   0.98363     0.98334
alpha_1                               0.98348      0.00015   0.98363     0.98333
alpha_2                               0.98348      0.00015   0.98363     0.98333
alpha_3                               0.98349      0.00015   0.98363     0.98334
alpha_4                               0.98348      0.00015   0.98363     0.98334
alpha_5                               0.98350      0.00015   0.98365     0.98335
alpha_6                               0.98348      0.00015   0.98362     0.98333
alpha_7                               0.98348      0.00015   0.98363     0.98333
alpha_8                               0.98348      0.00015   0.98362     0.98333
alpha_9                               0.98348      0.00015   0.98362     0.98333
Alpha_loss                            -0.10944     0.00117   -0.10826    -0.11061
Training/policy_loss                  -2.66831     0.01016   -2.65815    -2.67847
Training/qf1_loss                     1127.50079   69.05133  1196.55212  1058.44946
Training/qf2_loss                     1127.51270   69.05042  1196.56311  1058.46228
Training/pf_norm                      0.37590      0.04541   0.42131     0.33049
Training/qf1_norm                     20.77779     0.03613   20.81391    20.74166
Training/qf2_norm                     20.54828     0.03239   20.58067    20.51589
log_std/mean                          -0.02517     0.00032   -0.02485    -0.02549
log_std/std                           0.00151      0.00002   0.00152     0.00149
log_std/max                           -0.02237     0.00029   -0.02208    -0.02267
log_std/min                           -0.02841     0.00038   -0.02802    -0.02879
log_probs/mean                        -2.69075     0.01042   -2.68033    -2.70117
log_probs/std                         0.38094      0.00765   0.38859     0.37329
log_probs/max                         -1.39525     0.00155   -1.39370    -1.39680
log_probs/min                         -4.32412     0.35365   -3.97047    -4.67777
mean/mean                             0.00030      0.00002   0.00031     0.00028
mean/std                              0.00224      0.00004   0.00228     0.00220
mean/max                              0.00408      0.00000   0.00409     0.00408
mean/min                              -0.00316     0.00014   -0.00302    -0.00329
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 28 50
freq 22
sample: [0, 4, 6, 7, 8, 3, 2, 9, 1, 5]
replay_buffer._size: [4500 4500 4500 4500 4500 4500 4500 4500 4500 4500]
train_time 0.1336040496826172
eval time 0.0024840831756591797
snapshot at best
2023-08-25 14:04:03,909 MainThread INFO: EPOCH:28
2023-08-25 14:04:03,909 MainThread INFO: Time Consumed:0.7518999576568604s
2023-08-25 14:04:03,909 MainThread INFO: Total Frames:43500s
 14%|█▍        | 29/200 [00:28<02:27,  1.16it/s]------------------------------------  -----------  ---------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1193.87156
Train_Epoch_Reward                    14396.19228
Running_Training_Average_Rewards      1711.76770
Explore_Time                          0.00281
Train___Time                          0.13360
Eval____Time                          0.00248
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.37323
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.62716
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.84077
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.70082
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.15135
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.83003
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.93216
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12208.67874
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.73761
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.36626
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           7.61200      1.04233    8.65433     6.56967
alpha_0                               0.98289      0.00015    0.98304     0.98275
alpha_1                               0.98289      0.00015    0.98303     0.98274
alpha_2                               0.98289      0.00015    0.98304     0.98274
alpha_3                               0.98290      0.00015    0.98304     0.98275
alpha_4                               0.98289      0.00015    0.98304     0.98275
alpha_5                               0.98291      0.00015    0.98306     0.98276
alpha_6                               0.98289      0.00015    0.98303     0.98274
alpha_7                               0.98289      0.00015    0.98304     0.98274
alpha_8                               0.98289      0.00015    0.98303     0.98274
alpha_9                               0.98289      0.00015    0.98303     0.98274
Alpha_loss                            -0.11358     0.00107    -0.11251    -0.11464
Training/policy_loss                  -2.67534     0.00341    -2.67193    -2.67876
Training/qf1_loss                     1126.67914   322.10724  1448.78638  804.57190
Training/qf2_loss                     1126.69214   322.10791  1448.80005  804.58423
Training/pf_norm                      0.35880      0.00388    0.36268     0.35493
Training/qf1_norm                     21.41660     2.18480    23.60139    19.23180
Training/qf2_norm                     21.18324     2.16363    23.34687    19.01962
log_std/mean                          -0.02647     0.00033    -0.02614    -0.02680
log_std/std                           0.00157      0.00002    0.00159     0.00156
log_std/max                           -0.02357     0.00031    -0.02326    -0.02387
log_std/min                           -0.02994     0.00038    -0.02955    -0.03032
log_probs/mean                        -2.69827     0.00355    -2.69471    -2.70182
log_probs/std                         0.37885      0.00379    0.38263     0.37506
log_probs/max                         -1.39567     0.00645    -1.38923    -1.40212
log_probs/min                         -4.00160     0.12168    -3.87992    -4.12328
mean/mean                             0.00022      0.00002    0.00024     0.00019
mean/std                              0.00238      0.00001    0.00239     0.00237
mean/max                              0.00403      0.00005    0.00408     0.00399
mean/min                              -0.00369     0.00006    -0.00364    -0.00375
------------------------------------  -----------  ---------  ----------  ---------
epoch, update_end_epoch 29 50
freq 22
sample: [8, 1, 2, 3, 9, 0, 6, 5, 4, 7]
replay_buffer._size: [4650 4650 4650 4650 4650 4650 4650 4650 4650 4650]
train_time 0.1572399139404297
eval time 0.0034668445587158203
snapshot at best
2023-08-25 14:04:04,773 MainThread INFO: EPOCH:29
2023-08-25 14:04:04,774 MainThread INFO: Time Consumed:0.773874044418335s
2023-08-25 14:04:04,774 MainThread INFO: Total Frames:45000s
 15%|█▌        | 30/200 [00:29<02:27,  1.15it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1196.50117
Train_Epoch_Reward                    12817.33359
Running_Training_Average_Rewards      1428.91805
Explore_Time                          0.00258
Train___Time                          0.15724
Eval____Time                          0.00347
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -53.82990
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.64790
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.86122
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.67840
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16120
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.83335
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.93625
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12265.51130
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.75787
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.30929
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.76119      0.34634   8.10753     7.41485
alpha_0                               0.98230      0.00015   0.98245     0.98216
alpha_1                               0.98230      0.00015   0.98244     0.98215
alpha_2                               0.98230      0.00015   0.98245     0.98215
alpha_3                               0.98231      0.00015   0.98245     0.98216
alpha_4                               0.98230      0.00015   0.98245     0.98216
alpha_5                               0.98232      0.00015   0.98247     0.98217
alpha_6                               0.98230      0.00015   0.98244     0.98215
alpha_7                               0.98230      0.00015   0.98245     0.98215
alpha_8                               0.98230      0.00015   0.98244     0.98215
alpha_9                               0.98230      0.00015   0.98244     0.98215
Alpha_loss                            -0.11753     0.00105   -0.11648    -0.11858
Training/policy_loss                  -2.67112     0.00262   -2.66851    -2.67374
Training/qf1_loss                     1106.36993   95.78790  1202.15784  1010.58203
Training/qf2_loss                     1106.38776   95.78668  1202.17444  1010.60107
Training/pf_norm                      0.37778      0.00073   0.37851     0.37706
Training/qf1_norm                     21.74467     0.72024   22.46491    21.02442
Training/qf2_norm                     21.48328     0.72232   22.20560    20.76096
log_std/mean                          -0.02780     0.00034   -0.02746    -0.02813
log_std/std                           0.00164      0.00002   0.00166     0.00162
log_std/max                           -0.02481     0.00030   -0.02451    -0.02512
log_std/min                           -0.03149     0.00039   -0.03109    -0.03188
log_probs/mean                        -2.69430     0.00274   -2.69157    -2.69704
log_probs/std                         0.37842      0.01237   0.39080     0.36605
log_probs/max                         -1.51733     0.03141   -1.48593    -1.54874
log_probs/min                         -4.22005     0.39486   -3.82520    -4.61491
mean/mean                             0.00017      0.00002   0.00019     0.00015
mean/std                              0.00234      0.00002   0.00236     0.00231
mean/max                              0.00373      0.00013   0.00386     0.00359
mean/min                              -0.00375     0.00002   -0.00374    -0.00377
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 30 50
freq 22
sample: [3, 0, 4, 2, 7, 6, 1, 5, 9, 8]
replay_buffer._size: [4800 4800 4800 4800 4800 4800 4800 4800 4800 4800]
train_time 0.1591494083404541
eval time 0.003732919692993164
snapshot at best
2023-08-25 14:04:05,640 MainThread INFO: EPOCH:30
2023-08-25 14:04:05,641 MainThread INFO: Time Consumed:0.7701907157897949s
2023-08-25 14:04:05,641 MainThread INFO: Total Frames:46500s
 16%|█▌        | 31/200 [00:30<02:26,  1.16it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1200.08289
Train_Epoch_Reward                    9885.77767
Running_Training_Average_Rewards      1236.64345
Explore_Time                          0.00275
Train___Time                          0.15915
Eval____Time                          0.00373
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.48392
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.64520
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.87310
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.65650
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16061
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.82859
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.93150
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12290.48959
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.75428
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.28440
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.93992      0.01722   7.95714     7.92269
alpha_0                               0.98172      0.00015   0.98186     0.98157
alpha_1                               0.98171      0.00015   0.98185     0.98156
alpha_2                               0.98171      0.00015   0.98186     0.98156
alpha_3                               0.98172      0.00015   0.98187     0.98157
alpha_4                               0.98171      0.00015   0.98186     0.98157
alpha_5                               0.98173      0.00015   0.98188     0.98158
alpha_6                               0.98171      0.00015   0.98185     0.98156
alpha_7                               0.98171      0.00015   0.98186     0.98157
alpha_8                               0.98171      0.00015   0.98185     0.98156
alpha_9                               0.98171      0.00015   0.98185     0.98156
Alpha_loss                            -0.12173     0.00088   -0.12085    -0.12261
Training/policy_loss                  -2.68103     0.00692   -2.67411    -2.68794
Training/qf1_loss                     1241.76770   20.60266  1262.37036  1221.16504
Training/qf2_loss                     1241.79010   20.60333  1262.39343  1221.18677
Training/pf_norm                      0.32711      0.02178   0.34889     0.30533
Training/qf1_norm                     22.17892     0.06179   22.24071    22.11713
Training/qf2_norm                     21.89039     0.07276   21.96316    21.81763
log_std/mean                          -0.02915     0.00034   -0.02881    -0.02949
log_std/std                           0.00172      0.00002   0.00174     0.00170
log_std/max                           -0.02600     0.00031   -0.02569    -0.02632
log_std/min                           -0.03310     0.00040   -0.03269    -0.03350
log_probs/mean                        -2.70469     0.00697   -2.69772    -2.71167
log_probs/std                         0.37631      0.01066   0.38697     0.36565
log_probs/max                         -1.54470     0.06931   -1.47539    -1.61400
log_probs/min                         -3.86645     0.13129   -3.73516    -3.99774
mean/mean                             0.00013      0.00001   0.00013     0.00012
mean/std                              0.00239      0.00003   0.00242     0.00236
mean/max                              0.00363      0.00005   0.00369     0.00358
mean/min                              -0.00394     0.00003   -0.00391    -0.00398
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 31 50
freq 22
sample: [6, 3, 5, 4, 0, 8, 7, 1, 9, 2]
replay_buffer._size: [4950 4950 4950 4950 4950 4950 4950 4950 4950 4950]
train_time 0.1459944248199463
eval time 0.0026073455810546875
snapshot at best
2023-08-25 14:04:06,437 MainThread INFO: EPOCH:31
2023-08-25 14:04:06,437 MainThread INFO: Time Consumed:0.7058758735656738s
2023-08-25 14:04:06,437 MainThread INFO: Total Frames:48000s
 16%|█▌        | 32/200 [00:31<02:21,  1.18it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1204.04407
Train_Epoch_Reward                    41809.12114
Running_Training_Average_Rewards      2150.40775
Explore_Time                          0.00249
Train___Time                          0.14599
Eval____Time                          0.00261
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -43.91353
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.64844
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.88334
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.63750
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16069
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.82536
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.92843
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12315.96641
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.75686
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.25752
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.84569      0.09298   7.93867     7.75272
alpha_0                               0.98113      0.00015   0.98127     0.98098
alpha_1                               0.98112      0.00015   0.98126     0.98097
alpha_2                               0.98112      0.00015   0.98127     0.98097
alpha_3                               0.98113      0.00015   0.98128     0.98098
alpha_4                               0.98112      0.00015   0.98127     0.98098
alpha_5                               0.98114      0.00015   0.98129     0.98099
alpha_6                               0.98112      0.00015   0.98126     0.98097
alpha_7                               0.98112      0.00015   0.98127     0.98098
alpha_8                               0.98112      0.00015   0.98127     0.98097
alpha_9                               0.98112      0.00015   0.98126     0.98097
Alpha_loss                            -0.12560     0.00093   -0.12468    -0.12653
Training/policy_loss                  -2.67245     0.00418   -2.66827    -2.67664
Training/qf1_loss                     1220.76074   27.79712  1248.55786  1192.96362
Training/qf2_loss                     1220.78589   27.79688  1248.58276  1192.98901
Training/pf_norm                      0.34432      0.00797   0.35229     0.33636
Training/qf1_norm                     22.00434     0.17678   22.18113    21.82756
Training/qf2_norm                     21.70854     0.17137   21.87992    21.53717
log_std/mean                          -0.03052     0.00034   -0.03017    -0.03086
log_std/std                           0.00180      0.00002   0.00182     0.00178
log_std/max                           -0.02728     0.00030   -0.02698    -0.02758
log_std/min                           -0.03470     0.00040   -0.03430    -0.03510
log_probs/mean                        -2.69624     0.00420   -2.69204    -2.70044
log_probs/std                         0.37198      0.00279   0.37477     0.36920
log_probs/max                         -1.43732     0.01707   -1.42025    -1.45439
log_probs/min                         -4.57792     0.01313   -4.56479    -4.59105
mean/mean                             0.00016      0.00001   0.00017     0.00016
mean/std                              0.00253      0.00004   0.00257     0.00249
mean/max                              0.00390      0.00008   0.00398     0.00381
mean/min                              -0.00410     0.00003   -0.00407    -0.00413
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 32 50
freq 22
sample: [6, 4, 8, 2, 7, 1, 0, 5, 9, 3]
replay_buffer._size: [5100 5100 5100 5100 5100 5100 5100 5100 5100 5100]
train_time 0.1938152313232422
eval time 0.09287428855895996
snapshot at best
2023-08-25 14:04:07,354 MainThread INFO: EPOCH:32
2023-08-25 14:04:07,354 MainThread INFO: Time Consumed:0.8256072998046875s
2023-08-25 14:04:07,354 MainThread INFO: Total Frames:49500s
 16%|█▋        | 33/200 [00:32<02:24,  1.16it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1206.58167
Train_Epoch_Reward                    3050.25503
Running_Training_Average_Rewards      1824.83846
Explore_Time                          0.00262
Train___Time                          0.19382
Eval____Time                          0.09287
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.74085
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.65169
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.89240
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.62292
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16544
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.82297
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.92618
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12337.44222
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.75947
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.23644
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.69226      0.14002   7.83228     7.55224
alpha_0                               0.98054      0.00015   0.98069     0.98039
alpha_1                               0.98053      0.00015   0.98067     0.98038
alpha_2                               0.98053      0.00015   0.98068     0.98038
alpha_3                               0.98054      0.00015   0.98069     0.98039
alpha_4                               0.98054      0.00015   0.98068     0.98039
alpha_5                               0.98055      0.00015   0.98070     0.98041
alpha_6                               0.98053      0.00015   0.98067     0.98038
alpha_7                               0.98054      0.00015   0.98068     0.98039
alpha_8                               0.98053      0.00015   0.98068     0.98038
alpha_9                               0.98053      0.00015   0.98068     0.98038
Alpha_loss                            -0.12969     0.00084   -0.12885    -0.13053
Training/policy_loss                  -2.67575     0.00858   -2.66717    -2.68433
Training/qf1_loss                     1104.66675   35.21191  1139.87866  1069.45483
Training/qf2_loss                     1104.69409   35.21069  1139.90479  1069.48340
Training/pf_norm                      0.34049      0.00909   0.34958     0.33140
Training/qf1_norm                     21.74422     0.25673   22.00095    21.48749
Training/qf2_norm                     21.44907     0.26021   21.70928    21.18886
log_std/mean                          -0.03190     0.00035   -0.03156    -0.03225
log_std/std                           0.00187      0.00002   0.00188     0.00185
log_std/max                           -0.02857     0.00035   -0.02823    -0.02892
log_std/min                           -0.03631     0.00041   -0.03591    -0.03672
log_probs/mean                        -2.69986     0.00869   -2.69117    -2.70855
log_probs/std                         0.37883      0.00814   0.38698     0.37069
log_probs/max                         -1.45933     0.00114   -1.45819    -1.46048
log_probs/min                         -5.10159     1.02419   -4.07740    -6.12577
mean/mean                             0.00020      0.00001   0.00021     0.00019
mean/std                              0.00267      0.00004   0.00271     0.00263
mean/max                              0.00405      0.00000   0.00405     0.00404
mean/min                              -0.00427     0.00007   -0.00421    -0.00434
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 33 50
freq 22
sample: [8, 6, 9, 4, 2, 0, 7, 1, 3, 5]
replay_buffer._size: [5250 5250 5250 5250 5250 5250 5250 5250 5250 5250]
train_time 0.14818382263183594
eval time 0.0032749176025390625
snapshot at best
2023-08-25 14:04:08,194 MainThread INFO: EPOCH:33
2023-08-25 14:04:08,194 MainThread INFO: Time Consumed:0.7511787414550781s
2023-08-25 14:04:08,194 MainThread INFO: Total Frames:51000s
 17%|█▋        | 34/200 [00:33<02:22,  1.16it/s]------------------------------------  -----------  --------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1209.17739
Train_Epoch_Reward                    8052.04724
Running_Training_Average_Rewards      1763.71411
Explore_Time                          0.00263
Train___Time                          0.14818
Eval____Time                          0.00327
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.49777
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.65074
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.90272
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.60242
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16477
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.81453
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.91729
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12363.26363
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.75737
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.21292
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.49826      0.04602   7.54428     7.45225
alpha_0                               0.97995      0.00015   0.98010     0.97980
alpha_1                               0.97994      0.00015   0.98009     0.97979
alpha_2                               0.97994      0.00015   0.98009     0.97979
alpha_3                               0.97995      0.00015   0.98010     0.97981
alpha_4                               0.97995      0.00015   0.98009     0.97980
alpha_5                               0.97996      0.00015   0.98011     0.97982
alpha_6                               0.97994      0.00015   0.98008     0.97979
alpha_7                               0.97995      0.00015   0.98009     0.97980
alpha_8                               0.97994      0.00015   0.98009     0.97979
alpha_9                               0.97994      0.00015   0.98009     0.97979
Alpha_loss                            -0.13397     0.00101   -0.13295    -0.13498
Training/policy_loss                  -2.68789     0.00022   -2.68767    -2.68811
Training/qf1_loss                     984.06738    24.18811  1008.25549  959.87927
Training/qf2_loss                     984.09854    24.18723  1008.28577  959.91132
Training/pf_norm                      0.29259      0.01437   0.30696     0.27822
Training/qf1_norm                     21.33288     0.08298   21.41586    21.24990
Training/qf2_norm                     21.02719     0.08091   21.10810    20.94627
log_std/mean                          -0.03330     0.00035   -0.03295    -0.03366
log_std/std                           0.00194      0.00002   0.00195     0.00192
log_std/max                           -0.02990     0.00034   -0.02957    -0.03024
log_std/min                           -0.03792     0.00041   -0.03751    -0.03833
log_probs/mean                        -2.71249     0.00028   -2.71222    -2.71277
log_probs/std                         0.35541      0.00811   0.36352     0.34730
log_probs/max                         -1.60453     0.02704   -1.57750    -1.63157
log_probs/min                         -3.66995     0.03709   -3.63286    -3.70704
mean/mean                             0.00021      0.00001   0.00022     0.00020
mean/std                              0.00282      0.00005   0.00287     0.00278
mean/max                              0.00405      0.00000   0.00405     0.00404
mean/min                              -0.00457     0.00012   -0.00445    -0.00468
------------------------------------  -----------  --------  ----------  ---------
epoch, update_end_epoch 34 50
freq 22
sample: [9, 6, 5, 7, 2, 4, 0, 1, 3, 8]
replay_buffer._size: [5400 5400 5400 5400 5400 5400 5400 5400 5400 5400]
train_time 1.5423626899719238
eval time 0.002561330795288086
snapshot at best
2023-08-25 14:04:10,508 MainThread INFO: EPOCH:34
2023-08-25 14:04:10,508 MainThread INFO: Time Consumed:2.1926448345184326s
2023-08-25 14:04:10,508 MainThread INFO: Total Frames:52500s
 18%|█▊        | 35/200 [00:35<03:34,  1.30s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1212.28963
Train_Epoch_Reward                    4476.22455
Running_Training_Average_Rewards      519.28423
Explore_Time                          0.00329
Train___Time                          1.54236
Eval____Time                          0.00256
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.53319
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.64137
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.93352
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.56243
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.17012
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.81046
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.91287
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12420.80061
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.74584
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.16885
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.23393      0.67939    8.91333     7.55454
alpha_0                               0.97936      0.00015    0.97951     0.97921
alpha_1                               0.97935      0.00015    0.97950     0.97920
alpha_2                               0.97935      0.00015    0.97950     0.97920
alpha_3                               0.97937      0.00015    0.97951     0.97922
alpha_4                               0.97936      0.00015    0.97951     0.97921
alpha_5                               0.97938      0.00015    0.97952     0.97923
alpha_6                               0.97935      0.00015    0.97949     0.97920
alpha_7                               0.97936      0.00015    0.97951     0.97921
alpha_8                               0.97935      0.00015    0.97950     0.97921
alpha_9                               0.97935      0.00015    0.97950     0.97921
Alpha_loss                            -0.13776     0.00117    -0.13659    -0.13893
Training/policy_loss                  -2.67607     0.00783    -2.66824    -2.68390
Training/qf1_loss                     1434.12640   257.42609  1691.55249  1176.70032
Training/qf2_loss                     1434.16357   257.42554  1691.58911  1176.73804
Training/pf_norm                      0.33948      0.01645    0.35593     0.32303
Training/qf1_norm                     22.92159     1.42247    24.34405    21.49912
Training/qf2_norm                     22.57685     1.41964    23.99649    21.15721
log_std/mean                          -0.03470     0.00035    -0.03435    -0.03506
log_std/std                           0.00202      0.00002    0.00204     0.00199
log_std/max                           -0.03118     0.00033    -0.03085    -0.03150
log_std/min                           -0.03955     0.00041    -0.03914    -0.03996
log_probs/mean                        -2.70069     0.00805    -2.69265    -2.70874
log_probs/std                         0.36524      0.00496    0.37019     0.36028
log_probs/max                         -1.55756     0.02037    -1.53719    -1.57794
log_probs/min                         -3.99236     0.00683    -3.98552    -3.99919
mean/mean                             0.00019      0.00001    0.00020     0.00017
mean/std                              0.00298      0.00005    0.00303     0.00293
mean/max                              0.00411      0.00004    0.00415     0.00407
mean/min                              -0.00498     0.00014    -0.00483    -0.00512
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 35 50
freq 22
sample: [2, 6, 4, 5, 1, 3, 8, 7, 9, 0]
replay_buffer._size: [5550 5550 5550 5550 5550 5550 5550 5550 5550 5550]
train_time 0.15642285346984863
eval time 0.003192901611328125
snapshot at best
2023-08-25 14:04:11,377 MainThread INFO: EPOCH:35
2023-08-25 14:04:11,378 MainThread INFO: Time Consumed:0.7633752822875977s
2023-08-25 14:04:11,378 MainThread INFO: Total Frames:54000s
 18%|█▊        | 36/200 [00:36<03:11,  1.17s/it]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1216.52467
Train_Epoch_Reward                    18076.69505
Running_Training_Average_Rewards      1020.16556
Explore_Time                          0.00314
Train___Time                          0.15642
Eval____Time                          0.00319
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.46402
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.65514
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.95513
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.54962
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.18639
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.81859
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.92127
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12467.11473
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.75887
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.13068
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           8.24641      0.19559   8.44200     8.05083
alpha_0                               0.97877      0.00015   0.97892     0.97863
alpha_1                               0.97876      0.00015   0.97891     0.97862
alpha_2                               0.97876      0.00015   0.97891     0.97861
alpha_3                               0.97878      0.00015   0.97892     0.97863
alpha_4                               0.97877      0.00015   0.97892     0.97862
alpha_5                               0.97879      0.00015   0.97893     0.97864
alpha_6                               0.97876      0.00015   0.97891     0.97861
alpha_7                               0.97877      0.00015   0.97892     0.97863
alpha_8                               0.97877      0.00015   0.97891     0.97862
alpha_9                               0.97877      0.00015   0.97891     0.97862
Alpha_loss                            -0.14184     0.00089   -0.14094    -0.14273
Training/policy_loss                  -2.67849     0.00536   -2.67313    -2.68384
Training/qf1_loss                     1421.40674   99.88940  1521.29614  1321.51733
Training/qf2_loss                     1421.44977   99.89093  1521.34070  1321.55884
Training/pf_norm                      0.32667      0.01048   0.33714     0.31619
Training/qf1_norm                     23.01114     0.44517   23.45631    22.56597
Training/qf2_norm                     22.64249     0.44743   23.08993    22.19506
log_std/mean                          -0.03612     0.00036   -0.03576    -0.03647
log_std/std                           0.00210      0.00002   0.00213     0.00208
log_std/max                           -0.03246     0.00029   -0.03218    -0.03275
log_std/min                           -0.04121     0.00043   -0.04078    -0.04164
log_probs/mean                        -2.70336     0.00544   -2.69793    -2.70880
log_probs/std                         0.36104      0.00041   0.36145     0.36063
log_probs/max                         -1.55087     0.02523   -1.52564    -1.57610
log_probs/min                         -4.43709     0.11787   -4.31922    -4.55496
mean/mean                             0.00020      0.00002   0.00022     0.00018
mean/std                              0.00314      0.00001   0.00315     0.00312
mean/max                              0.00407      0.00006   0.00413     0.00400
mean/min                              -0.00538     0.00005   -0.00532    -0.00543
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 36 50
freq 22
sample: [4, 8, 0, 7, 1, 5, 9, 3, 2, 6]
replay_buffer._size: [5700 5700 5700 5700 5700 5700 5700 5700 5700 5700]
train_time 0.16650795936584473
eval time 0.002766132354736328
snapshot at best
2023-08-25 14:04:12,164 MainThread INFO: EPOCH:36
2023-08-25 14:04:12,165 MainThread INFO: Time Consumed:0.697460412979126s
2023-08-25 14:04:12,165 MainThread INFO: Total Frames:55500s
 18%|█▊        | 37/200 [00:37<02:51,  1.05s/it]------------------------------------  -----------  ---------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1221.64952
Train_Epoch_Reward                    5224.49196
Running_Training_Average_Rewards      925.91372
Explore_Time                          0.00239
Train___Time                          0.16651
Eval____Time                          0.00277
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.41159
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.67463
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.97599
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.53582
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.20466
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.83119
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.93425
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12515.92281
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.77778
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.08839
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           7.66943      0.93266    8.60209     6.73677
alpha_0                               0.97818      0.00015    0.97833     0.97804
alpha_1                               0.97817      0.00015    0.97832     0.97803
alpha_2                               0.97817      0.00015    0.97832     0.97803
alpha_3                               0.97819      0.00015    0.97834     0.97804
alpha_4                               0.97818      0.00015    0.97833     0.97804
alpha_5                               0.97820      0.00015    0.97835     0.97805
alpha_6                               0.97817      0.00015    0.97832     0.97802
alpha_7                               0.97818      0.00015    0.97833     0.97804
alpha_8                               0.97818      0.00015    0.97833     0.97803
alpha_9                               0.97818      0.00015    0.97833     0.97803
Alpha_loss                            -0.14617     0.00096    -0.14521    -0.14713
Training/policy_loss                  -2.69228     0.00233    -2.68995    -2.69461
Training/qf1_loss                     1135.28214   188.26242  1323.54456  947.01971
Training/qf2_loss                     1135.32147   188.26569  1323.58716  947.05579
Training/pf_norm                      0.28267      0.02064    0.30331     0.26203
Training/qf1_norm                     21.82721     2.00047    23.82768    19.82673
Training/qf2_norm                     21.49701     1.96475    23.46176    19.53226
log_std/mean                          -0.03754     0.00036    -0.03718    -0.03790
log_std/std                           0.00220      0.00002    0.00223     0.00218
log_std/max                           -0.03370     0.00031    -0.03338    -0.03401
log_std/min                           -0.04289     0.00043    -0.04247    -0.04332
log_probs/mean                        -2.71766     0.00231    -2.71535    -2.71997
log_probs/std                         0.35904      0.00817    0.36721     0.35087
log_probs/max                         -1.61634     0.05585    -1.56049    -1.67219
log_probs/min                         -4.36903     0.47575    -3.89328    -4.84478
mean/mean                             0.00034      0.00004    0.00038     0.00030
mean/std                              0.00329      0.00006    0.00334     0.00323
mean/max                              0.00405      0.00003    0.00407     0.00402
mean/min                              -0.00559     0.00008    -0.00551    -0.00567
------------------------------------  -----------  ---------  ----------  ---------
epoch, update_end_epoch 37 50
freq 22
sample: [0, 9, 6, 7, 3, 1, 8, 5, 4, 2]
replay_buffer._size: [5850 5850 5850 5850 5850 5850 5850 5850 5850 5850]
train_time 0.14636969566345215
eval time 0.003317594528198242
snapshot at best
2023-08-25 14:04:12,986 MainThread INFO: EPOCH:37
2023-08-25 14:04:12,986 MainThread INFO: Time Consumed:0.7349472045898438s
2023-08-25 14:04:12,987 MainThread INFO: Total Frames:57000s
 19%|█▉        | 38/200 [00:37<02:39,  1.02it/s]------------------------------------  -----------  --------  ---------  ---------
Name                                  Value
Running_Average_Rewards               1227.30333
Train_Epoch_Reward                    15608.55037
Running_Training_Average_Rewards      1296.99125
Explore_Time                          0.00261
Train___Time                          0.14637
Eval____Time                          0.00332
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.26753
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.65918
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.02807
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.50265
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.24858
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.86397
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.96875
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12585.28255
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.76021
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.04736
mean_success_rate                     0.00000

Name                                  Mean         Std       Max        Min
Reward_Mean                           7.21995      0.06226   7.28221    7.15769
alpha_0                               0.97760      0.00015   0.97774    0.97745
alpha_1                               0.97759      0.00015   0.97773    0.97744
alpha_2                               0.97758      0.00015   0.97773    0.97744
alpha_3                               0.97760      0.00015   0.97775    0.97746
alpha_4                               0.97759      0.00015   0.97774    0.97745
alpha_5                               0.97761      0.00015   0.97776    0.97746
alpha_6                               0.97758      0.00015   0.97773    0.97743
alpha_7                               0.97760      0.00015   0.97774    0.97745
alpha_8                               0.97759      0.00015   0.97774    0.97744
alpha_9                               0.97759      0.00015   0.97774    0.97745
Alpha_loss                            -0.15005     0.00095   -0.14910   -0.15101
Training/policy_loss                  -2.68537     0.00242   -2.68295   -2.68779
Training/qf1_loss                     937.99399    11.50815  949.50214  926.48584
Training/qf2_loss                     938.03720    11.51004  949.54724  926.52716
Training/pf_norm                      0.33381      0.00381   0.33762    0.33000
Training/qf1_norm                     20.87967     0.11330   20.99297   20.76637
Training/qf2_norm                     20.54129     0.12869   20.66999   20.41260
log_std/mean                          -0.03897     0.00036   -0.03860   -0.03933
log_std/std                           0.00230      0.00002   0.00233    0.00228
log_std/max                           -0.03493     0.00030   -0.03463   -0.03522
log_std/min                           -0.04459     0.00042   -0.04418   -0.04501
log_probs/mean                        -2.71075     0.00244   -2.70831   -2.71319
log_probs/std                         0.36310      0.00326   0.36637    0.35984
log_probs/max                         -1.54484     0.04601   -1.49883   -1.59086
log_probs/min                         -4.72700     0.25546   -4.47154   -4.98246
mean/mean                             0.00049      0.00005   0.00054    0.00044
mean/std                              0.00350      0.00004   0.00354    0.00346
mean/max                              0.00419      0.00004   0.00423    0.00415
mean/min                              -0.00588     0.00005   -0.00584   -0.00593
------------------------------------  -----------  --------  ---------  ---------
epoch, update_end_epoch 38 50
freq 22
sample: [5, 6, 2, 7, 9, 3, 1, 4, 8, 0]
replay_buffer._size: [6000 6000 6000 6000 6000 6000 6000 6000 6000 6000]
train_time 0.15844154357910156
eval time 0.0029783248901367188
snapshot at best
2023-08-25 14:04:13,798 MainThread INFO: EPOCH:38
2023-08-25 14:04:13,799 MainThread INFO: Time Consumed:0.7180204391479492s
2023-08-25 14:04:13,799 MainThread INFO: Total Frames:58500s
 20%|█▉        | 39/200 [00:38<02:29,  1.07it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1234.23626
Train_Epoch_Reward                    2974.71453
Running_Training_Average_Rewards      793.59190
Explore_Time                          0.00238
Train___Time                          0.15844
Eval____Time                          0.00298
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.15742
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.64741
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.08415
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.45807
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.28543
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.88901
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.99496
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12669.91920
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.74606
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.99372
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.38256      0.61565    8.99821     7.76690
alpha_0                               0.97701      0.00015    0.97716     0.97686
alpha_1                               0.97700      0.00015    0.97715     0.97685
alpha_2                               0.97700      0.00015    0.97714     0.97685
alpha_3                               0.97702      0.00015    0.97716     0.97687
alpha_4                               0.97701      0.00015    0.97715     0.97686
alpha_5                               0.97702      0.00015    0.97717     0.97688
alpha_6                               0.97699      0.00015    0.97714     0.97685
alpha_7                               0.97701      0.00015    0.97716     0.97686
alpha_8                               0.97700      0.00015    0.97715     0.97686
alpha_9                               0.97700      0.00015    0.97715     0.97686
Alpha_loss                            -0.15408     0.00093    -0.15315    -0.15502
Training/policy_loss                  -2.68505     0.00320    -2.68184    -2.68825
Training/qf1_loss                     1300.04675   114.73120  1414.77795  1185.31555
Training/qf2_loss                     1300.09906   114.73651  1414.83557  1185.36255
Training/pf_norm                      0.29941      0.00504    0.30444     0.29437
Training/qf1_norm                     23.42145     1.33908    24.76053    22.08237
Training/qf2_norm                     23.02726     1.29964    24.32690    21.72762
log_std/mean                          -0.04042     0.00036    -0.04006    -0.04078
log_std/std                           0.00241      0.00003    0.00243     0.00238
log_std/max                           -0.03615     0.00032    -0.03583    -0.03646
log_std/min                           -0.04631     0.00043    -0.04588    -0.04674
log_probs/mean                        -2.71063     0.00321    -2.70741    -2.71384
log_probs/std                         0.35102      0.01365    0.36467     0.33737
log_probs/max                         -1.61459     0.00373    -1.61086    -1.61832
log_probs/min                         -4.92991     1.14405    -3.78586    -6.07396
mean/mean                             0.00068      0.00004    0.00072     0.00064
mean/std                              0.00370      0.00005    0.00375     0.00365
mean/max                              0.00444      0.00008    0.00451     0.00436
mean/min                              -0.00611     0.00008    -0.00603    -0.00619
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 39 50
freq 22
sample: [2, 7, 4, 0, 6, 8, 9, 3, 1, 5]
replay_buffer._size: [6150 6150 6150 6150 6150 6150 6150 6150 6150 6150]
train_time 0.15332412719726562
eval time 0.0029325485229492188
snapshot at best
2023-08-25 14:04:14,605 MainThread INFO: EPOCH:39
2023-08-25 14:04:14,606 MainThread INFO: Time Consumed:0.7198145389556885s
2023-08-25 14:04:14,606 MainThread INFO: Total Frames:60000s
 20%|██        | 40/200 [00:39<02:23,  1.12it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1241.94813
Train_Epoch_Reward                    9220.38454
Running_Training_Average_Rewards      926.78831
Explore_Time                          0.00255
Train___Time                          0.15332
Eval____Time                          0.00293
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.75282
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.62956
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.14408
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.41781
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.33092
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.92049
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.02822
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12749.74345
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.72592
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.94901
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           8.04903      0.00299   8.05201     8.04604
alpha_0                               0.97642      0.00015   0.97657     0.97627
alpha_1                               0.97641      0.00015   0.97656     0.97627
alpha_2                               0.97641      0.00015   0.97656     0.97626
alpha_3                               0.97643      0.00015   0.97658     0.97628
alpha_4                               0.97642      0.00015   0.97657     0.97627
alpha_5                               0.97644      0.00015   0.97658     0.97629
alpha_6                               0.97641      0.00015   0.97655     0.97626
alpha_7                               0.97642      0.00015   0.97657     0.97628
alpha_8                               0.97642      0.00015   0.97656     0.97627
alpha_9                               0.97642      0.00015   0.97656     0.97627
Alpha_loss                            -0.15796     0.00115   -0.15681    -0.15912
Training/policy_loss                  -2.67853     0.00602   -2.67251    -2.68455
Training/qf1_loss                     1321.22308   21.49908  1342.72217  1299.72400
Training/qf2_loss                     1321.28552   21.50195  1342.78748  1299.78357
Training/pf_norm                      0.32361      0.01132   0.33493     0.31228
Training/qf1_norm                     22.78774     0.00668   22.79442    22.78106
Training/qf2_norm                     22.34358     0.01630   22.35988    22.32727
log_std/mean                          -0.04188     0.00037   -0.04151    -0.04225
log_std/std                           0.00252      0.00003   0.00255     0.00250
log_std/max                           -0.03735     0.00030   -0.03705    -0.03765
log_std/min                           -0.04810     0.00045   -0.04764    -0.04855
log_probs/mean                        -2.70412     0.00617   -2.69795    -2.71028
log_probs/std                         0.33905      0.00305   0.34210     0.33600
log_probs/max                         -1.53174     0.00352   -1.52822    -1.53526
log_probs/min                         -3.91783     0.17233   -3.74550    -4.09017
mean/mean                             0.00076      0.00002   0.00078     0.00074
mean/std                              0.00384      0.00002   0.00386     0.00382
mean/max                              0.00450      0.00002   0.00452     0.00448
mean/min                              -0.00645     0.00005   -0.00640    -0.00650
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 40 50
freq 22
sample: [2, 5, 6, 3, 9, 4, 7, 8, 0, 1]
replay_buffer._size: [6300 6300 6300 6300 6300 6300 6300 6300 6300 6300]
train_time 0.1422104835510254
eval time 0.0026416778564453125
snapshot at best
2023-08-25 14:04:15,438 MainThread INFO: EPOCH:40
2023-08-25 14:04:15,438 MainThread INFO: Time Consumed:0.7390384674072266s
2023-08-25 14:04:15,438 MainThread INFO: Total Frames:61500s
 20%|██        | 41/200 [00:40<02:19,  1.14it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1249.27201
Train_Epoch_Reward                    15564.11776
Running_Training_Average_Rewards      925.30723
Explore_Time                          0.00242
Train___Time                          0.14221
Eval____Time                          0.00264
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -43.43155
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.64696
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.17957
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.41963
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.38628
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.96731
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.07740
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12798.42638
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.74336
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.92159
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.85215      0.35860   8.21075     7.49355
alpha_0                               0.97583      0.00015   0.97598     0.97569
alpha_1                               0.97583      0.00015   0.97597     0.97568
alpha_2                               0.97582      0.00015   0.97597     0.97568
alpha_3                               0.97584      0.00015   0.97599     0.97570
alpha_4                               0.97583      0.00015   0.97598     0.97569
alpha_5                               0.97585      0.00015   0.97600     0.97571
alpha_6                               0.97582      0.00015   0.97597     0.97567
alpha_7                               0.97584      0.00015   0.97598     0.97569
alpha_8                               0.97583      0.00015   0.97598     0.97568
alpha_9                               0.97583      0.00015   0.97598     0.97568
Alpha_loss                            -0.16202     0.00107   -0.16095    -0.16309
Training/policy_loss                  -2.67957     0.00244   -2.67713    -2.68201
Training/qf1_loss                     1350.86249   83.11139  1433.97388  1267.75110
Training/qf2_loss                     1350.92682   83.11041  1434.03723  1267.81641
Training/pf_norm                      0.30931      0.03349   0.34280     0.27582
Training/qf1_norm                     22.40679     0.76664   23.17344    21.64015
Training/qf2_norm                     21.95949     0.77015   22.72964    21.18934
log_std/mean                          -0.04337     0.00037   -0.04299    -0.04374
log_std/std                           0.00265      0.00003   0.00268     0.00262
log_std/max                           -0.03856     0.00032   -0.03824    -0.03888
log_std/min                           -0.04990     0.00046   -0.04944    -0.05036
log_probs/mean                        -2.70526     0.00250   -2.70276    -2.70776
log_probs/std                         0.35340      0.00291   0.35631     0.35049
log_probs/max                         -1.65579     0.03713   -1.61866    -1.69291
log_probs/min                         -5.54133     0.43868   -5.10265    -5.98001
mean/mean                             0.00070      0.00002   0.00072     0.00068
mean/std                              0.00396      0.00003   0.00399     0.00393
mean/max                              0.00456      0.00006   0.00462     0.00450
mean/min                              -0.00683     0.00009   -0.00674    -0.00692
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 41 50
freq 22
sample: [3, 7, 0, 1, 4, 8, 9, 2, 6, 5]
replay_buffer._size: [6450 6450 6450 6450 6450 6450 6450 6450 6450 6450]
train_time 0.1689133644104004
eval time 0.0032427310943603516
snapshot at best
2023-08-25 14:04:16,240 MainThread INFO: EPOCH:41
2023-08-25 14:04:16,241 MainThread INFO: Time Consumed:0.7096748352050781s
2023-08-25 14:04:16,241 MainThread INFO: Total Frames:63000s
 21%|██        | 42/200 [00:41<02:14,  1.17it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1254.87513
Train_Epoch_Reward                    16703.89581
Running_Training_Average_Rewards      1382.94660
Explore_Time                          0.00274
Train___Time                          0.16891
Eval____Time                          0.00324
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -42.87602
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.66608
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.20265
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.42178
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.42023
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.99512
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.10622
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12834.10447
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.76247
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.89742
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           7.94555      0.00047    7.94602     7.94508
alpha_0                               0.97525      0.00015    0.97540     0.97510
alpha_1                               0.97524      0.00015    0.97539     0.97509
alpha_2                               0.97524      0.00015    0.97539     0.97509
alpha_3                               0.97526      0.00015    0.97540     0.97511
alpha_4                               0.97525      0.00015    0.97539     0.97510
alpha_5                               0.97527      0.00015    0.97541     0.97512
alpha_6                               0.97524      0.00015    0.97538     0.97509
alpha_7                               0.97525      0.00015    0.97540     0.97511
alpha_8                               0.97524      0.00015    0.97539     0.97510
alpha_9                               0.97524      0.00015    0.97539     0.97510
Alpha_loss                            -0.16594     0.00091    -0.16503    -0.16684
Training/policy_loss                  -2.67508     0.00390    -2.67119    -2.67898
Training/qf1_loss                     1439.93921   123.92480  1563.86401  1316.01440
Training/qf2_loss                     1440.00140   123.92780  1563.92920  1316.07361
Training/pf_norm                      0.32603      0.02027    0.34630     0.30576
Training/qf1_norm                     22.60924     0.01976    22.62900    22.58948
Training/qf2_norm                     22.18476     0.00521    22.18997    22.17955
log_std/mean                          -0.04487     0.00038    -0.04449    -0.04525
log_std/std                           0.00277      0.00003    0.00280     0.00274
log_std/max                           -0.03981     0.00032    -0.03950    -0.04013
log_std/min                           -0.05164     0.00048    -0.05116    -0.05212
log_probs/mean                        -2.70077     0.00399    -2.69678    -2.70476
log_probs/std                         0.34947      0.00263    0.35210     0.34684
log_probs/max                         -1.62135     0.03591    -1.58544    -1.65726
log_probs/min                         -4.71625     0.21764    -4.49861    -4.93389
mean/mean                             0.00062      0.00002    0.00064     0.00060
mean/std                              0.00404      0.00000    0.00404     0.00403
mean/max                              0.00473      0.00001    0.00474     0.00473
mean/min                              -0.00709     0.00005    -0.00703    -0.00714
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 42 50
freq 22
sample: [7, 1, 0, 5, 4, 3, 9, 6, 8, 2]
replay_buffer._size: [6600 6600 6600 6600 6600 6600 6600 6600 6600 6600]
train_time 0.12801003456115723
eval time 0.002468585968017578
snapshot at best
2023-08-25 14:04:16,983 MainThread INFO: EPOCH:42
2023-08-25 14:04:16,983 MainThread INFO: Time Consumed:0.6553089618682861s
2023-08-25 14:04:16,983 MainThread INFO: Total Frames:64500s
 22%|██▏       | 43/200 [00:41<02:08,  1.22it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1258.77034
Train_Epoch_Reward                    22557.51389
Running_Training_Average_Rewards      1827.51758
Explore_Time                          0.00256
Train___Time                          0.12801
Eval____Time                          0.00247
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -56.92314
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.68112
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.22194
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.40946
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.43282
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.00421
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.11494
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12872.13438
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.77698
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.86874
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.98111      0.30280   8.28391     7.67831
alpha_0                               0.97466      0.00015   0.97481     0.97452
alpha_1                               0.97465      0.00015   0.97480     0.97451
alpha_2                               0.97465      0.00015   0.97480     0.97451
alpha_3                               0.97467      0.00015   0.97482     0.97452
alpha_4                               0.97466      0.00015   0.97481     0.97452
alpha_5                               0.97468      0.00015   0.97483     0.97453
alpha_6                               0.97465      0.00015   0.97480     0.97450
alpha_7                               0.97467      0.00015   0.97481     0.97452
alpha_8                               0.97466      0.00015   0.97481     0.97451
alpha_9                               0.97466      0.00015   0.97480     0.97451
Alpha_loss                            -0.17039     0.00110   -0.16929    -0.17149
Training/policy_loss                  -2.69145     0.00353   -2.68792    -2.69498
Training/qf1_loss                     1213.55939   74.04999  1287.60938  1139.50940
Training/qf2_loss                     1213.62988   74.05542  1287.68530  1139.57446
Training/pf_norm                      0.27628      0.02317   0.29945     0.25311
Training/qf1_norm                     22.73557     0.67257   23.40814    22.06299
Training/qf2_norm                     22.27933     0.64099   22.92032    21.63833
log_std/mean                          -0.04640     0.00038   -0.04602    -0.04679
log_std/std                           0.00288      0.00003   0.00291     0.00286
log_std/max                           -0.04113     0.00034   -0.04079    -0.04147
log_std/min                           -0.05350     0.00042   -0.05309    -0.05392
log_probs/mean                        -2.71768     0.00361   -2.71408    -2.72129
log_probs/std                         0.33968      0.00002   0.33970     0.33967
log_probs/max                         -1.70530     0.04837   -1.65693    -1.75367
log_probs/min                         -4.31930     0.10769   -4.21162    -4.42699
mean/mean                             0.00053      0.00001   0.00055     0.00052
mean/std                              0.00397      0.00003   0.00400     0.00394
mean/max                              0.00455      0.00010   0.00466     0.00445
mean/min                              -0.00714     0.00001   -0.00712    -0.00715
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 43 50
freq 22
sample: [8, 4, 9, 2, 5, 6, 7, 0, 1, 3]
replay_buffer._size: [6750 6750 6750 6750 6750 6750 6750 6750 6750 6750]
train_time 0.13361096382141113
eval time 0.002485513687133789
2023-08-25 14:04:17,209 MainThread INFO: EPOCH:43
2023-08-25 14:04:17,209 MainThread INFO: Time Consumed:0.13892626762390137s
2023-08-25 14:04:17,209 MainThread INFO: Total Frames:66000s
 22%|██▏       | 44/200 [00:42<01:41,  1.54it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1260.72453
Train_Epoch_Reward                    8641.80844
Running_Training_Average_Rewards      1596.77394
Explore_Time                          0.00238
Train___Time                          0.13361
Eval____Time                          0.00249
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.47034
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.70542
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.21507
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.43000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.45088
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.01923
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.12952
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12862.37980
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.80229
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.87875
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.20635      0.80092    9.00727     7.40544
alpha_0                               0.97408      0.00015    0.97422     0.97393
alpha_1                               0.97407      0.00015    0.97421     0.97392
alpha_2                               0.97407      0.00015    0.97421     0.97392
alpha_3                               0.97409      0.00015    0.97423     0.97394
alpha_4                               0.97408      0.00015    0.97422     0.97393
alpha_5                               0.97410      0.00015    0.97424     0.97395
alpha_6                               0.97407      0.00015    0.97421     0.97392
alpha_7                               0.97408      0.00015    0.97423     0.97393
alpha_8                               0.97407      0.00015    0.97422     0.97393
alpha_9                               0.97407      0.00015    0.97422     0.97393
Alpha_loss                            -0.17428     0.00119    -0.17310    -0.17547
Training/policy_loss                  -2.68593     0.00663    -2.67930    -2.69256
Training/qf1_loss                     1400.43488   253.33588  1653.77075  1147.09900
Training/qf2_loss                     1400.51135   253.34485  1653.85620  1147.16650
Training/pf_norm                      0.31649      0.00329    0.31978     0.31320
Training/qf1_norm                     23.32532     1.75870    25.08402    21.56662
Training/qf2_norm                     22.83525     1.70297    24.53822    21.13227
log_std/mean                          -0.04793     0.00038    -0.04755    -0.04831
log_std/std                           0.00299      0.00002    0.00301     0.00296
log_std/max                           -0.04245     0.00035    -0.04210    -0.04281
log_std/min                           -0.05529     0.00041    -0.05488    -0.05570
log_probs/mean                        -2.71207     0.00685    -2.70523    -2.71892
log_probs/std                         0.34680      0.00258    0.34938     0.34423
log_probs/max                         -1.62340     0.01428    -1.60912    -1.63768
log_probs/min                         -4.55612     0.73948    -3.81664    -5.29560
mean/mean                             0.00051      0.00000    0.00052     0.00051
mean/std                              0.00384      0.00004    0.00388     0.00379
mean/max                              0.00422      0.00009    0.00431     0.00412
mean/min                              -0.00695     0.00009    -0.00685    -0.00704
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 44 50
freq 22
start to update mask
sample: [5, 4, 3, 8, 6, 1, 7, 2, 9, 0]
replay_buffer._size: [6900 6900 6900 6900 6900 6900 6900 6900 6900 6900]
train_time 0.13369488716125488
eval time 0.002841472625732422
2023-08-25 14:04:21,031 MainThread INFO: EPOCH:44
2023-08-25 14:04:21,032 MainThread INFO: Time Consumed:0.1417543888092041s
2023-08-25 14:04:21,032 MainThread INFO: Total Frames:67500s
 22%|██▎       | 45/200 [00:45<04:08,  1.60s/it]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1259.93868
Train_Epoch_Reward                    12565.53575
Running_Training_Average_Rewards      1458.82860
Explore_Time                          0.00424
Train___Time                          0.13369
Eval____Time                          0.00284
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -53.49870
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.73271
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.19516
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.47591
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.47195
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.03790
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.14820
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12821.49152
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.83157
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.91827
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           8.39698      0.04385   8.44082     8.35313
alpha_0                               0.97349      0.00015   0.97364     0.97335
alpha_1                               0.97348      0.00015   0.97363     0.97333
alpha_2                               0.97348      0.00015   0.97363     0.97334
alpha_3                               0.97350      0.00015   0.97365     0.97335
alpha_4                               0.97349      0.00015   0.97364     0.97335
alpha_5                               0.97351      0.00015   0.97366     0.97336
alpha_6                               0.97348      0.00015   0.97363     0.97333
alpha_7                               0.97350      0.00015   0.97364     0.97335
alpha_8                               0.97349      0.00015   0.97363     0.97334
alpha_9                               0.97349      0.00015   0.97363     0.97334
Alpha_loss                            -0.17826     0.00095   -0.17731    -0.17920
Training/policy_loss                  -2.68354     0.00231   -2.68124    -2.68585
Training/qf1_loss                     1617.93323   25.15344  1643.08667  1592.77979
Training/qf2_loss                     1618.01611   25.15503  1643.17114  1592.86108
Training/pf_norm                      0.30240      0.01399   0.31639     0.28842
Training/qf1_norm                     23.75574     0.09938   23.85512    23.65636
Training/qf2_norm                     23.24947     0.08669   23.33615    23.16278
log_std/mean                          -0.04835     0.00038   -0.04798    -0.04873
log_std/std                           0.00233      0.00001   0.00234     0.00232
log_std/max                           -0.04383     0.00035   -0.04349    -0.04418
log_std/min                           -0.05369     0.00038   -0.05331    -0.05407
log_probs/mean                        -2.70987     0.00235   -2.70751    -2.71222
log_probs/std                         0.33350      0.00185   0.33535     0.33166
log_probs/max                         -1.66226     0.07369   -1.58856    -1.73595
log_probs/min                         -4.81564     0.68710   -4.12854    -5.50274
mean/mean                             0.00040      0.00002   0.00042     0.00038
mean/std                              0.00350      0.00007   0.00357     0.00342
mean/max                              0.00367      0.00000   0.00367     0.00366
mean/min                              -0.00600     0.00014   -0.00586    -0.00614
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 45 50
freq 22
sample: [0, 3, 4, 5, 7, 1, 9, 2, 6, 8]
replay_buffer._size: [7050 7050 7050 7050 7050 7050 7050 7050 7050 7050]
train_time 0.14103031158447266
eval time 0.002744436264038086
2023-08-25 14:04:21,293 MainThread INFO: EPOCH:45
2023-08-25 14:04:21,293 MainThread INFO: Time Consumed:0.14690494537353516s
2023-08-25 14:04:21,293 MainThread INFO: Total Frames:69000s
 23%|██▎       | 46/200 [00:46<03:03,  1.19s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1256.13785
Train_Epoch_Reward                    55901.27260
Running_Training_Average_Rewards      2570.28723
Explore_Time                          0.00253
Train___Time                          0.14103
Eval____Time                          0.00274
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -46.74486
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.74666
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.14748
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.51637
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.44588
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.01792
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.12615
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12748.24665
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.84711
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.97807
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.87690      0.18953    9.06642     8.68737
alpha_0                               0.97291      0.00015    0.97305     0.97276
alpha_1                               0.97290      0.00015    0.97304     0.97275
alpha_2                               0.97290      0.00015    0.97304     0.97275
alpha_3                               0.97292      0.00015    0.97306     0.97277
alpha_4                               0.97291      0.00015    0.97305     0.97276
alpha_5                               0.97293      0.00015    0.97307     0.97278
alpha_6                               0.97290      0.00015    0.97304     0.97275
alpha_7                               0.97291      0.00015    0.97306     0.97276
alpha_8                               0.97290      0.00015    0.97305     0.97276
alpha_9                               0.97290      0.00015    0.97305     0.97276
Alpha_loss                            -0.18219     0.00098    -0.18121    -0.18317
Training/policy_loss                  -2.68000     0.00096    -2.67904    -2.68097
Training/qf1_loss                     1974.35602   178.74921  2153.10522  1795.60681
Training/qf2_loss                     1974.44806   178.74457  2153.19263  1795.70349
Training/pf_norm                      0.28269      0.01362    0.29630     0.26907
Training/qf1_norm                     24.86985     0.37075    25.24061    24.49910
Training/qf2_norm                     24.31891     0.39382    24.71273    23.92509
log_std/mean                          -0.04988     0.00038    -0.04950    -0.05027
log_std/std                           0.00239      0.00001    0.00240     0.00237
log_std/max                           -0.04522     0.00035    -0.04487    -0.04557
log_std/min                           -0.05531     0.00040    -0.05492    -0.05571
log_probs/mean                        -2.70623     0.00101    -2.70523    -2.70724
log_probs/std                         0.33813      0.00097    0.33909     0.33716
log_probs/max                         -1.68943     0.00203    -1.68740    -1.69147
log_probs/min                         -3.84561     0.01954    -3.82608    -3.86515
mean/mean                             0.00041      0.00001    0.00042     0.00040
mean/std                              0.00321      0.00007    0.00327     0.00314
mean/max                              0.00373      0.00003    0.00376     0.00369
mean/min                              -0.00539     0.00015    -0.00524    -0.00554
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 46 50
freq 22
sample: [2, 8, 6, 4, 0, 7, 9, 3, 1, 5]
replay_buffer._size: [7200 7200 7200 7200 7200 7200 7200 7200 7200 7200]
train_time 0.1412339210510254
eval time 0.002807140350341797
2023-08-25 14:04:21,534 MainThread INFO: EPOCH:46
2023-08-25 14:04:21,534 MainThread INFO: Time Consumed:0.14687418937683105s
2023-08-25 14:04:21,534 MainThread INFO: Total Frames:70500s
 24%|██▎       | 47/200 [00:46<02:19,  1.10it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1248.70208
Train_Epoch_Reward                    5628.97633
Running_Training_Average_Rewards      2469.85949
Explore_Time                          0.00235
Train___Time                          0.14123
Eval____Time                          0.00281
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.47549
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.73999
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.09784
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.54984
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.31844
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.98751
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.07609
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12642.34614
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.84138
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.05433
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           7.49986      0.46199   7.96186     7.03787
alpha_0                               0.97232      0.00015   0.97247     0.97218
alpha_1                               0.97231      0.00015   0.97246     0.97217
alpha_2                               0.97231      0.00015   0.97246     0.97217
alpha_3                               0.97233      0.00015   0.97248     0.97218
alpha_4                               0.97232      0.00015   0.97247     0.97218
alpha_5                               0.97234      0.00015   0.97249     0.97220
alpha_6                               0.97231      0.00015   0.97246     0.97216
alpha_7                               0.97233      0.00015   0.97247     0.97218
alpha_8                               0.97232      0.00015   0.97246     0.97217
alpha_9                               0.97232      0.00015   0.97246     0.97217
Alpha_loss                            -0.18660     0.00101   -0.18559    -0.18761
Training/policy_loss                  -2.69321     0.00008   -2.69313    -2.69329
Training/qf1_loss                     1159.09412   30.38135  1189.47546  1128.71277
Training/qf2_loss                     1159.18158   30.38312  1189.56470  1128.79846
Training/pf_norm                      0.28564      0.00900   0.29465     0.27664
Training/qf1_norm                     21.90873     1.01921   22.92794    20.88952
Training/qf2_norm                     21.40643     1.00570   22.41213    20.40073
log_std/mean                          -0.05143     0.00039   -0.05104    -0.05182
log_std/std                           0.00243      0.00001   0.00244     0.00242
log_std/max                           -0.04664     0.00036   -0.04628    -0.04700
log_std/min                           -0.05689     0.00041   -0.05649    -0.05730
log_probs/mean                        -2.71988     0.00007   -2.71980    -2.71995
log_probs/std                         0.33881      0.00810   0.34692     0.33071
log_probs/max                         -1.66379     0.00834   -1.65544    -1.67213
log_probs/min                         -4.85124     0.68509   -4.16615    -5.53633
mean/mean                             0.00050      0.00001   0.00051     0.00049
mean/std                              0.00297      0.00006   0.00303     0.00291
mean/max                              0.00389      0.00001   0.00390     0.00389
mean/min                              -0.00476     0.00014   -0.00462    -0.00490
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 47 50
freq 22
sample: [9, 3, 5, 4, 1, 7, 8, 0, 6, 2]
replay_buffer._size: [7350 7326 7327 7350 7350 7350 7350 7350 7350 7350]
train_time 0.13045930862426758
eval time 0.0024886131286621094
2023-08-25 14:04:21,772 MainThread INFO: EPOCH:47
2023-08-25 14:04:21,772 MainThread INFO: Time Consumed:0.13597321510314941s
2023-08-25 14:04:21,772 MainThread INFO: Total Frames:72000s
 24%|██▍       | 48/200 [00:46<01:47,  1.41it/s]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1240.26895
Train_Epoch_Reward                    35834.16469
Running_Training_Average_Rewards      3245.48045
Explore_Time                          0.00253
Train___Time                          0.13046
Eval____Time                          0.00249
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -56.66769
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.71918
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.06306
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.57202
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.29866
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.96242
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.05523
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12571.47145
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.82101
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.12512
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           9.08319      0.35689   9.44008     8.72631
alpha_0                               0.97174      0.00015   0.97188     0.97159
alpha_1                               0.97173      0.00015   0.97187     0.97158
alpha_2                               0.97173      0.00015   0.97187     0.97158
alpha_3                               0.97175      0.00015   0.97189     0.97160
alpha_4                               0.97174      0.00015   0.97189     0.97159
alpha_5                               0.97176      0.00015   0.97190     0.97161
alpha_6                               0.97173      0.00015   0.97187     0.97158
alpha_7                               0.97174      0.00015   0.97189     0.97160
alpha_8                               0.97173      0.00015   0.97188     0.97159
alpha_9                               0.97173      0.00015   0.97188     0.97159
Alpha_loss                            -0.19043     0.00084   -0.18959    -0.19127
Training/policy_loss                  -2.68613     0.00581   -2.68032    -2.69194
Training/qf1_loss                     1896.14160   91.90955  1988.05115  1804.23206
Training/qf2_loss                     1896.24121   91.91602  1988.15723  1804.32520
Training/pf_norm                      0.28651      0.04456   0.33107     0.24196
Training/qf1_norm                     25.44741     0.83084   26.27825    24.61656
Training/qf2_norm                     24.86482     0.79444   25.65926    24.07038
log_std/mean                          -0.05298     0.00039   -0.05260    -0.05337
log_std/std                           0.00247      0.00001   0.00248     0.00246
log_std/max                           -0.04808     0.00035   -0.04773    -0.04843
log_std/min                           -0.05858     0.00045   -0.05812    -0.05903
log_probs/mean                        -2.71257     0.00598   -2.70658    -2.71855
log_probs/std                         0.33165      0.00653   0.33818     0.32513
log_probs/max                         -1.68577     0.01672   -1.66905    -1.70250
log_probs/min                         -4.53942     0.22018   -4.31924    -4.75960
mean/mean                             0.00051      0.00001   0.00052     0.00050
mean/std                              0.00276      0.00005   0.00281     0.00271
mean/max                              0.00381      0.00005   0.00386     0.00376
mean/min                              -0.00430     0.00009   -0.00421    -0.00438
------------------------------------  -----------  --------  ----------  ----------
epoch, update_end_epoch 48 50
freq 22
sample: [2, 4, 5, 9, 6, 3, 8, 7, 0, 1]
replay_buffer._size: [7491 7437 7487 7494 7492 7447 7435 7488 7435 7470]
train_time 0.19127750396728516
eval time 0.003094911575317383
2023-08-25 14:04:22,074 MainThread INFO: EPOCH:48
2023-08-25 14:04:22,074 MainThread INFO: Time Consumed:0.19744062423706055s
2023-08-25 14:04:22,074 MainThread INFO: Total Frames:73500s
 24%|██▍       | 49/200 [00:46<01:28,  1.70it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1232.36434
Train_Epoch_Reward                    2658.17753
Running_Training_Average_Rewards      1470.71062
Explore_Time                          0.00233
Train___Time                          0.19128
Eval____Time                          0.00309
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -58.42109
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.68970
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.03787
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.57372
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.26929
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.93151
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.02624
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12522.45155
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.79100
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.17331
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.87225      0.57170    9.44395     8.30055
alpha_0                               0.97115      0.00015    0.97130     0.97101
alpha_1                               0.97115      0.00015    0.97129     0.97100
alpha_2                               0.97114      0.00015    0.97129     0.97100
alpha_3                               0.97116      0.00015    0.97131     0.97102
alpha_4                               0.97116      0.00015    0.97130     0.97101
alpha_5                               0.97117      0.00015    0.97132     0.97103
alpha_6                               0.97114      0.00015    0.97129     0.97100
alpha_7                               0.97116      0.00015    0.97130     0.97101
alpha_8                               0.97115      0.00015    0.97130     0.97100
alpha_9                               0.97115      0.00015    0.97130     0.97100
Alpha_loss                            -0.19465     0.00084    -0.19381    -0.19549
Training/policy_loss                  -2.69229     0.00566    -2.68663    -2.69795
Training/qf1_loss                     1759.26221   104.46960  1863.73181  1654.79260
Training/qf2_loss                     1759.37695   104.47729  1863.85425  1654.89966
Training/pf_norm                      0.28467      0.02714    0.31181     0.25752
Training/qf1_norm                     25.08039     1.25796    26.33835    23.82243
Training/qf2_norm                     24.43114     1.20815    25.63929    23.22299
log_std/mean                          -0.05457     0.00040    -0.05417    -0.05497
log_std/std                           0.00252      0.00001    0.00253     0.00250
log_std/max                           -0.04954     0.00034    -0.04920    -0.04989
log_std/min                           -0.06024     0.00047    -0.05977    -0.06070
log_probs/mean                        -2.71886     0.00584    -2.71301    -2.72470
log_probs/std                         0.32785      0.00611    0.33395     0.32174
log_probs/max                         -1.61535     0.02428    -1.59107    -1.63962
log_probs/min                         -4.63023     0.08697    -4.54326    -4.71720
mean/mean                             0.00054      0.00001    0.00055     0.00053
mean/std                              0.00262      0.00001    0.00264     0.00261
mean/max                              0.00376      0.00003    0.00378     0.00373
mean/min                              -0.00395     0.00006    -0.00389    -0.00401
------------------------------------  -----------  ---------  ----------  ----------
epoch, update_end_epoch 49 50
freq 22
sample: [4, 0, 2, 9, 8, 7, 3, 1, 6, 5]
replay_buffer._size: [7615 7625 7624 7583 7625 7615 7579 7616 7604 7590]
train_time 0.21384596824645996
eval time 0.0036385059356689453
2023-08-25 14:04:22,400 MainThread INFO: EPOCH:49
2023-08-25 14:04:22,400 MainThread INFO: Time Consumed:0.2201063632965088s
2023-08-25 14:04:22,400 MainThread INFO: Total Frames:75000s
 24%|██▍       | 49/200 [00:47<02:25,  1.04it/s]
Process Process-13:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 379, in eval_worker_process
    embedding_input[env_info.env_rank] = 1
KeyboardInterrupt
Process Process-21:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 380, in eval_worker_process
    embedding_input = embedding_input.unsqueeze(0).to(env_info.device)
KeyboardInterrupt
Process Process-20:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 378, in eval_worker_process
    embedding_input = torch.zeros(env_info.num_tasks)
KeyboardInterrupt
info {'reachDist': 0.1608338726856642, 'goalDist': 0.19996791944214265, 'epRew': -0.1608338726856642, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.228705461433576, 'goalDist': 0.20000002384185783, 'epRew': -0.228705461433576, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.05489581985087086, 'goalDist': 0.19998649123785328, 'epRew': -0.05489581985087086, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.14405523018614164, 'goalDist': 0.17081745392876146, 'epRew': -0.14405523018614164, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.07410525169650194, 'goalDist': 0.19974993724712498, 'epRew': -0.07410525169650194, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.16502473761613481, 'goalDist': 0.20000002384185783, 'epRew': -0.16502473761613481, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.09460700116924553, 'goalDist': 0.20000002384185783, 'epRew': -0.09460700116924553, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.30793392072326314, 'goalDist': 0.20000002384185783, 'epRew': -0.30793392072326314, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.2685023162414442, 'goalDist': 0.19621484357794006, 'epRew': -0.2685023162414442, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.2278793134062479, 'goalDist': 0.19977635873543265, 'epRew': -0.2278793134062479, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.12191784846956587, 'goalDist': 0.19264318808631664, 'epRew': -0.12191784846956587, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.11219258666447927, 'goalDist': 0.200036531280593, 'epRew': -0.11219258666447927, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.10897413433065374, 'goalDist': 0.20000002384185783, 'epRew': -0.10897413433065374, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.13319624409002742, 'goalDist': 0.20000002384185783, 'epRew': -0.13319624409002742, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.11600466240729979, 'goalDist': 0.20014342187728135, 'epRew': -0.11600466240729979, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.06881057289775473, 'goalDist': 0.20001011142149278, 'epRew': -0.06881057289775473, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.1177251168418934, 'goalDist': 0.19973998496840162, 'epRew': -0.1177251168418934, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.07340579773313426, 'goalDist': 0.19972237924193337, 'epRew': -0.07340579773313426, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.07627005237106849, 'goalDist': 0.2000931532140578, 'epRew': -0.07627005237106849, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.1346655026381936, 'goalDist': 0.20000002384185783, 'epRew': -0.1346655026381936, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.1104414472545996, 'goalDist': 0.20004872437213034, 'epRew': -0.1104414472545996, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.3251091936173641, 'goalDist': 0.20000002384185783, 'epRew': -0.3251091936173641, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.09343063771793067, 'goalDist': 0.007042657789109619, 'epRew': -0.09343063771793067, 'pickRew': None, 'success': 1.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.2355239403022764, 'goalDist': 0.20000002384185783, 'epRew': -0.2355239403022764, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.05220094138558543, 'goalDist': 0.19631366129598438, 'epRew': -0.05220094138558543, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.05759644973929692, 'goalDist': 0.20002902355704683, 'epRew': -0.05759644973929692, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.04695930183909813, 'goalDist': 0.1807782286459647, 'epRew': 57.25442744338146, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.1423644264521084, 'goalDist': 0.17820973656358396, 'epRew': -0.1423644264521084, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.24218757452122053, 'goalDist': 0.20000002384185783, 'epRew': -0.24218757452122053, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.21388900153690424, 'goalDist': 0.20000002384185783, 'epRew': -0.21388900153690424, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.1472763711264719, 'goalDist': 0.1992514174680622, 'epRew': -0.1472763711264719, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.06497205007203477, 'goalDist': 0.20003185944683904, 'epRew': -0.06497205007203477, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.0773148028910827, 'goalDist': 0.19648746954559115, 'epRew': -0.0773148028910827, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.09881136077728411, 'goalDist': 0.19999666128094407, 'epRew': -0.09881136077728411, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.17246335618349626, 'goalDist': 0.19898481001977308, 'epRew': -0.17246335618349626, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.2027123357706529, 'goalDist': 0.19632026181329243, 'epRew': -0.2027123357706529, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.17954594580188793, 'goalDist': 0.1683218889160265, 'epRew': -0.17954594580188793, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.097554254041828, 'goalDist': 0.18856136163126003, 'epRew': -0.097554254041828, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.08433177041576558, 'goalDist': 0.19992456218817245, 'epRew': -0.08433177041576558, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.07951299294106494, 'goalDist': 0.18294792847241692, 'epRew': -0.07951299294106494, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.13564013935548788, 'goalDist': 0.19993677179821545, 'epRew': -0.13564013935548788, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.12303483265234852, 'goalDist': 0.19486706577221335, 'epRew': -0.12303483265234852, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.17536336337173453, 'goalDist': 0.20000002384185783, 'epRew': -0.17536336337173453, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.1115202788341103, 'goalDist': 0.19967217287102712, 'epRew': -0.1115202788341103, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.1338541391927461, 'goalDist': 0.1999499861930225, 'epRew': -0.1338541391927461, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.19850728680165658, 'goalDist': 0.17527966959480312, 'epRew': -0.19850728680165658, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.09125525718402272, 'goalDist': 0.20004448716195683, 'epRew': -0.09125525718402272, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.21232558362200177, 'goalDist': 0.20000002384185783, 'epRew': -0.21232558362200177, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.10655935976456121, 'goalDist': 0.08647098709690315, 'epRew': -0.10655935976456121, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.1549330193595063, 'goalDist': 0.20000002384185783, 'epRew': -0.1549330193595063, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
info {'reachDist': 0.13130867093570203, 'goalDist': 0.10346253065545419, 'epRew': -0.13130867093570203, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.7 , 0.04])}
Process Process-7:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 183, in take_actions
    replay_buffer.add_sample(sample_dict, env_info.env_rank)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/replay_buffers/shared/base.py", line 66, in add_sample
    self._advance(worker_rank)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/replay_buffers/shared/base.py", line 74, in _advance
    if self._size[worker_rank] < self._max_replay_buffer_size:
KeyboardInterrupt
Process Process-17:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 383, in eval_worker_process
    eval_ob, r, done, info = env_info.env.step( act )
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_drawer_close.py", line 128, in step
    obs_dict = self._get_obs_dict()
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_drawer_close.py", line 164, in _get_obs_dict
    return dict(
KeyboardInterrupt
Process Process-12:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 383, in eval_worker_process
    eval_ob, r, done, info = env_info.env.step( act )
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_reach_push_pick_place.py", line 153, in step
    self.do_simulation([action[-1], -action[-1]])
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 120, in do_simulation
    self.sim.step()
KeyboardInterrupt
info {'reachDist': 0.32263595403195655, 'goalDist': 0.20000000149011615, 'epRew': -0.32263595403195655, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.28501906159620644, 'goalDist': 0.20000000149011604, 'epRew': -0.28501906159620644, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2461344778499648, 'goalDist': 0.2000000014901164, 'epRew': -0.2461344778499648, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.23036183003688226, 'goalDist': 0.20000000149011612, 'epRew': -0.23036183003688226, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.1438129673445257, 'goalDist': 0.20000000149011696, 'epRew': -0.1438129673445257, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2100339574925826, 'goalDist': 0.20000000149010977, 'epRew': -0.2100339574925826, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.26567393286828134, 'goalDist': 0.20000000149011612, 'epRew': -0.26567393286828134, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.20377145693112983, 'goalDist': 0.20000000149011618, 'epRew': -0.20377145693112983, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2922954112066454, 'goalDist': 0.2000000014901189, 'epRew': -0.2922954112066454, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.11080063294972356, 'goalDist': 0.20000000149013025, 'epRew': -0.11080063294972356, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2722819695406608, 'goalDist': 0.2000000014900795, 'epRew': -0.2722819695406608, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2685812612656838, 'goalDist': 0.20000000149011943, 'epRew': -0.2685812612656838, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.11435498167939367, 'goalDist': 0.20000000149011288, 'epRew': -0.11435498167939367, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.10841002923775307, 'goalDist': 0.20000000149011612, 'epRew': -0.10841002923775307, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.23398202608303972, 'goalDist': 0.200000001490116, 'epRew': -0.23398202608303972, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.19629598023169173, 'goalDist': 0.2000000014901189, 'epRew': -0.19629598023169173, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.26507906940129683, 'goalDist': 0.200000001490116, 'epRew': -0.26507906940129683, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.07472824404898848, 'goalDist': 0.20000000149011626, 'epRew': -0.07472824404898848, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.16234526263882137, 'goalDist': 0.2000000014901189, 'epRew': -0.16234526263882137, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.17435180666167355, 'goalDist': 0.20000000149011637, 'epRew': -0.17435180666167355, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.21002213327500652, 'goalDist': 0.2000000014901189, 'epRew': -0.21002213327500652, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.17814062838102349, 'goalDist': 0.20000000149012065, 'epRew': -0.17814062838102349, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.06889349883669434, 'goalDist': 0.20055115982496108, 'epRew': -0.06889349883669434, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.19688557099409568, 'goalDist': 0.2000000014901189, 'epRew': -0.19688557099409568, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.15040079953522875, 'goalDist': 0.2000000014901189, 'epRew': -0.15040079953522875, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2560152930699324, 'goalDist': 0.20000000149011096, 'epRew': -0.2560152930699324, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.22018225618474307, 'goalDist': 0.20000000149011615, 'epRew': -0.22018225618474307, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2937814774503367, 'goalDist': 0.20000000149011626, 'epRew': -0.2937814774503367, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.17735677787625662, 'goalDist': 0.20000000149011612, 'epRew': -0.17735677787625662, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.40348116642554255, 'goalDist': 0.20000000149012964, 'epRew': -0.40348116642554255, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2822320532820946, 'goalDist': 0.20000000148986752, 'epRew': -0.2822320532820946, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.11840192020455982, 'goalDist': 0.20000000149011612, 'epRew': -0.11840192020455982, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2533365549274471, 'goalDist': 0.20000000149011612, 'epRew': -0.2533365549274471, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.275222509279221, 'goalDist': 0.20000000149010969, 'epRew': -0.275222509279221, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.1317849697381638, 'goalDist': 0.2000000014901189, 'epRew': -0.1317849697381638, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.10215651434568539, 'goalDist': 0.2000000014901189, 'epRew': -0.10215651434568539, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.1777521021755639, 'goalDist': 0.2000000014901189, 'epRew': -0.1777521021755639, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.1884693201961639, 'goalDist': 0.2000000014901196, 'epRew': -0.1884693201961639, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2524264069207976, 'goalDist': 0.20000000149011965, 'epRew': -0.2524264069207976, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.27598002755656265, 'goalDist': 0.20000000149011016, 'epRew': -0.27598002755656265, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.18465027912605797, 'goalDist': 0.2000000014901189, 'epRew': -0.18465027912605797, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.14611204826594476, 'goalDist': 0.2000000014900899, 'epRew': -0.14611204826594476, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.26203382906382405, 'goalDist': 0.20000000149011624, 'epRew': -0.26203382906382405, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.1386758496321945, 'goalDist': 0.20000000148992914, 'epRew': -0.1386758496321945, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.23401400626211175, 'goalDist': 0.2000000014901165, 'epRew': -0.23401400626211175, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.2110127102301567, 'goalDist': 0.20000000149014885, 'epRew': -0.2110127102301567, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.17129997995158203, 'goalDist': 0.2000000014901189, 'epRew': -0.17129997995158203, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.21226816409235086, 'goalDist': 0.20000000149012043, 'epRew': -0.21226816409235086, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.29055801608806076, 'goalDist': 0.20000000149011612, 'epRew': -0.29055801608806076, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.19711494511840147, 'goalDist': 0.2000000014901189, 'epRew': -0.19711494511840147, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.37507496607307494, 'goalDist': 0.20000000149011654, 'epRew': -0.37507496607307494, 'pickRew': None, 'success': 0.0, 'goal': array([-0.08 ,  0.785,  0.15 ])}
info {'reachDist': 0.4537333987397539, 'goalDist': 0.02988535722272241, 'epRew': -0.4537333987397539, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3877873942266494, 'goalDist': 0.02988535722270995, 'epRew': -0.3877873942266494, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.42016358077984706, 'goalDist': 0.029885051210668354, 'epRew': -0.42016358077984706, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2596772795434348, 'goalDist': 0.029885510228952375, 'epRew': -0.2596772795434348, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2170941383404121, 'goalDist': 0.029885204216487285, 'epRew': -0.2170941383404121, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3400432581829217, 'goalDist': 0.02988551022894441, 'epRew': -0.3400432581829217, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3211790467387353, 'goalDist': 0.02988474514326893, 'epRew': -0.3211790467387353, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.32419624209701425, 'goalDist': 0.029885510228941273, 'epRew': -0.32419624209701425, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3696558741474414, 'goalDist': 0.02988505115588312, 'epRew': -0.3696558741474414, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2470095053553182, 'goalDist': 0.029885357222709727, 'epRew': -0.2470095053553182, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2815957074860781, 'goalDist': 0.029885206074761292, 'epRew': -0.2815957074860781, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.28133042854836376, 'goalDist': 0.0298855102289413, 'epRew': -0.28133042854836376, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.5242631734673191, 'goalDist': 0.029885204216481512, 'epRew': -0.5242631734673191, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.27803903218739806, 'goalDist': 0.029885204211575964, 'epRew': -0.27803903218739806, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.38427906301172604, 'goalDist': 0.0298855102289413, 'epRew': -0.38427906301172604, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3396145445973063, 'goalDist': 0.029885510228941245, 'epRew': -0.3396145445973063, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2842903074088955, 'goalDist': 0.02988551022894119, 'epRew': -0.2842903074088955, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.35300606716089616, 'goalDist': 0.029885357222709755, 'epRew': -0.35300606716089616, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.42610923196460826, 'goalDist': 0.02988551022894119, 'epRew': -0.42610923196460826, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.1988726301101278, 'goalDist': 0.029884898204024554, 'epRew': -0.1988726301101278, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.22627964296607778, 'goalDist': 0.029885357222709893, 'epRew': -0.22627964296607778, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.5067993914046774, 'goalDist': 0.029885204162086718, 'epRew': -0.5067993914046774, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3473238033372247, 'goalDist': 0.02988551022893822, 'epRew': -0.3473238033372247, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.25810672770882986, 'goalDist': 0.02988535722270992, 'epRew': -0.25810672770882986, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.29501443232608376, 'goalDist': 0.029885204216481345, 'epRew': -0.29501443232608376, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.38366269140517784, 'goalDist': 0.029884898149628844, 'epRew': -0.38366269140517784, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2843340038100891, 'goalDist': 0.02988551022895229, 'epRew': -0.2843340038100891, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2931976163838982, 'goalDist': 0.029885510228963505, 'epRew': -0.2931976163838982, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2935506086041673, 'goalDist': 0.02988521345687488, 'epRew': -0.2935506086041673, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.37314874902413314, 'goalDist': 0.029884898149629954, 'epRew': -0.37314874902413314, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.31859186181977, 'goalDist': 0.029885051210258792, 'epRew': -0.31859186181977, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3851878855480609, 'goalDist': 0.0298847451434006, 'epRew': -0.3851878855480609, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.29105330055053874, 'goalDist': 0.029885510228941356, 'epRew': -0.29105330055053874, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3160508494094148, 'goalDist': 0.029885357222710504, 'epRew': -0.3160508494094148, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.24166686999463122, 'goalDist': 0.029885357222716222, 'epRew': -0.24166686999463122, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.45385075804399494, 'goalDist': 0.029885357222709893, 'epRew': -0.45385075804399494, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.40125320400653613, 'goalDist': 0.029885357222710754, 'epRew': -0.40125320400653613, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2902864928438196, 'goalDist': 0.029884904391948117, 'epRew': -0.2902864928438196, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3213942969435173, 'goalDist': 0.029884745135935295, 'epRew': -0.3213942969435173, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.33248049361321275, 'goalDist': 0.029885357222709782, 'epRew': -0.33248049361321275, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.4158485937358544, 'goalDist': 0.029884745143400518, 'epRew': -0.4158485937358544, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.48399983965356946, 'goalDist': 0.02988551022894116, 'epRew': -0.48399983965356946, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3829233595484542, 'goalDist': 0.029885510228941217, 'epRew': -0.3829233595484542, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3225804523196114, 'goalDist': 0.029885051210253102, 'epRew': -0.3225804523196114, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2461560537444812, 'goalDist': 0.02988520421648143, 'epRew': -0.2461560537444812, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.4579958383397528, 'goalDist': 0.029885357222710032, 'epRew': -0.4579958383397528, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.347495159244562, 'goalDist': 0.029885510228952625, 'epRew': -0.347495159244562, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.29425416578415126, 'goalDist': 0.029885510228938192, 'epRew': -0.29425416578415126, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.23892444534045687, 'goalDist': 0.029885051285405625, 'epRew': -0.23892444534045687, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.2864014589056707, 'goalDist': 0.02988551022894441, 'epRew': -0.2864014589056707, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
info {'reachDist': 0.3799026755677926, 'goalDist': 0.02988443912947085, 'epRew': -0.3799026755677926, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.88, 0.1 ])}
Process Process-14:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 383, in eval_worker_process
    eval_ob, r, done, info = env_info.env.step( act )
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_reach_push_pick_place.py", line 153, in step
    self.do_simulation([action[-1], -action[-1]])
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 120, in do_simulation
    self.sim.step()
KeyboardInterrupt
Process Process-8:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 163, in take_actions
    next_ob, reward, done, info = env_info.env.step(act)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_button_press_topdown.py", line 123, in step
    self.do_simulation([action[-1], -action[-1]])
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 120, in do_simulation
    self.sim.step()
KeyboardInterrupt
Process Process-11:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 163, in take_actions
    next_ob, reward, done, info = env_info.env.step(act)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_window_close.py", line 128, in step
    self.do_simulation([action[-1], -action[-1]])
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 120, in do_simulation
    self.sim.step()
KeyboardInterrupt
info {'reachDist': 0.09088093409374656, 'goalDist': 0.19976222084185724, 'epRew': -0.09088093409374656, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.14878871973707927, 'goalDist': 0.19999997615814213, 'epRew': -0.14878871973707927, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.1448307109482797, 'goalDist': 0.19999997615814213, 'epRew': -0.1448307109482797, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.374197885197965, 'goalDist': 0.19999997615814213, 'epRew': -0.374197885197965, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.16820902382321273, 'goalDist': 0.19999997615814213, 'epRew': -0.16820902382321273, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.23155403548179238, 'goalDist': 0.19999997615814213, 'epRew': -0.23155403548179238, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.23427636839311067, 'goalDist': 0.19999997615814213, 'epRew': -0.23427636839311067, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.097534206772745, 'goalDist': 0.19999997615814213, 'epRew': -0.097534206772745, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.09307390351048589, 'goalDist': 0.20002416004733403, 'epRew': -0.09307390351048589, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.25476345269181566, 'goalDist': 0.19999997615814213, 'epRew': -0.25476345269181566, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.20550894325156827, 'goalDist': 0.19999997615814213, 'epRew': -0.20550894325156827, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.16772865495333217, 'goalDist': 0.19999997615814213, 'epRew': -0.16772865495333217, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.13177609006972515, 'goalDist': 0.19999997615814213, 'epRew': -0.13177609006972515, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.1325549205374337, 'goalDist': 0.19981645412864268, 'epRew': -0.1325549205374337, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.29706858519504625, 'goalDist': 0.19999997615814213, 'epRew': -0.29706858519504625, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.11595254110822782, 'goalDist': 0.19999997615814213, 'epRew': -0.11595254110822782, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.28924012467853805, 'goalDist': 0.19999997615814213, 'epRew': -0.28924012467853805, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.1484653521513937, 'goalDist': 0.19999997615814213, 'epRew': -0.1484653521513937, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.13318600857912744, 'goalDist': 0.1176120884433709, 'epRew': -0.13318600857912744, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.1433000863530837, 'goalDist': 0.19999997615814213, 'epRew': -0.1433000863530837, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.13826168575478756, 'goalDist': 0.19999997615814213, 'epRew': -0.13826168575478756, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.25920595130573887, 'goalDist': 0.19999997615814213, 'epRew': -0.25920595130573887, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.2796590222865079, 'goalDist': 0.19999997615814213, 'epRew': -0.2796590222865079, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.24813725088211558, 'goalDist': 0.19999997615814213, 'epRew': -0.24813725088211558, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.11446206395849072, 'goalDist': 0.19999997615814213, 'epRew': -0.11446206395849072, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.19672621412541194, 'goalDist': 0.19999997615814213, 'epRew': -0.19672621412541194, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.21985712310468178, 'goalDist': 0.19999997615814213, 'epRew': -0.21985712310468178, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.11365691777959451, 'goalDist': 0.19999997615814213, 'epRew': -0.11365691777959451, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.2349694677929588, 'goalDist': 0.19999997615814213, 'epRew': -0.2349694677929588, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.3429946594375294, 'goalDist': 0.19999997615814213, 'epRew': -0.3429946594375294, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.21043580320983218, 'goalDist': 0.19999997615814213, 'epRew': -0.21043580320983218, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.15372552360095143, 'goalDist': 0.19999997615814213, 'epRew': -0.15372552360095143, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.08654254483044131, 'goalDist': 0.19260120593001218, 'epRew': -0.08654254483044131, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.1714296938107771, 'goalDist': 0.19999997615814213, 'epRew': -0.1714296938107771, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.11811940405590707, 'goalDist': 0.19999997615814213, 'epRew': -0.11811940405590707, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.24965061802797275, 'goalDist': 0.19999997615814213, 'epRew': -0.24965061802797275, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.21584002977049047, 'goalDist': 0.19999997615814213, 'epRew': -0.21584002977049047, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.162629579278944, 'goalDist': 0.19999997615814213, 'epRew': -0.162629579278944, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.26975678664321023, 'goalDist': 0.19999997615814213, 'epRew': -0.26975678664321023, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.22445520107775135, 'goalDist': 0.19999997615814213, 'epRew': -0.22445520107775135, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.2556483422583756, 'goalDist': 0.19999997615814213, 'epRew': -0.2556483422583756, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.19100961225158902, 'goalDist': 0.19999997615814213, 'epRew': -0.19100961225158902, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.15511367162566578, 'goalDist': 0.19999997615814213, 'epRew': -0.15511367162566578, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.13921885997319414, 'goalDist': 0.19999997615814213, 'epRew': -0.13921885997319414, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.3098680013847935, 'goalDist': 0.19999997615814213, 'epRew': -0.3098680013847935, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.3063655358103207, 'goalDist': 0.19999997615814213, 'epRew': -0.3063655358103207, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.34146995196970215, 'goalDist': 0.19999997615814213, 'epRew': -0.34146995196970215, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.15565618219779387, 'goalDist': 0.19999997615814213, 'epRew': -0.15565618219779387, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.19130514594608297, 'goalDist': 0.19999997615814213, 'epRew': -0.19130514594608297, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.2764921974825025, 'goalDist': 0.19999997615814213, 'epRew': -0.2764921974825025, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
info {'reachDist': 0.14718107463189847, 'goalDist': 0.19999997615814213, 'epRew': -0.14718107463189847, 'pickRew': None, 'success': 0.0, 'goal': array([0.  , 0.55, 0.04])}
Process Process-6:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 163, in take_actions
    next_ob, reward, done, info = env_info.env.step(act)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_drawer_open.py", line 125, in step
    self.do_simulation([action[-1], -action[-1]])
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 120, in do_simulation
    self.sim.step()
KeyboardInterrupt
info {'reachDist': 0.23979890284238262, 'goalDist': 0.4123105611161413, 'epRew': -0.23979890284238262, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.30868579850016603, 'goalDist': 0.4123105611161413, 'epRew': -0.30868579850016603, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2774620345701256, 'goalDist': 0.4123105611161413, 'epRew': -0.2774620345701256, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.3491306062732345, 'goalDist': 0.4123105611161413, 'epRew': -0.3491306062732345, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.14975913730360338, 'goalDist': 0.4123105611161413, 'epRew': -0.14975913730360338, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.1941008984617834, 'goalDist': 0.4123105611161413, 'epRew': -0.1941008984617834, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.3322014791077562, 'goalDist': 0.4123105611161413, 'epRew': -0.3322014791077562, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.30536816818301576, 'goalDist': 0.4123105611161413, 'epRew': -0.30536816818301576, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.34078354113412, 'goalDist': 0.4123105611161413, 'epRew': -0.34078354113412, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.3136564596967147, 'goalDist': 0.4123105611161413, 'epRew': -0.3136564596967147, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.23125042181720948, 'goalDist': 0.4123105611161413, 'epRew': -0.23125042181720948, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.43120127517330387, 'goalDist': 0.4123105611161413, 'epRew': -0.43120127517330387, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2108398226176713, 'goalDist': 0.4123105611161413, 'epRew': -0.2108398226176713, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.3494773759439346, 'goalDist': 0.4123105611161413, 'epRew': -0.3494773759439346, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.4562297621908497, 'goalDist': 0.4123105611161413, 'epRew': -0.4562297621908497, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.324260907974054, 'goalDist': 0.4123105611161413, 'epRew': -0.324260907974054, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2541237986126732, 'goalDist': 0.4123105611161413, 'epRew': -0.2541237986126732, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.3034999090481252, 'goalDist': 0.4123105611161413, 'epRew': -0.3034999090481252, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.296067667999019, 'goalDist': 0.4123105611161413, 'epRew': -0.296067667999019, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.30877985510905503, 'goalDist': 0.4123105611161413, 'epRew': -0.30877985510905503, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.3754556515656919, 'goalDist': 0.4123105611161413, 'epRew': -0.3754556515656919, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.28121195835510365, 'goalDist': 0.4123105611161413, 'epRew': -0.28121195835510365, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.20812309006437116, 'goalDist': 0.4123105611161413, 'epRew': -0.20812309006437116, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2695919381724792, 'goalDist': 0.4123105611161413, 'epRew': -0.2695919381724792, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2255846862287128, 'goalDist': 0.4123105611161413, 'epRew': -0.2255846862287128, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2570832023467479, 'goalDist': 0.4123105611161413, 'epRew': -0.2570832023467479, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.4280997932071518, 'goalDist': 0.4123105611161413, 'epRew': -0.4280997932071518, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2743967114616353, 'goalDist': 0.4123105611161413, 'epRew': -0.2743967114616353, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.17398865175552874, 'goalDist': 0.4123105611161413, 'epRew': -0.17398865175552874, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.27786681459295515, 'goalDist': 0.4123105611161413, 'epRew': -0.27786681459295515, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.38281316643507063, 'goalDist': 0.4123105611161413, 'epRew': -0.38281316643507063, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2481714729745051, 'goalDist': 0.4123105611161413, 'epRew': -0.2481714729745051, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.25357756398876063, 'goalDist': 0.4123105611161413, 'epRew': -0.25357756398876063, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.279645963929434, 'goalDist': 0.4123105611161413, 'epRew': -0.279645963929434, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.3294225668112066, 'goalDist': 0.4123105611161413, 'epRew': -0.3294225668112066, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.29269687185394105, 'goalDist': 0.4123105611161413, 'epRew': -0.29269687185394105, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.3624227288350447, 'goalDist': 0.4123105611161413, 'epRew': -0.3624227288350447, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.32785009939848714, 'goalDist': 0.4123105611161413, 'epRew': -0.32785009939848714, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.44805467231598717, 'goalDist': 0.4123105611161413, 'epRew': -0.44805467231598717, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.26874524255212207, 'goalDist': 0.4123105611161413, 'epRew': -0.26874524255212207, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.283410981021656, 'goalDist': 0.4123105611161413, 'epRew': -0.283410981021656, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.31896015006731526, 'goalDist': 0.4123105611161413, 'epRew': -0.31896015006731526, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.12491316065223983, 'goalDist': 0.4123105611161413, 'epRew': -0.12491316065223983, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2784191441501726, 'goalDist': 0.4123105611161413, 'epRew': -0.2784191441501726, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.32250394647060554, 'goalDist': 0.4123105611161413, 'epRew': -0.32250394647060554, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.4187150385597512, 'goalDist': 0.4123105611161413, 'epRew': -0.4187150385597512, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.29295162717828577, 'goalDist': 0.4123105611161413, 'epRew': -0.29295162717828577, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.2596208389863674, 'goalDist': 0.4123105611161413, 'epRew': -0.2596208389863674, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.34055525080586013, 'goalDist': 0.4123105611161413, 'epRew': -0.34055525080586013, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.12094972889215229, 'goalDist': 0.4123105611161413, 'epRew': -0.12094972889215229, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
info {'reachDist': 0.33123054061475365, 'goalDist': 0.4123105611161413, 'epRew': -0.33123054061475365, 'pickRew': None, 'success': 0.0, 'goal': array([-0.2 ,  0.7 ,  0.15])}
Process Process-5:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 149, in take_actions
    out = pf.explore(torch.Tensor( ob ).to(env_info.device).unsqueeze(0),
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/continuous_policy.py", line 111, in explore
    action = dis.rsample(return_pretanh_value=False)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/distribution.py", line 76, in rsample
    return torch.tanh(z)
KeyboardInterrupt
Process Process-19:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 381, in eval_worker_process
    act = pf.eval_act( torch.Tensor( eval_ob ).to(env_info.device).unsqueeze(0), mask_this_task)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/continuous_policy.py", line 81, in eval_act
    mean, _, _ = self.forward(x, neuron_masks)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/continuous_policy.py", line 70, in forward
    x = super().forward(x, neuron_masks,enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/networks/nets.py", line 91, in forward
    mask_out = self.activation_func(layer(mask_out)) * neuron_masks[idx]
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/torch/nn/functional.py", line 1136, in relu
    result = torch.relu(input)
KeyboardInterrupt
info {'reachDist': 0.10634456178500688, 'pickRew': 0, 'epRew': -0.26364220421368695, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.15917581334746883, 'pickRew': 0, 'epRew': -0.09749703474208653, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.2244155522778036, 'pickRew': 0, 'epRew': -0.2244155522778036, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.13432014985369628, 'pickRew': 0, 'epRew': -0.12393724087571115, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.24054209061335033, 'pickRew': 0, 'epRew': -0.12121980061075951, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.13536499105327193, 'pickRew': 0, 'epRew': -0.13536499105327193, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.16881706142350186, 'pickRew': 0, 'epRew': -0.0810635160588957, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.2093061434077573, 'pickRew': 0, 'epRew': -0.16298618426610312, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.29702979683672054, 'pickRew': 0, 'epRew': -0.2451124493880959, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.09106443395360098, 'pickRew': 0, 'epRew': -0.09106443395360098, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.07970896691300393, 'pickRew': 0, 'epRew': -0.07970896691300393, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.3150845383708898, 'pickRew': 0, 'epRew': -0.26036656491713905, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.03951532333421202, 'pickRew': 4.551209201547845, 'epRew': 4.527978940049876, 'goalDist': 0.21225037342946704, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.19571461256664618, 'pickRew': 0, 'epRew': -0.10269509190499793, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.22469496730820535, 'pickRew': 0, 'epRew': -0.22469496730820535, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.17442993018463632, 'pickRew': 0, 'epRew': -0.297619053099013, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.19826604986737834, 'pickRew': 0, 'epRew': -0.13737083216098755, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.17217221188203488, 'pickRew': 0, 'epRew': -0.20166498299992983, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.19612561193688222, 'pickRew': 0, 'epRew': -0.08848751887193552, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.15607911202715055, 'pickRew': 0, 'epRew': -0.15607911202715055, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.10868792206551714, 'pickRew': 0, 'epRew': -0.23744795929128906, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.17458821374637085, 'pickRew': 0, 'epRew': -0.1324391734254583, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.17635322700181075, 'pickRew': 0, 'epRew': -0.17635322700181075, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.1528968263635014, 'pickRew': 0, 'epRew': -0.10785425070215603, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.2887484466461082, 'pickRew': 0, 'epRew': -0.2887484466461082, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.14594858606493613, 'pickRew': 0, 'epRew': -0.17606099171532075, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.11121382845591102, 'pickRew': 0, 'epRew': -0.22480213269544336, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.14473835595148643, 'pickRew': 0, 'epRew': -0.14566711595406875, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.22924938261463498, 'pickRew': 0, 'epRew': -0.22924938261463498, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.06956093048926215, 'pickRew': 0, 'epRew': -0.19626869440531425, 'goalDist': 0.22821907394426064, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.2232002076590241, 'pickRew': 0, 'epRew': -0.14615116658425234, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.12330184327029495, 'pickRew': 0, 'epRew': -0.18843141441497863, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.221556302880101, 'pickRew': 0, 'epRew': -0.221556302880101, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.18057158853884153, 'pickRew': 0, 'epRew': -0.1499738612924328, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.23687342948566126, 'pickRew': 0, 'epRew': -0.1453304063331209, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.16272868556545494, 'pickRew': 0, 'epRew': -0.07182300851658055, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.3260248480263178, 'pickRew': 0, 'epRew': -0.1979759004408201, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.18784536721768622, 'pickRew': 0, 'epRew': -0.06504932278877175, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.1962971093263985, 'pickRew': 0, 'epRew': -0.10743429355884672, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.12905863834584602, 'pickRew': 0, 'epRew': -0.1833583759609363, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.289133317632823, 'pickRew': 0, 'epRew': -0.23059551490832328, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.1650356339383487, 'pickRew': 0, 'epRew': -0.18786384845161147, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.21054225707679747, 'pickRew': 0, 'epRew': -0.1799803659017848, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.2570280353586187, 'pickRew': 0, 'epRew': -0.21124082909518233, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.16214961844351267, 'pickRew': 0, 'epRew': -0.08344789911312347, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.20110394337639942, 'pickRew': 0, 'epRew': -0.1262291438156664, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.0464533078392456, 'pickRew': 0, 'epRew': -0.0464533078392456, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.2217898781008531, 'pickRew': 0, 'epRew': -0.2217898781008531, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.2123556165067072, 'pickRew': 0, 'epRew': -0.2123556165067072, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.08241879334499913, 'pickRew': 0, 'epRew': -0.08241879334499913, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
info {'reachDist': 0.1320798668123156, 'pickRew': 0, 'epRew': -0.14340256540503796, 'goalDist': 0.22825429145451645, 'success': 0.0, 'goal': array([-0.3 ,  0.6 ,  0.05])}
Process Process-9:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 163, in take_actions
    next_ob, reward, done, info = env_info.env.step(act)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_peg_insertion_side.py", line 133, in step
    reward , reachRew, reachDist, pickRew, placeRew , placingDist = self.compute_reward(action, obs_dict, mode = self.rewMode)
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_peg_insertion_side.py", line 341, in compute_reward
    reachRew, reachDist = reachReward()
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_peg_insertion_side.py", line 273, in reachReward
    zRew = np.linalg.norm(fingerCOM[-1] - self.init_fingerCOM[-1])
  File "<__array_function__ internals>", line 200, in norm
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/linalg/linalg.py", line 2342, in norm
    @array_function_dispatch(_norm_dispatcher)
KeyboardInterrupt
info {'reachDist': 0.13841408698976362, 'goalDist': 0.20000000149010966, 'epRew': -0.13841408698976362, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.24232433368745546, 'goalDist': 0.20000000149010966, 'epRew': -0.24232433368745546, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.14331731993013416, 'goalDist': 0.20000000149010966, 'epRew': -0.14331731993013416, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.29827139339193215, 'goalDist': 0.20000000149010966, 'epRew': -0.29827139339193215, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.18236471278529526, 'goalDist': 0.20000000149010966, 'epRew': -0.18236471278529526, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.22650012662022045, 'goalDist': 0.20000000149010966, 'epRew': -0.22650012662022045, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.1627149297693536, 'goalDist': 0.20000000149010966, 'epRew': -0.1627149297693536, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.14473359085364773, 'goalDist': 0.20000000149010966, 'epRew': -0.14473359085364773, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.21393491770852982, 'goalDist': 0.20000000149010966, 'epRew': -0.21393491770852982, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.3555264230407041, 'goalDist': 0.20000000149010966, 'epRew': -0.3555264230407041, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.21819307787611805, 'goalDist': 0.20000000149010966, 'epRew': -0.21819307787611805, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.14052210525987513, 'goalDist': 0.20000000149010966, 'epRew': -0.14052210525987513, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.14517941758441652, 'goalDist': 0.20000000149010966, 'epRew': -0.14517941758441652, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.19560926697162234, 'goalDist': 0.20000000149010966, 'epRew': -0.19560926697162234, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.1856439416019531, 'goalDist': 0.20000000149010966, 'epRew': -0.1856439416019531, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.1539179393248874, 'goalDist': 0.20000000149010966, 'epRew': -0.1539179393248874, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.1765990120013007, 'goalDist': 0.20000000149010966, 'epRew': -0.1765990120013007, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.11486244805607955, 'goalDist': 0.20000000149010966, 'epRew': -0.11486244805607955, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.1724745332035503, 'goalDist': 0.20000000149010966, 'epRew': -0.1724745332035503, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.0692297890253982, 'goalDist': 0.19992250378420776, 'epRew': -0.0692297890253982, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.2328838842282938, 'goalDist': 0.20000000149010966, 'epRew': -0.2328838842282938, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.2215095854530418, 'goalDist': 0.20000000149010966, 'epRew': -0.2215095854530418, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.277182562994836, 'goalDist': 0.20000000149010966, 'epRew': -0.277182562994836, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.23032617879184913, 'goalDist': 0.20000000149010966, 'epRew': -0.23032617879184913, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.1697993405212569, 'goalDist': 0.20000000149010966, 'epRew': -0.1697993405212569, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.2537681345436677, 'goalDist': 0.20000000149010966, 'epRew': -0.2537681345436677, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.2565339393178058, 'goalDist': 0.20000000149010966, 'epRew': -0.2565339393178058, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.30847650492445644, 'goalDist': 0.20000000149010966, 'epRew': -0.30847650492445644, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.16748625935619368, 'goalDist': 0.20000000149010966, 'epRew': -0.16748625935619368, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.2676458105143901, 'goalDist': 0.20000000149010966, 'epRew': -0.2676458105143901, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.10756269819353259, 'goalDist': 0.20000000149010966, 'epRew': -0.10756269819353259, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.17499702669622458, 'goalDist': 0.20000000149010966, 'epRew': -0.17499702669622458, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.2848372725406243, 'goalDist': 0.20000000149010966, 'epRew': -0.2848372725406243, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.08320611303098752, 'goalDist': 0.20000000149010966, 'epRew': -0.08320611303098752, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.14428139285179062, 'goalDist': 0.20000000149010966, 'epRew': -0.14428139285179062, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.14639083315830392, 'goalDist': 0.20000000149010966, 'epRew': -0.14639083315830392, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.2126497229738945, 'goalDist': 0.20000000149010966, 'epRew': -0.2126497229738945, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.1694366416613029, 'goalDist': 0.20000000149010966, 'epRew': -0.1694366416613029, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.22615418819708996, 'goalDist': 0.20000000149010966, 'epRew': -0.22615418819708996, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.25519833398337993, 'goalDist': 0.20000000149010966, 'epRew': -0.25519833398337993, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.24062473702327294, 'goalDist': 0.20000000149010966, 'epRew': -0.24062473702327294, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.12694687447872705, 'goalDist': 0.20000000149010966, 'epRew': -0.12694687447872705, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.317068134302734, 'goalDist': 0.20000000149010966, 'epRew': -0.317068134302734, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.14346403023838708, 'goalDist': 0.20000000149010966, 'epRew': -0.14346403023838708, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.17029687718635766, 'goalDist': 0.20000000149010966, 'epRew': -0.17029687718635766, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.18175684927934543, 'goalDist': 0.20000000149010966, 'epRew': -0.18175684927934543, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.21397811587113386, 'goalDist': 0.20000000149010966, 'epRew': -0.21397811587113386, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.18303444235052116, 'goalDist': 0.20000000149010966, 'epRew': -0.18303444235052116, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.27036411951832096, 'goalDist': 0.20000000149010966, 'epRew': -0.27036411951832096, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.15371235239302222, 'goalDist': 0.20000000149010966, 'epRew': -0.15371235239302222, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.05070159344003249, 'goalDist': 0.1998583633983744, 'epRew': -0.05070159344003249, 'pickRew': None, 'success': 0.0, 'goal': array([0.08 , 0.785, 0.15 ])}
info {'reachDist': 0.23641373022352766, 'pickRew': 0, 'epRew': -0.2378978250608446, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1473500967439008, 'pickRew': 0, 'epRew': -0.29106096325311775, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.25729856314909183, 'pickRew': 0, 'epRew': -0.20668456008089167, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.21731045580457506, 'pickRew': 0, 'epRew': -0.290293758099745, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.26969815357179133, 'pickRew': 0, 'epRew': -0.24279207262646044, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1010532896282683, 'pickRew': 0, 'epRew': -0.1010532896282683, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.05984485819257199, 'pickRew': 0, 'epRew': -0.05984485819257199, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.25336420392246456, 'pickRew': 0, 'epRew': -0.18714831853353744, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.12519807801075178, 'pickRew': 0, 'epRew': -0.2245375236708752, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.2057151834940122, 'pickRew': 0, 'epRew': -0.09037499603620266, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1386125537408447, 'pickRew': 0, 'epRew': -0.4577276818606022, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.09800942377160644, 'pickRew': 0, 'epRew': -0.36513984507505703, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1303403000343501, 'pickRew': 0, 'epRew': -0.1910098160131871, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.20436643432733873, 'pickRew': 0, 'epRew': -0.09589093862166446, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.0882513074035802, 'pickRew': 0, 'epRew': -0.36308615576012876, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.30502567284381155, 'pickRew': 0, 'epRew': -0.31746800815576626, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.238078764871464, 'pickRew': 0, 'epRew': -0.15616771393507062, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.12417248710583727, 'pickRew': 0, 'epRew': -0.12417248710583727, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.2139528645441683, 'pickRew': 0, 'epRew': -0.2416398004492248, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.06811346933168651, 'pickRew': 2.883730673584901, 'epRew': 2.578586239324638, 'goalDist': 0.28169812363260316, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.12131612021957373, 'pickRew': 0, 'epRew': -0.3975152168250561, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.09928830972347696, 'pickRew': 0, 'epRew': -0.09928830972347696, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.29991884127058793, 'pickRew': 0, 'epRew': -0.3088010863840919, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1556091452369889, 'pickRew': 0, 'epRew': -0.1556091452369889, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.2242606149605898, 'pickRew': 0, 'epRew': -0.2242606149605898, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.18958169103918987, 'pickRew': 0, 'epRew': -0.10210406048855467, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.06791662468052792, 'pickRew': 3.0737098966690786, 'epRew': 3.0057932719885505, 'goalDist': 0.2877985530043557, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.09520902808335589, 'pickRew': 0, 'epRew': -0.3118371486299126, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.2589357248334635, 'pickRew': 0, 'epRew': -0.20888931553736795, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1296296520252118, 'pickRew': 0, 'epRew': -0.21483365675069316, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.0805875251756327, 'pickRew': 0, 'epRew': -0.3038957983388839, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.13976029954422917, 'pickRew': 0, 'epRew': -0.2401964584088031, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.06524245307298256, 'pickRew': 0, 'epRew': -0.06524245307298256, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.15812934191435454, 'pickRew': 0, 'epRew': -0.49454426957271835, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1613852787805679, 'pickRew': 0, 'epRew': -0.4005071192851512, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.14990585300435846, 'pickRew': 0, 'epRew': -0.4813032286948423, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1544752471605305, 'pickRew': 0, 'epRew': -0.24210897704556322, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.27461199792786073, 'pickRew': 0, 'epRew': -0.2525272150094264, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.2295866744679606, 'pickRew': 0, 'epRew': -0.15349562510963638, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.0648607648611528, 'pickRew': 0, 'epRew': -0.0648607648611528, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.09227032497247731, 'pickRew': 0, 'epRew': -0.27274759585348934, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.25440327060840895, 'pickRew': 0, 'epRew': -0.20419466875040268, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1961521498524299, 'pickRew': 0, 'epRew': -0.11730266414788072, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.12679230004624378, 'pickRew': 0, 'epRew': -0.12679230004624378, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1715788178090234, 'pickRew': 0, 'epRew': -0.1715788178090234, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.170766213239744, 'pickRew': 0, 'epRew': -0.35667972121614844, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.17697410418061418, 'pickRew': 0, 'epRew': -0.17697410418061418, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.14592145569572612, 'pickRew': 0, 'epRew': -0.15431987354994864, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.04861350956623601, 'pickRew': 0, 'epRew': -0.04861350956623601, 'goalDist': 0.29001880388732343, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.1969786297855985, 'pickRew': 0, 'epRew': -0.07902229660538465, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
info {'reachDist': 0.09881237382517112, 'pickRew': 0, 'epRew': -0.31069596776770925, 'goalDist': 0.29021778227915884, 'success': 0.0, 'goal': array([0.1, 0.8, 0.2])}
Process Process-10:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 149, in take_actions
    out = pf.explore(torch.Tensor( ob ).to(env_info.device).unsqueeze(0),
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/continuous_policy.py", line 90, in explore
    ent = dis.entropy().sum(-1, keepdim=True)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/distribution.py", line 79, in entropy
    return self.normal.entropy()
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/torch/distributions/normal.py", line 89, in entropy
    return 0.5 + 0.5 * math.log(2 * math.pi) + torch.log(self.scale)
KeyboardInterrupt
Process Process-4:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 149, in take_actions
    out = pf.explore(torch.Tensor( ob ).to(env_info.device).unsqueeze(0),
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/continuous_policy.py", line 111, in explore
    action = dis.rsample(return_pretanh_value=False)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/distribution.py", line 67, in rsample
    Normal(
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/torch/distributions/normal.py", line 63, in sample
    return torch.normal(self.loc.expand(shape), self.scale.expand(shape))
KeyboardInterrupt
info {'reachDist': 0.15708005782332277, 'pickRew': None, 'epRew': 231.0101231896138, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.39162495620266446, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.292236744497976, 'pickRew': None, 'epRew': 11.245041710784262, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.3643752570170798, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.13031814748455067, 'pickRew': None, 'epRew': 355.9658879095521, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.15364016316103896, 'pickRew': None, 'epRew': 244.01637431806046, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.2567409806067999, 'pickRew': None, 'epRew': 47.91721329573974, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.22983884358375795, 'pickRew': None, 'epRew': 78.52675911229906, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.26294787908110395, 'pickRew': None, 'epRew': 41.332079140976234, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.17692248005690725, 'pickRew': None, 'epRew': 170.0748216839802, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.33705985261450633, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.372623934657818, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.20484178120813892, 'pickRew': None, 'epRew': 113.5000053846635, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.24953527028726794, 'pickRew': None, 'epRew': 55.726864561862975, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.24511437513297002, 'pickRew': None, 'epRew': 60.630707454065906, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.3853365993866493, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.20995640471511898, 'pickRew': None, 'epRew': 105.50738903993341, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.35901070521216993, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.3194352016949435, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.22068319203106865, 'pickRew': None, 'epRew': 90.27569249155871, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.33187352250590746, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.24757497381961244, 'pickRew': None, 'epRew': 57.88939239817515, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.2306884932187448, 'pickRew': None, 'epRew': 77.48220272682434, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.24759087195949597, 'pickRew': None, 'epRew': 57.87178034467839, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.3485250027845154, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.32530364523952104, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.2739053937769762, 'pickRew': None, 'epRew': 29.932696888831728, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.18823135601247373, 'pickRew': None, 'epRew': 143.97838472881674, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.2768195747678689, 'pickRew': None, 'epRew': 26.93670541015668, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.219584434416007, 'pickRew': None, 'epRew': 91.75473124588571, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.24233102372410062, 'pickRew': None, 'epRew': 63.77133249479103, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.12407740824760656, 'pickRew': None, 'epRew': 393.69309860528534, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.3598591258819479, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.292494432866718, 'pickRew': None, 'epRew': 10.984430483296546, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.32104939148899886, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.2077503220411942, 'pickRew': None, 'epRew': 108.88894664112092, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.28895608279008644, 'pickRew': None, 'epRew': 14.566758541307612, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.25679880196568056, 'pickRew': None, 'epRew': 47.85532445857323, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.31627864810247186, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.24694961913054841, 'pickRew': None, 'epRew': 58.58315554901958, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.2519568565211625, 'pickRew': None, 'epRew': 53.0793337199171, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.18776925279353412, 'pickRew': None, 'epRew': 144.94742647498035, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.2033586971285893, 'pickRew': None, 'epRew': 115.92269422470241, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.2585228675840839, 'pickRew': None, 'epRew': 46.01498004566333, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.2255764075150658, 'pickRew': None, 'epRew': 83.87737291189005, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.08865755020301595, 'pickRew': None, 'epRew': 670.6710671253055, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.24795969494460307, 'pickRew': None, 'epRew': 57.46354212008068, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.11128484197343022, 'pickRew': None, 'epRew': 481.8441256363639, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.35905259917962573, 'pickRew': None, 'epRew': 0, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.19742408265111455, 'pickRew': None, 'epRew': 126.1522643090311, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
info {'reachDist': 0.29341093630306664, 'pickRew': None, 'epRew': 10.05786157832159, 'goalDist': None, 'success': 0.0, 'goal': array([-0.1,  0.8,  0.2])}
Process Process-2:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 149, in take_actions
    out = pf.explore(torch.Tensor( ob ).to(env_info.device).unsqueeze(0),
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/continuous_policy.py", line 90, in explore
    ent = dis.entropy().sum(-1, keepdim=True)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/distribution.py", line 79, in entropy
    return self.normal.entropy()
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/torch/distributions/normal.py", line 89, in entropy
    return 0.5 + 0.5 * math.log(2 * math.pi) + torch.log(self.scale)
KeyboardInterrupt
Process Process-15:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 383, in eval_worker_process
    eval_ob, r, done, info = env_info.env.step( act )
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 303, in step
    action = self.action(action)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/env/continuous_wrapper.py", line 144, in action
    return np.clip(scaled_action, lb, ub)
  File "<__array_function__ internals>", line 200, in clip
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 2180, in clip
    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 57, in _wrapfunc
    return bound(*args, **kwds)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/_methods.py", line 134, in _clip
    if not _clip_dep_is_byte_swapped(a) and not _clip_dep_is_byte_swapped(out):
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/_methods.py", line 104, in _clip_dep_is_byte_swapped
    if isinstance(a, mu.ndarray):
KeyboardInterrupt
Process Process-18:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 383, in eval_worker_process
    eval_ob, r, done, info = env_info.env.step( act )
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 303, in step
    action = self.action(action)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/env/continuous_wrapper.py", line 144, in action
    return np.clip(scaled_action, lb, ub)
  File "<__array_function__ internals>", line 200, in clip
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 2180, in clip
    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 57, in _wrapfunc
    return bound(*args, **kwds)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/_methods.py", line 161, in _clip
    return _clip_dep_invoke_with_casting(
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/_methods.py", line 108, in _clip_dep_invoke_with_casting
    def _clip_dep_invoke_with_casting(ufunc, *args, out=None, casting=None, **kwargs):
KeyboardInterrupt
info {'reachDist': 0.1646212829263575, 'pickRew': None, 'epRew': -0.1646212829263575, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.07502866165257792, 'pickRew': None, 'epRew': -0.07502866165257792, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.15265102872993208, 'pickRew': None, 'epRew': -0.15265102872993208, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.08975020082561037, 'pickRew': None, 'epRew': -0.08975020082561037, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.22781792984753094, 'pickRew': None, 'epRew': -0.22781792984753094, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.22850490982536298, 'pickRew': None, 'epRew': -0.22850490982536298, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.2798113409390387, 'pickRew': None, 'epRew': -0.2798113409390387, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.21771189920058134, 'pickRew': None, 'epRew': -0.21771189920058134, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.3067652352999387, 'pickRew': None, 'epRew': -0.3067652352999387, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.1755213890312811, 'pickRew': None, 'epRew': -0.1755213890312811, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.24078274272371622, 'pickRew': None, 'epRew': -0.24078274272371622, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.13791131949785695, 'pickRew': None, 'epRew': -0.13791131949785695, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.05255913923062116, 'pickRew': None, 'epRew': -0.05255913923062116, 'goalDist': 0.23470981208797057, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.24105027865603734, 'pickRew': None, 'epRew': -0.24105027865603734, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.16046222825005285, 'pickRew': None, 'epRew': -0.16046222825005285, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.20953702736478205, 'pickRew': None, 'epRew': -0.20953702736478205, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.26370686250462655, 'pickRew': None, 'epRew': -0.26370686250462655, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.2413360448750925, 'pickRew': None, 'epRew': -0.2413360448750925, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.207443765735638, 'pickRew': None, 'epRew': -0.207443765735638, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.08951357367086185, 'pickRew': None, 'epRew': -0.08951357367086185, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.17705342512884564, 'pickRew': None, 'epRew': -0.17705342512884564, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.2261355841734089, 'pickRew': None, 'epRew': -0.2261355841734089, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.1693340460985525, 'pickRew': None, 'epRew': -0.1693340460985525, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.3382694927147983, 'pickRew': None, 'epRew': -0.3382694927147983, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.2641670628564218, 'pickRew': None, 'epRew': -0.2641670628564218, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.2562467807152545, 'pickRew': None, 'epRew': -0.2562467807152545, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.10324579383056294, 'pickRew': None, 'epRew': -0.10324579383056294, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.09234552285309314, 'pickRew': None, 'epRew': -0.09234552285309314, 'goalDist': 0.2236269478949933, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.18569977223966727, 'pickRew': None, 'epRew': -0.18569977223966727, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.18763173513502918, 'pickRew': None, 'epRew': -0.18763173513502918, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.15220692910641886, 'pickRew': None, 'epRew': -0.15220692910641886, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.16380940902091462, 'pickRew': None, 'epRew': -0.16380940902091462, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.1508647664585146, 'pickRew': None, 'epRew': -0.1508647664585146, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.21741703690238293, 'pickRew': None, 'epRew': -0.21741703690238293, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.16089033349786608, 'pickRew': None, 'epRew': -0.16089033349786608, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.160983967814801, 'pickRew': None, 'epRew': -0.160983967814801, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.1907061298343516, 'pickRew': None, 'epRew': -0.1907061298343516, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.23176773761892144, 'pickRew': None, 'epRew': -0.23176773761892144, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.15461925793767606, 'pickRew': None, 'epRew': -0.15461925793767606, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.19574328512011227, 'pickRew': None, 'epRew': -0.19574328512011227, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.23546194478858176, 'pickRew': None, 'epRew': -0.23546194478858176, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.266758819469876, 'pickRew': None, 'epRew': -0.266758819469876, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.2299300975509449, 'pickRew': None, 'epRew': -0.2299300975509449, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.25086529684372094, 'pickRew': None, 'epRew': -0.25086529684372094, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.1996449127649451, 'pickRew': None, 'epRew': -0.1996449127649451, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.31173663437710253, 'pickRew': None, 'epRew': -0.31173663437710253, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.22311760088378663, 'pickRew': None, 'epRew': -0.22311760088378663, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.2284712249275299, 'pickRew': None, 'epRew': -0.2284712249275299, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.13989663847138195, 'pickRew': None, 'epRew': -0.13989663847138195, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.20301399813819676, 'pickRew': None, 'epRew': -0.20301399813819676, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
info {'reachDist': 0.1275389893095714, 'pickRew': None, 'epRew': -0.1275389893095714, 'goalDist': 0.22360679774997905, 'success': 0.0, 'goal': array([0.1 , 0.8 , 0.02])}
Process Process-3:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 264, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 163, in take_actions
    next_ob, reward, done, info = env_info.env.step(act)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_reach_push_pick_place.py", line 148, in step
    self.set_xyz_action(action[:3])
  File "/scratch/qianxi/t3s/metaworld/metaworld/envs/mujoco/sawyer_xyz/base.py", line 104, in set_xyz_action
    new_mocap_pos[0, :] = np.clip(
  File "<__array_function__ internals>", line 200, in clip
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 2180, in clip
    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 57, in _wrapfunc
    return bound(*args, **kwds)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/_methods.py", line 136, in _clip
    if _clip_dep_is_scalar_nan(min):
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/numpy/core/_methods.py", line 95, in _clip_dep_is_scalar_nan
    from numpy.core.fromnumeric import ndim
KeyboardInterrupt
Process Process-16:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 381, in eval_worker_process
    act = pf.eval_act( torch.Tensor( eval_ob ).to(env_info.device).unsqueeze(0), mask_this_task)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/continuous_policy.py", line 81, in eval_act
    mean, _, _ = self.forward(x, neuron_masks)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/policies/continuous_policy.py", line 70, in forward
    x = super().forward(x, neuron_masks,enable_mask)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/networks/nets.py", line 91, in forward
    mask_out = self.activation_func(layer(mask_out)) * neuron_masks[idx]
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 92, in forward
    def forward(self, input: Tensor) -> Tensor:
KeyboardInterrupt
Traceback (most recent call last):
  File "starter/mt_must_sac.py", line 317, in <module>
    experiment(args)
  File "starter/mt_must_sac.py", line 312, in experiment
    agent.train(env.num_tasks,params,group_name)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/algo/rl_algo.py", line 416, in train
    self.logger.add_epoch_info(epoch, total_frames,
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/utils/logger.py", line 106, in add_epoch_info
    print( tabulate(tabulate_list) )
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/tabulate/__init__.py", line 2130, in tabulate
    coltypes = [_column_type(col, numparse=np) for col, np in zip(cols, numparses)]
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/tabulate/__init__.py", line 2130, in <listcomp>
    coltypes = [_column_type(col, numparse=np) for col, np in zip(cols, numparses)]
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/tabulate/__init__.py", line 1197, in _column_type
    types = [_type(s, has_invisible, numparse) for s in strings]
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/tabulate/__init__.py", line 1197, in <listcomp>
    types = [_type(s, has_invisible, numparse) for s in strings]
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/tabulate/__init__.py", line 900, in _type
    string = _strip_ansi(string)
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/tabulate/__init__.py", line 984, in _strip_ansi
    def _strip_ansi(s):
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
