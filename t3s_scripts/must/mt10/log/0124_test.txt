W&B enabled.
task start
Use seed 3
id_list ['reach-v1']
2024-01-23 00:58:19,775 MainThread INFO: Experiment Name:0123_itv5_pr0.5_sllr1e-4_slloss0_trajinfo0_selected1_select-tsk-id0
2024-01-23 00:58:19,776 MainThread INFO: {
  "env_name": "mt10",
  "selected_task_amount": 1,
  "env": {
    "reward_scale": 1,
    "obs_norm": false
  },
  "meta_env": {
    "obs_type": "with_goal_and_id"
  },
  "replay_buffer": {
    "size": 1000000.0
  },
  "net": {
    "hidden_shapes": [
      40,
      40,
      40
    ]
  },
  "task_embedding": {
    "em_hidden_shapes": [
      256,
      128
    ]
  },
  "traj_encoder": {
    "latent_size": 32
  },
  "generator": {
    "one_hot_mlp_hidden": 64,
    "generator_mlp_hidden": 32,
    "one_hot_result_dim": 64
  },
  "sparse_training": {
    "pruning_ratio": 0.5
  },
  "general_setting": {
    "discount": 0.99,
    "pretrain_epochs": 5,
    "num_epochs": 10000,
    "epoch_frames": 150,
    "max_episode_frames": 150,
    "generator_lr": 0.0001,
    "batch_size": 1280,
    "min_pool": 10000,
    "success_traj_update_only": true,
    "target_hard_update_period": 1000,
    "use_soft_update": true,
    "tau": 0.005,
    "opt_times": 5,
    "update_end_epoch": 10000,
    "mask_update_interval": 5,
    "eval_episodes": 3,
    "recent_traj_window": 10,
    "sl_optim_times": 5,
    "use_trajectory_info": 0,
    "use_sl_loss": 0
  },
  "sac": {
    "plr": 0.0001,
    "qlr": 0.0001,
    "reparameterization": true,
    "automatic_entropy_tuning": true,
    "policy_std_reg_weight": 0,
    "policy_mean_reg_weight": 0
  },
  "specify_single_task": 0
}
finish policy net init
mask generator finish initialization
/lustre04/scratch/qianxi/sparse_training/dec_must/must_env/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Could not seed environment <MTEnv instance>[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
wandb: Tracking run with wandb version 0.15.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
replay_buffer._size: [300]
2024-01-23 00:58:32,002 MainThread INFO: EPOCH:0
2024-01-23 00:58:32,003 MainThread INFO: Time Consumed:0.0021049976348876953s
2024-01-23 00:58:32,003 MainThread INFO: Total Frames:150s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                25028.79512
Running_Training_Average_Rewards  25028.79512

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [302]
2024-01-23 00:58:32,007 MainThread INFO: EPOCH:1
2024-01-23 00:58:32,007 MainThread INFO: Time Consumed:0.0010063648223876953s
2024-01-23 00:58:32,007 MainThread INFO: Total Frames:300s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                3183.08325
Running_Training_Average_Rewards  14105.93918

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [450]
2024-01-23 00:58:32,138 MainThread INFO: EPOCH:2
2024-01-23 00:58:32,138 MainThread INFO: Time Consumed:0.129364013671875s
2024-01-23 00:58:32,138 MainThread INFO: Total Frames:450s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                23467.43011
Running_Training_Average_Rewards  17226.43616

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [600]
2024-01-23 00:58:32,270 MainThread INFO: EPOCH:3
2024-01-23 00:58:32,270 MainThread INFO: Time Consumed:0.12968802452087402s
2024-01-23 00:58:32,270 MainThread INFO: Total Frames:600s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                5736.07480
Running_Training_Average_Rewards  14353.84582

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [750]
2024-01-23 00:58:32,402 MainThread INFO: EPOCH:4
2024-01-23 00:58:32,402 MainThread INFO: Time Consumed:0.12981820106506348s
2024-01-23 00:58:32,403 MainThread INFO: Total Frames:750s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                4722.52503
Running_Training_Average_Rewards  12427.58166

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
2024-01-23 00:58:32,404 MainThread INFO: Finished Pretrain
  0%|          | 0/10000 [00:00<?, ?it/s]inside rlalgo, task 0, sumup 70905
epoch first part time 7.152557373046875e-06
replay_buffer._size: [1050]
collect time 0.0011415481567382812
inside mustsac before update, task 0, sumup 70905
inside mustsac after update, task 0, sumup 70300
inner_dict_sum {'sac_diff0': 0.08288216590881348, 'sac_diff1': 0.050309181213378906, 'sac_diff2': 0.21409320831298828, 'sac_diff3': 0.015450716018676758, 'sac_diff4': 0.009193897247314453, 'sac_diff5': 0.10308003425598145, 'sac_diff6': 0.0017361640930175781, 'all': 0.4767453670501709}
diff5_list [0.05934500694274902, 0.011727333068847656, 0.010697364807128906, 0.010533571243286133, 0.010776758193969727]
time3 0.0045299530029296875
time4 0.4809229373931885
time5 0.48099684715270996
time7 0.009641885757446289
gen_weight_change tensor(-17.4823)
policy weight change tensor(-0.9667, grad_fn=<SumBackward0>)
time8 0.01967453956604004
train_time 1.2743847370147705
eval time 2.141240119934082
epoch last part time 9.775161743164062e-06
2024-01-23 00:58:36,302 MainThread INFO: EPOCH:0
2024-01-23 00:58:36,302 MainThread INFO: Time Consumed:3.419353723526001s
2024-01-23 00:58:36,302 MainThread INFO: Total Frames:900s
  0%|          | 1/10000 [00:03<10:58:06,  3.95s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11487.64205
Train_Epoch_Reward                37679.31239
Running_Training_Average_Rewards  16636.20345
Explore_Time                      0.00113
Train___Time                      1.27438
Eval____Time                      2.14124
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11487.64205
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.08509     2.24635     101.03618    94.43115
alpha_0                           0.99970      0.00014     0.99990      0.99950
Alpha_loss                        -0.00134     0.00094     -0.00000     -0.00267
Training/policy_loss              -2.68358     0.01872     -2.65907     -2.71235
Training/qf1_loss                 20779.53672  1119.98471  22334.72266  18867.05469
Training/qf2_loss                 20780.06250  1119.84256  22335.08984  18867.83398
Training/pf_norm                  0.47121      0.05410     0.54563      0.38575
Training/qf1_norm                 226.13343    5.74920     233.60341    216.94603
Training/qf2_norm                 223.14726    4.76834     231.31085    216.68394
log_std/mean                      0.00011      0.00055     0.00052      -0.00093
log_probs/mean                    -2.68396     0.01853     -2.65973     -2.71242
mean/mean                         -0.00019     0.00018     0.00003      -0.00042
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0508115291595459
epoch last part time3 0.002786397933959961
inside rlalgo, task 0, sumup 70300
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [1200]
collect time 0.0008893013000488281
inner_dict_sum {'sac_diff0': 0.0001842975616455078, 'sac_diff1': 0.0072176456451416016, 'sac_diff2': 0.00812387466430664, 'sac_diff3': 0.010193824768066406, 'sac_diff4': 0.006887197494506836, 'sac_diff5': 0.03164076805114746, 'sac_diff6': 0.00040459632873535156, 'all': 0.0646522045135498}
diff5_list [0.0067059993743896484, 0.006075143814086914, 0.00629425048828125, 0.0064220428466796875, 0.006143331527709961]
time3 0
time4 0.06542062759399414
time5 0.06546998023986816
time7 9.5367431640625e-07
gen_weight_change tensor(-17.4823)
policy weight change tensor(-0.8224, grad_fn=<SumBackward0>)
time8 0.0018892288208007812
train_time 0.07568621635437012
eval time 0.17133235931396484
epoch last part time 6.4373016357421875e-06
2024-01-23 00:58:36,608 MainThread INFO: EPOCH:1
2024-01-23 00:58:36,609 MainThread INFO: Time Consumed:0.2502868175506592s
2024-01-23 00:58:36,609 MainThread INFO: Total Frames:1050s
  0%|          | 2/10000 [00:04<4:57:39,  1.79s/it] --------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11487.64205
Train_Epoch_Reward                12873.06458
Running_Training_Average_Rewards  16098.61218
Explore_Time                      0.00088
Train___Time                      0.07569
Eval____Time                      0.17133
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11487.64205
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.10782     2.54141    100.44984    94.06927
alpha_0                           0.99920      0.00014    0.99940      0.99900
Alpha_loss                        -0.00468     0.00094    -0.00334     -0.00600
Training/policy_loss              -2.68104     0.00573    -2.66992     -2.68591
Training/qf1_loss                 20542.08437  998.94402  21722.38086  19432.06836
Training/qf2_loss                 20542.90859  998.95605  21723.13867  19432.96289
Training/pf_norm                  0.47016      0.02467    0.51193      0.44438
Training/qf1_norm                 220.38560    5.54344    227.64032    213.83766
Training/qf2_norm                 227.66740    5.77408    235.27228    220.89029
log_std/mean                      -0.00147     0.00044    -0.00084     -0.00209
log_probs/mean                    -2.68233     0.00546    -2.67173     -2.68666
mean/mean                         0.00007      0.00010    0.00023      -0.00006
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01915287971496582
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70300
epoch first part time 3.814697265625e-06
replay_buffer._size: [1350]
collect time 0.0009648799896240234
inner_dict_sum {'sac_diff0': 0.00021147727966308594, 'sac_diff1': 0.0075838565826416016, 'sac_diff2': 0.008733510971069336, 'sac_diff3': 0.010970830917358398, 'sac_diff4': 0.007241487503051758, 'sac_diff5': 0.03307390213012695, 'sac_diff6': 0.0004200935363769531, 'all': 0.06823515892028809}
diff5_list [0.007928133010864258, 0.006451129913330078, 0.006404399871826172, 0.006097555160522461, 0.006192684173583984]
time3 0
time4 0.0690155029296875
time5 0.0690603256225586
time7 4.76837158203125e-07
gen_weight_change tensor(-17.4823)
policy weight change tensor(-0.6008, grad_fn=<SumBackward0>)
time8 0.001935720443725586
train_time 0.0798494815826416
eval time 0.2146313190460205
epoch last part time 6.67572021484375e-06
2024-01-23 00:58:36,929 MainThread INFO: EPOCH:2
2024-01-23 00:58:36,930 MainThread INFO: Time Consumed:0.29795289039611816s
2024-01-23 00:58:36,930 MainThread INFO: Total Frames:1200s
  0%|          | 3/10000 [00:04<3:06:08,  1.12s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11462.29536
Train_Epoch_Reward                5858.85268
Running_Training_Average_Rewards  14818.64224
Explore_Time                      0.00096
Train___Time                      0.07985
Eval____Time                      0.21463
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11411.60197
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.39772     0.98475    92.81341     90.00455
alpha_0                           0.99870      0.00014    0.99890      0.99850
Alpha_loss                        -0.00803     0.00096    -0.00667     -0.00938
Training/policy_loss              -2.68663     0.01172    -2.67030     -2.70090
Training/qf1_loss                 19703.92852  854.45914  21130.02539  18521.16016
Training/qf2_loss                 19705.16602  854.49336  21131.19141  18522.22070
Training/pf_norm                  0.44276      0.02975    0.47802      0.40475
Training/qf1_norm                 212.12317    2.28753    215.16397    208.75720
Training/qf2_norm                 214.72623    2.27236    217.93912    211.50357
log_std/mean                      -0.00225     0.00037    -0.00174     -0.00278
log_probs/mean                    -2.68995     0.01209    -2.67307     -2.70450
mean/mean                         -0.00027     0.00005    -0.00018     -0.00032
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019205808639526367
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70300
epoch first part time 3.814697265625e-06
replay_buffer._size: [1500]
collect time 0.0009696483612060547
inner_dict_sum {'sac_diff0': 0.0001983642578125, 'sac_diff1': 0.007387399673461914, 'sac_diff2': 0.007843017578125, 'sac_diff3': 0.010447978973388672, 'sac_diff4': 0.0071566104888916016, 'sac_diff5': 0.0329282283782959, 'sac_diff6': 0.00039005279541015625, 'all': 0.06635165214538574}
diff5_list [0.00722503662109375, 0.006389617919921875, 0.006459951400756836, 0.006674766540527344, 0.006178855895996094]
time3 0
time4 0.06712794303894043
time5 0.06717061996459961
time7 7.152557373046875e-07
gen_weight_change tensor(-17.4823)
policy weight change tensor(-0.3592, grad_fn=<SumBackward0>)
time8 0.0019512176513671875
train_time 0.07751250267028809
eval time 0.22752904891967773
epoch last part time 5.9604644775390625e-06
2024-01-23 00:58:37,261 MainThread INFO: EPOCH:3
2024-01-23 00:58:37,261 MainThread INFO: Time Consumed:0.3084278106689453s
2024-01-23 00:58:37,261 MainThread INFO: Total Frames:1350s
  0%|          | 4/10000 [00:04<2:14:23,  1.24it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11441.90314
Train_Epoch_Reward                4175.39403
Running_Training_Average_Rewards  13636.05911
Explore_Time                      0.00096
Train___Time                      0.07751
Eval____Time                      0.22753
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11380.72649
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.19347     3.71808     93.80447     82.61259
alpha_0                           0.99820      0.00014     0.99840      0.99800
Alpha_loss                        -0.01134     0.00094     -0.00998     -0.01267
Training/policy_loss              -2.67000     0.01631     -2.65221     -2.69995
Training/qf1_loss                 16995.43555  1465.71417  19536.34570  15263.66895
Training/qf2_loss                 16996.53574  1465.80439  19537.70703  15264.70605
Training/pf_norm                  0.48761      0.05024     0.54068      0.40450
Training/qf1_norm                 202.87225    8.47200     218.01703    192.46877
Training/qf2_norm                 198.66709    8.14343     213.14162    188.59406
log_std/mean                      -0.00305     0.00035     -0.00257     -0.00355
log_probs/mean                    -2.67364     0.01634     -2.65528     -2.70337
mean/mean                         -0.00047     0.00008     -0.00037     -0.00057
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01852703094482422
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70300
epoch first part time 3.337860107421875e-06
replay_buffer._size: [1650]
collect time 0.0009217262268066406
inner_dict_sum {'sac_diff0': 0.00018739700317382812, 'sac_diff1': 0.007014274597167969, 'sac_diff2': 0.008054018020629883, 'sac_diff3': 0.01077127456665039, 'sac_diff4': 0.006979227066040039, 'sac_diff5': 0.032439231872558594, 'sac_diff6': 0.00039458274841308594, 'all': 0.06584000587463379}
diff5_list [0.0067789554595947266, 0.00644993782043457, 0.006675004959106445, 0.006207466125488281, 0.00632786750793457]
time3 0
time4 0.0666203498840332
time5 0.0666654109954834
time7 9.5367431640625e-07
gen_weight_change tensor(-17.4823)
policy weight change tensor(-0.1007, grad_fn=<SumBackward0>)
time8 0.0020859241485595703
train_time 0.07706713676452637
eval time 0.2527275085449219
epoch last part time 7.62939453125e-06
2024-01-23 00:58:37,616 MainThread INFO: EPOCH:4
2024-01-23 00:58:37,616 MainThread INFO: Time Consumed:0.33324289321899414s
2024-01-23 00:58:37,617 MainThread INFO: Total Frames:1500s
  0%|          | 5/10000 [00:05<1:47:17,  1.55it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11434.47427
Train_Epoch_Reward                10216.24858
Running_Training_Average_Rewards  13294.07806
Explore_Time                      0.00092
Train___Time                      0.07707
Eval____Time                      0.25273
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11404.75878
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       83.55034     1.86563    86.02214     81.18297
alpha_0                           0.99770      0.00014    0.99790      0.99750
Alpha_loss                        -0.01471     0.00095    -0.01339     -0.01605
Training/policy_loss              -2.68360     0.01045    -2.67177     -2.69889
Training/qf1_loss                 16177.85938  939.81517  17420.33203  14669.05469
Training/qf2_loss                 16179.34238  939.84682  17422.02930  14670.57031
Training/pf_norm                  0.43292      0.02364    0.47478      0.41122
Training/qf1_norm                 194.75174    4.23108    200.68306    189.53447
Training/qf2_norm                 192.60070    4.10148    198.07788    187.40726
log_std/mean                      -0.00418     0.00037    -0.00367     -0.00471
log_probs/mean                    -2.68802     0.01054    -2.67589     -2.70361
mean/mean                         0.00006      0.00007    0.00014      -0.00005
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01888418197631836
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70300
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [1800]
collect time 0.0009851455688476562
inside mustsac before update, task 0, sumup 70300
inside mustsac after update, task 0, sumup 70025
inner_dict_sum {'sac_diff0': 0.00019621849060058594, 'sac_diff1': 0.007754802703857422, 'sac_diff2': 0.008798599243164062, 'sac_diff3': 0.011213064193725586, 'sac_diff4': 0.0077915191650390625, 'sac_diff5': 0.05345797538757324, 'sac_diff6': 0.00043773651123046875, 'all': 0.08964991569519043}
diff5_list [0.012757062911987305, 0.011187314987182617, 0.009525537490844727, 0.00980687141418457, 0.010181188583374023]
time3 0.0009191036224365234
time4 0.09067082405090332
time5 0.0907280445098877
time7 0.009018898010253906
gen_weight_change tensor(-17.9520)
policy weight change tensor(0.0824, grad_fn=<SumBackward0>)
time8 0.001981973648071289
train_time 0.12018013000488281
eval time 0.22240376472473145
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:37,985 MainThread INFO: EPOCH:5
2024-01-23 00:58:37,985 MainThread INFO: Time Consumed:0.3460960388183594s
2024-01-23 00:58:37,985 MainThread INFO: Total Frames:1650s
  0%|          | 6/10000 [00:05<1:31:43,  1.82it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11431.55347
Train_Epoch_Reward                5712.89613
Running_Training_Average_Rewards  12604.87970
Explore_Time                      0.00098
Train___Time                      0.12018
Eval____Time                      0.22240
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11416.94950
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       81.66695     1.28763    83.45799     79.51894
alpha_0                           0.99720      0.00014    0.99740      0.99700
Alpha_loss                        -0.01805     0.00096    -0.01670     -0.01941
Training/policy_loss              -2.68038     0.00431    -2.67474     -2.68626
Training/qf1_loss                 15047.55273  653.79305  16075.68457  14433.04395
Training/qf2_loss                 15049.13633  653.78533  16077.15332  14434.60938
Training/pf_norm                  0.45185      0.01705    0.47815      0.42838
Training/qf1_norm                 188.29020    2.87151    191.82564    183.49651
Training/qf2_norm                 189.89952    4.86292    196.25018    182.14606
log_std/mean                      -0.00526     0.00050    -0.00441     -0.00589
log_probs/mean                    -2.68634     0.00469    -2.68008     -2.69325
mean/mean                         0.00022      0.00010    0.00039      0.00010
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019471406936645508
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70025
epoch first part time 3.337860107421875e-06
replay_buffer._size: [1950]
collect time 0.0008754730224609375
inner_dict_sum {'sac_diff0': 0.00018978118896484375, 'sac_diff1': 0.007272481918334961, 'sac_diff2': 0.008525848388671875, 'sac_diff3': 0.010429859161376953, 'sac_diff4': 0.007238149642944336, 'sac_diff5': 0.03222036361694336, 'sac_diff6': 0.0003943443298339844, 'all': 0.06627082824707031}
diff5_list [0.006802082061767578, 0.0063190460205078125, 0.006451606750488281, 0.0062999725341796875, 0.00634765625]
time3 0
time4 0.06705808639526367
time5 0.06710481643676758
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9520)
policy weight change tensor(0.3854, grad_fn=<SumBackward0>)
time8 0.0019428730010986328
train_time 0.07739114761352539
eval time 0.27045631408691406
epoch last part time 8.344650268554688e-06
2024-01-23 00:58:38,359 MainThread INFO: EPOCH:6
2024-01-23 00:58:38,359 MainThread INFO: Time Consumed:0.3513054847717285s
2024-01-23 00:58:38,359 MainThread INFO: Total Frames:1800s
  0%|          | 7/10000 [00:05<1:22:05,  2.03it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11440.23743
Train_Epoch_Reward                11412.52165
Running_Training_Average_Rewards  12505.51653
Explore_Time                      0.00087
Train___Time                      0.07739
Eval____Time                      0.27046
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11492.34118
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       82.69908     1.58261    85.63114     81.15532
alpha_0                           0.99671      0.00014    0.99690      0.99651
Alpha_loss                        -0.02136     0.00096    -0.02002     -0.02269
Training/policy_loss              -2.66821     0.00861    -2.65277     -2.67817
Training/qf1_loss                 15783.43828  669.66654  16931.76367  15036.62012
Training/qf2_loss                 15785.50937  669.76372  16933.98438  15038.51562
Training/pf_norm                  0.48520      0.03275    0.54423      0.45391
Training/qf1_norm                 189.51614    3.64876    196.24361    185.90811
Training/qf2_norm                 191.78871    3.52877    198.33034    188.41556
log_std/mean                      -0.00634     0.00039    -0.00580     -0.00691
log_probs/mean                    -2.67468     0.00878    -2.65893     -2.68494
mean/mean                         -0.00036     0.00010    -0.00023     -0.00049
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019215822219848633
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70025
epoch first part time 3.337860107421875e-06
replay_buffer._size: [2100]
collect time 0.0010671615600585938
inner_dict_sum {'sac_diff0': 0.00019407272338867188, 'sac_diff1': 0.007378816604614258, 'sac_diff2': 0.008554458618164062, 'sac_diff3': 0.011073589324951172, 'sac_diff4': 0.007691383361816406, 'sac_diff5': 0.034346580505371094, 'sac_diff6': 0.00040221214294433594, 'all': 0.06964111328125}
diff5_list [0.006722450256347656, 0.006398200988769531, 0.006631612777709961, 0.0073451995849609375, 0.007249116897583008]
time3 0
time4 0.07044482231140137
time5 0.07050013542175293
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9520)
policy weight change tensor(0.7591, grad_fn=<SumBackward0>)
time8 0.0022292137145996094
train_time 0.08145642280578613
eval time 0.29859089851379395
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:38,766 MainThread INFO: EPOCH:7
2024-01-23 00:58:38,766 MainThread INFO: Time Consumed:0.3837559223175049s
2024-01-23 00:58:38,766 MainThread INFO: Total Frames:1950s
  0%|          | 8/10000 [00:06<1:17:31,  2.15it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11448.22461
Train_Epoch_Reward                3906.80989
Running_Training_Average_Rewards  11844.07756
Explore_Time                      0.00106
Train___Time                      0.08146
Eval____Time                      0.29859
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11504.13489
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       76.91974     1.29105    78.70177     75.40928
alpha_0                           0.99621      0.00014    0.99641      0.99601
Alpha_loss                        -0.02478     0.00096    -0.02344     -0.02616
Training/policy_loss              -2.69044     0.00607    -2.68049     -2.69963
Training/qf1_loss                 13136.19355  457.46828  13619.06445  12395.95996
Training/qf2_loss                 13137.83906  457.50864  13620.72461  12397.64453
Training/pf_norm                  0.42530      0.01979    0.45951      0.39801
Training/qf1_norm                 178.83931    3.02508    183.16681    175.52705
Training/qf2_norm                 177.89757    2.87914    181.85014    174.53339
log_std/mean                      -0.00833     0.00040    -0.00778     -0.00891
log_probs/mean                    -2.69734     0.00635    -2.68708     -2.70710
mean/mean                         0.00093      0.00007    0.00099      0.00081
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01915454864501953
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70025
epoch first part time 3.337860107421875e-06
replay_buffer._size: [2250]
collect time 0.0010242462158203125
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.00794363021850586, 'sac_diff2': 0.008833885192871094, 'sac_diff3': 0.012021541595458984, 'sac_diff4': 0.007904767990112305, 'sac_diff5': 0.0365443229675293, 'sac_diff6': 0.0004239082336425781, 'all': 0.07388186454772949}
diff5_list [0.007769107818603516, 0.007181882858276367, 0.007216215133666992, 0.007128000259399414, 0.007249116897583008]
time3 0
time4 0.07474684715270996
time5 0.07479596138000488
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9520)
policy weight change tensor(1.2077, grad_fn=<SumBackward0>)
time8 0.002124309539794922
train_time 0.08595442771911621
eval time 0.2982962131500244
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:39,177 MainThread INFO: EPOCH:8
2024-01-23 00:58:39,177 MainThread INFO: Time Consumed:0.3877997398376465s
2024-01-23 00:58:39,177 MainThread INFO: Total Frames:2100s
  0%|          | 9/10000 [00:06<1:14:39,  2.23it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11459.19504
Train_Epoch_Reward                8800.33561
Running_Training_Average_Rewards  11626.66742
Explore_Time                      0.00102
Train___Time                      0.08595
Eval____Time                      0.29830
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11546.95846
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       86.72725     1.96855    90.44969     85.00995
alpha_0                           0.99571      0.00014    0.99591      0.99551
Alpha_loss                        -0.02809     0.00092    -0.02679     -0.02934
Training/policy_loss              -2.67860     0.01447    -2.65922     -2.69267
Training/qf1_loss                 16331.69883  738.04790  17665.86523  15671.40430
Training/qf2_loss                 16334.36270  738.10312  17668.73828  15674.21582
Training/pf_norm                  0.44287      0.05757    0.51887      0.37917
Training/qf1_norm                 205.81912    4.61160    214.67996    201.85132
Training/qf2_norm                 200.89875    4.38265    209.18515    197.13031
log_std/mean                      -0.00902     0.00039    -0.00848     -0.00958
log_probs/mean                    -2.68731     0.01433    -2.66838     -2.70171
mean/mean                         0.00064      0.00001    0.00066      0.00063
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019191503524780273
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70025
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [2400]
collect time 0.0009777545928955078
inner_dict_sum {'sac_diff0': 0.0002002716064453125, 'sac_diff1': 0.007128477096557617, 'sac_diff2': 0.008105754852294922, 'sac_diff3': 0.010864973068237305, 'sac_diff4': 0.007394552230834961, 'sac_diff5': 0.03338479995727539, 'sac_diff6': 0.0004036426544189453, 'all': 0.06748247146606445}
diff5_list [0.007382631301879883, 0.0064733028411865234, 0.006609201431274414, 0.006340503692626953, 0.006579160690307617]
time3 0
time4 0.06834006309509277
time5 0.06840014457702637
time7 9.5367431640625e-07
gen_weight_change tensor(-17.9520)
policy weight change tensor(1.7133, grad_fn=<SumBackward0>)
time8 0.0020918846130371094
train_time 0.07924938201904297
eval time 0.32203078269958496
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:39,608 MainThread INFO: EPOCH:9
2024-01-23 00:58:39,608 MainThread INFO: Time Consumed:0.40802431106567383s
2024-01-23 00:58:39,609 MainThread INFO: Total Frames:2250s
  0%|          | 10/10000 [00:07<1:13:50,  2.25it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11473.05637
Train_Epoch_Reward                33267.48258
Running_Training_Average_Rewards  13069.38843
Explore_Time                      0.00097
Train___Time                      0.07925
Eval____Time                      0.32203
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11597.80829
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.67407     1.45831    95.40163     91.08626
alpha_0                           0.99521      0.00014    0.99541      0.99501
Alpha_loss                        -0.03144     0.00093    -0.03015     -0.03269
Training/policy_loss              -2.67751     0.01258    -2.66007     -2.69487
Training/qf1_loss                 18564.83437  968.23114  20023.89258  17080.27734
Training/qf2_loss                 18568.32578  968.22612  20027.33398  17083.66797
Training/pf_norm                  0.43077      0.03647    0.48060      0.39071
Training/qf1_norm                 218.19599    3.23899    221.65694    212.25670
Training/qf2_norm                 218.03292    3.26716    221.85597    212.19371
log_std/mean                      -0.01070     0.00039    -0.01017     -0.01127
log_probs/mean                    -2.68871     0.01251    -2.67173     -2.70643
mean/mean                         0.00008      0.00000    0.00009      0.00008
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.023975372314453125
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70025
epoch first part time 1.5974044799804688e-05
replay_buffer._size: [2550]
collect time 0.001453399658203125
inside mustsac before update, task 0, sumup 70025
inside mustsac after update, task 0, sumup 70809
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.008287429809570312, 'sac_diff2': 0.009404897689819336, 'sac_diff3': 0.012439250946044922, 'sac_diff4': 0.00835418701171875, 'sac_diff5': 0.05650162696838379, 'sac_diff6': 0.0004506111145019531, 'all': 0.09564781188964844}
diff5_list [0.01200413703918457, 0.01074075698852539, 0.01183176040649414, 0.011565923690795898, 0.010359048843383789]
time3 0.0009906291961669922
time4 0.09668517112731934
time5 0.09674668312072754
time7 0.016028881072998047
gen_weight_change tensor(-18.3939)
policy weight change tensor(2.0652, grad_fn=<SumBackward0>)
time8 0.0028421878814697266
train_time 0.13487911224365234
eval time 0.2676401138305664
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:40,039 MainThread INFO: EPOCH:10
2024-01-23 00:58:40,039 MainThread INFO: Time Consumed:0.40652918815612793s
2024-01-23 00:58:40,039 MainThread INFO: Total Frames:2400s
  0%|          | 11/10000 [00:07<1:13:18,  2.27it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11488.15637
Train_Epoch_Reward                26601.07619
Running_Training_Average_Rewards  13915.11891
Explore_Time                      0.00142
Train___Time                      0.13488
Eval____Time                      0.26764
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11638.64212
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.10190     1.59718    91.72630     87.46233
alpha_0                           0.99471      0.00014    0.99491      0.99451
Alpha_loss                        -0.03479     0.00093    -0.03338     -0.03602
Training/policy_loss              -2.67749     0.01415    -2.65782     -2.69490
Training/qf1_loss                 16894.21719  525.94225  17897.95508  16446.17773
Training/qf2_loss                 16897.41133  526.01694  17901.27734  16449.08594
Training/pf_norm                  0.42995      0.04071    0.50710      0.39780
Training/qf1_norm                 209.55192    4.71056    216.15776    203.27664
Training/qf2_norm                 204.31721    3.07012    209.31355    200.21478
log_std/mean                      -0.01134     0.00082    -0.00988     -0.01215
log_probs/mean                    -2.68889     0.01392    -2.67010     -2.70575
mean/mean                         0.00084      0.00021    0.00124      0.00064
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019753694534301758
epoch last part time3 0.0027899742126464844
inside rlalgo, task 0, sumup 70809
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [2700]
collect time 0.0008962154388427734
inner_dict_sum {'sac_diff0': 0.0001957416534423828, 'sac_diff1': 0.007181882858276367, 'sac_diff2': 0.008540868759155273, 'sac_diff3': 0.011472463607788086, 'sac_diff4': 0.007345438003540039, 'sac_diff5': 0.033034324645996094, 'sac_diff6': 0.0004074573516845703, 'all': 0.06817817687988281}
diff5_list [0.006883144378662109, 0.006648063659667969, 0.00624394416809082, 0.006667137145996094, 0.0065920352935791016]
time3 0
time4 0.06898045539855957
time5 0.06902575492858887
time7 9.5367431640625e-07
gen_weight_change tensor(-18.3939)
policy weight change tensor(2.6130, grad_fn=<SumBackward0>)
time8 0.0019843578338623047
train_time 0.0796656608581543
eval time 0.3303260803222656
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:40,478 MainThread INFO: EPOCH:11
2024-01-23 00:58:40,479 MainThread INFO: Time Consumed:0.41339540481567383s
2024-01-23 00:58:40,479 MainThread INFO: Total Frames:2550s
  0%|          | 12/10000 [00:08<1:13:03,  2.28it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11488.82080
Train_Epoch_Reward                5968.20275
Running_Training_Average_Rewards  13447.65326
Explore_Time                      0.00089
Train___Time                      0.07967
Eval____Time                      0.33033
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11494.28630
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.25867     1.49084    93.95697     89.88550
alpha_0                           0.99422      0.00014    0.99441      0.99402
Alpha_loss                        -0.03814     0.00097    -0.03675     -0.03957
Training/policy_loss              -2.67922     0.00996    -2.66531     -2.69296
Training/qf1_loss                 17140.41875  416.14408  17639.49609  16596.10547
Training/qf2_loss                 17143.19453  416.13811  17642.43750  16598.76953
Training/pf_norm                  0.43038      0.03631    0.46343      0.36234
Training/qf1_norm                 204.96010    3.47184    211.01118    201.55437
Training/qf2_norm                 213.58916    3.36625    219.65851    210.45726
log_std/mean                      -0.01248     0.00040    -0.01193     -0.01307
log_probs/mean                    -2.69097     0.01017    -2.67725     -2.70533
mean/mean                         0.00082      0.00004    0.00089      0.00076
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01878666877746582
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70809
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [2850]
collect time 0.0008993148803710938
inner_dict_sum {'sac_diff0': 0.00019121170043945312, 'sac_diff1': 0.006882190704345703, 'sac_diff2': 0.007891178131103516, 'sac_diff3': 0.009886503219604492, 'sac_diff4': 0.006773948669433594, 'sac_diff5': 0.031536102294921875, 'sac_diff6': 0.00039839744567871094, 'all': 0.06355953216552734}
diff5_list [0.006865024566650391, 0.006075143814086914, 0.006255149841308594, 0.006069660186767578, 0.0062711238861083984]
time3 0
time4 0.06433510780334473
time5 0.06437969207763672
time7 1.1920928955078125e-06
gen_weight_change tensor(-18.3939)
policy weight change tensor(3.1680, grad_fn=<SumBackward0>)
time8 0.0019114017486572266
train_time 0.0747060775756836
eval time 0.35701894760131836
epoch last part time 8.821487426757812e-06
2024-01-23 00:58:40,936 MainThread INFO: EPOCH:12
2024-01-23 00:58:40,936 MainThread INFO: Time Consumed:0.4351329803466797s
2024-01-23 00:58:40,936 MainThread INFO: Total Frames:2700s
  0%|          | 13/10000 [00:08<1:14:00,  2.25it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11497.12637
Train_Epoch_Reward                18609.96105
Running_Training_Average_Rewards  13734.44813
Explore_Time                      0.00089
Train___Time                      0.07471
Eval____Time                      0.35702
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11494.65771
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.24430     1.47343    93.85387     89.47353
alpha_0                           0.99372      0.00014    0.99392      0.99352
Alpha_loss                        -0.04149     0.00094    -0.04015     -0.04288
Training/policy_loss              -2.67725     0.01019    -2.66058     -2.68961
Training/qf1_loss                 16846.46953  409.36850  17174.92773  16041.56250
Training/qf2_loss                 16850.43770  409.36177  17178.85547  16045.51270
Training/pf_norm                  0.44271      0.06480    0.52398      0.35344
Training/qf1_norm                 217.96243    3.25461    221.34998    211.90128
Training/qf2_norm                 210.25837    3.22519    213.78285    204.19987
log_std/mean                      -0.01411     0.00045    -0.01350     -0.01476
log_probs/mean                    -2.69049     0.01018    -2.67398     -2.70266
mean/mean                         0.00121      0.00015    0.00142      0.00099
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019417524337768555
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70809
epoch first part time 3.337860107421875e-06
replay_buffer._size: [3000]
collect time 0.0008819103240966797
inner_dict_sum {'sac_diff0': 0.0002009868621826172, 'sac_diff1': 0.007071733474731445, 'sac_diff2': 0.008503198623657227, 'sac_diff3': 0.010671854019165039, 'sac_diff4': 0.006924629211425781, 'sac_diff5': 0.03253364562988281, 'sac_diff6': 0.00038886070251464844, 'all': 0.06629490852355957}
diff5_list [0.006776094436645508, 0.006525993347167969, 0.006545543670654297, 0.0064105987548828125, 0.0062754154205322266]
time3 0
time4 0.06707334518432617
time5 0.06712174415588379
time7 1.1920928955078125e-06
gen_weight_change tensor(-18.3939)
policy weight change tensor(3.7055, grad_fn=<SumBackward0>)
time8 0.0019044876098632812
train_time 0.07756638526916504
eval time 0.3685894012451172
epoch last part time 7.62939453125e-06
2024-01-23 00:58:41,408 MainThread INFO: EPOCH:13
2024-01-23 00:58:41,408 MainThread INFO: Time Consumed:0.44954538345336914s
2024-01-23 00:58:41,409 MainThread INFO: Total Frames:2850s
  0%|          | 14/10000 [00:09<1:15:24,  2.21it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11510.04443
Train_Epoch_Reward                12362.46483
Running_Training_Average_Rewards  13662.23849
Explore_Time                      0.00088
Train___Time                      0.07757
Eval____Time                      0.36859
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11509.90706
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.11712     0.85424    92.06268     89.83225
alpha_0                           0.99322      0.00014    0.99342      0.99302
Alpha_loss                        -0.04487     0.00097    -0.04344     -0.04619
Training/policy_loss              -2.68060     0.01447    -2.66301     -2.69919
Training/qf1_loss                 16433.59863  467.88667  17040.48633  15930.50195
Training/qf2_loss                 16437.64570  467.77393  17044.43359  15934.59961
Training/pf_norm                  0.41487      0.02821    0.45356      0.38032
Training/qf1_norm                 212.72082    1.60769    214.85056    210.13976
Training/qf2_norm                 210.61328    1.86436    212.73021    207.78879
log_std/mean                      -0.01517     0.00042    -0.01459     -0.01578
log_probs/mean                    -2.69515     0.01463    -2.67744     -2.71360
mean/mean                         0.00171      0.00005    0.00178      0.00164
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019341468811035156
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70809
epoch first part time 3.337860107421875e-06
replay_buffer._size: [3150]
collect time 0.0008993148803710938
inner_dict_sum {'sac_diff0': 0.0002002716064453125, 'sac_diff1': 0.007337093353271484, 'sac_diff2': 0.008345603942871094, 'sac_diff3': 0.010647773742675781, 'sac_diff4': 0.007103919982910156, 'sac_diff5': 0.03306770324707031, 'sac_diff6': 0.00038814544677734375, 'all': 0.06709051132202148}
diff5_list [0.007266998291015625, 0.006689310073852539, 0.006189823150634766, 0.006471872329711914, 0.006449699401855469]
time3 0
time4 0.06787776947021484
time5 0.06792330741882324
time7 7.152557373046875e-07
gen_weight_change tensor(-18.3939)
policy weight change tensor(4.2373, grad_fn=<SumBackward0>)
time8 0.0020203590393066406
train_time 0.07865738868713379
eval time 0.3920552730560303
epoch last part time 7.3909759521484375e-06
2024-01-23 00:58:41,905 MainThread INFO: EPOCH:14
2024-01-23 00:58:41,906 MainThread INFO: Time Consumed:0.474240779876709s
2024-01-23 00:58:41,906 MainThread INFO: Total Frames:3000s
  0%|          | 15/10000 [00:09<1:17:38,  2.14it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11521.19611
Train_Epoch_Reward                10925.72773
Running_Training_Average_Rewards  13525.41295
Explore_Time                      0.00089
Train___Time                      0.07866
Eval____Time                      0.39206
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11516.27557
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.71515     1.12398    92.71119     89.60100
alpha_0                           0.99272      0.00014    0.99292      0.99253
Alpha_loss                        -0.04823     0.00096    -0.04688     -0.04967
Training/policy_loss              -2.68118     0.01009    -2.66375     -2.69362
Training/qf1_loss                 15864.53906  406.71565  16551.66016  15347.24414
Training/qf2_loss                 15868.84277  406.65694  16555.93945  15351.60449
Training/pf_norm                  0.41602      0.02478    0.46547      0.40201
Training/qf1_norm                 211.56494    2.25836    215.72815    209.35968
Training/qf2_norm                 212.11007    2.52000    216.59882    209.66187
log_std/mean                      -0.01671     0.00046    -0.01608     -0.01737
log_probs/mean                    -2.69771     0.01018    -2.68042     -2.71079
mean/mean                         0.00138      0.00004    0.00142      0.00133
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019938945770263672
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70809
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [3300]
collect time 0.0009615421295166016
inside mustsac before update, task 0, sumup 70809
inside mustsac after update, task 0, sumup 71261
inner_dict_sum {'sac_diff0': 0.00019025802612304688, 'sac_diff1': 0.007299184799194336, 'sac_diff2': 0.008959770202636719, 'sac_diff3': 0.011485099792480469, 'sac_diff4': 0.0077512264251708984, 'sac_diff5': 0.05297422409057617, 'sac_diff6': 0.0004284381866455078, 'all': 0.08908820152282715}
diff5_list [0.010932683944702148, 0.010081052780151367, 0.010075807571411133, 0.01115107536315918, 0.010733604431152344]
time3 0.0008995532989501953
time4 0.0900421142578125
time5 0.09009623527526855
time7 0.009563684463500977
gen_weight_change tensor(-18.6864)
policy weight change tensor(4.6931, grad_fn=<SumBackward0>)
time8 0.0020453929901123047
train_time 0.11975622177124023
eval time 0.3558042049407959
epoch last part time 8.344650268554688e-06
2024-01-23 00:58:42,408 MainThread INFO: EPOCH:15
2024-01-23 00:58:42,408 MainThread INFO: Time Consumed:0.47921252250671387s
2024-01-23 00:58:42,409 MainThread INFO: Total Frames:3150s
  0%|          | 16/10000 [00:10<1:19:24,  2.10it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11533.77621
Train_Epoch_Reward                19331.28154
Running_Training_Average_Rewards  13801.88288
Explore_Time                      0.00096
Train___Time                      0.11976
Eval____Time                      0.35580
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11542.75050
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.51558     1.62952    92.58643     87.91309
alpha_0                           0.99223      0.00014    0.99243      0.99203
Alpha_loss                        -0.05157     0.00097    -0.05023     -0.05298
Training/policy_loss              -2.67885     0.00524    -2.67116     -2.68758
Training/qf1_loss                 15651.06699  435.85610  16104.50293  15133.14648
Training/qf2_loss                 15656.01934  436.13014  16109.71289  15137.98242
Training/pf_norm                  0.41183      0.01290    0.42385      0.39181
Training/qf1_norm                 216.00061    6.62834    223.35617    205.49216
Training/qf2_norm                 207.93531    4.88755    211.88002    198.86726
log_std/mean                      -0.01797     0.00101    -0.01664     -0.01912
log_probs/mean                    -2.69581     0.00512    -2.68808     -2.70421
mean/mean                         0.00190      0.00020    0.00212      0.00154
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019141435623168945
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71261
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [3450]
collect time 0.0010256767272949219
inner_dict_sum {'sac_diff0': 0.00020003318786621094, 'sac_diff1': 0.007184505462646484, 'sac_diff2': 0.008571386337280273, 'sac_diff3': 0.010717630386352539, 'sac_diff4': 0.00724029541015625, 'sac_diff5': 0.03289341926574707, 'sac_diff6': 0.00040078163146972656, 'all': 0.06720805168151855}
diff5_list [0.006976127624511719, 0.006449460983276367, 0.006336212158203125, 0.006421804428100586, 0.0067098140716552734]
time3 0
time4 0.06800460815429688
time5 0.06805109977722168
time7 1.1920928955078125e-06
gen_weight_change tensor(-18.6864)
policy weight change tensor(5.2755, grad_fn=<SumBackward0>)
time8 0.0020117759704589844
train_time 0.0787208080291748
eval time 0.4362616539001465
epoch last part time 7.3909759521484375e-06
2024-01-23 00:58:42,949 MainThread INFO: EPOCH:16
2024-01-23 00:58:42,950 MainThread INFO: Time Consumed:0.5185141563415527s
2024-01-23 00:58:42,950 MainThread INFO: Total Frames:3300s
  0%|          | 17/10000 [00:10<1:22:34,  2.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11555.56704
Train_Epoch_Reward                7276.93926
Running_Training_Average_Rewards  13505.29454
Explore_Time                      0.00102
Train___Time                      0.07872
Eval____Time                      0.43626
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11710.24946
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.66840     2.99376     97.41829     89.44975
alpha_0                           0.99173      0.00014     0.99193      0.99153
Alpha_loss                        -0.05486     0.00097     -0.05355     -0.05628
Training/policy_loss              -2.67026     0.00770     -2.65813     -2.67922
Training/qf1_loss                 16971.67109  1220.65964  18770.04883  15506.08105
Training/qf2_loss                 16976.58691  1220.88725  18775.14062  15510.57227
Training/pf_norm                  0.44607      0.01832     0.47754      0.42109
Training/qf1_norm                 220.66469    7.15916     229.24017    210.37309
Training/qf2_norm                 218.54451    6.74798     226.98671    208.98302
log_std/mean                      -0.01995     0.00049     -0.01928     -0.02066
log_probs/mean                    -2.68811     0.00789     -2.67561     -2.69769
mean/mean                         0.00179      0.00002     0.00181      0.00176
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018537521362304688
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71261
epoch first part time 3.337860107421875e-06
replay_buffer._size: [3600]
collect time 0.0009465217590332031
inner_dict_sum {'sac_diff0': 0.0001976490020751953, 'sac_diff1': 0.007227420806884766, 'sac_diff2': 0.00862884521484375, 'sac_diff3': 0.011016845703125, 'sac_diff4': 0.007304668426513672, 'sac_diff5': 0.03305935859680176, 'sac_diff6': 0.0003876686096191406, 'all': 0.06782245635986328}
diff5_list [0.007213115692138672, 0.0064661502838134766, 0.006529331207275391, 0.006397247314453125, 0.006453514099121094]
time3 0
time4 0.06861138343811035
time5 0.06865859031677246
time7 9.5367431640625e-07
gen_weight_change tensor(-18.6864)
policy weight change tensor(5.8894, grad_fn=<SumBackward0>)
time8 0.002156972885131836
train_time 0.07947850227355957
eval time 0.4481353759765625
epoch last part time 8.344650268554688e-06
2024-01-23 00:58:43,503 MainThread INFO: EPOCH:17
2024-01-23 00:58:43,503 MainThread INFO: Time Consumed:0.5310962200164795s
2024-01-23 00:58:43,503 MainThread INFO: Total Frames:3450s
  0%|          | 18/10000 [00:11<1:25:27,  1.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11575.00968
Train_Epoch_Reward                11883.61392
Running_Training_Average_Rewards  13434.78668
Explore_Time                      0.00094
Train___Time                      0.07948
Eval____Time                      0.44814
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11698.56137
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.98296     2.65234     101.53384    94.34529
alpha_0                           0.99124      0.00014     0.99143      0.99104
Alpha_loss                        -0.05827     0.00096     -0.05691     -0.05968
Training/policy_loss              -2.67534     0.00470     -2.66950     -2.68283
Training/qf1_loss                 18272.49570  1079.18580  19871.98828  16956.78320
Training/qf2_loss                 18278.76836  1079.26825  19878.19727  16962.68359
Training/pf_norm                  0.41600      0.02738     0.45932      0.37928
Training/qf1_norm                 236.56351    6.17466     244.10818    227.52277
Training/qf2_norm                 230.91077    6.03404     238.96815    222.64233
log_std/mean                      -0.02256     0.00056     -0.02180     -0.02337
log_probs/mean                    -2.69508     0.00486     -2.68945     -2.70317
mean/mean                         0.00135      0.00003     0.00140      0.00131
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01931929588317871
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 71261
epoch first part time 3.337860107421875e-06
replay_buffer._size: [3750]
collect time 0.0009512901306152344
inner_dict_sum {'sac_diff0': 0.00020074844360351562, 'sac_diff1': 0.007238626480102539, 'sac_diff2': 0.008542537689208984, 'sac_diff3': 0.010935783386230469, 'sac_diff4': 0.007122993469238281, 'sac_diff5': 0.03287172317504883, 'sac_diff6': 0.0003905296325683594, 'all': 0.06730294227600098}
diff5_list [0.006986141204833984, 0.0067043304443359375, 0.006481647491455078, 0.0062825679779052734, 0.006417036056518555]
time3 0
time4 0.06810450553894043
time5 0.06815242767333984
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6864)
policy weight change tensor(6.5120, grad_fn=<SumBackward0>)
time8 0.0019028186798095703
train_time 0.07885551452636719
eval time 0.46591687202453613
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:44,074 MainThread INFO: EPOCH:18
2024-01-23 00:58:44,075 MainThread INFO: Time Consumed:0.5482971668243408s
2024-01-23 00:58:44,075 MainThread INFO: Total Frames:3600s
  0%|          | 19/10000 [00:11<1:28:18,  1.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11588.38737
Train_Epoch_Reward                36616.99072
Running_Training_Average_Rewards  14400.71185
Explore_Time                      0.00095
Train___Time                      0.07886
Eval____Time                      0.46592
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11680.73536
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       101.09083    2.02405    104.44301    98.10230
alpha_0                           0.99074      0.00014    0.99094      0.99054
Alpha_loss                        -0.06159     0.00105    -0.06017     -0.06314
Training/policy_loss              -2.67188     0.01146    -2.66292     -2.69324
Training/qf1_loss                 20154.76211  976.36737  21784.27148  18901.09375
Training/qf2_loss                 20162.15313  976.41784  21791.61328  18908.41211
Training/pf_norm                  0.41969      0.03549    0.46295      0.35544
Training/qf1_norm                 246.05220    4.43249    253.11174    239.43907
Training/qf2_norm                 232.45038    4.44764    239.81055    225.87263
log_std/mean                      -0.02262     0.00052    -0.02190     -0.02337
log_probs/mean                    -2.69230     0.01191    -2.68294     -2.71440
mean/mean                         0.00125      0.00004    0.00130      0.00118
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018705368041992188
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71261
epoch first part time 3.337860107421875e-06
replay_buffer._size: [3900]
collect time 0.0009658336639404297
inner_dict_sum {'sac_diff0': 0.00020456314086914062, 'sac_diff1': 0.007325172424316406, 'sac_diff2': 0.008365869522094727, 'sac_diff3': 0.010912179946899414, 'sac_diff4': 0.0072977542877197266, 'sac_diff5': 0.03343987464904785, 'sac_diff6': 0.0003948211669921875, 'all': 0.06794023513793945}
diff5_list [0.007223844528198242, 0.006434202194213867, 0.0063703060150146484, 0.00640559196472168, 0.007005929946899414]
time3 0
time4 0.06873965263366699
time5 0.0687873363494873
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6864)
policy weight change tensor(7.1457, grad_fn=<SumBackward0>)
time8 0.0020384788513183594
train_time 0.07973074913024902
eval time 0.48076295852661133
epoch last part time 8.106231689453125e-06
2024-01-23 00:58:44,661 MainThread INFO: EPOCH:19
2024-01-23 00:58:44,661 MainThread INFO: Time Consumed:0.5639753341674805s
2024-01-23 00:58:44,661 MainThread INFO: Total Frames:3750s
  0%|          | 20/10000 [00:12<1:31:06,  1.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11600.25466
Train_Epoch_Reward                37520.32799
Running_Training_Average_Rewards  15325.49650
Explore_Time                      0.00096
Train___Time                      0.07973
Eval____Time                      0.48076
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11716.48120
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       102.50552    4.05384     108.76555    97.55891
alpha_0                           0.99024      0.00014     0.99044      0.99005
Alpha_loss                        -0.06501     0.00092     -0.06375     -0.06628
Training/policy_loss              -2.67752     0.00656     -2.67008     -2.68650
Training/qf1_loss                 20311.26758  1852.13370  23287.74805  18257.41016
Training/qf2_loss                 20318.76250  1852.37771  23295.51953  18264.54102
Training/pf_norm                  0.39872      0.03430     0.45608      0.34976
Training/qf1_norm                 244.11163    9.23174     258.12064    232.55962
Training/qf2_norm                 239.13716    9.12141     253.23360    228.02536
log_std/mean                      -0.02244     0.00047     -0.02179     -0.02313
log_probs/mean                    -2.69926     0.00642     -2.69192     -2.70779
mean/mean                         0.00173      0.00004     0.00177      0.00167
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019649744033813477
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71261
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [4050]
collect time 0.0008857250213623047
inside mustsac before update, task 0, sumup 71261
inside mustsac after update, task 0, sumup 70024
inner_dict_sum {'sac_diff0': 0.00021910667419433594, 'sac_diff1': 0.0077478885650634766, 'sac_diff2': 0.00903177261352539, 'sac_diff3': 0.011564016342163086, 'sac_diff4': 0.007936954498291016, 'sac_diff5': 0.0543217658996582, 'sac_diff6': 0.0004916191101074219, 'all': 0.09131312370300293}
diff5_list [0.011291980743408203, 0.010285615921020508, 0.010634422302246094, 0.010785341262817383, 0.011324405670166016]
time3 0.0009281635284423828
time4 0.09231925010681152
time5 0.09237408638000488
time7 0.00945901870727539
gen_weight_change tensor(-18.9865)
policy weight change tensor(7.6532, grad_fn=<SumBackward0>)
time8 0.002833127975463867
train_time 0.12322354316711426
eval time 0.4349980354309082
epoch last part time 8.58306884765625e-06
2024-01-23 00:58:45,245 MainThread INFO: EPOCH:20
2024-01-23 00:58:45,246 MainThread INFO: Time Consumed:0.5616683959960938s
2024-01-23 00:58:45,246 MainThread INFO: Total Frames:3900s
  0%|          | 21/10000 [00:12<1:33:04,  1.79it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11615.79520
Train_Epoch_Reward                14239.42939
Running_Training_Average_Rewards  15283.72468
Explore_Time                      0.00088
Train___Time                      0.12322
Eval____Time                      0.43500
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11794.04750
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.72433     2.83284     102.14110    94.59453
alpha_0                           0.98975      0.00014     0.98995      0.98955
Alpha_loss                        -0.06830     0.00091     -0.06712     -0.06970
Training/policy_loss              -2.67133     0.01025     -2.65831     -2.68808
Training/qf1_loss                 18527.08437  1235.77395  20321.44141  17356.36328
Training/qf2_loss                 18534.23477  1235.70406  20328.38086  17363.41992
Training/pf_norm                  0.43153      0.04435     0.47180      0.35155
Training/qf1_norm                 237.26815    5.55320     243.01266    229.83832
Training/qf2_norm                 228.26682    8.06145     239.99774    217.13640
log_std/mean                      -0.02536     0.00085     -0.02416     -0.02657
log_probs/mean                    -2.69324     0.01039     -2.67940     -2.70968
mean/mean                         0.00191      0.00022     0.00210      0.00149
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018951892852783203
epoch last part time3 0.002905130386352539
inside rlalgo, task 0, sumup 70024
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [4200]
collect time 0.0010159015655517578
inner_dict_sum {'sac_diff0': 0.00021576881408691406, 'sac_diff1': 0.007834911346435547, 'sac_diff2': 0.009082555770874023, 'sac_diff3': 0.011834144592285156, 'sac_diff4': 0.008152961730957031, 'sac_diff5': 0.03675580024719238, 'sac_diff6': 0.00041985511779785156, 'all': 0.0742959976196289}
diff5_list [0.007458209991455078, 0.0070343017578125, 0.0072634220123291016, 0.007189273834228516, 0.0078105926513671875]
time3 0
time4 0.07517123222351074
time5 0.07522416114807129
time7 1.1920928955078125e-06
gen_weight_change tensor(-18.9865)
policy weight change tensor(8.2819, grad_fn=<SumBackward0>)
time8 0.0023260116577148438
train_time 0.08713889122009277
eval time 0.5135204792022705
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:45,875 MainThread INFO: EPOCH:21
2024-01-23 00:58:45,875 MainThread INFO: Time Consumed:0.604311466217041s
2024-01-23 00:58:45,875 MainThread INFO: Total Frames:4050s
  0%|          | 22/10000 [00:13<1:36:26,  1.72it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11652.15919
Train_Epoch_Reward                9435.02070
Running_Training_Average_Rewards  15067.10602
Explore_Time                      0.00101
Train___Time                      0.08714
Eval____Time                      0.51352
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11857.92614
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       102.13350    1.30945    104.38320    100.74150
alpha_0                           0.98925      0.00014    0.98945      0.98906
Alpha_loss                        -0.07163     0.00093    -0.07035     -0.07297
Training/policy_loss              -2.66785     0.00309    -2.66584     -2.67396
Training/qf1_loss                 19799.83320  807.16609  21061.10547  18561.81641
Training/qf2_loss                 19808.45898  807.30826  21069.75977  18570.02344
Training/pf_norm                  0.41464      0.02038    0.43465      0.38464
Training/qf1_norm                 253.71037    3.16821    258.76447    249.16628
Training/qf2_norm                 237.35749    2.99048    242.46062    234.15659
log_std/mean                      -0.02570     0.00051    -0.02500     -0.02643
log_probs/mean                    -2.69187     0.00291    -2.68984     -2.69750
mean/mean                         0.00214      0.00004    0.00218      0.00208
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01972365379333496
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70024
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [4350]
collect time 0.0009710788726806641
inner_dict_sum {'sac_diff0': 0.0002155303955078125, 'sac_diff1': 0.0073239803314208984, 'sac_diff2': 0.008493185043334961, 'sac_diff3': 0.010458230972290039, 'sac_diff4': 0.0072901248931884766, 'sac_diff5': 0.03265547752380371, 'sac_diff6': 0.00039839744567871094, 'all': 0.06683492660522461}
diff5_list [0.007057666778564453, 0.006599903106689453, 0.006345272064208984, 0.0062482357025146484, 0.006404399871826172]
time3 0
time4 0.06761908531188965
time5 0.06766653060913086
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9865)
policy weight change tensor(8.9180, grad_fn=<SumBackward0>)
time8 0.0020017623901367188
train_time 0.07855677604675293
eval time 0.511688232421875
epoch last part time 8.106231689453125e-06
2024-01-23 00:58:46,492 MainThread INFO: EPOCH:22
2024-01-23 00:58:46,492 MainThread INFO: Time Consumed:0.593745231628418s
2024-01-23 00:58:46,493 MainThread INFO: Total Frames:4200s
  0%|          | 23/10000 [00:14<1:38:16,  1.69it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11691.87198
Train_Epoch_Reward                26208.46964
Running_Training_Average_Rewards  15465.01186
Explore_Time                      0.00097
Train___Time                      0.07856
Eval____Time                      0.51169
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11891.78567
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       102.50845    2.76436    105.30640    97.56139
alpha_0                           0.98876      0.00014    0.98896      0.98856
Alpha_loss                        -0.07507     0.00098    -0.07374     -0.07635
Training/policy_loss              -2.67558     0.00977    -2.66139     -2.69020
Training/qf1_loss                 20019.43281  721.16665  21035.09766  19065.37500
Training/qf2_loss                 20028.14922  721.21302  21043.94141  19073.84180
Training/pf_norm                  0.37046      0.02681    0.42073      0.34616
Training/qf1_norm                 256.27035    6.27945    262.53516    244.69852
Training/qf2_norm                 234.55293    6.07814    240.78455    223.65401
log_std/mean                      -0.02783     0.00051    -0.02714     -0.02857
log_probs/mean                    -2.70016     0.00996    -2.68554     -2.71523
mean/mean                         0.00221      0.00003    0.00224      0.00215
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019533157348632812
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70024
epoch first part time 4.291534423828125e-06
replay_buffer._size: [4500]
collect time 0.0009121894836425781
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.007066965103149414, 'sac_diff2': 0.00825047492980957, 'sac_diff3': 0.010606050491333008, 'sac_diff4': 0.007237434387207031, 'sac_diff5': 0.032175540924072266, 'sac_diff6': 0.0003876686096191406, 'all': 0.06593155860900879}
diff5_list [0.006982088088989258, 0.006420612335205078, 0.006258964538574219, 0.0062367916107177734, 0.0062770843505859375]
time3 0
time4 0.06670832633972168
time5 0.06675601005554199
time7 9.5367431640625e-07
gen_weight_change tensor(-18.9865)
policy weight change tensor(9.5647, grad_fn=<SumBackward0>)
time8 0.0020024776458740234
train_time 0.07759714126586914
eval time 0.6934773921966553
epoch last part time 1.430511474609375e-05
2024-01-23 00:58:47,290 MainThread INFO: EPOCH:23
2024-01-23 00:58:47,291 MainThread INFO: Time Consumed:0.7750484943389893s
2024-01-23 00:58:47,291 MainThread INFO: Total Frames:4350s
  0%|          | 24/10000 [00:14<1:48:41,  1.53it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11736.36950
Train_Epoch_Reward                9826.42582
Running_Training_Average_Rewards  15270.57786
Explore_Time                      0.00091
Train___Time                      0.07760
Eval____Time                      0.69348
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11954.88219
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       102.20001    2.25925     104.46375    98.81979
alpha_0                           0.98826      0.00014     0.98846      0.98807
Alpha_loss                        -0.07847     0.00097     -0.07702     -0.07984
Training/policy_loss              -2.67809     0.00711     -2.66971     -2.69004
Training/qf1_loss                 19618.84453  1026.40414  21037.01172  18270.96680
Training/qf2_loss                 19628.16563  1026.52858  21046.51367  18280.17188
Training/pf_norm                  0.37111      0.02217     0.40394      0.34422
Training/qf1_norm                 254.41706    5.70134     259.80289    246.82570
Training/qf2_norm                 229.46486    4.92940     234.32109    222.11487
log_std/mean                      -0.03029     0.00054     -0.02955     -0.03107
log_probs/mean                    -2.70360     0.00727     -2.69459     -2.71543
mean/mean                         0.00193      0.00004     0.00198      0.00188
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021458864212036133
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70024
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [4650]
collect time 0.0009148120880126953
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.007236003875732422, 'sac_diff2': 0.008867502212524414, 'sac_diff3': 0.01042032241821289, 'sac_diff4': 0.007125139236450195, 'sac_diff5': 0.0317838191986084, 'sac_diff6': 0.00039505958557128906, 'all': 0.06603860855102539}
diff5_list [0.006890535354614258, 0.006516456604003906, 0.006319761276245117, 0.006251811981201172, 0.005805253982543945]
time3 0
time4 0.06679821014404297
time5 0.06685018539428711
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9865)
policy weight change tensor(10.1931, grad_fn=<SumBackward0>)
time8 0.0019102096557617188
train_time 0.07819390296936035
eval time 0.5286242961883545
epoch last part time 7.62939453125e-06
2024-01-23 00:58:47,926 MainThread INFO: EPOCH:24
2024-01-23 00:58:47,926 MainThread INFO: Time Consumed:0.6103963851928711s
2024-01-23 00:58:47,926 MainThread INFO: Total Frames:4500s
  0%|          | 25/10000 [00:15<1:47:45,  1.54it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11783.44731
Train_Epoch_Reward                18481.12063
Running_Training_Average_Rewards  15377.59595
Explore_Time                      0.00091
Train___Time                      0.07819
Eval____Time                      0.52862
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11987.05373
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       102.42710    2.32340    105.70116    98.64302
alpha_0                           0.98777      0.00014    0.98797      0.98757
Alpha_loss                        -0.08183     0.00097    -0.08041     -0.08317
Training/policy_loss              -2.67851     0.00371    -2.67307     -2.68377
Training/qf1_loss                 19917.75234  729.17204  20922.54297  18836.79102
Training/qf2_loss                 19926.41211  729.52337  20931.79883  18844.98047
Training/pf_norm                  0.37594      0.03332    0.42414      0.33561
Training/qf1_norm                 244.31375    5.95141    252.97285    234.99361
Training/qf2_norm                 239.49947    5.24257    246.89818    230.98795
log_std/mean                      -0.03084     0.00053    -0.03010     -0.03160
log_probs/mean                    -2.70416     0.00387    -2.69812     -2.70921
mean/mean                         0.00185      0.00003    0.00191      0.00182
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02088761329650879
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70024
epoch first part time 3.814697265625e-06
replay_buffer._size: [4800]
collect time 0.0009014606475830078
inside mustsac before update, task 0, sumup 70024
inside mustsac after update, task 0, sumup 70144
inner_dict_sum {'sac_diff0': 0.00020551681518554688, 'sac_diff1': 0.012396812438964844, 'sac_diff2': 0.013718605041503906, 'sac_diff3': 0.013206005096435547, 'sac_diff4': 0.009459257125854492, 'sac_diff5': 0.05753779411315918, 'sac_diff6': 0.0004458427429199219, 'all': 0.10696983337402344}
diff5_list [0.011020183563232422, 0.010973930358886719, 0.009608983993530273, 0.009529352188110352, 0.016405344009399414]
time3 0.0008947849273681641
time4 0.10801410675048828
time5 0.10809683799743652
time7 0.08893060684204102
gen_weight_change tensor(-19.3523)
policy weight change tensor(10.7360, grad_fn=<SumBackward0>)
time8 0.002323627471923828
train_time 0.2177281379699707
eval time 0.5428316593170166
epoch last part time 1.5497207641601562e-05
2024-01-23 00:58:48,715 MainThread INFO: EPOCH:25
2024-01-23 00:58:48,715 MainThread INFO: Time Consumed:0.76405930519104s
2024-01-23 00:58:48,715 MainThread INFO: Total Frames:4650s
  0%|          | 26/10000 [00:16<1:54:43,  1.45it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11829.84661
Train_Epoch_Reward                7414.80884
Running_Training_Average_Rewards  14790.46308
Explore_Time                      0.00090
Train___Time                      0.21773
Eval____Time                      0.54283
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12006.74343
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       100.37835    1.67905    102.37689    98.61024
alpha_0                           0.98728      0.00014    0.98747      0.98708
Alpha_loss                        -0.08507     0.00090    -0.08382     -0.08630
Training/policy_loss              -2.66734     0.00699    -2.65797     -2.67600
Training/qf1_loss                 19495.17227  721.71109  20367.20117  18510.75000
Training/qf2_loss                 19505.16836  721.78007  20376.49609  18520.61328
Training/pf_norm                  0.40489      0.01820    0.42733      0.37605
Training/qf1_norm                 250.32738    5.18444    259.88248    245.23692
Training/qf2_norm                 230.39611    3.03654    235.80540    226.94574
log_std/mean                      -0.03535     0.00108    -0.03418     -0.03728
log_probs/mean                    -2.69538     0.00693    -2.68614     -2.70377
mean/mean                         0.00190      0.00057    0.00276      0.00099
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.020281076431274414
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70144
epoch first part time 5.0067901611328125e-06
replay_buffer._size: [4950]
collect time 0.0010519027709960938
inner_dict_sum {'sac_diff0': 0.00020551681518554688, 'sac_diff1': 0.007236480712890625, 'sac_diff2': 0.008219003677368164, 'sac_diff3': 0.010361194610595703, 'sac_diff4': 0.006788969039916992, 'sac_diff5': 0.03210806846618652, 'sac_diff6': 0.0003795623779296875, 'all': 0.06529879570007324}
diff5_list [0.007151603698730469, 0.006186008453369141, 0.006237030029296875, 0.006270170211791992, 0.006263256072998047]
time3 0
time4 0.06604671478271484
time5 0.06609249114990234
time7 7.152557373046875e-07
gen_weight_change tensor(-19.3523)
policy weight change tensor(11.3588, grad_fn=<SumBackward0>)
time8 0.0018873214721679688
train_time 0.07758164405822754
eval time 0.5468637943267822
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:49,366 MainThread INFO: EPOCH:26
2024-01-23 00:58:49,367 MainThread INFO: Time Consumed:0.6279914379119873s
2024-01-23 00:58:49,367 MainThread INFO: Total Frames:4800s
  0%|          | 27/10000 [00:16<1:52:45,  1.47it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11859.47651
Train_Epoch_Reward                6418.00767
Running_Training_Average_Rewards  14898.29389
Explore_Time                      0.00104
Train___Time                      0.07758
Eval____Time                      0.54686
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12006.54852
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.28265     2.44275     103.70612    96.60572
alpha_0                           0.98678      0.00014     0.98698      0.98658
Alpha_loss                        -0.08868     0.00103     -0.08723     -0.09015
Training/policy_loss              -2.68611     0.00571     -2.67934     -2.69484
Training/qf1_loss                 18667.94336  1213.46151  20853.06641  17425.23828
Training/qf2_loss                 18677.25078  1213.49341  20862.43555  17434.78516
Training/pf_norm                  0.36112      0.02793     0.41450      0.33820
Training/qf1_norm                 242.51931    5.45394     252.01183    235.45068
Training/qf2_norm                 235.17497    5.58088     245.27448    229.09361
log_std/mean                      -0.03620     0.00059     -0.03539     -0.03705
log_probs/mean                    -2.71430     0.00615     -2.70689     -2.72369
mean/mean                         0.00135      0.00001     0.00136      0.00134
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019385099411010742
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70144
epoch first part time 3.337860107421875e-06
replay_buffer._size: [5100]
collect time 0.0009708404541015625
inner_dict_sum {'sac_diff0': 0.0001983642578125, 'sac_diff1': 0.0071794986724853516, 'sac_diff2': 0.008799076080322266, 'sac_diff3': 0.011543512344360352, 'sac_diff4': 0.0072479248046875, 'sac_diff5': 0.03225231170654297, 'sac_diff6': 0.000385284423828125, 'all': 0.06760597229003906}
diff5_list [0.006608486175537109, 0.006267547607421875, 0.006292819976806641, 0.006684064865112305, 0.006399393081665039]
time3 0
time4 0.06839513778686523
time5 0.06844019889831543
time7 9.5367431640625e-07
gen_weight_change tensor(-19.3523)
policy weight change tensor(11.9920, grad_fn=<SumBackward0>)
time8 0.0019414424896240234
train_time 0.07903718948364258
eval time 0.5696167945861816
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:50,042 MainThread INFO: EPOCH:27
2024-01-23 00:58:50,042 MainThread INFO: Time Consumed:0.6521871089935303s
2024-01-23 00:58:50,042 MainThread INFO: Total Frames:4950s
  0%|          | 28/10000 [00:17<1:52:35,  1.48it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11891.49568
Train_Epoch_Reward                15163.96015
Running_Training_Average_Rewards  14621.51156
Explore_Time                      0.00097
Train___Time                      0.07904
Eval____Time                      0.56962
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12018.75304
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       101.58828    1.90155    103.75254    98.50697
alpha_0                           0.98629      0.00014    0.98648      0.98609
Alpha_loss                        -0.09193     0.00105    -0.09054     -0.09351
Training/policy_loss              -2.67750     0.00889    -2.66903     -2.69392
Training/qf1_loss                 19270.40078  662.70697  20265.92578  18393.93945
Training/qf2_loss                 19281.43320  662.82399  20276.82422  18404.45508
Training/pf_norm                  0.36463      0.03438    0.39772      0.30013
Training/qf1_norm                 253.35872    4.69909    257.41953    245.22487
Training/qf2_norm                 233.21696    4.19267    237.90625    226.40361
log_std/mean                      -0.03752     0.00060    -0.03671     -0.03839
log_probs/mean                    -2.70652     0.00931    -2.69793     -2.72371
mean/mean                         0.00244      0.00005    0.00251      0.00237
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019400358200073242
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70144
epoch first part time 3.337860107421875e-06
replay_buffer._size: [5250]
collect time 0.0009548664093017578
inner_dict_sum {'sac_diff0': 0.0002028942108154297, 'sac_diff1': 0.007256269454956055, 'sac_diff2': 0.008641481399536133, 'sac_diff3': 0.011338472366333008, 'sac_diff4': 0.007231235504150391, 'sac_diff5': 0.03409457206726074, 'sac_diff6': 0.0003998279571533203, 'all': 0.06916475296020508}
diff5_list [0.006846427917480469, 0.0068132877349853516, 0.006338596343994141, 0.00648045539855957, 0.007615804672241211]
time3 0
time4 0.06998419761657715
time5 0.07003521919250488
time7 7.152557373046875e-07
gen_weight_change tensor(-19.3523)
policy weight change tensor(12.5969, grad_fn=<SumBackward0>)
time8 0.0020143985748291016
train_time 0.08109331130981445
eval time 0.5728914737701416
epoch last part time 7.3909759521484375e-06
2024-01-23 00:58:50,722 MainThread INFO: EPOCH:28
2024-01-23 00:58:50,723 MainThread INFO: Time Consumed:0.6574554443359375s
2024-01-23 00:58:50,723 MainThread INFO: Total Frames:5100s
  0%|          | 29/10000 [00:18<1:52:43,  1.47it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11926.17221
Train_Epoch_Reward                19907.70526
Running_Training_Average_Rewards  15093.89924
Explore_Time                      0.00095
Train___Time                      0.08109
Eval____Time                      0.57289
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12027.50069
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       100.31202    1.98467    103.70619    98.17864
alpha_0                           0.98579      0.00014    0.98599      0.98560
Alpha_loss                        -0.09535     0.00098    -0.09406     -0.09677
Training/policy_loss              -2.67906     0.00509    -2.67016     -2.68344
Training/qf1_loss                 18403.95391  644.10713  19450.71484  17563.44141
Training/qf2_loss                 18415.37617  644.36748  19462.91016  17574.88867
Training/pf_norm                  0.35949      0.02986    0.39903      0.32887
Training/qf1_norm                 250.59274    5.23878    260.17957    245.34258
Training/qf2_norm                 234.61869    4.41817    242.20782    229.85248
log_std/mean                      -0.03819     0.00056    -0.03742     -0.03901
log_probs/mean                    -2.71090     0.00530    -2.70159     -2.71588
mean/mean                         0.00149      0.00002    0.00153      0.00146
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01880359649658203
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70144
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [5400]
collect time 0.0009455680847167969
inner_dict_sum {'sac_diff0': 0.00019884109497070312, 'sac_diff1': 0.0072994232177734375, 'sac_diff2': 0.008337736129760742, 'sac_diff3': 0.010703325271606445, 'sac_diff4': 0.007017374038696289, 'sac_diff5': 0.03228020668029785, 'sac_diff6': 0.00038814544677734375, 'all': 0.06622505187988281}
diff5_list [0.0068569183349609375, 0.0064945220947265625, 0.006384134292602539, 0.006209373474121094, 0.006335258483886719]
time3 0
time4 0.0670022964477539
time5 0.0670473575592041
time7 4.76837158203125e-07
gen_weight_change tensor(-19.3523)
policy weight change tensor(13.1937, grad_fn=<SumBackward0>)
time8 0.0019664764404296875
train_time 0.07787895202636719
eval time 0.5856409072875977
epoch last part time 2.5987625122070312e-05
2024-01-23 00:58:51,413 MainThread INFO: EPOCH:29
2024-01-23 00:58:51,414 MainThread INFO: Time Consumed:0.668522834777832s
2024-01-23 00:58:51,414 MainThread INFO: Total Frames:5250s
  0%|          | 30/10000 [00:19<1:53:21,  1.47it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11958.57547
Train_Epoch_Reward                18356.49861
Running_Training_Average_Rewards  15548.36503
Explore_Time                      0.00094
Train___Time                      0.07788
Eval____Time                      0.58564
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12040.51379
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       102.47117    1.63817     104.46733    100.14208
alpha_0                           0.98530      0.00014     0.98550      0.98510
Alpha_loss                        -0.09851     0.00088     -0.09733     -0.09986
Training/policy_loss              -2.66397     0.00771     -2.65558     -2.67578
Training/qf1_loss                 18709.98164  1299.51259  20412.85742  16909.37891
Training/qf2_loss                 18721.95625  1299.43971  20424.69727  16921.47266
Training/pf_norm                  0.39371      0.03960     0.45547      0.33290
Training/qf1_norm                 254.95188    3.20032     258.69754    250.50644
Training/qf2_norm                 240.54623    3.68456     244.96948    235.30939
log_std/mean                      -0.03998     0.00057     -0.03920     -0.04082
log_probs/mean                    -2.69675     0.00757     -2.68823     -2.70819
mean/mean                         0.00131      0.00005     0.00138      0.00124
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0194091796875
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70144
epoch first part time 5.7220458984375e-06
replay_buffer._size: [5550]
collect time 0.0010013580322265625
inside mustsac before update, task 0, sumup 70144
inside mustsac after update, task 0, sumup 70708
inner_dict_sum {'sac_diff0': 0.00020933151245117188, 'sac_diff1': 0.007449626922607422, 'sac_diff2': 0.008946418762207031, 'sac_diff3': 0.011325359344482422, 'sac_diff4': 0.007658243179321289, 'sac_diff5': 0.05189394950866699, 'sac_diff6': 0.0004374980926513672, 'all': 0.0879204273223877}
diff5_list [0.010541915893554688, 0.009754180908203125, 0.011755704879760742, 0.010164499282836914, 0.009677648544311523]
time3 0.0008940696716308594
time4 0.08884406089782715
time5 0.0888967514038086
time7 0.015348434448242188
gen_weight_change tensor(-19.7186)
policy weight change tensor(13.7502, grad_fn=<SumBackward0>)
time8 0.0027632713317871094
train_time 0.12534594535827637
eval time 0.5581965446472168
epoch last part time 7.867813110351562e-06
2024-01-23 00:58:52,124 MainThread INFO: EPOCH:30
2024-01-23 00:58:52,124 MainThread INFO: Time Consumed:0.6870434284210205s
2024-01-23 00:58:52,124 MainThread INFO: Total Frames:5400s
  0%|          | 31/10000 [00:19<1:54:52,  1.45it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11981.74021
Train_Epoch_Reward                14397.12110
Running_Training_Average_Rewards  14772.29198
Explore_Time                      0.00099
Train___Time                      0.12535
Eval____Time                      0.55820
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12025.69484
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.08136     2.52579    98.72034     92.08589
alpha_0                           0.98481      0.00014    0.98500      0.98461
Alpha_loss                        -0.10210     0.00112    -0.10075     -0.10375
Training/policy_loss              -2.67858     0.01467    -2.65704     -2.69819
Training/qf1_loss                 17124.71055  991.22514  18029.73047  15252.31445
Training/qf2_loss                 17137.15176  991.20069  18041.92383  15264.74707
Training/pf_norm                  0.32897      0.05997    0.41366      0.23294
Training/qf1_norm                 248.29715    7.69471    258.32727    237.79445
Training/qf2_norm                 221.79940    8.54869    226.87462    204.77568
log_std/mean                      -0.04236     0.00184    -0.03929     -0.04500
log_probs/mean                    -2.71225     0.01480    -2.69012     -2.73241
mean/mean                         0.00120      0.00014    0.00135      0.00094
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018785953521728516
epoch last part time3 0.002955913543701172
inside rlalgo, task 0, sumup 70708
epoch first part time 3.337860107421875e-06
replay_buffer._size: [5700]
collect time 0.0009775161743164062
inner_dict_sum {'sac_diff0': 0.0002079010009765625, 'sac_diff1': 0.007071256637573242, 'sac_diff2': 0.008407831192016602, 'sac_diff3': 0.010499954223632812, 'sac_diff4': 0.007188558578491211, 'sac_diff5': 0.03236794471740723, 'sac_diff6': 0.0003867149353027344, 'all': 0.06613016128540039}
diff5_list [0.007148265838623047, 0.006656646728515625, 0.006182670593261719, 0.006289243698120117, 0.006091117858886719]
time3 0
time4 0.06688857078552246
time5 0.06693458557128906
time7 4.76837158203125e-07
gen_weight_change tensor(-19.7186)
policy weight change tensor(14.3647, grad_fn=<SumBackward0>)
time8 0.0021080970764160156
train_time 0.07830524444580078
eval time 0.6463367938995361
epoch last part time 6.4373016357421875e-06
2024-01-23 00:58:52,877 MainThread INFO: EPOCH:31
2024-01-23 00:58:52,877 MainThread INFO: Time Consumed:0.7281651496887207s
2024-01-23 00:58:52,877 MainThread INFO: Total Frames:5550s
  0%|          | 32/10000 [00:20<1:57:47,  1.41it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12007.90706
Train_Epoch_Reward                1747.83755
Running_Training_Average_Rewards  14401.45108
Explore_Time                      0.00097
Train___Time                      0.07831
Eval____Time                      0.64634
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12119.59467
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.00315     1.99454     99.29320     93.56560
alpha_0                           0.98431      0.00014     0.98451      0.98412
Alpha_loss                        -0.10527     0.00090     -0.10396     -0.10658
Training/policy_loss              -2.66663     0.00552     -2.66033     -2.67604
Training/qf1_loss                 17190.10508  1261.28994  19509.06055  15685.02148
Training/qf2_loss                 17202.54258  1261.70527  19522.36133  15697.14258
Training/pf_norm                  0.37575      0.02424     0.40693      0.33869
Training/qf1_norm                 247.36426    5.48447     257.12552    241.23691
Training/qf2_norm                 226.59138    4.52872     234.07358    221.11420
log_std/mean                      -0.04166     0.00057     -0.04088     -0.04249
log_probs/mean                    -2.70028     0.00535     -2.69415     -2.70956
mean/mean                         0.00153      0.00005     0.00161      0.00147
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01888418197631836
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70708
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [5850]
collect time 0.0008759498596191406
inner_dict_sum {'sac_diff0': 0.00021195411682128906, 'sac_diff1': 0.00728297233581543, 'sac_diff2': 0.008623600006103516, 'sac_diff3': 0.011504173278808594, 'sac_diff4': 0.007474422454833984, 'sac_diff5': 0.03510594367980957, 'sac_diff6': 0.00040435791015625, 'all': 0.07060742378234863}
diff5_list [0.006505489349365234, 0.006281614303588867, 0.007163047790527344, 0.007620573043823242, 0.007535219192504883]
time3 0
time4 0.07143807411193848
time5 0.07148933410644531
time7 7.152557373046875e-07
gen_weight_change tensor(-19.7186)
policy weight change tensor(14.9932, grad_fn=<SumBackward0>)
time8 0.001955270767211914
train_time 0.08207130432128906
eval time 0.6525866985321045
epoch last part time 8.58306884765625e-06
2024-01-23 00:58:53,637 MainThread INFO: EPOCH:32
2024-01-23 00:58:53,638 MainThread INFO: Time Consumed:0.7381861209869385s
2024-01-23 00:58:53,638 MainThread INFO: Total Frames:5700s
  0%|          | 33/10000 [00:21<2:00:27,  1.38it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12022.06045
Train_Epoch_Reward                16111.49389
Running_Training_Average_Rewards  14743.20579
Explore_Time                      0.00087
Train___Time                      0.08207
Eval____Time                      0.65259
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12033.31963
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.06163     0.85259    98.74155     96.44354
alpha_0                           0.98382      0.00014    0.98402      0.98362
Alpha_loss                        -0.10884     0.00092    -0.10744     -0.11018
Training/policy_loss              -2.67729     0.01082    -2.65883     -2.68912
Training/qf1_loss                 17655.04609  561.66337  18523.37109  16989.68555
Training/qf2_loss                 17669.01094  561.63043  18537.01953  17003.58008
Training/pf_norm                  0.32324      0.04145    0.37149      0.26935
Training/qf1_norm                 248.51261    2.53770    253.35361    246.26366
Training/qf2_norm                 224.58828    1.92605    228.38803    223.17145
log_std/mean                      -0.04829     0.00065    -0.04740     -0.04923
log_probs/mean                    -2.71332     0.01088    -2.69482     -2.72534
mean/mean                         0.00127      0.00001    0.00129      0.00124
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0211484432220459
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70708
epoch first part time 2.86102294921875e-06
replay_buffer._size: [6000]
collect time 0.0008113384246826172
inner_dict_sum {'sac_diff0': 0.00021696090698242188, 'sac_diff1': 0.007935523986816406, 'sac_diff2': 0.009474992752075195, 'sac_diff3': 0.012125015258789062, 'sac_diff4': 0.008010625839233398, 'sac_diff5': 0.03680849075317383, 'sac_diff6': 0.000415802001953125, 'all': 0.07498741149902344}
diff5_list [0.0075418949127197266, 0.007031679153442383, 0.007196187973022461, 0.0077970027923583984, 0.007241725921630859]
time3 0
time4 0.07590222358703613
time5 0.0759577751159668
time7 7.152557373046875e-07
gen_weight_change tensor(-19.7186)
policy weight change tensor(15.6113, grad_fn=<SumBackward0>)
time8 0.0019655227661132812
train_time 0.0868380069732666
eval time 0.6724939346313477
epoch last part time 8.106231689453125e-06
2024-01-23 00:58:54,424 MainThread INFO: EPOCH:33
2024-01-23 00:58:54,425 MainThread INFO: Time Consumed:0.7628118991851807s
2024-01-23 00:58:54,425 MainThread INFO: Total Frames:5850s
  0%|          | 34/10000 [00:22<2:03:31,  1.34it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12026.48523
Train_Epoch_Reward                4943.41360
Running_Training_Average_Rewards  14768.80644
Explore_Time                      0.00081
Train___Time                      0.08684
Eval____Time                      0.67249
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11999.12998
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.18917     2.27957     98.25712     91.32113
alpha_0                           0.98333      0.00014     0.98352      0.98313
Alpha_loss                        -0.11209     0.00086     -0.11089     -0.11331
Training/policy_loss              -2.67172     0.00737     -2.66271     -2.68078
Training/qf1_loss                 15910.56973  1182.07441  17988.99414  14766.27344
Training/qf2_loss                 15923.27637  1182.22527  18002.21680  14779.02539
Training/pf_norm                  0.31722      0.03087     0.36820      0.28498
Training/qf1_norm                 239.99918    5.41260     249.96004    234.18791
Training/qf2_norm                 220.99083    5.08755     230.04959    214.58699
log_std/mean                      -0.04595     0.00056     -0.04519     -0.04676
log_probs/mean                    -2.70664     0.00721     -2.69748     -2.71532
mean/mean                         0.00067      0.00002     0.00070      0.00065
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020870447158813477
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70708
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [6150]
collect time 0.0008819103240966797
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.007394075393676758, 'sac_diff2': 0.008974552154541016, 'sac_diff3': 0.011349916458129883, 'sac_diff4': 0.007409811019897461, 'sac_diff5': 0.03377938270568848, 'sac_diff6': 0.0004048347473144531, 'all': 0.06951761245727539}
diff5_list [0.007096767425537109, 0.0069866180419921875, 0.006947040557861328, 0.006300210952758789, 0.0064487457275390625]
time3 0
time4 0.07029938697814941
time5 0.07034659385681152
time7 7.152557373046875e-07
gen_weight_change tensor(-19.7186)
policy weight change tensor(16.2335, grad_fn=<SumBackward0>)
time8 0.0019528865814208984
train_time 0.0812387466430664
eval time 0.6788637638092041
epoch last part time 7.152557373046875e-06
2024-01-23 00:58:55,212 MainThread INFO: EPOCH:34
2024-01-23 00:58:55,213 MainThread INFO: Time Consumed:0.7634963989257812s
2024-01-23 00:58:55,213 MainThread INFO: Total Frames:6000s
  0%|          | 35/10000 [00:22<2:05:48,  1.32it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12027.95488
Train_Epoch_Reward                14246.22791
Running_Training_Average_Rewards  14903.13909
Explore_Time                      0.00088
Train___Time                      0.08124
Eval____Time                      0.67886
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12001.75021
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       96.57507     1.13298    98.29057     95.09145
alpha_0                           0.98283      0.00014    0.98303      0.98264
Alpha_loss                        -0.11566     0.00087    -0.11423     -0.11665
Training/policy_loss              -2.68133     0.01192    -2.66055     -2.69399
Training/qf1_loss                 17368.78398  691.79971  18394.55078  16563.89453
Training/qf2_loss                 17384.71719  691.86615  18410.97852  16580.18555
Training/pf_norm                  0.29774      0.01784    0.33057      0.28177
Training/qf1_norm                 260.68862    2.97148    265.85391    256.66202
Training/qf2_norm                 228.14962    2.56888    232.08484    224.82021
log_std/mean                      -0.04801     0.00060    -0.04718     -0.04889
log_probs/mean                    -2.71874     0.01195    -2.69814     -2.73163
mean/mean                         0.00062      0.00007    0.00070      0.00052
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0226287841796875
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70708
epoch first part time 7.3909759521484375e-06
replay_buffer._size: [6300]
collect time 0.001424551010131836
inside mustsac before update, task 0, sumup 70708
inside mustsac after update, task 0, sumup 71313
inner_dict_sum {'sac_diff0': 0.00021529197692871094, 'sac_diff1': 0.008529424667358398, 'sac_diff2': 0.010637283325195312, 'sac_diff3': 0.012626171112060547, 'sac_diff4': 0.008831262588500977, 'sac_diff5': 0.056690216064453125, 'sac_diff6': 0.0004355907440185547, 'all': 0.09796524047851562}
diff5_list [0.01443624496459961, 0.010948896408081055, 0.010231733322143555, 0.010716676712036133, 0.010356664657592773]
time3 0.0009326934814453125
time4 0.09888720512390137
time5 0.09894943237304688
time7 0.08295059204101562
gen_weight_change tensor(-20.1900)
policy weight change tensor(16.7924, grad_fn=<SumBackward0>)
time8 0.0023279190063476562
train_time 0.20385241508483887
eval time 0.6910252571105957
epoch last part time 7.152557373046875e-06
2024-01-23 00:58:56,138 MainThread INFO: EPOCH:35
2024-01-23 00:58:56,139 MainThread INFO: Time Consumed:0.8988254070281982s
2024-01-23 00:58:56,139 MainThread INFO: Total Frames:6150s
  0%|          | 36/10000 [00:23<2:14:00,  1.24it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12027.21219
Train_Epoch_Reward                3074.05365
Running_Training_Average_Rewards  14815.17767
Explore_Time                      0.00142
Train___Time                      0.20385
Eval____Time                      0.69103
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11999.31654
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.86342     1.57110    95.48549     91.28242
alpha_0                           0.98234      0.00014    0.98254      0.98215
Alpha_loss                        -0.11904     0.00099    -0.11751     -0.12029
Training/policy_loss              -2.68067     0.00658    -2.67034     -2.68715
Training/qf1_loss                 16505.68555  648.73507  17122.26562  15544.96582
Training/qf2_loss                 16519.70137  649.21054  17138.67578  15558.51367
Training/pf_norm                  0.29287      0.03241    0.33461      0.24091
Training/qf1_norm                 241.55304    10.09318   258.98480    230.22849
Training/qf2_norm                 215.63338    5.25680    225.59351    210.08264
log_std/mean                      -0.05181     0.00200    -0.04983     -0.05553
log_probs/mean                    -2.71955     0.00674    -2.70896     -2.72614
mean/mean                         0.00084      0.00037    0.00129      0.00030
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019022464752197266
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71313
epoch first part time 3.337860107421875e-06
replay_buffer._size: [6450]
collect time 0.0008602142333984375
inner_dict_sum {'sac_diff0': 0.00019931793212890625, 'sac_diff1': 0.007315158843994141, 'sac_diff2': 0.008627653121948242, 'sac_diff3': 0.01114201545715332, 'sac_diff4': 0.007134675979614258, 'sac_diff5': 0.033025264739990234, 'sac_diff6': 0.0004093647003173828, 'all': 0.06785345077514648}
diff5_list [0.00687098503112793, 0.006520986557006836, 0.0063822269439697266, 0.006726264953613281, 0.006524801254272461]
time3 0
time4 0.06863999366760254
time5 0.06868934631347656
time7 7.152557373046875e-07
gen_weight_change tensor(-20.1900)
policy weight change tensor(17.4194, grad_fn=<SumBackward0>)
time8 0.0019307136535644531
train_time 0.07937073707580566
eval time 0.6957893371582031
epoch last part time 7.152557373046875e-06
2024-01-23 00:58:56,939 MainThread INFO: EPOCH:36
2024-01-23 00:58:56,940 MainThread INFO: Time Consumed:0.7784125804901123s
2024-01-23 00:58:56,940 MainThread INFO: Total Frames:6300s
  0%|          | 37/10000 [00:24<2:13:41,  1.24it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12011.53696
Train_Epoch_Reward                14405.83910
Running_Training_Average_Rewards  14914.95492
Explore_Time                      0.00086
Train___Time                      0.07937
Eval____Time                      0.69579
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11849.79619
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.42744     1.67780    94.24580     89.66241
alpha_0                           0.98185      0.00014    0.98205      0.98165
Alpha_loss                        -0.12229     0.00094    -0.12097     -0.12368
Training/policy_loss              -2.67410     0.00470    -2.66526     -2.67914
Training/qf1_loss                 16102.41133  833.01071  16826.12695  14557.64648
Training/qf2_loss                 16116.76797  833.35167  16840.71094  14571.20020
Training/pf_norm                  0.32087      0.03045    0.35434      0.26663
Training/qf1_norm                 245.78147    5.07627    252.02599    237.41302
Training/qf2_norm                 214.13043    3.77320    218.20352    207.90085
log_std/mean                      -0.05305     0.00063    -0.05219     -0.05396
log_probs/mean                    -2.71303     0.00470    -2.70429     -2.71817
mean/mean                         0.00035      0.00010    0.00047      0.00018
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018615007400512695
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71313
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [6600]
collect time 0.0009527206420898438
inner_dict_sum {'sac_diff0': 0.0002014636993408203, 'sac_diff1': 0.0071604251861572266, 'sac_diff2': 0.008390426635742188, 'sac_diff3': 0.010927200317382812, 'sac_diff4': 0.00733184814453125, 'sac_diff5': 0.03412175178527832, 'sac_diff6': 0.0004055500030517578, 'all': 0.06853866577148438}
diff5_list [0.00655674934387207, 0.006302356719970703, 0.006580352783203125, 0.0073468685150146484, 0.0073354244232177734]
time3 0
time4 0.0693202018737793
time5 0.06936860084533691
time7 9.5367431640625e-07
gen_weight_change tensor(-20.1900)
policy weight change tensor(18.0457, grad_fn=<SumBackward0>)
time8 0.0020613670349121094
train_time 0.08008098602294922
eval time 0.6796789169311523
epoch last part time 6.9141387939453125e-06
2024-01-23 00:58:57,725 MainThread INFO: EPOCH:37
2024-01-23 00:58:57,725 MainThread INFO: Time Consumed:0.7632105350494385s
2024-01-23 00:58:57,725 MainThread INFO: Total Frames:6450s
  0%|          | 38/10000 [00:25<2:12:41,  1.25it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11993.29472
Train_Epoch_Reward                2581.56189
Running_Training_Average_Rewards  14870.77999
Explore_Time                      0.00095
Train___Time                      0.08008
Eval____Time                      0.67968
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11836.33063
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.98271     1.37088    94.63342     91.33649
alpha_0                           0.98136      0.00014    0.98155      0.98116
Alpha_loss                        -0.12564     0.00104    -0.12426     -0.12724
Training/policy_loss              -2.67227     0.00706    -2.66564     -2.68513
Training/qf1_loss                 15683.08906  359.84429  16120.79492  15243.16699
Training/qf2_loss                 15699.32441  360.05739  16136.85742  15259.45117
Training/pf_norm                  0.34775      0.02783    0.39269      0.31386
Training/qf1_norm                 252.82956    3.90281    258.67831    247.63861
Training/qf2_norm                 221.82920    3.11288    225.52318    218.08438
log_std/mean                      -0.05409     0.00062    -0.05324     -0.05500
log_probs/mean                    -2.71208     0.00742    -2.70505     -2.72573
mean/mean                         0.00127      0.00011    0.00143      0.00112
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01867842674255371
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71313
epoch first part time 1.3589859008789062e-05
replay_buffer._size: [6750]
collect time 0.0009493827819824219
inner_dict_sum {'sac_diff0': 0.00019884109497070312, 'sac_diff1': 0.007056236267089844, 'sac_diff2': 0.008160591125488281, 'sac_diff3': 0.010138750076293945, 'sac_diff4': 0.006847858428955078, 'sac_diff5': 0.03232145309448242, 'sac_diff6': 0.0003936290740966797, 'all': 0.06511735916137695}
diff5_list [0.006503582000732422, 0.006211996078491211, 0.006600379943847656, 0.006646633148193359, 0.0063588619232177734]
time3 0
time4 0.06591367721557617
time5 0.06596159934997559
time7 4.76837158203125e-07
gen_weight_change tensor(-20.1900)
policy weight change tensor(18.6580, grad_fn=<SumBackward0>)
time8 0.002020120620727539
train_time 0.0767669677734375
eval time 0.6988246440887451
epoch last part time 6.9141387939453125e-06
2024-01-23 00:58:58,526 MainThread INFO: EPOCH:38
2024-01-23 00:58:58,526 MainThread INFO: Time Consumed:0.7790513038635254s
2024-01-23 00:58:58,527 MainThread INFO: Total Frames:6600s
  0%|          | 39/10000 [00:26<2:12:48,  1.25it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11969.67472
Train_Epoch_Reward                26253.12516
Running_Training_Average_Rewards  15452.53964
Explore_Time                      0.00094
Train___Time                      0.07677
Eval____Time                      0.69882
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11791.30074
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.57584     3.34163     96.82312     88.05486
alpha_0                           0.98087      0.00014     0.98106      0.98067
Alpha_loss                        -0.12907     0.00082     -0.12766     -0.13006
Training/policy_loss              -2.67353     0.01414     -2.65506     -2.69347
Training/qf1_loss                 16236.64258  1237.63575  17465.92383  14048.36523
Training/qf2_loss                 16255.07031  1238.00773  17484.49609  14065.97168
Training/pf_norm                  0.32417      0.04917     0.37695      0.24615
Training/qf1_norm                 262.25069    8.22128     269.62848    247.94922
Training/qf2_norm                 217.30605    7.45219     224.49933    204.95029
log_std/mean                      -0.05680     0.00063     -0.05592     -0.05772
log_probs/mean                    -2.71567     0.01418     -2.69738     -2.73572
mean/mean                         0.00100      0.00006     0.00108      0.00090
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019078731536865234
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71313
epoch first part time 3.337860107421875e-06
replay_buffer._size: [6900]
collect time 0.0009286403656005859
inner_dict_sum {'sac_diff0': 0.0001933574676513672, 'sac_diff1': 0.007030010223388672, 'sac_diff2': 0.008091449737548828, 'sac_diff3': 0.010247230529785156, 'sac_diff4': 0.006819009780883789, 'sac_diff5': 0.03181791305541992, 'sac_diff6': 0.0003876686096191406, 'all': 0.06458663940429688}
diff5_list [0.006628990173339844, 0.006297588348388672, 0.005938053131103516, 0.006532907485961914, 0.0064203739166259766]
time3 0
time4 0.0653378963470459
time5 0.0653839111328125
time7 7.152557373046875e-07
gen_weight_change tensor(-20.1900)
policy weight change tensor(19.2804, grad_fn=<SumBackward0>)
time8 0.0019152164459228516
train_time 0.07618904113769531
eval time 0.705437421798706
epoch last part time 7.3909759521484375e-06
2024-01-23 00:58:59,334 MainThread INFO: EPOCH:39
2024-01-23 00:58:59,334 MainThread INFO: Time Consumed:0.785057544708252s
2024-01-23 00:58:59,334 MainThread INFO: Total Frames:6750s
  0%|          | 40/10000 [00:26<2:13:08,  1.25it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11938.43871
Train_Epoch_Reward                8896.06389
Running_Training_Average_Rewards  14640.15901
Explore_Time                      0.00092
Train___Time                      0.07619
Eval____Time                      0.70544
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11728.15363
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.37446     2.23715     96.79362     90.79454
alpha_0                           0.98037      0.00014     0.98057      0.98018
Alpha_loss                        -0.13244     0.00101     -0.13090     -0.13393
Training/policy_loss              -2.67320     0.01061     -2.66219     -2.69052
Training/qf1_loss                 16527.61797  1038.88525  17372.41797  14891.44727
Training/qf2_loss                 16545.16621  1039.30950  17389.90039  14908.32520
Training/pf_norm                  0.29678      0.03475     0.35782      0.25444
Training/qf1_norm                 260.00595    5.89732     267.16177    250.30190
Training/qf2_norm                 221.09852    5.02964     226.43478    212.95975
log_std/mean                      -0.05561     0.00061     -0.05477     -0.05651
log_probs/mean                    -2.71614     0.01088     -2.70491     -2.73352
mean/mean                         -0.00053     0.00009     -0.00039     -0.00063
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01854681968688965
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71313
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [7050]
collect time 0.0009145736694335938
inside mustsac before update, task 0, sumup 71313
inside mustsac after update, task 0, sumup 69753
inner_dict_sum {'sac_diff0': 0.00022172927856445312, 'sac_diff1': 0.007312774658203125, 'sac_diff2': 0.008673906326293945, 'sac_diff3': 0.011109113693237305, 'sac_diff4': 0.007658243179321289, 'sac_diff5': 0.05323338508605957, 'sac_diff6': 0.0004317760467529297, 'all': 0.08864092826843262}
diff5_list [0.010568618774414062, 0.01031041145324707, 0.01000666618347168, 0.012276172637939453, 0.010071516036987305]
time3 0.0008783340454101562
time4 0.08952450752258301
time5 0.08959770202636719
time7 0.12118220329284668
gen_weight_change tensor(-21.0092)
policy weight change tensor(19.8490, grad_fn=<SumBackward0>)
time8 0.002827167510986328
train_time 0.23152899742126465
eval time 0.801990270614624
epoch last part time 7.867813110351562e-06
2024-01-23 00:59:00,392 MainThread INFO: EPOCH:40
2024-01-23 00:59:00,393 MainThread INFO: Time Consumed:1.0369939804077148s
2024-01-23 00:59:00,393 MainThread INFO: Total Frames:6900s
  0%|          | 41/10000 [00:28<2:26:04,  1.14it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11910.93261
Train_Epoch_Reward                13149.38239
Running_Training_Average_Rewards  14191.76922
Explore_Time                      0.00091
Train___Time                      0.23153
Eval____Time                      0.80199
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11750.63390
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.47746     2.35903    95.93399     89.67775
alpha_0                           0.97988      0.00014    0.98008      0.97969
Alpha_loss                        -0.13585     0.00094    -0.13450     -0.13717
Training/policy_loss              -2.67357     0.00318    -2.66909     -2.67878
Training/qf1_loss                 15590.09160  823.06949  16386.55273  14405.27637
Training/qf2_loss                 15607.91328  823.29895  16405.47266  14423.21387
Training/pf_norm                  0.30342      0.02876    0.35792      0.27438
Training/qf1_norm                 253.66064    6.43737    259.59769    241.76978
Training/qf2_norm                 216.76589    5.39208    222.59529    210.22713
log_std/mean                      -0.05834     0.00244    -0.05517     -0.06113
log_probs/mean                    -2.71804     0.00321    -2.71326     -2.72303
mean/mean                         -0.00025     0.00037    0.00014      -0.00074
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018590688705444336
epoch last part time3 0.002963542938232422
inside rlalgo, task 0, sumup 69753
epoch first part time 3.337860107421875e-06
replay_buffer._size: [7200]
collect time 0.000911712646484375
inner_dict_sum {'sac_diff0': 0.00023174285888671875, 'sac_diff1': 0.00767827033996582, 'sac_diff2': 0.008929967880249023, 'sac_diff3': 0.01145029067993164, 'sac_diff4': 0.00766301155090332, 'sac_diff5': 0.03450274467468262, 'sac_diff6': 0.0004153251647949219, 'all': 0.07087135314941406}
diff5_list [0.0069811344146728516, 0.007151126861572266, 0.006636381149291992, 0.006654024124145508, 0.007080078125]
time3 0
time4 0.07174253463745117
time5 0.07180213928222656
time7 7.152557373046875e-07
gen_weight_change tensor(-21.0092)
policy weight change tensor(20.4472, grad_fn=<SumBackward0>)
time8 0.0020520687103271484
train_time 0.08301496505737305
eval time 0.7438597679138184
epoch last part time 8.344650268554688e-06
2024-01-23 00:59:01,248 MainThread INFO: EPOCH:41
2024-01-23 00:59:01,248 MainThread INFO: Time Consumed:0.8302826881408691s
2024-01-23 00:59:01,248 MainThread INFO: Total Frames:7050s
  0%|          | 42/10000 [00:28<2:24:39,  1.15it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11866.63046
Train_Epoch_Reward                9667.08983
Running_Training_Average_Rewards  14315.06546
Explore_Time                      0.00090
Train___Time                      0.08301
Eval____Time                      0.74386
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11676.57314
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.14729     1.49031    93.81413     89.41631
alpha_0                           0.97939      0.00014    0.97959      0.97920
Alpha_loss                        -0.13944     0.00096    -0.13825     -0.14079
Training/policy_loss              -2.68456     0.00720    -2.67167     -2.69260
Training/qf1_loss                 15918.43652  787.51763  16767.18555  14543.37500
Training/qf2_loss                 15936.92559  787.63457  16785.73438  14561.30664
Training/pf_norm                  0.25366      0.03751    0.32409      0.21928
Training/qf1_norm                 252.40661    4.18122    256.03201    245.03294
Training/qf2_norm                 213.26149    3.34420    217.06175    207.17384
log_std/mean                      -0.06260     0.00065    -0.06172     -0.06354
log_probs/mean                    -2.72900     0.00732    -2.71557     -2.73666
mean/mean                         -0.00041     0.00001    -0.00039     -0.00043
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0186159610748291
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 69753
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [7350]
collect time 0.00096893310546875
inner_dict_sum {'sac_diff0': 0.000202178955078125, 'sac_diff1': 0.006871223449707031, 'sac_diff2': 0.008015632629394531, 'sac_diff3': 0.010425329208374023, 'sac_diff4': 0.006723642349243164, 'sac_diff5': 0.03203535079956055, 'sac_diff6': 0.00038552284240722656, 'all': 0.06465888023376465}
diff5_list [0.0066335201263427734, 0.006197929382324219, 0.006299018859863281, 0.006495952606201172, 0.0064089298248291016]
time3 0
time4 0.06540679931640625
time5 0.06545233726501465
time7 4.76837158203125e-07
gen_weight_change tensor(-21.0092)
policy weight change tensor(21.0689, grad_fn=<SumBackward0>)
time8 0.0018579959869384766
train_time 0.07590174674987793
eval time 0.7579026222229004
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:02,107 MainThread INFO: EPOCH:42
2024-01-23 00:59:02,107 MainThread INFO: Time Consumed:0.8372552394866943s
2024-01-23 00:59:02,107 MainThread INFO: Total Frames:7200s
  0%|          | 43/10000 [00:29<2:24:07,  1.15it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11835.26854
Train_Epoch_Reward                6974.94987
Running_Training_Average_Rewards  13927.23175
Explore_Time                      0.00096
Train___Time                      0.07590
Eval____Time                      0.75790
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11719.70042
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       94.64558     2.25601    97.76626     91.58985
alpha_0                           0.97890      0.00014    0.97910      0.97870
Alpha_loss                        -0.14237     0.00097    -0.14078     -0.14357
Training/policy_loss              -2.66190     0.00847    -2.65084     -2.67470
Training/qf1_loss                 17390.19922  931.64061  18648.56250  16154.27344
Training/qf2_loss                 17410.59746  932.18363  18669.32422  16173.48145
Training/pf_norm                  0.32231      0.03963    0.38684      0.28046
Training/qf1_norm                 269.94618    7.48054    280.74094    259.69308
Training/qf2_norm                 216.04558    4.95406    222.89081    209.31491
log_std/mean                      -0.06432     0.00067    -0.06340     -0.06531
log_probs/mean                    -2.70795     0.00864    -2.69612     -2.72075
mean/mean                         -0.00054     0.00004    -0.00047     -0.00059
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.020380020141601562
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69753
epoch first part time 2.86102294921875e-06
replay_buffer._size: [7500]
collect time 0.00086212158203125
inner_dict_sum {'sac_diff0': 0.00019621849060058594, 'sac_diff1': 0.0070574283599853516, 'sac_diff2': 0.008181571960449219, 'sac_diff3': 0.010176420211791992, 'sac_diff4': 0.006834745407104492, 'sac_diff5': 0.03144383430480957, 'sac_diff6': 0.0003814697265625, 'all': 0.06427168846130371}
diff5_list [0.006680727005004883, 0.006237030029296875, 0.006361722946166992, 0.006229400634765625, 0.005934953689575195]
time3 0
time4 0.0650489330291748
time5 0.0650949478149414
time7 7.152557373046875e-07
gen_weight_change tensor(-21.0092)
policy weight change tensor(21.7070, grad_fn=<SumBackward0>)
time8 0.0018644332885742188
train_time 0.07558393478393555
eval time 0.7486214637756348
epoch last part time 7.3909759521484375e-06
2024-01-23 00:59:02,958 MainThread INFO: EPOCH:43
2024-01-23 00:59:02,959 MainThread INFO: Time Consumed:0.8275594711303711s
2024-01-23 00:59:02,959 MainThread INFO: Total Frames:7350s
  0%|          | 44/10000 [00:30<2:23:10,  1.16it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11814.62714
Train_Epoch_Reward                45662.02854
Running_Training_Average_Rewards  15037.21721
Explore_Time                      0.00086
Train___Time                      0.07558
Eval____Time                      0.74862
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11792.71602
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       103.32119    2.11951     107.24318    101.31408
alpha_0                           0.97841      0.00014     0.97861      0.97821
Alpha_loss                        -0.14601     0.00106     -0.14457     -0.14752
Training/policy_loss              -2.67321     0.00727     -2.66334     -2.68316
Training/qf1_loss                 21430.85664  1350.84800  23050.08789  19653.16992
Training/qf2_loss                 21455.29102  1350.91035  23074.28320  19677.25391
Training/pf_norm                  0.27301      0.00978     0.28445      0.25919
Training/qf1_norm                 300.92752    4.74045     309.93787    296.73633
Training/qf2_norm                 242.58695    4.78958     251.43117    238.07037
log_std/mean                      -0.06867     0.00072     -0.06768     -0.06972
log_probs/mean                    -2.72086     0.00765     -2.71050     -2.73104
mean/mean                         -0.00037     0.00002     -0.00034     -0.00041
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018625497817993164
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69753
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [7650]
collect time 0.0009849071502685547
inner_dict_sum {'sac_diff0': 0.000194549560546875, 'sac_diff1': 0.0066564083099365234, 'sac_diff2': 0.00800013542175293, 'sac_diff3': 0.009987592697143555, 'sac_diff4': 0.006678581237792969, 'sac_diff5': 0.030854463577270508, 'sac_diff6': 0.00037670135498046875, 'all': 0.06274843215942383}
diff5_list [0.006510019302368164, 0.006033897399902344, 0.005959749221801758, 0.006199836730957031, 0.006150960922241211]
time3 0
time4 0.06349301338195801
time5 0.0635378360748291
time7 9.5367431640625e-07
gen_weight_change tensor(-21.0092)
policy weight change tensor(22.3287, grad_fn=<SumBackward0>)
time8 0.001882314682006836
train_time 0.07389235496520996
eval time 0.7910208702087402
epoch last part time 7.62939453125e-06
2024-01-23 00:59:03,849 MainThread INFO: EPOCH:44
2024-01-23 00:59:03,849 MainThread INFO: Time Consumed:0.8683714866638184s
2024-01-23 00:59:03,849 MainThread INFO: Total Frames:7500s
  0%|          | 45/10000 [00:31<2:24:31,  1.15it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11801.15897
Train_Epoch_Reward                48616.66165
Running_Training_Average_Rewards  16293.58167
Explore_Time                      0.00098
Train___Time                      0.07389
Eval____Time                      0.79102
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11867.06846
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.72920     1.49645     100.15067    96.08147
alpha_0                           0.97792      0.00014     0.97812      0.97772
Alpha_loss                        -0.14943     0.00092     -0.14816     -0.15071
Training/policy_loss              -2.67361     0.00523     -2.66486     -2.67773
Training/qf1_loss                 19255.01719  1098.65564  20687.44141  18086.19922
Training/qf2_loss                 19278.62773  1098.59437  20710.90234  18109.75586
Training/pf_norm                  0.26108      0.02421     0.28526      0.21627
Training/qf1_norm                 286.22804    2.93309     291.27090    283.15665
Training/qf2_norm                 224.07556    3.27894     229.36880    220.44925
log_std/mean                      -0.07079     0.00068     -0.06986     -0.07177
log_probs/mean                    -2.72282     0.00521     -2.71387     -2.72711
mean/mean                         -0.00102     0.00001     -0.00101     -0.00104
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01861119270324707
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69753
epoch first part time 2.86102294921875e-06
replay_buffer._size: [7800]
collect time 0.0009486675262451172
inside mustsac before update, task 0, sumup 69753
inside mustsac after update, task 0, sumup 69580
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.007025480270385742, 'sac_diff2': 0.008227825164794922, 'sac_diff3': 0.010483503341674805, 'sac_diff4': 0.007275819778442383, 'sac_diff5': 0.0496211051940918, 'sac_diff6': 0.0004215240478515625, 'all': 0.0832681655883789}
diff5_list [0.010077714920043945, 0.00965428352355957, 0.00978994369506836, 0.009785652160644531, 0.01031351089477539]
time3 0.0008776187896728516
time4 0.08412456512451172
time5 0.08417844772338867
time7 0.3177030086517334
gen_weight_change tensor(-21.6299)
policy weight change tensor(22.8892, grad_fn=<SumBackward0>)
time8 0.002081632614135742
train_time 0.4219686985015869
eval time 0.5800943374633789
epoch last part time 7.3909759521484375e-06
2024-01-23 00:59:04,876 MainThread INFO: EPOCH:45
2024-01-23 00:59:04,877 MainThread INFO: Time Consumed:1.0055296421051025s
2024-01-23 00:59:04,877 MainThread INFO: Total Frames:7650s
  0%|          | 46/10000 [00:32<2:32:16,  1.09it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11793.33050
Train_Epoch_Reward                10881.94005
Running_Training_Average_Rewards  16011.93696
Explore_Time                      0.00094
Train___Time                      0.42197
Eval____Time                      0.58009
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11921.03191
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       98.93060     2.54504    101.60139    94.83933
alpha_0                           0.97743      0.00014    0.97763      0.97723
Alpha_loss                        -0.15269     0.00092    -0.15136     -0.15398
Training/policy_loss              -2.66802     0.00153    -2.66617     -2.67036
Training/qf1_loss                 19584.95664  821.97798  20448.48438  18389.01172
Training/qf2_loss                 19608.14570  821.64083  20471.27734  18413.29883
Training/pf_norm                  0.25108      0.01138    0.26739      0.23213
Training/qf1_norm                 278.62325    7.58877    292.64713    270.61243
Training/qf2_norm                 228.38217    7.71385    234.82672    215.78954
log_std/mean                      -0.06943     0.00450    -0.06059     -0.07263
log_probs/mean                    -2.71792     0.00169    -2.71552     -2.72031
mean/mean                         0.00003      0.00062    0.00054      -0.00104
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018117189407348633
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69580
epoch first part time 2.86102294921875e-06
replay_buffer._size: [7950]
collect time 0.0008451938629150391
inner_dict_sum {'sac_diff0': 0.00019550323486328125, 'sac_diff1': 0.006979227066040039, 'sac_diff2': 0.00817561149597168, 'sac_diff3': 0.010067224502563477, 'sac_diff4': 0.0069315433502197266, 'sac_diff5': 0.031745195388793945, 'sac_diff6': 0.0003788471221923828, 'all': 0.06447315216064453}
diff5_list [0.006555318832397461, 0.006216526031494141, 0.006162166595458984, 0.0064334869384765625, 0.006377696990966797]
time3 0
time4 0.0652318000793457
time5 0.06528496742248535
time7 9.5367431640625e-07
gen_weight_change tensor(-21.6299)
policy weight change tensor(23.5176, grad_fn=<SumBackward0>)
time8 0.0018138885498046875
train_time 0.07564640045166016
eval time 0.8042440414428711
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:05,781 MainThread INFO: EPOCH:46
2024-01-23 00:59:05,781 MainThread INFO: Time Consumed:0.8831703662872314s
2024-01-23 00:59:05,781 MainThread INFO: Total Frames:7800s
  0%|          | 47/10000 [00:33<2:31:41,  1.09it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11835.32756
Train_Epoch_Reward                8611.23585
Running_Training_Average_Rewards  16056.41351
Explore_Time                      0.00084
Train___Time                      0.07565
Eval____Time                      0.80424
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12269.76674
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       99.92950     2.08955    102.23773    96.42314
alpha_0                           0.97694      0.00014    0.97714      0.97674
Alpha_loss                        -0.15616     0.00096    -0.15517     -0.15778
Training/policy_loss              -2.67241     0.01122    -2.65861     -2.68793
Training/qf1_loss                 20468.35391  976.27671  21365.44531  18675.12305
Training/qf2_loss                 20494.57773  976.82490  21391.60547  18700.06641
Training/pf_norm                  0.24921      0.04588    0.29021      0.17373
Training/qf1_norm                 295.88945    6.67413    304.09247    284.37952
Training/qf2_norm                 238.71575    4.80913    243.93605    230.61124
log_std/mean                      -0.07464     0.00072    -0.07368     -0.07570
log_probs/mean                    -2.72262     0.01143    -2.70850     -2.73795
mean/mean                         0.00066      0.00013    0.00086      0.00050
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019865036010742188
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69580
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [8100]
collect time 0.0008482933044433594
inner_dict_sum {'sac_diff0': 0.0001995563507080078, 'sac_diff1': 0.007215023040771484, 'sac_diff2': 0.008451461791992188, 'sac_diff3': 0.010759592056274414, 'sac_diff4': 0.007078886032104492, 'sac_diff5': 0.03191995620727539, 'sac_diff6': 0.00038170814514160156, 'all': 0.06600618362426758}
diff5_list [0.006600618362426758, 0.006465911865234375, 0.0064013004302978516, 0.006193637847900391, 0.006258487701416016]
time3 0
time4 0.06675863265991211
time5 0.0668039321899414
time7 4.76837158203125e-07
gen_weight_change tensor(-21.6299)
policy weight change tensor(24.1574, grad_fn=<SumBackward0>)
time8 0.0020227432250976562
train_time 0.07750630378723145
eval time 0.8250827789306641
epoch last part time 8.106231689453125e-06
2024-01-23 00:59:06,710 MainThread INFO: EPOCH:47
2024-01-23 00:59:06,710 MainThread INFO: Time Consumed:0.9059569835662842s
2024-01-23 00:59:06,711 MainThread INFO: Total Frames:7950s
  0%|          | 48/10000 [00:34<2:32:20,  1.09it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11886.67286
Train_Epoch_Reward                2385.10545
Running_Training_Average_Rewards  15739.79656
Explore_Time                      0.00084
Train___Time                      0.07751
Eval____Time                      0.82508
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12349.78366
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.21747     1.21746    98.63391     95.37257
alpha_0                           0.97645      0.00014    0.97665      0.97625
Alpha_loss                        -0.15935     0.00097    -0.15782     -0.16061
Training/policy_loss              -2.66325     0.00593    -2.65604     -2.67233
Training/qf1_loss                 19322.40859  629.09787  20445.05078  18560.43750
Training/qf2_loss                 19349.42266  629.53756  20473.19141  18587.28906
Training/pf_norm                  0.30910      0.03848    0.34087      0.24024
Training/qf1_norm                 290.27747    3.86711    296.71823    285.96927
Training/qf2_norm                 230.44924    2.82217    233.70374    226.22502
log_std/mean                      -0.06752     0.00065    -0.06664     -0.06847
log_probs/mean                    -2.71482     0.00608    -2.70690     -2.72412
mean/mean                         0.00068      0.00012    0.00085      0.00052
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018368959426879883
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69580
epoch first part time 2.86102294921875e-06
replay_buffer._size: [8250]
collect time 0.0008759498596191406
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.0071485042572021484, 'sac_diff2': 0.008649110794067383, 'sac_diff3': 0.010638952255249023, 'sac_diff4': 0.006959676742553711, 'sac_diff5': 0.03378725051879883, 'sac_diff6': 0.0003979206085205078, 'all': 0.06780648231506348}
diff5_list [0.007208824157714844, 0.007604837417602539, 0.006285667419433594, 0.006273508071899414, 0.0064144134521484375]
time3 0
time4 0.06862068176269531
time5 0.0686795711517334
time7 7.152557373046875e-07
gen_weight_change tensor(-21.6299)
policy weight change tensor(24.7928, grad_fn=<SumBackward0>)
time8 0.0019075870513916016
train_time 0.07952356338500977
eval time 0.8488037586212158
epoch last part time 6.4373016357421875e-06
2024-01-23 00:59:07,664 MainThread INFO: EPOCH:48
2024-01-23 00:59:07,664 MainThread INFO: Time Consumed:0.9316873550415039s
2024-01-23 00:59:07,664 MainThread INFO: Total Frames:8100s
  0%|          | 49/10000 [00:35<2:34:05,  1.08it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11947.21137
Train_Epoch_Reward                8990.89895
Running_Training_Average_Rewards  14818.92683
Explore_Time                      0.00086
Train___Time                      0.07952
Eval____Time                      0.84880
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12396.68583
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.26406     1.68112    100.37517    95.78563
alpha_0                           0.97596      0.00014    0.97616      0.97577
Alpha_loss                        -0.16281     0.00080    -0.16168     -0.16388
Training/policy_loss              -2.66537     0.00709    -2.65366     -2.67485
Training/qf1_loss                 19167.81367  797.05190  20361.49609  17866.39844
Training/qf2_loss                 19194.54336  797.21172  20389.01562  17893.51172
Training/pf_norm                  0.27250      0.04208    0.32866      0.21829
Training/qf1_norm                 298.81346    5.27974    307.97202    292.38004
Training/qf2_norm                 225.02965    3.76729    231.99092    221.68140
log_std/mean                      -0.06941     0.00065    -0.06852     -0.07037
log_probs/mean                    -2.71877     0.00691    -2.70731     -2.72794
mean/mean                         0.00050      0.00005    0.00055      0.00041
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01863551139831543
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69580
epoch first part time 3.337860107421875e-06
replay_buffer._size: [8400]
collect time 0.000873565673828125
inner_dict_sum {'sac_diff0': 0.0002300739288330078, 'sac_diff1': 0.008200645446777344, 'sac_diff2': 0.009790420532226562, 'sac_diff3': 0.010992050170898438, 'sac_diff4': 0.00759434700012207, 'sac_diff5': 0.032501220703125, 'sac_diff6': 0.0004048347473144531, 'all': 0.06971359252929688}
diff5_list [0.007280826568603516, 0.006037235260009766, 0.00638270378112793, 0.006159782409667969, 0.00664067268371582]
time3 0
time4 0.07052206993103027
time5 0.07058453559875488
time7 7.152557373046875e-07
gen_weight_change tensor(-21.6299)
policy weight change tensor(25.4078, grad_fn=<SumBackward0>)
time8 0.002373933792114258
train_time 0.08622574806213379
eval time 0.8411195278167725
epoch last part time 7.867813110351562e-06
2024-01-23 00:59:08,617 MainThread INFO: EPOCH:49
2024-01-23 00:59:08,618 MainThread INFO: Time Consumed:0.9309914112091064s
2024-01-23 00:59:08,618 MainThread INFO: Total Frames:8250s
  0%|          | 50/10000 [00:36<2:35:18,  1.07it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12015.00937
Train_Epoch_Reward                11056.66157
Running_Training_Average_Rewards  13936.80462
Explore_Time                      0.00087
Train___Time                      0.08623
Eval____Time                      0.84112
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12406.13361
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       95.44791     2.25200    97.49399     91.52592
alpha_0                           0.97547      0.00014    0.97567      0.97528
Alpha_loss                        -0.16649     0.00105    -0.16533     -0.16815
Training/policy_loss              -2.67765     0.00945    -2.66301     -2.68920
Training/qf1_loss                 18297.61055  857.64120  19568.95508  17221.83594
Training/qf2_loss                 18325.15234  858.44095  19597.78711  17248.58789
Training/pf_norm                  0.23252      0.03504    0.27105      0.17583
Training/qf1_norm                 287.70295    6.91074    295.95358    277.17831
Training/qf2_norm                 227.00631    5.12587    231.69743    218.08270
log_std/mean                      -0.08005     0.00068    -0.07912     -0.08104
log_probs/mean                    -2.73158     0.00979    -2.71657     -2.74397
mean/mean                         0.00035      0.00007    0.00046      0.00026
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019701242446899414
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69580
epoch first part time 5.4836273193359375e-06
replay_buffer._size: [8550]
collect time 0.0009274482727050781
inside mustsac before update, task 0, sumup 69580
inside mustsac after update, task 0, sumup 69607
inner_dict_sum {'sac_diff0': 0.00022649765014648438, 'sac_diff1': 0.007678508758544922, 'sac_diff2': 0.009038448333740234, 'sac_diff3': 0.011539936065673828, 'sac_diff4': 0.008164167404174805, 'sac_diff5': 0.05745291709899902, 'sac_diff6': 0.0004341602325439453, 'all': 0.09453463554382324}
diff5_list [0.014107704162597656, 0.011705160140991211, 0.009671926498413086, 0.010236024856567383, 0.011732101440429688]
time3 0.0008857250213623047
time4 0.09543871879577637
time5 0.09550070762634277
time7 0.1390535831451416
gen_weight_change tensor(-22.0690)
policy weight change tensor(25.9514, grad_fn=<SumBackward0>)
time8 0.002798318862915039
train_time 0.2566204071044922
eval time 0.6915104389190674
epoch last part time 6.9141387939453125e-06
2024-01-23 00:59:09,592 MainThread INFO: EPOCH:50
2024-01-23 00:59:09,592 MainThread INFO: Time Consumed:0.9515669345855713s
2024-01-23 00:59:09,592 MainThread INFO: Total Frames:8400s
  1%|          | 51/10000 [00:37<2:37:18,  1.05it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12075.89710
Train_Epoch_Reward                9824.14614
Running_Training_Average_Rewards  13789.62851
Explore_Time                      0.00092
Train___Time                      0.25662
Eval____Time                      0.69151
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12359.51117
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       95.77575     1.18120    97.70515     94.59145
alpha_0                           0.97498      0.00014    0.97518      0.97479
Alpha_loss                        -0.16994     0.00086    -0.16872     -0.17118
Training/policy_loss              -2.67873     0.00382    -2.67489     -2.68427
Training/qf1_loss                 18856.17031  485.89145  19771.57227  18354.29688
Training/qf2_loss                 18883.34453  486.07176  19799.33984  18381.22266
Training/pf_norm                  0.23857      0.03569    0.29827      0.19552
Training/qf1_norm                 284.77000    6.50451    294.11136    274.87067
Training/qf2_norm                 222.83170    4.26752    226.42137    214.74391
log_std/mean                      -0.08056     0.00380    -0.07442     -0.08580
log_probs/mean                    -2.73427     0.00404    -2.72988     -2.73967
mean/mean                         0.00088      0.00058    0.00155      -0.00002
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018607616424560547
epoch last part time3 0.0028073787689208984
inside rlalgo, task 0, sumup 69607
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [8700]
collect time 0.0008578300476074219
inner_dict_sum {'sac_diff0': 0.00020074844360351562, 'sac_diff1': 0.007736682891845703, 'sac_diff2': 0.008975028991699219, 'sac_diff3': 0.011508941650390625, 'sac_diff4': 0.007333517074584961, 'sac_diff5': 0.03382301330566406, 'sac_diff6': 0.00040602684020996094, 'all': 0.06998395919799805}
diff5_list [0.00710296630859375, 0.0073888301849365234, 0.006788969039916992, 0.00621485710144043, 0.006327390670776367]
time3 0
time4 0.07082366943359375
time5 0.07087516784667969
time7 4.76837158203125e-07
gen_weight_change tensor(-22.0690)
policy weight change tensor(26.5368, grad_fn=<SumBackward0>)
time8 0.0019342899322509766
train_time 0.08201313018798828
eval time 1.0323002338409424
epoch last part time 7.62939453125e-06
2024-01-23 00:59:10,734 MainThread INFO: EPOCH:51
2024-01-23 00:59:10,734 MainThread INFO: Time Consumed:1.1176483631134033s
2024-01-23 00:59:10,735 MainThread INFO: Total Frames:8550s
  1%|          | 52/10000 [00:38<2:46:47,  1.01s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12140.13081
Train_Epoch_Reward                6624.87774
Running_Training_Average_Rewards  13695.95708
Explore_Time                      0.00085
Train___Time                      0.08201
Eval____Time                      1.03230
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12318.91027
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.83208     2.90060     96.71713     89.11015
alpha_0                           0.97449      0.00014     0.97469      0.97430
Alpha_loss                        -0.17308     0.00102     -0.17155     -0.17441
Training/policy_loss              -2.66925     0.00682     -2.66292     -2.68242
Training/qf1_loss                 18505.66484  1323.75068  19473.12891  15881.83789
Training/qf2_loss                 18531.39707  1324.80485  19499.67383  15905.47363
Training/pf_norm                  0.23056      0.04064     0.27667      0.17943
Training/qf1_norm                 272.22005    8.87480     277.92148    254.55891
Training/qf2_norm                 223.99175    6.63565     228.28511    210.90483
log_std/mean                      -0.08172     0.00071     -0.08074     -0.08273
log_probs/mean                    -2.72500     0.00711     -2.71796     -2.73852
mean/mean                         0.00079      0.00004     0.00086      0.00075
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019073963165283203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69607
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [8850]
collect time 0.0009589195251464844
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.0071561336517333984, 'sac_diff2': 0.008038997650146484, 'sac_diff3': 0.010286092758178711, 'sac_diff4': 0.006863117218017578, 'sac_diff5': 0.03154611587524414, 'sac_diff6': 0.0004241466522216797, 'all': 0.06452536582946777}
diff5_list [0.006520509719848633, 0.006315946578979492, 0.006088733673095703, 0.006194114685058594, 0.006426811218261719]
time3 0
time4 0.06527900695800781
time5 0.0653221607208252
time7 7.152557373046875e-07
gen_weight_change tensor(-22.0690)
policy weight change tensor(27.1124, grad_fn=<SumBackward0>)
time8 0.0019795894622802734
train_time 0.0762031078338623
eval time 0.8972508907318115
epoch last part time 6.9141387939453125e-06
2024-01-23 00:59:11,734 MainThread INFO: EPOCH:52
2024-01-23 00:59:11,734 MainThread INFO: Time Consumed:0.9768948554992676s
2024-01-23 00:59:11,734 MainThread INFO: Total Frames:8700s
  1%|          | 53/10000 [00:39<2:46:28,  1.00s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12192.21713
Train_Epoch_Reward                9216.58890
Running_Training_Average_Rewards  13129.56105
Explore_Time                      0.00095
Train___Time                      0.07620
Eval____Time                      0.89725
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12240.56366
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.59183     1.91996     96.44582     91.43433
alpha_0                           0.97401      0.00014     0.97420      0.97381
Alpha_loss                        -0.17643     0.00091     -0.17494     -0.17745
Training/policy_loss              -2.66750     0.00778     -2.65494     -2.67630
Training/qf1_loss                 18152.34961  1029.51188  19005.71094  16367.33008
Training/qf2_loss                 18182.32266  1030.21769  19035.58203  16395.66797
Training/pf_norm                  0.21013      0.03580     0.25722      0.15670
Training/qf1_norm                 293.78106    6.35411     300.50842    282.25385
Training/qf2_norm                 220.39743    4.35364     224.59953    213.19965
log_std/mean                      -0.08665     0.00071     -0.08568     -0.08769
log_probs/mean                    -2.72421     0.00785     -2.71185     -2.73324
mean/mean                         0.00097      0.00008     0.00107      0.00086
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019367456436157227
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69607
epoch first part time 3.337860107421875e-06
replay_buffer._size: [9000]
collect time 0.0010280609130859375
inner_dict_sum {'sac_diff0': 0.000202178955078125, 'sac_diff1': 0.0071258544921875, 'sac_diff2': 0.008311748504638672, 'sac_diff3': 0.010468482971191406, 'sac_diff4': 0.00690460205078125, 'sac_diff5': 0.03238105773925781, 'sac_diff6': 0.0003921985626220703, 'all': 0.06578612327575684}
diff5_list [0.00707697868347168, 0.00654149055480957, 0.006323337554931641, 0.006228446960449219, 0.006210803985595703]
time3 0
time4 0.06655645370483398
time5 0.06660580635070801
time7 4.76837158203125e-07
gen_weight_change tensor(-22.0690)
policy weight change tensor(27.7024, grad_fn=<SumBackward0>)
time8 0.0019528865814208984
train_time 0.07775139808654785
eval time 1.0614418983459473
epoch last part time 8.106231689453125e-06
2024-01-23 00:59:12,900 MainThread INFO: EPOCH:53
2024-01-23 00:59:12,900 MainThread INFO: Time Consumed:1.142754077911377s
2024-01-23 00:59:12,900 MainThread INFO: Total Frames:8850s
  1%|          | 54/10000 [00:40<2:54:29,  1.05s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12232.56363
Train_Epoch_Reward                3397.44432
Running_Training_Average_Rewards  12915.26167
Explore_Time                      0.00102
Train___Time                      0.07775
Eval____Time                      1.06144
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12196.18100
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.24644     1.82771     98.89142     93.59196
alpha_0                           0.97352      0.00014     0.97371      0.97332
Alpha_loss                        -0.17999     0.00096     -0.17862     -0.18123
Training/policy_loss              -2.67150     0.00501     -2.66550     -2.67929
Training/qf1_loss                 19323.11953  1078.21209  20836.73047  17833.63086
Training/qf2_loss                 19355.10000  1078.98598  20869.93359  17864.05664
Training/pf_norm                  0.23161      0.03210     0.27888      0.18173
Training/qf1_norm                 297.60201    6.22351     306.72134    287.44644
Training/qf2_norm                 222.82924    4.10815     228.75592    216.83749
log_std/mean                      -0.08937     0.00076     -0.08832     -0.09048
log_probs/mean                    -2.73129     0.00512     -2.72514     -2.73958
mean/mean                         0.00090      0.00004     0.00097      0.00086
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019306421279907227
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69607
epoch first part time 3.337860107421875e-06
replay_buffer._size: [9150]
collect time 0.0008976459503173828
inner_dict_sum {'sac_diff0': 0.00021314620971679688, 'sac_diff1': 0.007065534591674805, 'sac_diff2': 0.008353710174560547, 'sac_diff3': 0.010459184646606445, 'sac_diff4': 0.0071566104888916016, 'sac_diff5': 0.032349586486816406, 'sac_diff6': 0.00040841102600097656, 'all': 0.06600618362426758}
diff5_list [0.0066070556640625, 0.006284952163696289, 0.006249189376831055, 0.006642818450927734, 0.006565570831298828]
time3 0
time4 0.06677746772766113
time5 0.06682538986206055
time7 9.5367431640625e-07
gen_weight_change tensor(-22.0690)
policy weight change tensor(28.2488, grad_fn=<SumBackward0>)
time8 0.0019409656524658203
train_time 0.07779383659362793
eval time 0.9375317096710205
epoch last part time 6.67572021484375e-06
2024-01-23 00:59:13,942 MainThread INFO: EPOCH:54
2024-01-23 00:59:13,942 MainThread INFO: Time Consumed:1.0187511444091797s
2024-01-23 00:59:13,942 MainThread INFO: Total Frames:9000s
  1%|          | 55/10000 [00:41<2:53:55,  1.05s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12271.22090
Train_Epoch_Reward                11529.39468
Running_Training_Average_Rewards  12683.53747
Explore_Time                      0.00089
Train___Time                      0.07779
Eval____Time                      0.93753
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12253.64115
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.35789     2.60215    94.61609     88.42952
alpha_0                           0.97303      0.00014    0.97322      0.97283
Alpha_loss                        -0.18332     0.00079    -0.18245     -0.18469
Training/policy_loss              -2.67009     0.01047    -2.65652     -2.68790
Training/qf1_loss                 17830.90547  937.98467  18737.62891  16241.19531
Training/qf2_loss                 17859.75273  938.49206  18766.52734  16269.52148
Training/pf_norm                  0.22153      0.03686    0.26911      0.17071
Training/qf1_norm                 277.15043    7.08619    283.48520    267.71597
Training/qf2_norm                 218.29019    5.88977    223.29573    209.37137
log_std/mean                      -0.08310     0.00060    -0.08229     -0.08399
log_probs/mean                    -2.72941     0.01050    -2.71574     -2.74716
mean/mean                         0.00132      0.00011    0.00145      0.00116
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0189054012298584
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69607
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [9300]
collect time 0.0009717941284179688
inside mustsac before update, task 0, sumup 69607
inside mustsac after update, task 0, sumup 70548
inner_dict_sum {'sac_diff0': 0.0002028942108154297, 'sac_diff1': 0.007425546646118164, 'sac_diff2': 0.009380578994750977, 'sac_diff3': 0.011893510818481445, 'sac_diff4': 0.008288145065307617, 'sac_diff5': 0.051611900329589844, 'sac_diff6': 0.0004267692565917969, 'all': 0.08922934532165527}
diff5_list [0.010283946990966797, 0.010223627090454102, 0.010578632354736328, 0.009948968887329102, 0.010576725006103516]
time3 0.00090789794921875
time4 0.09016704559326172
time5 0.09022140502929688
time7 0.15517282485961914
gen_weight_change tensor(-22.1906)
policy weight change tensor(28.7818, grad_fn=<SumBackward0>)
time8 0.002125263214111328
train_time 0.2666354179382324
eval time 0.7519850730895996
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:14,986 MainThread INFO: EPOCH:55
2024-01-23 00:59:14,987 MainThread INFO: Time Consumed:1.0221343040466309s
2024-01-23 00:59:14,987 MainThread INFO: Total Frames:9150s
  1%|          | 56/10000 [00:42<2:53:41,  1.05s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12311.23485
Train_Epoch_Reward                4961.39176
Running_Training_Average_Rewards  12601.75690
Explore_Time                      0.00097
Train___Time                      0.26664
Eval____Time                      0.75199
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12321.17137
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.52142     2.34194     98.03357     91.63005
alpha_0                           0.97254      0.00014     0.97274      0.97235
Alpha_loss                        -0.18660     0.00084     -0.18534     -0.18777
Training/policy_loss              -2.66514     0.00498     -2.65745     -2.67060
Training/qf1_loss                 18020.92344  1311.74255  20562.69141  16833.36719
Training/qf2_loss                 18052.82734  1310.68764  20593.04297  16866.98438
Training/pf_norm                  0.21289      0.04022     0.26056      0.15613
Training/qf1_norm                 292.54327    7.20053     301.38800    281.32422
Training/qf2_norm                 216.56284    5.33330     225.92697    210.84569
log_std/mean                      -0.09128     0.00256     -0.08877     -0.09518
log_probs/mean                    -2.72594     0.00537     -2.71919     -2.73193
mean/mean                         0.00103      0.00054     0.00164      0.00028
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01884913444519043
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70548
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [9450]
collect time 0.0008568763732910156
inner_dict_sum {'sac_diff0': 0.0002117156982421875, 'sac_diff1': 0.007275581359863281, 'sac_diff2': 0.008818864822387695, 'sac_diff3': 0.010717630386352539, 'sac_diff4': 0.007383584976196289, 'sac_diff5': 0.03367161750793457, 'sac_diff6': 0.0003972053527832031, 'all': 0.06847620010375977}
diff5_list [0.006780385971069336, 0.007706642150878906, 0.006527900695800781, 0.006335735321044922, 0.006320953369140625]
time3 0
time4 0.06929874420166016
time5 0.06935405731201172
time7 7.152557373046875e-07
gen_weight_change tensor(-22.1906)
policy weight change tensor(29.3080, grad_fn=<SumBackward0>)
time8 0.0018701553344726562
train_time 0.08021283149719238
eval time 0.9603068828582764
epoch last part time 7.3909759521484375e-06
2024-01-23 00:59:16,053 MainThread INFO: EPOCH:56
2024-01-23 00:59:16,053 MainThread INFO: Time Consumed:1.0440104007720947s
2024-01-23 00:59:16,054 MainThread INFO: Total Frames:9300s
  1%|          | 57/10000 [00:43<2:58:03,  1.07s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12293.59614
Train_Epoch_Reward                3080.37860
Running_Training_Average_Rewards  12490.50260
Explore_Time                      0.00085
Train___Time                      0.08021
Eval____Time                      0.96031
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12093.37968
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.09192     3.29630     97.51826     87.73415
alpha_0                           0.97205      0.00014     0.97225      0.97186
Alpha_loss                        -0.18992     0.00091     -0.18855     -0.19109
Training/policy_loss              -2.66314     0.00511     -2.65639     -2.67198
Training/qf1_loss                 17929.72734  1568.18307  20257.49023  15530.55469
Training/qf2_loss                 17959.95645  1569.14223  20288.47266  15559.04980
Training/pf_norm                  0.19362      0.02613     0.23359      0.15928
Training/qf1_norm                 285.70901    9.87845     297.04276    269.55612
Training/qf2_norm                 219.54489    7.45011     229.52811    207.42642
log_std/mean                      -0.09867     0.00072     -0.09769     -0.09972
log_probs/mean                    -2.72417     0.00509     -2.71777     -2.73327
mean/mean                         0.00043      0.00002     0.00045      0.00038
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.08872032165527344
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70548
epoch first part time 4.291534423828125e-06
replay_buffer._size: [9600]
collect time 0.0010585784912109375
inner_dict_sum {'sac_diff0': 0.0002014636993408203, 'sac_diff1': 0.00722956657409668, 'sac_diff2': 0.010065078735351562, 'sac_diff3': 0.010396003723144531, 'sac_diff4': 0.007419109344482422, 'sac_diff5': 0.03240180015563965, 'sac_diff6': 0.00041294097900390625, 'all': 0.06812596321105957}
diff5_list [0.0070154666900634766, 0.0064508914947509766, 0.006268978118896484, 0.006144285202026367, 0.006522178649902344]
time3 0
time4 0.06897401809692383
time5 0.06904029846191406
time7 1.6689300537109375e-06
gen_weight_change tensor(-22.1906)
policy weight change tensor(29.7895, grad_fn=<SumBackward0>)
time8 0.0019919872283935547
train_time 0.08128786087036133
eval time 0.9027583599090576
epoch last part time 1.0251998901367188e-05
2024-01-23 00:59:17,221 MainThread INFO: EPOCH:57
2024-01-23 00:59:17,221 MainThread INFO: Time Consumed:0.9876904487609863s
2024-01-23 00:59:17,221 MainThread INFO: Total Frames:9450s
  1%|          | 58/10000 [00:44<2:59:29,  1.08s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12258.59291
Train_Epoch_Reward                25589.32590
Running_Training_Average_Rewards  12838.01479
Explore_Time                      0.00105
Train___Time                      0.08129
Eval____Time                      0.90276
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11999.75138
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.26396     2.93411     95.22813     87.35920
alpha_0                           0.97157      0.00014     0.97176      0.97137
Alpha_loss                        -0.19338     0.00094     -0.19236     -0.19489
Training/policy_loss              -2.66262     0.00783     -2.65317     -2.67419
Training/qf1_loss                 16954.76270  1443.57394  18924.29102  14794.40332
Training/qf2_loss                 16987.66016  1444.26318  18957.89453  14825.88477
Training/pf_norm                  0.18772      0.05516     0.27595      0.11264
Training/qf1_norm                 288.71364    8.00525     298.95279    276.47043
Training/qf2_norm                 210.47281    6.46978     219.12230    201.84001
log_std/mean                      -0.09164     0.00061     -0.09083     -0.09255
log_probs/mean                    -2.72723     0.00799     -2.71722     -2.73859
mean/mean                         -0.00024     0.00005     -0.00019     -0.00031
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.024839401245117188
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70548
epoch first part time 3.337860107421875e-06
replay_buffer._size: [9750]
collect time 0.0009222030639648438
inner_dict_sum {'sac_diff0': 0.0002486705780029297, 'sac_diff1': 0.007894754409790039, 'sac_diff2': 0.009692668914794922, 'sac_diff3': 0.01171731948852539, 'sac_diff4': 0.008211851119995117, 'sac_diff5': 0.04010772705078125, 'sac_diff6': 0.0005137920379638672, 'all': 0.07838678359985352}
diff5_list [0.009613275527954102, 0.007718801498413086, 0.007486581802368164, 0.0072765350341796875, 0.008012533187866211]
time3 0
time4 0.07922601699829102
time5 0.07928133010864258
time7 4.76837158203125e-07
gen_weight_change tensor(-22.1906)
policy weight change tensor(30.2598, grad_fn=<SumBackward0>)
time8 0.0020034313201904297
train_time 0.09110069274902344
eval time 0.9649980068206787
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:18,308 MainThread INFO: EPOCH:58
2024-01-23 00:59:18,309 MainThread INFO: Time Consumed:1.0595858097076416s
2024-01-23 00:59:18,309 MainThread INFO: Total Frames:9600s
  1%|          | 59/10000 [00:45<2:59:25,  1.08s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12206.76088
Train_Epoch_Reward                27989.28283
Running_Training_Average_Rewards  13107.40071
Explore_Time                      0.00091
Train___Time                      0.09110
Eval____Time                      0.96500
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11878.36552
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.46460     1.33142    94.89034     91.82583
alpha_0                           0.97108      0.00014    0.97127      0.97088
Alpha_loss                        -0.19672     0.00083    -0.19586     -0.19798
Training/policy_loss              -2.66250     0.00950    -2.64983     -2.67915
Training/qf1_loss                 18518.84297  742.87494  19574.87305  17734.52930
Training/qf2_loss                 18553.89766  743.21716  19610.68750  17768.94141
Training/pf_norm                  0.22486      0.02733    0.24966      0.17250
Training/qf1_norm                 300.16570    4.77097    305.06927    293.96802
Training/qf2_norm                 217.16897    2.95221    220.29945    213.52116
log_std/mean                      -0.09542     0.00064    -0.09456     -0.09636
log_probs/mean                    -2.72615     0.00956    -2.71309     -2.74275
mean/mean                         -0.00074     0.00006    -0.00067     -0.00081
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019240856170654297
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70548
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [9900]
collect time 0.001088857650756836
inner_dict_sum {'sac_diff0': 0.0002841949462890625, 'sac_diff1': 0.007855415344238281, 'sac_diff2': 0.009314298629760742, 'sac_diff3': 0.012223958969116211, 'sac_diff4': 0.007994413375854492, 'sac_diff5': 0.03776121139526367, 'sac_diff6': 0.0004730224609375, 'all': 0.07590651512145996}
diff5_list [0.007816314697265625, 0.007569313049316406, 0.0076084136962890625, 0.007161378860473633, 0.007605791091918945]
time3 0
time4 0.07665205001831055
time5 0.07669758796691895
time7 7.152557373046875e-07
gen_weight_change tensor(-22.1906)
policy weight change tensor(30.7123, grad_fn=<SumBackward0>)
time8 0.002118349075317383
train_time 0.0886387825012207
eval time 0.9880795478820801
epoch last part time 7.62939453125e-06
2024-01-23 00:59:19,412 MainThread INFO: EPOCH:59
2024-01-23 00:59:19,412 MainThread INFO: Time Consumed:1.0803229808807373s
2024-01-23 00:59:19,412 MainThread INFO: Total Frames:9750s
  1%|          | 60/10000 [00:47<3:00:25,  1.09s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12142.83248
Train_Epoch_Reward                4068.45337
Running_Training_Average_Rewards  12631.13254
Explore_Time                      0.00108
Train___Time                      0.08864
Eval____Time                      0.98808
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11766.84964
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       95.33388     2.53068    98.96758     91.12307
alpha_0                           0.97059      0.00014    0.97079      0.97040
Alpha_loss                        -0.20040     0.00099    -0.19886     -0.20176
Training/policy_loss              -2.67277     0.00344    -2.66719     -2.67698
Training/qf1_loss                 18583.90352  741.37915  19475.56055  17521.57227
Training/qf2_loss                 18619.82383  742.34921  19512.06641  17555.23828
Training/pf_norm                  0.17990      0.03401    0.23760      0.14014
Training/qf1_norm                 302.10818    8.44839    313.15201    287.07278
Training/qf2_norm                 229.01973    5.90029    237.50926    219.21251
log_std/mean                      -0.10138     0.00064    -0.10049     -0.10230
log_probs/mean                    -2.73645     0.00365    -2.73015     -2.74052
mean/mean                         0.00038      0.00003    0.00042      0.00033
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01919102668762207
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70548
epoch first part time 3.337860107421875e-06
replay_buffer._size: [10050]
collect time 0.0010890960693359375
inside mustsac before update, task 0, sumup 70548
inside mustsac after update, task 0, sumup 71220
inner_dict_sum {'sac_diff0': 0.0002720355987548828, 'sac_diff1': 0.009699344635009766, 'sac_diff2': 0.011529684066772461, 'sac_diff3': 0.013633251190185547, 'sac_diff4': 0.010140180587768555, 'sac_diff5': 0.06529450416564941, 'sac_diff6': 0.0005214214324951172, 'all': 0.11109042167663574}
diff5_list [0.013943672180175781, 0.012596845626831055, 0.011841773986816406, 0.014492034912109375, 0.012420177459716797]
time3 0.0014624595642089844
time4 0.11198973655700684
time5 0.11206603050231934
time7 0.3259096145629883
gen_weight_change tensor(-22.3271)
policy weight change tensor(31.2322, grad_fn=<SumBackward0>)
time8 0.0030798912048339844
train_time 0.4657573699951172
eval time 0.9472112655639648
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:20,852 MainThread INFO: EPOCH:60
2024-01-23 00:59:20,852 MainThread INFO: Time Consumed:1.4166369438171387s
2024-01-23 00:59:20,852 MainThread INFO: Total Frames:9900s
  1%|          | 61/10000 [00:48<3:17:59,  1.20s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12078.54903
Train_Epoch_Reward                12677.06883
Running_Training_Average_Rewards  12573.79746
Explore_Time                      0.00108
Train___Time                      0.46576
Eval____Time                      0.94721
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11716.67663
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.31280     1.93479     98.33388     92.52611
alpha_0                           0.97011      0.00014     0.97030      0.96991
Alpha_loss                        -0.20358     0.00100     -0.20219     -0.20504
Training/policy_loss              -2.66348     0.00168     -2.66083     -2.66509
Training/qf1_loss                 18786.46641  1036.96114  19932.16016  17178.97070
Training/qf2_loss                 18824.77695  1035.53980  19968.20703  17215.43359
Training/pf_norm                  0.18954      0.04407     0.24055      0.12850
Training/qf1_norm                 315.42343    14.59408    342.40524    297.97388
Training/qf2_norm                 220.90689    5.75566     231.01987    215.22415
log_std/mean                      -0.10145     0.00387     -0.09515     -0.10512
log_probs/mean                    -2.72991     0.00210     -2.72705     -2.73329
mean/mean                         -0.00007     0.00038     0.00030      -0.00066
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0193939208984375
epoch last part time3 0.0030646324157714844
inside rlalgo, task 0, sumup 71220
epoch first part time 1.0013580322265625e-05
replay_buffer._size: [10200]
collect time 0.0010564327239990234
inner_dict_sum {'sac_diff0': 0.00021982192993164062, 'sac_diff1': 0.006994962692260742, 'sac_diff2': 0.008083820343017578, 'sac_diff3': 0.011036872863769531, 'sac_diff4': 0.007286548614501953, 'sac_diff5': 0.03276824951171875, 'sac_diff6': 0.00038743019104003906, 'all': 0.06677770614624023}
diff5_list [0.006621837615966797, 0.006238460540771484, 0.0064198970794677734, 0.006590604782104492, 0.006897449493408203]
time3 0
time4 0.06754875183105469
time5 0.0675969123840332
time7 9.5367431640625e-07
gen_weight_change tensor(-22.3271)
policy weight change tensor(31.6517, grad_fn=<SumBackward0>)
time8 0.0019199848175048828
train_time 0.07836222648620605
eval time 1.0411098003387451
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:22,000 MainThread INFO: EPOCH:61
2024-01-23 00:59:22,001 MainThread INFO: Time Consumed:1.122999668121338s
2024-01-23 00:59:22,001 MainThread INFO: Total Frames:10050s
  1%|          | 62/10000 [00:49<3:15:29,  1.18s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12038.09959
Train_Epoch_Reward                11521.37379
Running_Training_Average_Rewards  12899.58200
Explore_Time                      0.00105
Train___Time                      0.07836
Eval____Time                      1.04111
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11914.41585
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.60929     2.80597     97.96455     90.46419
alpha_0                           0.96962      0.00014     0.96981      0.96942
Alpha_loss                        -0.20732     0.00109     -0.20572     -0.20903
Training/policy_loss              -2.67551     0.00841     -2.66358     -2.68628
Training/qf1_loss                 17256.05059  1637.23034  20457.22266  16056.29102
Training/qf2_loss                 17295.43418  1638.89750  20499.75000  16094.37207
Training/pf_norm                  0.17747      0.04767     0.25972      0.12895
Training/qf1_norm                 306.52162    10.66954    326.42349    297.27383
Training/qf2_norm                 216.73158    6.29999     228.72963    211.79378
log_std/mean                      -0.10617     0.00056     -0.10540     -0.10699
log_probs/mean                    -2.74166     0.00882     -2.72962     -2.75331
mean/mean                         -0.00006     0.00005     0.00001      -0.00013
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018978357315063477
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71220
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [10350]
collect time 0.0009605884552001953
inner_dict_sum {'sac_diff0': 0.0001933574676513672, 'sac_diff1': 0.00675511360168457, 'sac_diff2': 0.00849008560180664, 'sac_diff3': 0.01049661636352539, 'sac_diff4': 0.007186174392700195, 'sac_diff5': 0.032266855239868164, 'sac_diff6': 0.000392913818359375, 'all': 0.0657811164855957}
diff5_list [0.0065419673919677734, 0.006325960159301758, 0.006961822509765625, 0.0062215328216552734, 0.006215572357177734]
time3 0
time4 0.06662654876708984
time5 0.06667494773864746
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3271)
policy weight change tensor(32.0187, grad_fn=<SumBackward0>)
time8 0.0018210411071777344
train_time 0.07721304893493652
eval time 1.0512850284576416
epoch last part time 6.67572021484375e-06
2024-01-23 00:59:23,155 MainThread INFO: EPOCH:62
2024-01-23 00:59:23,155 MainThread INFO: Time Consumed:1.1320061683654785s
2024-01-23 00:59:23,156 MainThread INFO: Total Frames:10200s
  1%|          | 63/10000 [00:50<3:14:11,  1.17s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12006.72028
Train_Epoch_Reward                26753.15247
Running_Training_Average_Rewards  13254.30396
Explore_Time                      0.00096
Train___Time                      0.07721
Eval____Time                      1.05129
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11926.77056
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.19428     2.61060     97.52467     90.96703
alpha_0                           0.96913      0.00014     0.96933      0.96894
Alpha_loss                        -0.21042     0.00076     -0.20958     -0.21154
Training/policy_loss              -2.66456     0.00864     -2.65689     -2.68105
Training/qf1_loss                 18036.98438  1019.92868  19402.33789  16785.23633
Training/qf2_loss                 18079.25273  1020.67501  19445.15039  16827.07812
Training/pf_norm                  0.13882      0.03869     0.20428      0.10007
Training/qf1_norm                 321.44050    7.67251     329.92444    311.73032
Training/qf2_norm                 220.90625    5.89830     228.40163    213.61064
log_std/mean                      -0.11232     0.00049     -0.11163     -0.11304
log_probs/mean                    -2.73269     0.00860     -2.72533     -2.74916
mean/mean                         0.00050      0.00006     0.00060      0.00043
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01866316795349121
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71220
epoch first part time 3.337860107421875e-06
replay_buffer._size: [10500]
collect time 0.0009853839874267578
inner_dict_sum {'sac_diff0': 0.00020575523376464844, 'sac_diff1': 0.007238626480102539, 'sac_diff2': 0.008433341979980469, 'sac_diff3': 0.011255502700805664, 'sac_diff4': 0.007270097732543945, 'sac_diff5': 0.032541751861572266, 'sac_diff6': 0.00038695335388183594, 'all': 0.06733202934265137}
diff5_list [0.006730556488037109, 0.006749391555786133, 0.006315469741821289, 0.006428956985473633, 0.0063173770904541016]
time3 0
time4 0.06809306144714355
time5 0.06814026832580566
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3271)
policy weight change tensor(32.3576, grad_fn=<SumBackward0>)
time8 0.0018956661224365234
train_time 0.07917547225952148
eval time 1.046111822128296
epoch last part time 6.67572021484375e-06
2024-01-23 00:59:24,306 MainThread INFO: EPOCH:63
2024-01-23 00:59:24,306 MainThread INFO: Time Consumed:1.1287343502044678s
2024-01-23 00:59:24,306 MainThread INFO: Total Frames:10350s
  1%|          | 64/10000 [00:51<3:13:05,  1.17s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11984.33315
Train_Epoch_Reward                4402.59938
Running_Training_Average_Rewards  13236.27681
Explore_Time                      0.00098
Train___Time                      0.07918
Eval____Time                      1.04611
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11972.30973
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.86448     1.86713     95.67446     90.32042
alpha_0                           0.96865      0.00014     0.96884      0.96845
Alpha_loss                        -0.21392     0.00087     -0.21263     -0.21531
Training/policy_loss              -2.66517     0.00776     -2.65306     -2.67691
Training/qf1_loss                 17972.75195  1261.20942  20136.22852  16480.42383
Training/qf2_loss                 18011.90664  1262.22990  20176.94141  16518.98242
Training/pf_norm                  0.16168      0.02804     0.20530      0.12708
Training/qf1_norm                 308.67459    7.32810     319.09000    299.16132
Training/qf2_norm                 215.82010    4.20977     222.18086    210.12660
log_std/mean                      -0.09708     0.00040     -0.09654     -0.09768
log_probs/mean                    -2.73629     0.00782     -2.72406     -2.74813
mean/mean                         0.00071      0.00003     0.00075      0.00065
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018837451934814453
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71220
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [10650]
collect time 0.0010194778442382812
inner_dict_sum {'sac_diff0': 0.0001976490020751953, 'sac_diff1': 0.0070934295654296875, 'sac_diff2': 0.008199214935302734, 'sac_diff3': 0.010684013366699219, 'sac_diff4': 0.006815910339355469, 'sac_diff5': 0.03364706039428711, 'sac_diff6': 0.0003910064697265625, 'all': 0.06702828407287598}
diff5_list [0.007061958312988281, 0.007944107055664062, 0.006324291229248047, 0.0061798095703125, 0.006136894226074219]
time3 0
time4 0.06780457496643066
time5 0.06786012649536133
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3271)
policy weight change tensor(32.7074, grad_fn=<SumBackward0>)
time8 0.0019636154174804688
train_time 0.07889032363891602
eval time 1.0580432415008545
epoch last part time 9.059906005859375e-06
2024-01-23 00:59:25,469 MainThread INFO: EPOCH:64
2024-01-23 00:59:25,469 MainThread INFO: Time Consumed:1.1404516696929932s
2024-01-23 00:59:25,469 MainThread INFO: Total Frames:10500s
  1%|          | 65/10000 [00:53<3:12:54,  1.17s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11959.53737
Train_Epoch_Reward                4663.70135
Running_Training_Average_Rewards  12916.85926
Explore_Time                      0.00101
Train___Time                      0.07889
Eval____Time                      1.05804
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12005.68332
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.90581     3.58910     96.91679     88.28137
alpha_0                           0.96816      0.00014     0.96835      0.96797
Alpha_loss                        -0.21724     0.00091     -0.21603     -0.21841
Training/policy_loss              -2.66528     0.00453     -2.65960     -2.67031
Training/qf1_loss                 17311.77480  1665.74225  19487.26172  14719.71875
Training/qf2_loss                 17354.12441  1667.03703  19530.46875  14760.47363
Training/pf_norm                  0.16586      0.02257     0.20384      0.14329
Training/qf1_norm                 319.11593    12.14667    333.55109    304.81427
Training/qf2_norm                 226.33827    8.38943     235.70926    215.47459
log_std/mean                      -0.11172     0.00050     -0.11104     -0.11246
log_probs/mean                    -2.73439     0.00451     -2.72903     -2.73905
mean/mean                         0.00034      0.00013     0.00052      0.00019
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01868271827697754
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71220
epoch first part time 3.337860107421875e-06
replay_buffer._size: [10800]
collect time 0.0010292530059814453
inside mustsac before update, task 0, sumup 71220
inside mustsac after update, task 0, sumup 70639
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.0075531005859375, 'sac_diff2': 0.008962869644165039, 'sac_diff3': 0.011877298355102539, 'sac_diff4': 0.007915496826171875, 'sac_diff5': 0.052774906158447266, 'sac_diff6': 0.00041747093200683594, 'all': 0.08971571922302246}
diff5_list [0.011376142501831055, 0.009882688522338867, 0.010427236557006836, 0.011083602905273438, 0.01000523567199707]
time3 0.0008904933929443359
time4 0.09060215950012207
time5 0.09065604209899902
time7 0.2029860019683838
gen_weight_change tensor(-22.5974)
policy weight change tensor(33.1871, grad_fn=<SumBackward0>)
time8 0.0021622180938720703
train_time 0.31461071968078613
eval time 0.9198470115661621
epoch last part time 8.106231689453125e-06
2024-01-23 00:59:26,729 MainThread INFO: EPOCH:65
2024-01-23 00:59:26,730 MainThread INFO: Time Consumed:1.2380506992340088s
2024-01-23 00:59:26,730 MainThread INFO: Total Frames:10650s
  1%|          | 66/10000 [00:54<3:17:39,  1.19s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11928.46648
Train_Epoch_Reward                9139.47791
Running_Training_Average_Rewards  13119.04007
Explore_Time                      0.00102
Train___Time                      0.31461
Eval____Time                      0.91985
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12010.46254
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.05287     2.39849    96.73429     89.20405
alpha_0                           0.96767      0.00014    0.96787      0.96748
Alpha_loss                        -0.22048     0.00104    -0.21886     -0.22178
Training/policy_loss              -2.65935     0.00472    -2.65104     -2.66517
Training/qf1_loss                 17370.13125  979.40370  19062.16016  16104.43555
Training/qf2_loss                 17412.41523  981.59499  19109.83984  16144.77930
Training/pf_norm                  0.17770      0.03255    0.21178      0.12989
Training/qf1_norm                 321.42467    19.87109   348.53873    300.19424
Training/qf2_norm                 215.50491    4.96559    224.59854    211.32222
log_std/mean                      -0.10503     0.00204    -0.10115     -0.10678
log_probs/mean                    -2.73037     0.00489    -2.72192     -2.73586
mean/mean                         -0.00077     0.00069    0.00051      -0.00131
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019070863723754883
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70639
epoch first part time 2.86102294921875e-06
replay_buffer._size: [10950]
collect time 0.0009613037109375
inner_dict_sum {'sac_diff0': 0.00019860267639160156, 'sac_diff1': 0.007517099380493164, 'sac_diff2': 0.008746147155761719, 'sac_diff3': 0.011316776275634766, 'sac_diff4': 0.007480621337890625, 'sac_diff5': 0.033829450607299805, 'sac_diff6': 0.0004048347473144531, 'all': 0.06949353218078613}
diff5_list [0.007597208023071289, 0.0074121952056884766, 0.006426095962524414, 0.006267547607421875, 0.00612640380859375]
time3 0
time4 0.07027173042297363
time5 0.07031941413879395
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5974)
policy weight change tensor(33.5112, grad_fn=<SumBackward0>)
time8 0.0019843578338623047
train_time 0.08165931701660156
eval time 1.2512166500091553
epoch last part time 7.867813110351562e-06
2024-01-23 00:59:28,089 MainThread INFO: EPOCH:66
2024-01-23 00:59:28,089 MainThread INFO: Time Consumed:1.3363707065582275s
2024-01-23 00:59:28,089 MainThread INFO: Total Frames:10800s
  1%|          | 67/10000 [00:55<3:25:50,  1.24s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11937.27382
Train_Epoch_Reward                13919.54311
Running_Training_Average_Rewards  13102.83021
Explore_Time                      0.00096
Train___Time                      0.08166
Eval____Time                      1.25122
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12181.45303
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.09299     2.31862     97.95218     91.69348
alpha_0                           0.96719      0.00014     0.96738      0.96700
Alpha_loss                        -0.22401     0.00109     -0.22227     -0.22533
Training/policy_loss              -2.66155     0.00867     -2.65083     -2.67188
Training/qf1_loss                 18199.62910  1224.18144  19534.90625  16223.84863
Training/qf2_loss                 18251.17695  1224.78134  19587.98047  16274.75000
Training/pf_norm                  0.14873      0.02073     0.18189      0.12510
Training/qf1_norm                 359.45426    6.91962     370.23105    350.45514
Training/qf2_norm                 220.02162    5.17352     226.37450    212.45265
log_std/mean                      -0.11179     0.00046     -0.11114     -0.11244
log_probs/mean                    -2.73488     0.00908     -2.72327     -2.74528
mean/mean                         -0.00084     0.00005     -0.00076     -0.00091
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01874995231628418
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70639
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [11100]
collect time 0.0010528564453125
inner_dict_sum {'sac_diff0': 0.00020194053649902344, 'sac_diff1': 0.0074176788330078125, 'sac_diff2': 0.00872492790222168, 'sac_diff3': 0.011239290237426758, 'sac_diff4': 0.007375478744506836, 'sac_diff5': 0.03470015525817871, 'sac_diff6': 0.00040841102600097656, 'all': 0.0700678825378418}
diff5_list [0.0073261260986328125, 0.007628679275512695, 0.006597042083740234, 0.006568193435668945, 0.0065801143646240234]
time3 0
time4 0.07089424133300781
time5 0.07094621658325195
time7 9.5367431640625e-07
gen_weight_change tensor(-22.5974)
policy weight change tensor(33.8223, grad_fn=<SumBackward0>)
time8 0.001903533935546875
train_time 0.08200740814208984
eval time 1.1250948905944824
epoch last part time 8.106231689453125e-06
2024-01-23 00:59:29,322 MainThread INFO: EPOCH:67
2024-01-23 00:59:29,322 MainThread INFO: Time Consumed:1.21063232421875s
2024-01-23 00:59:29,322 MainThread INFO: Total Frames:10950s
  1%|          | 68/10000 [00:56<3:25:19,  1.24s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11962.04478
Train_Epoch_Reward                26632.75007
Running_Training_Average_Rewards  13904.53648
Explore_Time                      0.00105
Train___Time                      0.08201
Eval____Time                      1.12509
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12247.46102
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.61638     1.65165    95.91095     91.16706
alpha_0                           0.96670      0.00014    0.96690      0.96651
Alpha_loss                        -0.22752     0.00111    -0.22591     -0.22918
Training/policy_loss              -2.66445     0.00551    -2.65744     -2.67249
Training/qf1_loss                 17269.58887  877.99847  18389.34961  16237.55762
Training/qf2_loss                 17316.89902  879.34894  18438.42383  16283.41895
Training/pf_norm                  0.15669      0.02947    0.21059      0.12923
Training/qf1_norm                 344.01248    7.96213    353.75824    333.23154
Training/qf2_norm                 217.29724    3.69847    222.49104    211.89197
log_std/mean                      -0.11394     0.00047    -0.11331     -0.11462
log_probs/mean                    -2.73892     0.00597    -2.73112     -2.74779
mean/mean                         -0.00021     0.00010    -0.00009     -0.00038
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018950939178466797
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70639
epoch first part time 3.814697265625e-06
replay_buffer._size: [11250]
collect time 0.0010974407196044922
inner_dict_sum {'sac_diff0': 0.0002071857452392578, 'sac_diff1': 0.006791353225708008, 'sac_diff2': 0.007840871810913086, 'sac_diff3': 0.009913206100463867, 'sac_diff4': 0.006676912307739258, 'sac_diff5': 0.03079843521118164, 'sac_diff6': 0.0003948211669921875, 'all': 0.0626227855682373}
diff5_list [0.006568431854248047, 0.0061740875244140625, 0.006058216094970703, 0.006098508834838867, 0.005899190902709961]
time3 0
time4 0.06334710121154785
time5 0.06339383125305176
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5974)
policy weight change tensor(34.0952, grad_fn=<SumBackward0>)
time8 0.0018050670623779297
train_time 0.07426834106445312
eval time 1.129241943359375
epoch last part time 7.3909759521484375e-06
2024-01-23 00:59:30,552 MainThread INFO: EPOCH:68
2024-01-23 00:59:30,552 MainThread INFO: Time Consumed:1.2071301937103271s
2024-01-23 00:59:30,553 MainThread INFO: Total Frames:11100s
  1%|          | 69/10000 [00:58<3:24:45,  1.24s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11995.01756
Train_Epoch_Reward                16808.03156
Running_Training_Average_Rewards  13589.70002
Explore_Time                      0.00109
Train___Time                      0.07427
Eval____Time                      1.12924
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12208.09332
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.16569     3.40066     97.92473     88.02430
alpha_0                           0.96622      0.00014     0.96641      0.96603
Alpha_loss                        -0.23078     0.00099     -0.22945     -0.23232
Training/policy_loss              -2.66008     0.00316     -2.65638     -2.66473
Training/qf1_loss                 16876.15449  1453.42167  18816.65234  14748.23242
Training/qf2_loss                 16926.47910  1455.68825  18871.24219  14796.28125
Training/pf_norm                  0.13091      0.01120     0.15010      0.11994
Training/qf1_norm                 347.36761    14.47743    372.31470    331.95389
Training/qf2_norm                 212.58652    7.54847     225.38486    203.43973
log_std/mean                      -0.11056     0.00039     -0.11003     -0.11112
log_probs/mean                    -2.73524     0.00334     -2.73168     -2.74061
mean/mean                         -0.00104     0.00034     -0.00060     -0.00154
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018085956573486328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70639
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [11400]
collect time 0.0009047985076904297
inner_dict_sum {'sac_diff0': 0.00021505355834960938, 'sac_diff1': 0.00711369514465332, 'sac_diff2': 0.008239984512329102, 'sac_diff3': 0.01107478141784668, 'sac_diff4': 0.007134914398193359, 'sac_diff5': 0.033223867416381836, 'sac_diff6': 0.0003952980041503906, 'all': 0.0673975944519043}
diff5_list [0.007073640823364258, 0.006383657455444336, 0.006430149078369141, 0.006948232650756836, 0.006388187408447266]
time3 0
time4 0.06816649436950684
time5 0.06821537017822266
time7 9.5367431640625e-07
gen_weight_change tensor(-22.5974)
policy weight change tensor(34.3803, grad_fn=<SumBackward0>)
time8 0.0019872188568115234
train_time 0.07917451858520508
eval time 1.2953810691833496
epoch last part time 7.62939453125e-06
2024-01-23 00:59:31,952 MainThread INFO: EPOCH:69
2024-01-23 00:59:31,952 MainThread INFO: Time Consumed:1.378037691116333s
2024-01-23 00:59:31,952 MainThread INFO: Total Frames:11250s
  1%|          | 70/10000 [00:59<3:33:05,  1.29s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12015.90276
Train_Epoch_Reward                6800.69577
Running_Training_Average_Rewards  13519.85442
Explore_Time                      0.00090
Train___Time                      0.07917
Eval____Time                      1.29538
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11975.70163
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.68633     0.85988    95.00813     92.68384
alpha_0                           0.96573      0.00014    0.96593      0.96554
Alpha_loss                        -0.23402     0.00106    -0.23252     -0.23543
Training/policy_loss              -2.65504     0.00312    -2.65130     -2.66039
Training/qf1_loss                 17206.28652  547.05455  17744.37695  16153.60645
Training/qf2_loss                 17257.81211  547.77611  17797.58789  16203.96289
Training/pf_norm                  0.14045      0.01374    0.15753      0.12555
Training/qf1_norm                 353.29311    4.17250    360.03183    347.60284
Training/qf2_norm                 223.90619    1.94388    226.86935    221.64383
log_std/mean                      -0.11284     0.00042    -0.11229     -0.11346
log_probs/mean                    -2.73133     0.00354    -2.72692     -2.73715
mean/mean                         -0.00184     0.00011    -0.00168     -0.00198
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.023793697357177734
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70639
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [11550]
collect time 0.0012478828430175781
inside mustsac before update, task 0, sumup 70639
inside mustsac after update, task 0, sumup 70266
inner_dict_sum {'sac_diff0': 0.00021147727966308594, 'sac_diff1': 0.008600950241088867, 'sac_diff2': 0.010372638702392578, 'sac_diff3': 0.011740684509277344, 'sac_diff4': 0.009071826934814453, 'sac_diff5': 0.05677032470703125, 'sac_diff6': 0.0004303455352783203, 'all': 0.0971982479095459}
diff5_list [0.013016462326049805, 0.009937763214111328, 0.01178121566772461, 0.01090693473815918, 0.011127948760986328]
time3 0.0009717941284179688
time4 0.09822678565979004
time5 0.09829473495483398
time7 0.18844819068908691
gen_weight_change tensor(-23.1529)
policy weight change tensor(34.7863, grad_fn=<SumBackward0>)
time8 0.003030538558959961
train_time 0.3104848861694336
eval time 0.9395184516906738
epoch last part time 7.3909759521484375e-06
2024-01-23 00:59:33,234 MainThread INFO: EPOCH:70
2024-01-23 00:59:33,234 MainThread INFO: Time Consumed:1.2538044452667236s
2024-01-23 00:59:33,235 MainThread INFO: Total Frames:11400s
  1%|          | 71/10000 [01:00<3:32:44,  1.29s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12029.75780
Train_Epoch_Reward                14288.51103
Running_Training_Average_Rewards  13557.82538
Explore_Time                      0.00124
Train___Time                      0.31048
Eval____Time                      0.93952
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11855.22705
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.81777     0.70773    98.45094     96.50807
alpha_0                           0.96525      0.00014    0.96544      0.96506
Alpha_loss                        -0.23747     0.00102    -0.23603     -0.23893
Training/policy_loss              -2.65641     0.00323    -2.65150     -2.66037
Training/qf1_loss                 18639.09258  343.29888  18902.30859  17972.56641
Training/qf2_loss                 18693.82461  343.50561  18957.28516  18028.09375
Training/pf_norm                  0.12086      0.03953    0.19491      0.07890
Training/qf1_norm                 367.90856    20.18179   397.51019    343.43219
Training/qf2_norm                 227.67317    3.93199    233.03041    222.69432
log_std/mean                      -0.11718     0.00315    -0.11167     -0.12067
log_probs/mean                    -2.73327     0.00335    -2.72796     -2.73650
mean/mean                         -0.00191     0.00048    -0.00114     -0.00253
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019544124603271484
epoch last part time3 0.002962350845336914
inside rlalgo, task 0, sumup 70266
epoch first part time 2.86102294921875e-06
replay_buffer._size: [11700]
collect time 0.0009703636169433594
inner_dict_sum {'sac_diff0': 0.00023055076599121094, 'sac_diff1': 0.007292509078979492, 'sac_diff2': 0.008673906326293945, 'sac_diff3': 0.010928153991699219, 'sac_diff4': 0.007531404495239258, 'sac_diff5': 0.033504486083984375, 'sac_diff6': 0.0004069805145263672, 'all': 0.06856799125671387}
diff5_list [0.006806373596191406, 0.006697416305541992, 0.007391929626464844, 0.006447315216064453, 0.00616145133972168]
time3 0
time4 0.06940984725952148
time5 0.06946396827697754
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1529)
policy weight change tensor(35.0715, grad_fn=<SumBackward0>)
time8 0.0018701553344726562
train_time 0.08035492897033691
eval time 1.1608264446258545
epoch last part time 8.344650268554688e-06
2024-01-23 00:59:34,504 MainThread INFO: EPOCH:71
2024-01-23 00:59:34,505 MainThread INFO: Time Consumed:1.2446362972259521s
2024-01-23 00:59:34,505 MainThread INFO: Total Frames:11550s
  1%|          | 72/10000 [01:02<3:31:45,  1.28s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12013.68477
Train_Epoch_Reward                20956.82502
Running_Training_Average_Rewards  13934.14988
Explore_Time                      0.00097
Train___Time                      0.08035
Eval____Time                      1.16083
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11753.68551
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       95.62224     1.53944    97.27892     93.24989
alpha_0                           0.96477      0.00014    0.96496      0.96457
Alpha_loss                        -0.24066     0.00093    -0.23913     -0.24187
Training/policy_loss              -2.65057     0.00520    -2.64610     -2.66035
Training/qf1_loss                 18976.60625  548.96816  19409.94727  17968.38281
Training/qf2_loss                 19032.38867  549.11460  19466.03320  18023.36328
Training/pf_norm                  0.17275      0.02943    0.23077      0.14946
Training/qf1_norm                 375.34693    4.80486    379.82526    367.80951
Training/qf2_norm                 222.58970    3.43622    226.35573    217.27126
log_std/mean                      -0.11741     0.00043    -0.11681     -0.11804
log_probs/mean                    -2.72826     0.00530    -2.72320     -2.73813
mean/mean                         -0.00275     0.00006    -0.00265     -0.00280
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018546342849731445
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70266
epoch first part time 2.86102294921875e-06
replay_buffer._size: [11850]
collect time 0.0008180141448974609
inner_dict_sum {'sac_diff0': 0.00020647048950195312, 'sac_diff1': 0.00673675537109375, 'sac_diff2': 0.007698535919189453, 'sac_diff3': 0.009981632232666016, 'sac_diff4': 0.007028818130493164, 'sac_diff5': 0.03166985511779785, 'sac_diff6': 0.0003814697265625, 'all': 0.06370353698730469}
diff5_list [0.0067174434661865234, 0.00625300407409668, 0.006191253662109375, 0.006461381912231445, 0.006046772003173828]
time3 0
time4 0.06445646286010742
time5 0.06450223922729492
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1529)
policy weight change tensor(35.3378, grad_fn=<SumBackward0>)
time8 0.0018024444580078125
train_time 0.07498502731323242
eval time 1.2771415710449219
epoch last part time 7.867813110351562e-06
2024-01-23 00:59:35,882 MainThread INFO: EPOCH:72
2024-01-23 00:59:35,882 MainThread INFO: Time Consumed:1.3554842472076416s
2024-01-23 00:59:35,882 MainThread INFO: Total Frames:11700s
  1%|          | 73/10000 [01:03<3:36:35,  1.31s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11987.57632
Train_Epoch_Reward                57293.68632
Running_Training_Average_Rewards  15611.44110
Explore_Time                      0.00081
Train___Time                      0.07499
Eval____Time                      1.27714
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11665.68608
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.29003     2.83621     100.75831    93.13181
alpha_0                           0.96428      0.00014     0.96448      0.96409
Alpha_loss                        -0.24433     0.00098     -0.24328     -0.24583
Training/policy_loss              -2.65681     0.00561     -2.64918     -2.66533
Training/qf1_loss                 19818.87539  1672.28787  23040.20703  18457.39453
Training/qf2_loss                 19876.10313  1673.02222  23098.82031  18514.69727
Training/pf_norm                  0.14623      0.03325     0.18918      0.09398
Training/qf1_norm                 378.40478    8.95474     391.95767    367.80301
Training/qf2_norm                 226.91523    6.43407     237.06490    219.72215
log_std/mean                      -0.12082     0.00040     -0.12026     -0.12139
log_probs/mean                    -2.73635     0.00581     -2.72816     -2.74463
mean/mean                         -0.00195     0.00005     -0.00191     -0.00204
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01860213279724121
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70266
epoch first part time 2.86102294921875e-06
replay_buffer._size: [12000]
collect time 0.0009584426879882812
inner_dict_sum {'sac_diff0': 0.000213623046875, 'sac_diff1': 0.007262229919433594, 'sac_diff2': 0.008131742477416992, 'sac_diff3': 0.011042356491088867, 'sac_diff4': 0.007356405258178711, 'sac_diff5': 0.03331756591796875, 'sac_diff6': 0.0004119873046875, 'all': 0.06773591041564941}
diff5_list [0.0075511932373046875, 0.0067021846771240234, 0.006195068359375, 0.006707668304443359, 0.00616145133972168]
time3 0
time4 0.0685112476348877
time5 0.06856131553649902
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1529)
policy weight change tensor(35.5608, grad_fn=<SumBackward0>)
time8 0.0019855499267578125
train_time 0.0796668529510498
eval time 1.243018388748169
epoch last part time 6.9141387939453125e-06
2024-01-23 00:59:37,230 MainThread INFO: EPOCH:73
2024-01-23 00:59:37,230 MainThread INFO: Time Consumed:1.3261380195617676s
2024-01-23 00:59:37,231 MainThread INFO: Total Frames:11850s
  1%|          | 74/10000 [01:04<3:38:31,  1.32s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11958.34093
Train_Epoch_Reward                6215.92624
Running_Training_Average_Rewards  14296.57102
Explore_Time                      0.00095
Train___Time                      0.07967
Eval____Time                      1.24302
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11679.95579
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.83000     4.73698     103.63484    89.07632
alpha_0                           0.96380      0.00014     0.96399      0.96361
Alpha_loss                        -0.24748     0.00096     -0.24622     -0.24888
Training/policy_loss              -2.65026     0.00204     -2.64705     -2.65336
Training/qf1_loss                 20254.17246  2830.37401  24348.23047  15655.96582
Training/qf2_loss                 20313.28418  2832.92981  24410.75391  15711.02832
Training/pf_norm                  0.16173      0.02570     0.20605      0.12915
Training/qf1_norm                 383.62241    17.49933    408.11407    355.36942
Training/qf2_norm                 229.81580    10.83932    245.40247    212.06889
log_std/mean                      -0.12175     0.00033     -0.12129     -0.12222
log_probs/mean                    -2.73000     0.00211     -2.72640     -2.73267
mean/mean                         -0.00277     0.00010     -0.00261     -0.00288
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01867079734802246
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70266
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [12150]
collect time 0.0010228157043457031
inner_dict_sum {'sac_diff0': 0.00021028518676757812, 'sac_diff1': 0.006977558135986328, 'sac_diff2': 0.007864236831665039, 'sac_diff3': 0.010579586029052734, 'sac_diff4': 0.00708460807800293, 'sac_diff5': 0.0323026180267334, 'sac_diff6': 0.000396728515625, 'all': 0.06541562080383301}
diff5_list [0.00735020637512207, 0.006165742874145508, 0.006256580352783203, 0.006239652633666992, 0.006290435791015625]
time3 0
time4 0.06618118286132812
time5 0.06622719764709473
time7 9.5367431640625e-07
gen_weight_change tensor(-23.1529)
policy weight change tensor(35.7210, grad_fn=<SumBackward0>)
time8 0.0018622875213623047
train_time 0.07702517509460449
eval time 1.2085046768188477
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:38,541 MainThread INFO: EPOCH:74
2024-01-23 00:59:38,542 MainThread INFO: Time Consumed:1.2890353202819824s
2024-01-23 00:59:38,542 MainThread INFO: Total Frames:12000s
  1%|          | 75/10000 [01:06<3:38:00,  1.32s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11928.79987
Train_Epoch_Reward                4659.57732
Running_Training_Average_Rewards  12831.33488
Explore_Time                      0.00102
Train___Time                      0.07703
Eval____Time                      1.20850
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11710.27274
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       94.34268     1.23043    95.40331     92.06414
alpha_0                           0.96332      0.00014    0.96351      0.96312
Alpha_loss                        -0.25120     0.00083    -0.25000     -0.25245
Training/policy_loss              -2.66067     0.00714    -2.64808     -2.66791
Training/qf1_loss                 19111.38359  729.87227  20298.61523  18187.79297
Training/qf2_loss                 19168.17227  731.14394  20356.55078  18242.10547
Training/pf_norm                  0.10658      0.01657    0.12779      0.08726
Training/qf1_norm                 359.95516    6.89128    368.06503    348.05386
Training/qf2_norm                 225.71318    2.88028    228.25438    220.38287
log_std/mean                      -0.13023     0.00025    -0.12989     -0.13060
log_probs/mean                    -2.73934     0.00718    -2.72654     -2.74686
mean/mean                         -0.00247     0.00005    -0.00238     -0.00251
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018558502197265625
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70266
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [12300]
collect time 0.0008697509765625
inside mustsac before update, task 0, sumup 70266
inside mustsac after update, task 0, sumup 70867
inner_dict_sum {'sac_diff0': 0.00019693374633789062, 'sac_diff1': 0.006755828857421875, 'sac_diff2': 0.007790088653564453, 'sac_diff3': 0.011881351470947266, 'sac_diff4': 0.007152080535888672, 'sac_diff5': 0.049141645431518555, 'sac_diff6': 0.00040411949157714844, 'all': 0.08332204818725586}
diff5_list [0.010318279266357422, 0.009495019912719727, 0.009557247161865234, 0.010101079940795898, 0.009670019149780273]
time3 0.0008633136749267578
time4 0.08416366577148438
time5 0.08422183990478516
time7 0.2138512134552002
gen_weight_change tensor(-23.6089)
policy weight change tensor(36.0415, grad_fn=<SumBackward0>)
time8 0.002047300338745117
train_time 0.31771039962768555
eval time 0.996314525604248
epoch last part time 8.344650268554688e-06
2024-01-23 00:59:39,881 MainThread INFO: EPOCH:75
2024-01-23 00:59:39,881 MainThread INFO: Time Consumed:1.3174011707305908s
2024-01-23 00:59:39,881 MainThread INFO: Total Frames:12150s
  1%|          | 76/10000 [01:07<3:39:03,  1.32s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11901.98129
Train_Epoch_Reward                4895.08557
Running_Training_Average_Rewards  12631.77306
Explore_Time                      0.00086
Train___Time                      0.31771
Eval____Time                      0.99631
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11742.27678
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.91207     1.36084    95.52563     92.10475
alpha_0                           0.96283      0.00014    0.96303      0.96264
Alpha_loss                        -0.25449     0.00101    -0.25319     -0.25610
Training/policy_loss              -2.65471     0.00410    -2.65078     -2.66226
Training/qf1_loss                 18583.99062  941.20010  20049.33398  17303.70117
Training/qf2_loss                 18643.25820  941.76533  20109.67773  17362.97852
Training/pf_norm                  0.13392      0.03086    0.17507      0.09023
Training/qf1_norm                 368.92374    5.10589    377.65533    363.13126
Training/qf2_norm                 218.85817    4.47170    224.29114    211.79727
log_std/mean                      -0.12091     0.00677    -0.10900     -0.12823
log_probs/mean                    -2.73696     0.00379    -2.73352     -2.74371
mean/mean                         -0.00215     0.00120    -0.00056     -0.00368
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018634796142578125
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70867
epoch first part time 2.86102294921875e-06
replay_buffer._size: [12450]
collect time 0.0009465217590332031
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.007042646408081055, 'sac_diff2': 0.007958650588989258, 'sac_diff3': 0.010207414627075195, 'sac_diff4': 0.0068206787109375, 'sac_diff5': 0.03214693069458008, 'sac_diff6': 0.00038743019104003906, 'all': 0.06477785110473633}
diff5_list [0.006732463836669922, 0.00629425048828125, 0.006165504455566406, 0.006766557693481445, 0.006188154220581055]
time3 0
time4 0.06554794311523438
time5 0.06559324264526367
time7 7.152557373046875e-07
gen_weight_change tensor(-23.6089)
policy weight change tensor(36.1661, grad_fn=<SumBackward0>)
time8 0.0019505023956298828
train_time 0.07654833793640137
eval time 1.370835304260254
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:41,354 MainThread INFO: EPOCH:76
2024-01-23 00:59:41,354 MainThread INFO: Time Consumed:1.4508137702941895s
2024-01-23 00:59:41,354 MainThread INFO: Total Frames:12300s
  1%|          | 77/10000 [01:08<3:46:24,  1.37s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11896.79863
Train_Epoch_Reward                11773.40093
Running_Training_Average_Rewards  12737.17856
Explore_Time                      0.00094
Train___Time                      0.07655
Eval____Time                      1.37084
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12129.62637
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.46041     2.95339     97.59296     89.28680
alpha_0                           0.96235      0.00014     0.96254      0.96216
Alpha_loss                        -0.25773     0.00104     -0.25627     -0.25938
Training/policy_loss              -2.65237     0.00466     -2.64707     -2.65914
Training/qf1_loss                 18201.79766  1785.42521  20500.08984  15444.25586
Training/qf2_loss                 18263.26387  1786.97328  20562.01953  15502.39355
Training/pf_norm                  0.17444      0.05642     0.27201      0.09838
Training/qf1_norm                 373.45369    11.94847    389.46548    353.35938
Training/qf2_norm                 232.00988    6.93887     239.43187    219.80482
log_std/mean                      -0.11953     0.00021     -0.11921     -0.11980
log_probs/mean                    -2.73342     0.00496     -2.72819     -2.74102
mean/mean                         -0.00288     0.00008     -0.00276     -0.00297
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01857447624206543
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70867
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [12600]
collect time 0.0009641647338867188
inner_dict_sum {'sac_diff0': 0.00021004676818847656, 'sac_diff1': 0.007942914962768555, 'sac_diff2': 0.009952545166015625, 'sac_diff3': 0.012777566909790039, 'sac_diff4': 0.00869441032409668, 'sac_diff5': 0.039398908615112305, 'sac_diff6': 0.0004668235778808594, 'all': 0.07944321632385254}
diff5_list [0.007059574127197266, 0.008214950561523438, 0.007857799530029297, 0.008236169815063477, 0.008030414581298828]
time3 0
time4 0.08039283752441406
time5 0.08046174049377441
time7 7.152557373046875e-07
gen_weight_change tensor(-23.6089)
policy weight change tensor(36.2505, grad_fn=<SumBackward0>)
time8 0.0019376277923583984
train_time 0.09180712699890137
eval time 1.3478689193725586
epoch last part time 6.67572021484375e-06
2024-01-23 00:59:42,819 MainThread INFO: EPOCH:77
2024-01-23 00:59:42,819 MainThread INFO: Time Consumed:1.4431307315826416s
2024-01-23 00:59:42,819 MainThread INFO: Total Frames:12450s
  1%|          | 78/10000 [01:10<3:51:08,  1.40s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11882.36600
Train_Epoch_Reward                25362.14478
Running_Training_Average_Rewards  13503.07987
Explore_Time                      0.00096
Train___Time                      0.09181
Eval____Time                      1.34787
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12103.13474
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.76992     2.44629     98.77600     91.95827
alpha_0                           0.96187      0.00014     0.96206      0.96167
Alpha_loss                        -0.26126     0.00095     -0.26002     -0.26257
Training/policy_loss              -2.65442     0.00231     -2.65184     -2.65779
Training/qf1_loss                 19473.38398  1512.88663  20782.78711  16635.95312
Training/qf2_loss                 19540.02109  1513.95678  20850.10352  16700.62695
Training/pf_norm                  0.10923      0.01902     0.14084      0.08663
Training/qf1_norm                 389.48936    8.05339     395.46616    374.09003
Training/qf2_norm                 233.12825    5.69019     237.83040    221.95380
log_std/mean                      -0.12264     0.00014     -0.12245     -0.12285
log_probs/mean                    -2.73721     0.00231     -2.73425     -2.74015
mean/mean                         -0.00320     0.00006     -0.00310     -0.00327
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018310546875
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70867
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [12750]
collect time 0.0008711814880371094
inner_dict_sum {'sac_diff0': 0.00024127960205078125, 'sac_diff1': 0.007389068603515625, 'sac_diff2': 0.009072065353393555, 'sac_diff3': 0.011778116226196289, 'sac_diff4': 0.0077135562896728516, 'sac_diff5': 0.034941911697387695, 'sac_diff6': 0.0004038810729980469, 'all': 0.07153987884521484}
diff5_list [0.006781816482543945, 0.00637507438659668, 0.007509708404541016, 0.007317304611206055, 0.0069580078125]
time3 0
time4 0.07235050201416016
time5 0.07239842414855957
time7 7.152557373046875e-07
gen_weight_change tensor(-23.6089)
policy weight change tensor(36.3231, grad_fn=<SumBackward0>)
time8 0.0019161701202392578
train_time 0.08313345909118652
eval time 1.2884738445281982
epoch last part time 7.867813110351562e-06
2024-01-23 00:59:44,216 MainThread INFO: EPOCH:78
2024-01-23 00:59:44,216 MainThread INFO: Time Consumed:1.3749699592590332s
2024-01-23 00:59:44,216 MainThread INFO: Total Frames:12600s
  1%|          | 79/10000 [01:11<3:51:07,  1.40s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11871.84563
Train_Epoch_Reward                21166.49348
Running_Training_Average_Rewards  13908.93302
Explore_Time                      0.00087
Train___Time                      0.08313
Eval____Time                      1.28847
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12102.88963
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       94.63781     1.63707    96.52998     92.16480
alpha_0                           0.96139      0.00014    0.96158      0.96119
Alpha_loss                        -0.26447     0.00096    -0.26344     -0.26598
Training/policy_loss              -2.64783     0.00770    -2.63333     -2.65623
Training/qf1_loss                 18081.42656  551.27299  19142.20703  17544.02734
Training/qf2_loss                 18150.65547  551.52765  19211.98242  17614.46484
Training/pf_norm                  0.10258      0.01492    0.12285      0.08792
Training/qf1_norm                 409.66751    8.86455    419.28265    395.22989
Training/qf2_norm                 221.57541    3.74400    225.90544    215.90108
log_std/mean                      -0.12970     0.00011    -0.12955     -0.12987
log_probs/mean                    -2.73305     0.00794    -2.71796     -2.74124
mean/mean                         -0.00252     0.00002    -0.00249     -0.00254
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019455432891845703
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70867
epoch first part time 2.86102294921875e-06
replay_buffer._size: [12900]
collect time 0.0009024143218994141
inner_dict_sum {'sac_diff0': 0.00020003318786621094, 'sac_diff1': 0.006501913070678711, 'sac_diff2': 0.007783651351928711, 'sac_diff3': 0.010218620300292969, 'sac_diff4': 0.00700068473815918, 'sac_diff5': 0.03214883804321289, 'sac_diff6': 0.0003840923309326172, 'all': 0.06423783302307129}
diff5_list [0.007272005081176758, 0.00648045539855957, 0.0062177181243896484, 0.006053447723388672, 0.006125211715698242]
time3 0
time4 0.06496548652648926
time5 0.06500840187072754
time7 9.5367431640625e-07
gen_weight_change tensor(-23.6089)
policy weight change tensor(36.4258, grad_fn=<SumBackward0>)
time8 0.0018963813781738281
train_time 0.07543110847473145
eval time 1.2645745277404785
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:45,582 MainThread INFO: EPOCH:79
2024-01-23 00:59:45,582 MainThread INFO: Time Consumed:1.343360185623169s
2024-01-23 00:59:45,582 MainThread INFO: Total Frames:12750s
  1%|          | 80/10000 [01:13<3:49:29,  1.39s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11880.68844
Train_Epoch_Reward                20286.49980
Running_Training_Average_Rewards  14216.59430
Explore_Time                      0.00090
Train___Time                      0.07543
Eval____Time                      1.26457
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12064.12974
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       95.38008     0.81470    96.25272     93.84330
alpha_0                           0.96090      0.00014    0.96110      0.96071
Alpha_loss                        -0.26798     0.00093    -0.26677     -0.26908
Training/policy_loss              -2.64978     0.00761    -2.63828     -2.65970
Training/qf1_loss                 19178.84492  848.55574  20379.64648  17826.35938
Training/qf2_loss                 19245.58711  848.24828  20445.35742  17893.38477
Training/pf_norm                  0.11927      0.01271    0.13256      0.09516
Training/qf1_norm                 385.93361    2.90190    391.23975    383.60559
Training/qf2_norm                 220.80039    1.78972    222.76668    217.43430
log_std/mean                      -0.13224     0.00021    -0.13195     -0.13253
log_probs/mean                    -2.73643     0.00782    -2.72502     -2.74703
mean/mean                         -0.00158     0.00002    -0.00155     -0.00162
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0186307430267334
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70867
epoch first part time 2.86102294921875e-06
replay_buffer._size: [13050]
collect time 0.0009412765502929688
inside mustsac before update, task 0, sumup 70867
inside mustsac after update, task 0, sumup 70791
inner_dict_sum {'sac_diff0': 0.00019598007202148438, 'sac_diff1': 0.006510734558105469, 'sac_diff2': 0.008033037185668945, 'sac_diff3': 0.010051250457763672, 'sac_diff4': 0.006996631622314453, 'sac_diff5': 0.049590349197387695, 'sac_diff6': 0.00039887428283691406, 'all': 0.08177685737609863}
diff5_list [0.010180234909057617, 0.010323762893676758, 0.009840965270996094, 0.009693145751953125, 0.009552240371704102]
time3 0.0008492469787597656
time4 0.08260679244995117
time5 0.0826578140258789
time7 0.22588753700256348
gen_weight_change tensor(-23.8592)
policy weight change tensor(36.6985, grad_fn=<SumBackward0>)
time8 0.002764463424682617
train_time 0.32906031608581543
eval time 1.0575339794158936
epoch last part time 6.9141387939453125e-06
2024-01-23 00:59:46,994 MainThread INFO: EPOCH:80
2024-01-23 00:59:46,994 MainThread INFO: Time Consumed:1.3900039196014404s
2024-01-23 00:59:46,994 MainThread INFO: Total Frames:12900s
  1%|          | 81/10000 [01:14<3:50:46,  1.40s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11896.63926
Train_Epoch_Reward                11781.08725
Running_Training_Average_Rewards  14281.82567
Explore_Time                      0.00094
Train___Time                      0.32906
Eval____Time                      1.05753
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12014.73519
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       94.99086     2.67105    98.06980     90.79045
alpha_0                           0.96042      0.00014    0.96061      0.96023
Alpha_loss                        -0.27102     0.00102    -0.26949     -0.27240
Training/policy_loss              -2.64073     0.00387    -2.63528     -2.64434
Training/qf1_loss                 18243.14004  978.41713  19254.69922  16376.34082
Training/qf2_loss                 18314.15820  979.86485  19327.48047  16444.80469
Training/pf_norm                  0.17267      0.03522    0.22263      0.11819
Training/qf1_norm                 406.05504    14.70485   425.34222    385.69464
Training/qf2_norm                 221.08352    6.71468    227.97646    212.81694
log_std/mean                      -0.12246     0.00199    -0.11971     -0.12567
log_probs/mean                    -2.72790     0.00434    -2.72247     -2.73349
mean/mean                         -0.00147     0.00050    -0.00068     -0.00219
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01849818229675293
epoch last part time3 0.0026865005493164062
inside rlalgo, task 0, sumup 70791
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [13200]
collect time 0.0008332729339599609
inner_dict_sum {'sac_diff0': 0.0001990795135498047, 'sac_diff1': 0.006760358810424805, 'sac_diff2': 0.00780034065246582, 'sac_diff3': 0.009997129440307617, 'sac_diff4': 0.006960153579711914, 'sac_diff5': 0.031964778900146484, 'sac_diff6': 0.0003821849822998047, 'all': 0.06406402587890625}
diff5_list [0.006367206573486328, 0.006391763687133789, 0.006598949432373047, 0.006386995315551758, 0.0062198638916015625]
time3 0
time4 0.06483960151672363
time5 0.06488704681396484
time7 4.76837158203125e-07
gen_weight_change tensor(-23.8592)
policy weight change tensor(36.8186, grad_fn=<SumBackward0>)
time8 0.0018639564514160156
train_time 0.07555913925170898
eval time 1.3152296543121338
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:48,412 MainThread INFO: EPOCH:81
2024-01-23 00:59:48,412 MainThread INFO: Time Consumed:1.3940763473510742s
2024-01-23 00:59:48,412 MainThread INFO: Total Frames:13050s
  1%|          | 82/10000 [01:16<3:51:44,  1.40s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11933.81094
Train_Epoch_Reward                25510.77467
Running_Training_Average_Rewards  14911.35557
Explore_Time                      0.00083
Train___Time                      0.07556
Eval____Time                      1.31523
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12125.40229
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       96.20334     1.05498    97.33022     94.69673
alpha_0                           0.95994      0.00014    0.96013      0.95975
Alpha_loss                        -0.27455     0.00086    -0.27342     -0.27595
Training/policy_loss              -2.64504     0.00727    -2.63445     -2.65462
Training/qf1_loss                 19496.32539  520.96958  20093.11914  18659.25391
Training/qf2_loss                 19575.56289  521.65789  20174.93164  18738.57812
Training/pf_norm                  0.12802      0.04077    0.18761      0.07437
Training/qf1_norm                 439.20091    6.12580    449.19107    429.96329
Training/qf2_norm                 233.41368    2.43552    236.08853    229.93445
log_std/mean                      -0.12850     0.00020    -0.12823     -0.12880
log_probs/mean                    -2.73192     0.00740    -2.72088     -2.74163
mean/mean                         -0.00230     0.00005    -0.00223     -0.00236
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01884150505065918
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70791
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [13350]
collect time 0.0009362697601318359
inner_dict_sum {'sac_diff0': 0.00022411346435546875, 'sac_diff1': 0.007321834564208984, 'sac_diff2': 0.008764982223510742, 'sac_diff3': 0.011005640029907227, 'sac_diff4': 0.007178544998168945, 'sac_diff5': 0.03333687782287598, 'sac_diff6': 0.00039458274841308594, 'all': 0.06822657585144043}
diff5_list [0.0072634220123291016, 0.0070421695709228516, 0.006397247314453125, 0.006463050842285156, 0.006170988082885742]
time3 0
time4 0.06902313232421875
time5 0.06907463073730469
time7 4.76837158203125e-07
gen_weight_change tensor(-23.8592)
policy weight change tensor(36.9743, grad_fn=<SumBackward0>)
time8 0.0019097328186035156
train_time 0.08004617691040039
eval time 1.4352772235870361
epoch last part time 7.867813110351562e-06
2024-01-23 00:59:49,953 MainThread INFO: EPOCH:82
2024-01-23 00:59:49,954 MainThread INFO: Time Consumed:1.5188250541687012s
2024-01-23 00:59:49,954 MainThread INFO: Total Frames:13200s
  1%|          | 83/10000 [01:17<3:58:40,  1.44s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11983.03438
Train_Epoch_Reward                5353.02366
Running_Training_Average_Rewards  14782.57006
Explore_Time                      0.00093
Train___Time                      0.08005
Eval____Time                      1.43528
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12157.92051
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       96.27878     2.29888    99.13920     92.62800
alpha_0                           0.95946      0.00014    0.95965      0.95927
Alpha_loss                        -0.27780     0.00118    -0.27610     -0.27930
Training/policy_loss              -2.64001     0.00628    -2.63246     -2.64811
Training/qf1_loss                 19303.93672  724.92027  20394.79102  18487.73828
Training/qf2_loss                 19384.09336  726.36253  20477.05078  18568.43164
Training/pf_norm                  0.12633      0.01557    0.14833      0.11249
Training/qf1_norm                 441.00983    10.85782   452.92932    420.93591
Training/qf2_norm                 228.20932    5.30862    234.76146    219.72203
log_std/mean                      -0.12176     0.00026    -0.12143     -0.12217
log_probs/mean                    -2.72879     0.00683    -2.72037     -2.73724
mean/mean                         -0.00235     0.00007    -0.00223     -0.00241
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019531726837158203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70791
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [13500]
collect time 0.0009245872497558594
inner_dict_sum {'sac_diff0': 0.0002110004425048828, 'sac_diff1': 0.007115364074707031, 'sac_diff2': 0.00812220573425293, 'sac_diff3': 0.010377645492553711, 'sac_diff4': 0.007192134857177734, 'sac_diff5': 0.03308463096618652, 'sac_diff6': 0.0003871917724609375, 'all': 0.06649017333984375}
diff5_list [0.006738424301147461, 0.0065174102783203125, 0.0065419673919677734, 0.006971120834350586, 0.006315708160400391]
time3 0
time4 0.0672750473022461
time5 0.06732511520385742
time7 4.76837158203125e-07
gen_weight_change tensor(-23.8592)
policy weight change tensor(37.1213, grad_fn=<SumBackward0>)
time8 0.0020143985748291016
train_time 0.07865428924560547
eval time 1.5012173652648926
epoch last part time 8.106231689453125e-06
2024-01-23 00:59:51,560 MainThread INFO: EPOCH:83
2024-01-23 00:59:51,560 MainThread INFO: Time Consumed:1.5833027362823486s
2024-01-23 00:59:51,560 MainThread INFO: Total Frames:13350s
  1%|          | 84/10000 [01:19<4:06:41,  1.49s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12033.76763
Train_Epoch_Reward                9655.11851
Running_Training_Average_Rewards  14991.15920
Explore_Time                      0.00092
Train___Time                      0.07865
Eval____Time                      1.50122
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12187.28832
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.40815     2.23745     100.03150    93.42352
alpha_0                           0.95898      0.00014     0.95917      0.95879
Alpha_loss                        -0.28149     0.00101     -0.27988     -0.28272
Training/policy_loss              -2.64672     0.00397     -2.64130     -2.65171
Training/qf1_loss                 19224.65977  1531.90796  21575.81250  17385.31055
Training/qf2_loss                 19304.72344  1533.67148  21658.78125  17463.79492
Training/pf_norm                  0.12088      0.01850     0.14855      0.10186
Training/qf1_norm                 444.85964    11.25186    461.18665    433.98511
Training/qf2_norm                 232.94450    5.22218     241.38753    225.99841
log_std/mean                      -0.12919     0.00021     -0.12890     -0.12951
log_probs/mean                    -2.73639     0.00420     -2.73021     -2.74133
mean/mean                         -0.00171     0.00007     -0.00165     -0.00184
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019164562225341797
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70791
epoch first part time 3.337860107421875e-06
replay_buffer._size: [13650]
collect time 0.0009660720825195312
inner_dict_sum {'sac_diff0': 0.0002269744873046875, 'sac_diff1': 0.008922576904296875, 'sac_diff2': 0.011592388153076172, 'sac_diff3': 0.013450145721435547, 'sac_diff4': 0.00981903076171875, 'sac_diff5': 0.04146313667297363, 'sac_diff6': 0.0005469322204589844, 'all': 0.08602118492126465}
diff5_list [0.007828950881958008, 0.011719465255737305, 0.008829355239868164, 0.006593942642211914, 0.006491422653198242]
time3 0
time4 0.08725333213806152
time5 0.0873258113861084
time7 4.76837158203125e-07
gen_weight_change tensor(-23.8592)
policy weight change tensor(37.2124, grad_fn=<SumBackward0>)
time8 0.0022950172424316406
train_time 0.0999295711517334
eval time 1.3521134853363037
epoch last part time 7.62939453125e-06
2024-01-23 00:59:53,039 MainThread INFO: EPOCH:84
2024-01-23 00:59:53,039 MainThread INFO: Time Consumed:1.4557900428771973s
2024-01-23 00:59:53,040 MainThread INFO: Total Frames:13500s
  1%|          | 85/10000 [01:20<4:06:00,  1.49s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12081.55798
Train_Epoch_Reward                1496.80493
Running_Training_Average_Rewards  14656.73954
Explore_Time                      0.00096
Train___Time                      0.09993
Eval____Time                      1.35211
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12188.17620
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.24504     1.90475     97.90942     92.65784
alpha_0                           0.95850      0.00014     0.95869      0.95831
Alpha_loss                        -0.28445     0.00109     -0.28298     -0.28592
Training/policy_loss              -2.63389     0.00538     -2.62518     -2.64141
Training/qf1_loss                 19360.83594  1306.78732  20647.72656  16907.22461
Training/qf2_loss                 19440.54102  1308.65716  20727.86328  16982.55469
Training/pf_norm                  0.13978      0.02444     0.16878      0.10003
Training/qf1_norm                 432.06883    10.17173    438.98782    411.99854
Training/qf2_norm                 222.63204    4.28199     226.36752    214.56223
log_std/mean                      -0.13633     0.00011     -0.13615     -0.13647
log_probs/mean                    -2.72676     0.00578     -2.71767     -2.73488
mean/mean                         -0.00222     0.00006     -0.00211     -0.00226
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020054101943969727
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70791
epoch first part time 5.0067901611328125e-06
replay_buffer._size: [13800]
collect time 0.0010788440704345703
inside mustsac before update, task 0, sumup 70791
inside mustsac after update, task 0, sumup 71349
inner_dict_sum {'sac_diff0': 0.0002079010009765625, 'sac_diff1': 0.007414579391479492, 'sac_diff2': 0.00859832763671875, 'sac_diff3': 0.011362791061401367, 'sac_diff4': 0.007839441299438477, 'sac_diff5': 0.054160356521606445, 'sac_diff6': 0.00042510032653808594, 'all': 0.09000849723815918}
diff5_list [0.013242721557617188, 0.00992727279663086, 0.010335206985473633, 0.010947942733764648, 0.009707212448120117]
time3 0.0008959770202636719
time4 0.09091615676879883
time5 0.09097123146057129
time7 0.24170422554016113
gen_weight_change tensor(-24.2716)
policy weight change tensor(37.4802, grad_fn=<SumBackward0>)
time8 0.002066373825073242
train_time 0.35399746894836426
eval time 1.099735975265503
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:54,520 MainThread INFO: EPOCH:85
2024-01-23 00:59:54,520 MainThread INFO: Time Consumed:1.4572935104370117s
2024-01-23 00:59:54,520 MainThread INFO: Total Frames:13650s
  1%|          | 86/10000 [01:22<4:05:33,  1.49s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12125.26020
Train_Epoch_Reward                10524.69992
Running_Training_Average_Rewards  14842.18315
Explore_Time                      0.00107
Train___Time                      0.35400
Eval____Time                      1.09974
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12179.29901
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.00789     1.95904     100.33954    94.45232
alpha_0                           0.95802      0.00014     0.95821      0.95783
Alpha_loss                        -0.28813     0.00111     -0.28631     -0.28963
Training/policy_loss              -2.64251     0.00732     -2.62989     -2.65008
Training/qf1_loss                 19455.62227  1095.37887  21053.89648  17853.88281
Training/qf2_loss                 19540.09648  1096.01449  21142.54297  17939.38477
Training/pf_norm                  0.15105      0.05417     0.20272      0.04862
Training/qf1_norm                 450.31790    12.34049    463.52634    428.01111
Training/qf2_norm                 232.08427    6.14283     239.91486    224.37631
log_std/mean                      -0.12210     0.00350     -0.11780     -0.12723
log_probs/mean                    -2.73386     0.00678     -2.72293     -2.74210
mean/mean                         -0.00180     0.00078     -0.00092     -0.00325
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018842220306396484
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71349
epoch first part time 2.86102294921875e-06
replay_buffer._size: [13950]
collect time 0.0008490085601806641
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.006792306900024414, 'sac_diff2': 0.008069992065429688, 'sac_diff3': 0.01055908203125, 'sac_diff4': 0.007042646408081055, 'sac_diff5': 0.03260183334350586, 'sac_diff6': 0.0003838539123535156, 'all': 0.06565475463867188}
diff5_list [0.006752967834472656, 0.006399869918823242, 0.0068359375, 0.0064280033111572266, 0.006185054779052734]
time3 0
time4 0.06641316413879395
time5 0.06645941734313965
time7 4.76837158203125e-07
gen_weight_change tensor(-24.2716)
policy weight change tensor(37.5262, grad_fn=<SumBackward0>)
time8 0.0019202232360839844
train_time 0.07730937004089355
eval time 1.3810484409332275
epoch last part time 6.4373016357421875e-06
2024-01-23 00:59:56,004 MainThread INFO: EPOCH:86
2024-01-23 00:59:56,004 MainThread INFO: Time Consumed:1.4616520404815674s
2024-01-23 00:59:56,004 MainThread INFO: Total Frames:13800s
  1%|          | 87/10000 [01:23<4:05:25,  1.49s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12117.50353
Train_Epoch_Reward                33680.31054
Running_Training_Average_Rewards  15862.18088
Explore_Time                      0.00084
Train___Time                      0.07731
Eval____Time                      1.38105
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12052.05965
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       95.59343     2.31480    98.61787     92.25970
alpha_0                           0.95754      0.00014    0.95773      0.95735
Alpha_loss                        -0.29156     0.00094    -0.29023     -0.29312
Training/policy_loss              -2.64238     0.00655    -2.63427     -2.65193
Training/qf1_loss                 19608.77813  955.78550  20736.98438  17988.73242
Training/qf2_loss                 19701.83281  958.12732  20831.88672  18078.30859
Training/pf_norm                  0.10380      0.01912    0.12908      0.08119
Training/qf1_norm                 477.42533    12.12149   490.73108    462.93463
Training/qf2_norm                 229.22297    5.41975    236.24525    221.44209
log_std/mean                      -0.12484     0.00005    -0.12480     -0.12494
log_probs/mean                    -2.73519     0.00676    -2.72699     -2.74487
mean/mean                         -0.00156     0.00008    -0.00145     -0.00165
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018822431564331055
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71349
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [14100]
collect time 0.00098419189453125
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.0071032047271728516, 'sac_diff2': 0.008443832397460938, 'sac_diff3': 0.010512351989746094, 'sac_diff4': 0.00708460807800293, 'sac_diff5': 0.03419756889343262, 'sac_diff6': 0.00040721893310546875, 'all': 0.0679633617401123}
diff5_list [0.007123231887817383, 0.0077228546142578125, 0.006682872772216797, 0.006538867950439453, 0.006129741668701172]
time3 0
time4 0.06878519058227539
time5 0.06883835792541504
time7 4.76837158203125e-07
gen_weight_change tensor(-24.2716)
policy weight change tensor(37.5788, grad_fn=<SumBackward0>)
time8 0.0019450187683105469
train_time 0.07989931106567383
eval time 1.5558500289916992
epoch last part time 7.3909759521484375e-06
2024-01-23 00:59:57,665 MainThread INFO: EPOCH:87
2024-01-23 00:59:57,666 MainThread INFO: Time Consumed:1.6392557621002197s
2024-01-23 00:59:57,666 MainThread INFO: Total Frames:13950s
  1%|          | 88/10000 [01:25<4:14:08,  1.54s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12106.37190
Train_Epoch_Reward                3441.55474
Running_Training_Average_Rewards  15123.92184
Explore_Time                      0.00098
Train___Time                      0.07990
Eval____Time                      1.55585
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11991.81846
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.02009     2.08355     98.79594     92.94173
alpha_0                           0.95706      0.00014     0.95725      0.95687
Alpha_loss                        -0.29506     0.00119     -0.29375     -0.29677
Training/policy_loss              -2.64498     0.00740     -2.63609     -2.65328
Training/qf1_loss                 19594.03555  1437.80279  21213.13281  17058.32227
Training/qf2_loss                 19680.80117  1439.46173  21301.47852  17142.26562
Training/pf_norm                  0.15148      0.03338     0.19053      0.09508
Training/qf1_norm                 447.17741    9.11249     455.38528    430.74832
Training/qf2_norm                 232.98358    4.83681     237.16425    223.53165
log_std/mean                      -0.12678     0.00008     -0.12666     -0.12688
log_probs/mean                    -2.73793     0.00795     -2.72864     -2.74689
mean/mean                         -0.00024     0.00022     0.00008      -0.00048
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018912315368652344
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71349
epoch first part time 3.337860107421875e-06
replay_buffer._size: [14250]
collect time 0.0008559226989746094
inner_dict_sum {'sac_diff0': 0.0002205371856689453, 'sac_diff1': 0.006959438323974609, 'sac_diff2': 0.00822901725769043, 'sac_diff3': 0.010655879974365234, 'sac_diff4': 0.007103919982910156, 'sac_diff5': 0.03228139877319336, 'sac_diff6': 0.00039267539978027344, 'all': 0.06584286689758301}
diff5_list [0.006546497344970703, 0.006308794021606445, 0.006681680679321289, 0.0064449310302734375, 0.006299495697021484]
time3 0
time4 0.06660151481628418
time5 0.06664872169494629
time7 4.76837158203125e-07
gen_weight_change tensor(-24.2716)
policy weight change tensor(37.6861, grad_fn=<SumBackward0>)
time8 0.0019061565399169922
train_time 0.07732105255126953
eval time 1.4412314891815186
epoch last part time 7.152557373046875e-06
2024-01-23 00:59:59,210 MainThread INFO: EPOCH:88
2024-01-23 00:59:59,210 MainThread INFO: Time Consumed:1.5218579769134521s
2024-01-23 00:59:59,210 MainThread INFO: Total Frames:14100s
  1%|          | 89/10000 [01:26<4:14:27,  1.54s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12091.02519
Train_Epoch_Reward                1439.62432
Running_Training_Average_Rewards  14238.93322
Explore_Time                      0.00085
Train___Time                      0.07732
Eval____Time                      1.44123
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11949.42251
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.36652     2.14566     99.46066     93.49644
alpha_0                           0.95658      0.00014     0.95677      0.95639
Alpha_loss                        -0.29811     0.00108     -0.29698     -0.29965
Training/policy_loss              -2.63766     0.00727     -2.62523     -2.64512
Training/qf1_loss                 18814.67852  1190.83508  21012.03516  17607.68750
Training/qf2_loss                 18897.28633  1191.88481  21096.53516  17689.26562
Training/pf_norm                  0.13170      0.01618     0.15376      0.10862
Training/qf1_norm                 435.86096    7.83842     449.80597    427.72583
Training/qf2_norm                 233.38626    5.03588     242.98700    228.98863
log_std/mean                      -0.11603     0.00019     -0.11579     -0.11633
log_probs/mean                    -2.73064     0.00767     -2.71766     -2.73872
mean/mean                         -0.00056     0.00005     -0.00051     -0.00064
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02013421058654785
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71349
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [14400]
collect time 0.0009005069732666016
inner_dict_sum {'sac_diff0': 0.0001971721649169922, 'sac_diff1': 0.0070934295654296875, 'sac_diff2': 0.008150815963745117, 'sac_diff3': 0.01080465316772461, 'sac_diff4': 0.006867408752441406, 'sac_diff5': 0.03228640556335449, 'sac_diff6': 0.0003943443298339844, 'all': 0.06579422950744629}
diff5_list [0.006814479827880859, 0.00654149055480957, 0.006398200988769531, 0.0061533451080322266, 0.006378889083862305]
time3 0
time4 0.06652665138244629
time5 0.06657099723815918
time7 7.152557373046875e-07
gen_weight_change tensor(-24.2716)
policy weight change tensor(37.7759, grad_fn=<SumBackward0>)
time8 0.0018100738525390625
train_time 0.07699918746948242
eval time 1.6019618511199951
epoch last part time 6.9141387939453125e-06
2024-01-23 01:00:00,916 MainThread INFO: EPOCH:89
2024-01-23 01:00:00,916 MainThread INFO: Time Consumed:1.6823184490203857s
2024-01-23 01:00:00,916 MainThread INFO: Total Frames:14250s
  1%|          | 90/10000 [01:28<4:22:33,  1.59s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12072.53681
Train_Epoch_Reward                3538.74486
Running_Training_Average_Rewards  14221.27627
Explore_Time                      0.00090
Train___Time                      0.07700
Eval____Time                      1.60196
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11879.24595
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       95.09208     0.93414    96.77000     93.98270
alpha_0                           0.95610      0.00014    0.95629      0.95591
Alpha_loss                        -0.30162     0.00084    -0.30042     -0.30262
Training/policy_loss              -2.63661     0.00411    -2.62870     -2.64046
Training/qf1_loss                 18597.25938  634.07926  19470.86914  17517.96875
Training/qf2_loss                 18688.02383  633.76807  19562.22656  17609.59766
Training/pf_norm                  0.12819      0.01584    0.14243      0.10149
Training/qf1_norm                 452.82670    3.65635    457.88757    446.46033
Training/qf2_norm                 225.36166    2.11733    229.18179    222.88823
log_std/mean                      -0.14415     0.00011    -0.14398     -0.14430
log_probs/mean                    -2.73363     0.00402    -2.72590     -2.73711
mean/mean                         -0.00130     0.00005    -0.00124     -0.00135
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018677234649658203
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71349
epoch first part time 3.337860107421875e-06
replay_buffer._size: [14550]
collect time 0.0009090900421142578
inside mustsac before update, task 0, sumup 71349
inside mustsac after update, task 0, sumup 71417
inner_dict_sum {'sac_diff0': 0.00019693374633789062, 'sac_diff1': 0.006739139556884766, 'sac_diff2': 0.008416414260864258, 'sac_diff3': 0.010595321655273438, 'sac_diff4': 0.007577419281005859, 'sac_diff5': 0.050331830978393555, 'sac_diff6': 0.0004088878631591797, 'all': 0.08426594734191895}
diff5_list [0.010386466979980469, 0.010572195053100586, 0.009757280349731445, 0.010022163391113281, 0.009593725204467773]
time3 0.0008695125579833984
time4 0.08510732650756836
time5 0.0851597785949707
time7 0.28856539726257324
gen_weight_change tensor(-24.7084)
policy weight change tensor(37.9947, grad_fn=<SumBackward0>)
time8 0.0027246475219726562
train_time 0.3945138454437256
eval time 1.1670341491699219
epoch last part time 8.106231689453125e-06
2024-01-23 01:00:02,503 MainThread INFO: EPOCH:90
2024-01-23 01:00:02,503 MainThread INFO: Time Consumed:1.5650908946990967s
2024-01-23 01:00:02,503 MainThread INFO: Total Frames:14400s
  1%|          | 91/10000 [01:30<4:22:41,  1.59s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12060.60259
Train_Epoch_Reward                5764.65474
Running_Training_Average_Rewards  13990.86247
Explore_Time                      0.00090
Train___Time                      0.39451
Eval____Time                      1.16703
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11895.39302
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.10240     2.04370     96.93472     91.09968
alpha_0                           0.95562      0.00014     0.95581      0.95543
Alpha_loss                        -0.30489     0.00096     -0.30321     -0.30602
Training/policy_loss              -2.63459     0.00509     -2.62716     -2.64214
Training/qf1_loss                 18042.52266  1062.46933  20018.27148  17114.32422
Training/qf2_loss                 18135.53672  1060.50539  20105.09766  17199.92578
Training/pf_norm                  0.13579      0.02829     0.17326      0.09295
Training/qf1_norm                 458.25759    31.36650    513.68677    423.98264
Training/qf2_norm                 222.59590    5.38823     230.89668    215.33960
log_std/mean                      -0.12988     0.01039     -0.11238     -0.14066
log_probs/mean                    -2.73129     0.00585     -2.72407     -2.74091
mean/mean                         -0.00050     0.00070     0.00086      -0.00107
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021315574645996094
epoch last part time3 0.0030112266540527344
inside rlalgo, task 0, sumup 71417
epoch first part time 3.337860107421875e-06
replay_buffer._size: [14700]
collect time 0.0009448528289794922
inner_dict_sum {'sac_diff0': 0.00022339820861816406, 'sac_diff1': 0.008182048797607422, 'sac_diff2': 0.00957942008972168, 'sac_diff3': 0.012275934219360352, 'sac_diff4': 0.008172035217285156, 'sac_diff5': 0.03760814666748047, 'sac_diff6': 0.0004284381866455078, 'all': 0.07646942138671875}
diff5_list [0.007308483123779297, 0.006745576858520508, 0.007773160934448242, 0.007726430892944336, 0.008054494857788086]
time3 0
time4 0.07737231254577637
time5 0.07742762565612793
time7 7.152557373046875e-07
gen_weight_change tensor(-24.7084)
policy weight change tensor(38.0394, grad_fn=<SumBackward0>)
time8 0.0020635128021240234
train_time 0.08882379531860352
eval time 1.5624125003814697
epoch last part time 6.9141387939453125e-06
2024-01-23 01:00:04,185 MainThread INFO: EPOCH:91
2024-01-23 01:00:04,186 MainThread INFO: Time Consumed:1.6547393798828125s
2024-01-23 01:00:04,186 MainThread INFO: Total Frames:14550s
  1%|          | 92/10000 [01:31<4:26:58,  1.62s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12003.55554
Train_Epoch_Reward                6950.54936
Running_Training_Average_Rewards  13838.50165
Explore_Time                      0.00094
Train___Time                      0.08882
Eval____Time                      1.56241
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11554.93174
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.48860     2.12147     94.54594     88.98563
alpha_0                           0.95514      0.00014     0.95533      0.95495
Alpha_loss                        -0.30813     0.00116     -0.30663     -0.30994
Training/policy_loss              -2.63163     0.00550     -2.62394     -2.64073
Training/qf1_loss                 17054.97832  1260.15603  18617.72266  15169.45605
Training/qf2_loss                 17137.52891  1262.68773  18702.79102  15248.86719
Training/pf_norm                  0.09873      0.03321     0.15712      0.05482
Training/qf1_norm                 443.86595    12.09517    457.72876    429.20828
Training/qf2_norm                 218.23239    4.89619     225.26929    212.51723
log_std/mean                      -0.13378     0.00005     -0.13371     -0.13384
log_probs/mean                    -2.72852     0.00601     -2.72048     -2.73858
mean/mean                         -0.00031     0.00005     -0.00023     -0.00038
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019472837448120117
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71417
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [14850]
collect time 0.0010213851928710938
inner_dict_sum {'sac_diff0': 0.00021839141845703125, 'sac_diff1': 0.008640527725219727, 'sac_diff2': 0.01031804084777832, 'sac_diff3': 0.013453006744384766, 'sac_diff4': 0.008918285369873047, 'sac_diff5': 0.041167497634887695, 'sac_diff6': 0.00047707557678222656, 'all': 0.08319282531738281}
diff5_list [0.008344888687133789, 0.008150339126586914, 0.008197307586669922, 0.008178472518920898, 0.008296489715576172]
time3 0
time4 0.08415484428405762
time5 0.08421516418457031
time7 9.5367431640625e-07
gen_weight_change tensor(-24.7084)
policy weight change tensor(38.1435, grad_fn=<SumBackward0>)
time8 0.00217437744140625
train_time 0.09626579284667969
eval time 1.5891644954681396
epoch last part time 7.152557373046875e-06
2024-01-23 01:00:05,898 MainThread INFO: EPOCH:92
2024-01-23 01:00:05,900 MainThread INFO: Time Consumed:1.6891117095947266s
2024-01-23 01:00:05,900 MainThread INFO: Total Frames:14700s
  1%|          | 93/10000 [01:33<4:31:57,  1.65s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11947.44416
Train_Epoch_Reward                16264.26092
Running_Training_Average_Rewards  13488.87194
Explore_Time                      0.00102
Train___Time                      0.09627
Eval____Time                      1.58916
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11596.80673
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.62793     2.89929     97.92280     89.25977
alpha_0                           0.95466      0.00014     0.95486      0.95447
Alpha_loss                        -0.31152     0.00101     -0.31007     -0.31298
Training/policy_loss              -2.63280     0.00509     -2.62435     -2.63991
Training/qf1_loss                 18006.17461  1095.36438  20129.52148  17053.62109
Training/qf2_loss                 18116.12773  1097.76185  20243.61328  17160.53906
Training/pf_norm                  0.16056      0.01436     0.18430      0.14123
Training/qf1_norm                 508.12270    13.82724    530.97021    492.33844
Training/qf2_norm                 227.52622    6.81578     239.95068    219.56000
log_std/mean                      -0.12397     0.00022     -0.12371     -0.12434
log_probs/mean                    -2.72892     0.00536     -2.72008     -2.73612
mean/mean                         -0.00047     0.00005     -0.00040     -0.00052
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.024981975555419922
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71417
epoch first part time 7.152557373046875e-06
replay_buffer._size: [15000]
collect time 0.0011157989501953125
inner_dict_sum {'sac_diff0': 0.0002372264862060547, 'sac_diff1': 0.008942842483520508, 'sac_diff2': 0.010496854782104492, 'sac_diff3': 0.013501167297363281, 'sac_diff4': 0.00898599624633789, 'sac_diff5': 0.0408935546875, 'sac_diff6': 0.000484466552734375, 'all': 0.0835421085357666}
diff5_list [0.00852203369140625, 0.008370637893676758, 0.00832056999206543, 0.007979393005371094, 0.007700920104980469]
time3 0
time4 0.0845191478729248
time5 0.0845787525177002
time7 1.1920928955078125e-06
gen_weight_change tensor(-24.7084)
policy weight change tensor(38.2119, grad_fn=<SumBackward0>)
time8 0.0021872520446777344
train_time 0.09692192077636719
eval time 1.660529375076294
epoch last part time 6.9141387939453125e-06
2024-01-23 01:00:07,688 MainThread INFO: EPOCH:93
2024-01-23 01:00:07,688 MainThread INFO: Time Consumed:1.7610743045806885s
2024-01-23 01:00:07,689 MainThread INFO: Total Frames:14850s
  1%|          | 94/10000 [01:35<4:38:48,  1.69s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11889.02434
Train_Epoch_Reward                8655.17261
Running_Training_Average_Rewards  13630.62438
Explore_Time                      0.00110
Train___Time                      0.09692
Eval____Time                      1.66053
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11603.09019
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.17032     1.74844     95.88953     90.97334
alpha_0                           0.95419      0.00014     0.95438      0.95399
Alpha_loss                        -0.31503     0.00092     -0.31354     -0.31603
Training/policy_loss              -2.63206     0.00810     -2.62447     -2.64677
Training/qf1_loss                 18079.73984  1310.03802  20209.78711  16804.00781
Training/qf2_loss                 18162.01367  1311.74039  20295.16406  16886.12891
Training/pf_norm                  0.16364      0.05487     0.21502      0.06159
Training/qf1_norm                 433.79735    8.65704     448.33044    425.12384
Training/qf2_norm                 223.86439    4.08392     230.23766    218.82613
log_std/mean                      -0.13607     0.00006     -0.13595     -0.13612
log_probs/mean                    -2.73186     0.00837     -2.72444     -2.74727
mean/mean                         0.00008      0.00001     0.00011      0.00006
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020414113998413086
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71417
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [15150]
collect time 0.0009617805480957031
inner_dict_sum {'sac_diff0': 0.0002162456512451172, 'sac_diff1': 0.008014917373657227, 'sac_diff2': 0.009525299072265625, 'sac_diff3': 0.011758089065551758, 'sac_diff4': 0.007611989974975586, 'sac_diff5': 0.03594374656677246, 'sac_diff6': 0.0004172325134277344, 'all': 0.07348752021789551}
diff5_list [0.007019996643066406, 0.007529258728027344, 0.007771492004394531, 0.006968259811401367, 0.0066547393798828125]
time3 0
time4 0.07433080673217773
time5 0.07438349723815918
time7 9.5367431640625e-07
gen_weight_change tensor(-24.7084)
policy weight change tensor(38.2657, grad_fn=<SumBackward0>)
time8 0.00205230712890625
train_time 0.08593416213989258
eval time 1.528925895690918
epoch last part time 6.67572021484375e-06
2024-01-23 01:00:09,331 MainThread INFO: EPOCH:94
2024-01-23 01:00:09,331 MainThread INFO: Time Consumed:1.6183090209960938s
2024-01-23 01:00:09,331 MainThread INFO: Total Frames:15000s
  1%|          | 95/10000 [01:36<4:36:22,  1.67s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11832.20315
Train_Epoch_Reward                8536.72277
Running_Training_Average_Rewards  13759.72509
Explore_Time                      0.00096
Train___Time                      0.08593
Eval____Time                      1.52893
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11619.96426
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.10146     2.30573     94.03471     87.28461
alpha_0                           0.95371      0.00014     0.95390      0.95352
Alpha_loss                        -0.31855     0.00104     -0.31727     -0.32015
Training/policy_loss              -2.63390     0.00711     -2.62186     -2.64239
Training/qf1_loss                 16894.53027  1106.73718  18062.74414  15155.76367
Training/qf2_loss                 16998.56211  1108.91748  18168.19336  15257.58301
Training/pf_norm                  0.13933      0.00965     0.14876      0.12614
Training/qf1_norm                 477.90376    11.62317    496.84097    464.08740
Training/qf2_norm                 216.76838    5.28140     223.48154    208.03635
log_std/mean                      -0.12952     0.00014     -0.12933     -0.12969
log_probs/mean                    -2.73499     0.00747     -2.72208     -2.74333
mean/mean                         0.00010      0.00003     0.00013      0.00006
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018288612365722656
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71417
epoch first part time 2.86102294921875e-06
replay_buffer._size: [15300]
collect time 0.0009267330169677734
inside mustsac before update, task 0, sumup 71417
inside mustsac after update, task 0, sumup 70320
inner_dict_sum {'sac_diff0': 0.00020265579223632812, 'sac_diff1': 0.007433891296386719, 'sac_diff2': 0.008983850479125977, 'sac_diff3': 0.011384963989257812, 'sac_diff4': 0.007822275161743164, 'sac_diff5': 0.052828073501586914, 'sac_diff6': 0.00043773651123046875, 'all': 0.08909344673156738}
diff5_list [0.010858774185180664, 0.011162519454956055, 0.010472297668457031, 0.00994729995727539, 0.010387182235717773]
time3 0.0008535385131835938
time4 0.09000706672668457
time5 0.09006214141845703
time7 0.29695677757263184
gen_weight_change tensor(-25.1067)
policy weight change tensor(38.4478, grad_fn=<SumBackward0>)
time8 0.002290964126586914
train_time 0.4082305431365967
eval time 1.3567054271697998
epoch last part time 6.67572021484375e-06
2024-01-23 01:00:11,121 MainThread INFO: EPOCH:95
2024-01-23 01:00:11,121 MainThread INFO: Time Consumed:1.7683887481689453s
2024-01-23 01:00:11,121 MainThread INFO: Total Frames:15150s
  1%|          | 96/10000 [01:38<4:42:05,  1.71s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11781.45507
Train_Epoch_Reward                16360.13285
Running_Training_Average_Rewards  14000.41359
Explore_Time                      0.00092
Train___Time                      0.40823
Eval____Time                      1.35671
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11671.81817
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.51112     2.18688    92.83115     86.79288
alpha_0                           0.95323      0.00014    0.95342      0.95304
Alpha_loss                        -0.32182     0.00111    -0.32036     -0.32313
Training/policy_loss              -2.62917     0.00558    -2.62402     -2.63992
Training/qf1_loss                 16532.07188  464.10703  17185.08203  15960.55664
Training/qf2_loss                 16631.24180  467.06894  17280.51953  16050.65039
Training/pf_norm                  0.14722      0.03560    0.20421      0.09821
Training/qf1_norm                 465.67551    24.61974   503.47263    432.97147
Training/qf2_norm                 209.06487    7.02609    216.58110    198.36868
log_std/mean                      -0.13116     0.00702    -0.12096     -0.14259
log_probs/mean                    -2.73272     0.00588    -2.72742     -2.74409
mean/mean                         0.00023      0.00095    0.00104      -0.00159
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018220901489257812
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70320
epoch first part time 3.337860107421875e-06
replay_buffer._size: [15450]
collect time 0.0010213851928710938
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.006929636001586914, 'sac_diff2': 0.008377313613891602, 'sac_diff3': 0.010917425155639648, 'sac_diff4': 0.007353067398071289, 'sac_diff5': 0.032305002212524414, 'sac_diff6': 0.00040030479431152344, 'all': 0.06649327278137207}
diff5_list [0.006505727767944336, 0.006155252456665039, 0.006083965301513672, 0.0069446563720703125, 0.006615400314331055]
time3 0
time4 0.06729555130004883
time5 0.06734418869018555
time7 7.152557373046875e-07
gen_weight_change tensor(-25.1067)
policy weight change tensor(38.4850, grad_fn=<SumBackward0>)
time8 0.001953601837158203
train_time 0.07829570770263672
eval time 1.743588924407959
epoch last part time 7.62939453125e-06
2024-01-23 01:00:12,968 MainThread INFO: EPOCH:96
2024-01-23 01:00:12,968 MainThread INFO: Time Consumed:1.8254358768463135s
2024-01-23 01:00:12,969 MainThread INFO: Total Frames:15300s
  1%|          | 97/10000 [01:40<4:48:58,  1.75s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11759.56414
Train_Epoch_Reward                4460.11924
Running_Training_Average_Rewards  13685.09946
Explore_Time                      0.00102
Train___Time                      0.07830
Eval____Time                      1.74359
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11833.15039
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.43265     4.06494     95.57721     83.99297
alpha_0                           0.95275      0.00014     0.95294      0.95256
Alpha_loss                        -0.32526     0.00095     -0.32394     -0.32655
Training/policy_loss              -2.63165     0.00132     -2.62989     -2.63290
Training/qf1_loss                 17120.02383  1830.00398  19446.57031  13969.55273
Training/qf2_loss                 17237.86523  1835.69571  19570.71484  14077.99805
Training/pf_norm                  0.11656      0.03175     0.16469      0.06532
Training/qf1_norm                 546.09063    25.70328    573.28821    502.55109
Training/qf2_norm                 219.90270    9.39921     229.48694    202.72153
log_std/mean                      -0.12924     0.00010     -0.12913     -0.12941
log_probs/mean                    -2.73420     0.00131     -2.73242     -2.73579
mean/mean                         -0.00141     0.00001     -0.00139     -0.00143
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01937699317932129
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70320
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [15600]
collect time 0.0008778572082519531
inner_dict_sum {'sac_diff0': 0.0001976490020751953, 'sac_diff1': 0.0066680908203125, 'sac_diff2': 0.007878541946411133, 'sac_diff3': 0.01000070571899414, 'sac_diff4': 0.006685018539428711, 'sac_diff5': 0.03196406364440918, 'sac_diff6': 0.0004112720489501953, 'all': 0.06380534172058105}
diff5_list [0.006570577621459961, 0.006165027618408203, 0.006472349166870117, 0.006319761276245117, 0.006436347961425781]
time3 0
time4 0.0645899772644043
time5 0.0646369457244873
time7 7.152557373046875e-07
gen_weight_change tensor(-25.1067)
policy weight change tensor(38.4360, grad_fn=<SumBackward0>)
time8 0.0019867420196533203
train_time 0.0754554271697998
eval time 1.5666072368621826
epoch last part time 7.3909759521484375e-06
2024-01-23 01:00:14,637 MainThread INFO: EPOCH:97
2024-01-23 01:00:14,637 MainThread INFO: Time Consumed:1.6454322338104248s
2024-01-23 01:00:14,637 MainThread INFO: Total Frames:15450s
  1%|          | 98/10000 [01:42<4:44:52,  1.73s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11749.29698
Train_Epoch_Reward                4660.86477
Running_Training_Average_Rewards  12952.70328
Explore_Time                      0.00087
Train___Time                      0.07546
Eval____Time                      1.56661
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11889.14687
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.26324     2.64281     98.19180     90.05309
alpha_0                           0.95228      0.00013     0.95247      0.95208
Alpha_loss                        -0.32866     0.00129     -0.32713     -0.33076
Training/policy_loss              -2.63072     0.00737     -2.62391     -2.64490
Training/qf1_loss                 19098.53086  1556.08779  21203.30078  16478.82227
Training/qf2_loss                 19222.10469  1558.65555  21330.74023  16598.02930
Training/pf_norm                  0.14415      0.04045     0.21334      0.09314
Training/qf1_norm                 548.06713    14.72586    567.38849    526.80017
Training/qf2_norm                 225.55880    6.12637     234.66690    215.83405
log_std/mean                      -0.14919     0.00021     -0.14885     -0.14941
log_probs/mean                    -2.73472     0.00804     -2.72729     -2.75015
mean/mean                         -0.00036     0.00004     -0.00030     -0.00041
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019409656524658203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70320
epoch first part time 2.86102294921875e-06
replay_buffer._size: [15750]
collect time 0.0009908676147460938
inner_dict_sum {'sac_diff0': 0.00020599365234375, 'sac_diff1': 0.0068683624267578125, 'sac_diff2': 0.008106708526611328, 'sac_diff3': 0.010198593139648438, 'sac_diff4': 0.007123708724975586, 'sac_diff5': 0.032236576080322266, 'sac_diff6': 0.00037860870361328125, 'all': 0.06511855125427246}
diff5_list [0.0074651241302490234, 0.0064852237701416016, 0.006095409393310547, 0.006109476089477539, 0.006081342697143555]
time3 0
time4 0.0658717155456543
time5 0.0659172534942627
time7 7.152557373046875e-07
gen_weight_change tensor(-25.1067)
policy weight change tensor(38.3693, grad_fn=<SumBackward0>)
time8 0.0018413066864013672
train_time 0.07666301727294922
eval time 1.5773611068725586
epoch last part time 7.3909759521484375e-06
2024-01-23 01:00:16,318 MainThread INFO: EPOCH:98
2024-01-23 01:00:16,318 MainThread INFO: Time Consumed:1.657487154006958s
2024-01-23 01:00:16,318 MainThread INFO: Total Frames:15600s
  1%|          | 99/10000 [01:43<4:42:32,  1.71s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11744.13502
Train_Epoch_Reward                45718.70139
Running_Training_Average_Rewards  13916.39228
Explore_Time                      0.00099
Train___Time                      0.07666
Eval____Time                      1.57736
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11897.80290
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.00247     2.97849     95.70268     86.97871
alpha_0                           0.95180      0.00013     0.95199      0.95161
Alpha_loss                        -0.33204     0.00133     -0.33010     -0.33406
Training/policy_loss              -2.62949     0.00854     -2.61860     -2.64187
Training/qf1_loss                 17587.59570  1035.51786  19124.55078  16353.42773
Training/qf2_loss                 17708.54180  1037.83088  19248.23828  16468.86133
Training/pf_norm                  0.15215      0.03662     0.19950      0.10836
Training/qf1_norm                 526.94149    14.52851    541.66656    501.17459
Training/qf2_norm                 216.48252    6.70455     224.75493    205.15585
log_std/mean                      -0.13164     0.00009     -0.13156     -0.13180
log_probs/mean                    -2.73477     0.00927     -2.72278     -2.74834
mean/mean                         0.00126      0.00015     0.00148      0.00107
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01817798614501953
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70320
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [15900]
collect time 0.0009534358978271484
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.006986856460571289, 'sac_diff2': 0.008158683776855469, 'sac_diff3': 0.010593891143798828, 'sac_diff4': 0.007001638412475586, 'sac_diff5': 0.03312420845031738, 'sac_diff6': 0.0003924369812011719, 'all': 0.06646275520324707}
diff5_list [0.006919384002685547, 0.0073926448822021484, 0.0066487789154052734, 0.0060346126556396484, 0.006128787994384766]
time3 0
time4 0.06726670265197754
time5 0.06731891632080078
time7 7.152557373046875e-07
gen_weight_change tensor(-25.1067)
policy weight change tensor(38.3391, grad_fn=<SumBackward0>)
time8 0.0018596649169921875
train_time 0.07804536819458008
eval time 1.6059842109680176
epoch last part time 6.9141387939453125e-06
2024-01-23 01:00:18,027 MainThread INFO: EPOCH:99
2024-01-23 01:00:18,027 MainThread INFO: Time Consumed:1.687568187713623s
2024-01-23 01:00:18,027 MainThread INFO: Total Frames:15750s
  1%|          | 100/10000 [01:45<4:42:23,  1.71s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11743.44498
Train_Epoch_Reward                10193.41531
Running_Training_Average_Rewards  14029.48293
Explore_Time                      0.00095
Train___Time                      0.07805
Eval____Time                      1.60598
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11872.34555
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.18164     1.29323    95.32664     91.56897
alpha_0                           0.95132      0.00013    0.95151      0.95113
Alpha_loss                        -0.33555     0.00077    -0.33447     -0.33638
Training/policy_loss              -2.63202     0.00546    -2.62161     -2.63770
Training/qf1_loss                 18119.75547  956.43335  19427.68359  16792.80859
Training/qf2_loss                 18248.26328  959.13634  19559.29297  16918.52734
Training/pf_norm                  0.12939      0.03057    0.18357      0.09547
Training/qf1_norm                 547.69231    10.64405   558.81464    532.32904
Training/qf2_norm                 225.00153    3.03345    230.00995    221.17223
log_std/mean                      -0.12953     0.00007    -0.12946     -0.12966
log_probs/mean                    -2.73758     0.00544    -2.72718     -2.74301
mean/mean                         -0.00017     0.00007    -0.00008     -0.00024
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01859116554260254
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70320
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [16050]
collect time 0.0008478164672851562
inside mustsac before update, task 0, sumup 70320
inside mustsac after update, task 0, sumup 70780
inner_dict_sum {'sac_diff0': 0.00020432472229003906, 'sac_diff1': 0.007111549377441406, 'sac_diff2': 0.008502483367919922, 'sac_diff3': 0.011550188064575195, 'sac_diff4': 0.007772684097290039, 'sac_diff5': 0.05279731750488281, 'sac_diff6': 0.0004284381866455078, 'all': 0.08836698532104492}
diff5_list [0.010933160781860352, 0.009514808654785156, 0.011690616607666016, 0.010162830352783203, 0.010495901107788086]
time3 0.0008749961853027344
time4 0.0892632007598877
time5 0.08931565284729004
time7 0.29072117805480957
gen_weight_change tensor(-25.3195)
policy weight change tensor(38.4871, grad_fn=<SumBackward0>)
time8 0.002873659133911133
train_time 0.40085458755493164
eval time 1.4133353233337402
epoch last part time 6.9141387939453125e-06
2024-01-23 01:00:19,867 MainThread INFO: EPOCH:100
2024-01-23 01:00:19,867 MainThread INFO: Time Consumed:1.8175153732299805s
2024-01-23 01:00:19,867 MainThread INFO: Total Frames:15900s
  1%|          | 101/10000 [01:47<4:48:49,  1.75s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11743.37747
Train_Epoch_Reward                6689.37907
Running_Training_Average_Rewards  13776.17853
Explore_Time                      0.00084
Train___Time                      0.40085
Eval____Time                      1.41334
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11894.71785
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.76939     1.66395    95.04915     89.90297
alpha_0                           0.95085      0.00013    0.95104      0.95065
Alpha_loss                        -0.33873     0.00105    -0.33742     -0.34002
Training/policy_loss              -2.62834     0.00632    -2.61637     -2.63415
Training/qf1_loss                 18226.11445  912.42813  19422.08008  16929.53516
Training/qf2_loss                 18351.70391  911.12931  19544.95703  17058.30664
Training/pf_norm                  0.13217      0.03170    0.18591      0.09361
Training/qf1_norm                 549.83942    12.58169   569.95837    535.11572
Training/qf2_norm                 223.91305    5.46170    229.96432    216.25580
log_std/mean                      -0.13180     0.00616    -0.12389     -0.14162
log_probs/mean                    -2.73366     0.00719    -2.72104     -2.74089
mean/mean                         0.00101      0.00066    0.00211      0.00017
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018314838409423828
epoch last part time3 0.0027396678924560547
inside rlalgo, task 0, sumup 70780
epoch first part time 3.337860107421875e-06
replay_buffer._size: [16200]
collect time 0.0008652210235595703
inner_dict_sum {'sac_diff0': 0.0001952648162841797, 'sac_diff1': 0.006720066070556641, 'sac_diff2': 0.007684946060180664, 'sac_diff3': 0.009857416152954102, 'sac_diff4': 0.006856203079223633, 'sac_diff5': 0.03128337860107422, 'sac_diff6': 0.0003762245178222656, 'all': 0.0629734992980957}
diff5_list [0.006494283676147461, 0.006215572357177734, 0.006163358688354492, 0.006297588348388672, 0.006112575531005859]
time3 0
time4 0.06372284889221191
time5 0.0637671947479248
time7 7.152557373046875e-07
gen_weight_change tensor(-25.3195)
policy weight change tensor(38.4167, grad_fn=<SumBackward0>)
time8 0.0018377304077148438
train_time 0.07433390617370605
eval time 1.736837387084961
epoch last part time 7.62939453125e-06
2024-01-23 01:00:21,705 MainThread INFO: EPOCH:101
2024-01-23 01:00:21,706 MainThread INFO: Time Consumed:1.814560890197754s
2024-01-23 01:00:21,706 MainThread INFO: Total Frames:16050s
  1%|          | 102/10000 [01:49<4:53:02,  1.78s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11783.97133
Train_Epoch_Reward                6305.97784
Running_Training_Average_Rewards  13287.81696
Explore_Time                      0.00086
Train___Time                      0.07433
Eval____Time                      1.73684
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11960.87042
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.21875     1.53245    93.84184     89.31564
alpha_0                           0.95037      0.00013    0.95056      0.95018
Alpha_loss                        -0.34207     0.00099    -0.34055     -0.34323
Training/policy_loss              -2.62383     0.00495    -2.61972     -2.63352
Training/qf1_loss                 17882.97891  748.03712  18980.85547  16819.99219
Training/qf2_loss                 18007.74805  749.28836  19106.19141  16939.56055
Training/pf_norm                  0.18357      0.04379    0.26262      0.14363
Training/qf1_norm                 549.82360    12.16820   562.75043    528.91406
Training/qf2_norm                 215.28476    3.47104    219.00771    208.73215
log_std/mean                      -0.13386     0.00013    -0.13364     -0.13402
log_probs/mean                    -2.73306     0.00519    -2.72927     -2.74323
mean/mean                         0.00071      0.00012    0.00087      0.00053
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018668174743652344
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70780
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [16350]
collect time 0.0009398460388183594
inner_dict_sum {'sac_diff0': 0.0002167224884033203, 'sac_diff1': 0.006954669952392578, 'sac_diff2': 0.007947683334350586, 'sac_diff3': 0.010627508163452148, 'sac_diff4': 0.006847858428955078, 'sac_diff5': 0.031391143798828125, 'sac_diff6': 0.00039768218994140625, 'all': 0.06438326835632324}
diff5_list [0.006593942642211914, 0.0062673091888427734, 0.006150960922241211, 0.0061187744140625, 0.0062601566314697266]
time3 0
time4 0.06512618064880371
time5 0.06517291069030762
time7 4.76837158203125e-07
gen_weight_change tensor(-25.3195)
policy weight change tensor(38.3329, grad_fn=<SumBackward0>)
time8 0.0018510818481445312
train_time 0.07582235336303711
eval time 1.6333935260772705
epoch last part time 7.152557373046875e-06
2024-01-23 01:00:23,440 MainThread INFO: EPOCH:102
2024-01-23 01:00:23,441 MainThread INFO: Time Consumed:1.712653398513794s
2024-01-23 01:00:23,441 MainThread INFO: Total Frames:16200s
  1%|          | 103/10000 [01:51<4:50:58,  1.76s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11826.34546
Train_Epoch_Reward                25817.42966
Running_Training_Average_Rewards  12238.60840
Explore_Time                      0.00094
Train___Time                      0.07582
Eval____Time                      1.63339
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12020.54797
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.27968     2.72444     96.47209     89.43694
alpha_0                           0.94989      0.00013     0.95008      0.94970
Alpha_loss                        -0.34572     0.00111     -0.34405     -0.34697
Training/policy_loss              -2.63049     0.00673     -2.62254     -2.64016
Training/qf1_loss                 17798.85645  1524.27192  19966.40234  15835.54199
Training/qf2_loss                 17939.40391  1527.69603  20113.14453  15971.18750
Training/pf_norm                  0.13956      0.03099     0.19328      0.10972
Training/qf1_norm                 595.84224    16.33500    622.36707    576.95020
Training/qf2_norm                 219.41966    6.22124     229.06624    212.95775
log_std/mean                      -0.13013     0.00010     -0.13000     -0.13030
log_probs/mean                    -2.73831     0.00717     -2.72995     -2.74876
mean/mean                         -0.00025     0.00008     -0.00014     -0.00033
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018937349319458008
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70780
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [16500]
collect time 0.0009348392486572266
inner_dict_sum {'sac_diff0': 0.0002079010009765625, 'sac_diff1': 0.0067424774169921875, 'sac_diff2': 0.008141040802001953, 'sac_diff3': 0.01037144660949707, 'sac_diff4': 0.0067670345306396484, 'sac_diff5': 0.03174233436584473, 'sac_diff6': 0.0003936290740966797, 'all': 0.06436586380004883}
diff5_list [0.006688356399536133, 0.006351947784423828, 0.0061833858489990234, 0.006252765655517578, 0.006265878677368164]
time3 0
time4 0.06510376930236816
time5 0.06515002250671387
time7 4.76837158203125e-07
gen_weight_change tensor(-25.3195)
policy weight change tensor(38.3044, grad_fn=<SumBackward0>)
time8 0.0018444061279296875
train_time 0.07580828666687012
eval time 1.7239398956298828
epoch last part time 7.62939453125e-06
2024-01-23 01:00:25,266 MainThread INFO: EPOCH:103
2024-01-23 01:00:25,266 MainThread INFO: Time Consumed:1.8031845092773438s
2024-01-23 01:00:25,267 MainThread INFO: Total Frames:16350s
  1%|          | 104/10000 [01:52<4:53:59,  1.78s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11867.24332
Train_Epoch_Reward                20088.46507
Running_Training_Average_Rewards  12701.02636
Explore_Time                      0.00093
Train___Time                      0.07581
Eval____Time                      1.72394
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12012.06879
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.12271     1.83017     96.41398     90.84221
alpha_0                           0.94942      0.00013     0.94961      0.94923
Alpha_loss                        -0.34876     0.00091     -0.34729     -0.34997
Training/policy_loss              -2.62283     0.00306     -2.61976     -2.62863
Training/qf1_loss                 18432.38711  1332.30302  20107.35938  16124.70508
Training/qf2_loss                 18562.28008  1333.67469  20242.96875  16253.87695
Training/pf_norm                  0.10535      0.02424     0.12322      0.05801
Training/qf1_norm                 568.38691    12.11844    591.64728    557.80090
Training/qf2_norm                 227.87247    4.29973     235.59888    222.55383
log_std/mean                      -0.13338     0.00005     -0.13332     -0.13345
log_probs/mean                    -2.73191     0.00307     -2.72921     -2.73776
mean/mean                         0.00042      0.00005     0.00048      0.00034
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018885374069213867
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70780
epoch first part time 3.337860107421875e-06
replay_buffer._size: [16650]
collect time 0.0010161399841308594
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.006947517395019531, 'sac_diff2': 0.008398294448852539, 'sac_diff3': 0.011139392852783203, 'sac_diff4': 0.007254362106323242, 'sac_diff5': 0.032968997955322266, 'sac_diff6': 0.0003991127014160156, 'all': 0.06732535362243652}
diff5_list [0.006687164306640625, 0.0063092708587646484, 0.0062143802642822266, 0.006193637847900391, 0.007564544677734375]
time3 0
time4 0.06810784339904785
time5 0.06815767288208008
time7 7.152557373046875e-07
gen_weight_change tensor(-25.3195)
policy weight change tensor(38.3632, grad_fn=<SumBackward0>)
time8 0.0019371509552001953
train_time 0.07931828498840332
eval time 1.769331932067871
epoch last part time 6.67572021484375e-06
2024-01-23 01:00:27,141 MainThread INFO: EPOCH:104
2024-01-23 01:00:27,141 MainThread INFO: Time Consumed:1.8523104190826416s
2024-01-23 01:00:27,142 MainThread INFO: Total Frames:16500s
  1%|          | 105/10000 [01:54<5:04:09,  1.84s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11901.33752
Train_Epoch_Reward                6741.96994
Running_Training_Average_Rewards  12770.43945
Explore_Time                      0.00101
Train___Time                      0.07932
Eval____Time                      1.76933
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11960.90624
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.21017     2.22021     95.71578     89.47668
alpha_0                           0.94894      0.00013     0.94913      0.94875
Alpha_loss                        -0.35214     0.00133     -0.35020     -0.35402
Training/policy_loss              -2.61993     0.00655     -2.60975     -2.62898
Training/qf1_loss                 17715.13848  1542.19422  20582.96484  15944.68262
Training/qf2_loss                 17855.22949  1545.62391  20729.62695  16080.44824
Training/pf_norm                  0.13831      0.03905     0.19143      0.09075
Training/qf1_norm                 601.65455    17.58526    628.04108    582.54840
Training/qf2_norm                 219.44583    5.07140     227.43513    213.27930
log_std/mean                      -0.12462     0.00016     -0.12442     -0.12486
log_probs/mean                    -2.73194     0.00729     -2.72066     -2.74204
mean/mean                         -0.00040     0.00017     -0.00022     -0.00067
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.13256478309631348
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70780
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [16800]
collect time 0.0010366439819335938
inside mustsac before update, task 0, sumup 70780
inside mustsac after update, task 0, sumup 70546
inner_dict_sum {'sac_diff0': 0.00022172927856445312, 'sac_diff1': 0.006940126419067383, 'sac_diff2': 0.008563756942749023, 'sac_diff3': 0.011216878890991211, 'sac_diff4': 0.007704734802246094, 'sac_diff5': 0.051274776458740234, 'sac_diff6': 0.0004265308380126953, 'all': 0.0863485336303711}
diff5_list [0.011056184768676758, 0.010085105895996094, 0.010061979293823242, 0.009982585906982422, 0.010088920593261719]
time3 0.0008740425109863281
time4 0.08723783493041992
time5 0.08729314804077148
time7 0.00944662094116211
gen_weight_change tensor(-25.4003)
policy weight change tensor(38.4476, grad_fn=<SumBackward0>)
time8 0.0018320083618164062
train_time 0.11687684059143066
eval time 1.4033918380737305
epoch last part time 7.867813110351562e-06
2024-01-23 01:00:28,924 MainThread INFO: EPOCH:105
2024-01-23 01:00:28,924 MainThread INFO: Time Consumed:1.5237901210784912s
2024-01-23 01:00:28,924 MainThread INFO: Total Frames:16650s
  1%|          | 106/10000 [01:56<4:55:27,  1.79s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11919.79626
Train_Epoch_Reward                7225.24823
Running_Training_Average_Rewards  12848.11154
Explore_Time                      0.00103
Train___Time                      0.11688
Eval____Time                      1.40339
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11856.40562
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.55440     1.72379     93.29927     88.08900
alpha_0                           0.94847      0.00013     0.94866      0.94828
Alpha_loss                        -0.35558     0.00100     -0.35376     -0.35647
Training/policy_loss              -2.62161     0.00762     -2.61130     -2.63332
Training/qf1_loss                 17399.30586  1441.69948  19229.88086  15333.65332
Training/qf2_loss                 17542.18594  1445.34453  19400.55078  15474.05664
Training/pf_norm                  0.12410      0.03578     0.18485      0.08739
Training/qf1_norm                 583.45713    54.52513    674.45233    524.48492
Training/qf2_norm                 216.92136    6.38430     227.01848    210.22876
log_std/mean                      -0.13145     0.00187     -0.12807     -0.13362
log_probs/mean                    -2.73332     0.00884     -2.72137     -2.74406
mean/mean                         0.00041      0.00073     0.00172      -0.00042
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018839597702026367
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70546
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [16950]
collect time 0.0008471012115478516
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.007210493087768555, 'sac_diff2': 0.013047933578491211, 'sac_diff3': 0.011613845825195312, 'sac_diff4': 0.008389949798583984, 'sac_diff5': 0.03339338302612305, 'sac_diff6': 0.0004057884216308594, 'all': 0.07427597045898438}
diff5_list [0.006554841995239258, 0.006227254867553711, 0.006403207778930664, 0.007706165313720703, 0.006501913070678711]
time3 0
time4 0.07506179809570312
time5 0.0751199722290039
time7 4.76837158203125e-07
gen_weight_change tensor(-25.4003)
policy weight change tensor(38.5053, grad_fn=<SumBackward0>)
time8 0.002283811569213867
train_time 0.087188720703125
eval time 1.819366693496704
epoch last part time 7.152557373046875e-06
2024-01-23 01:00:30,856 MainThread INFO: EPOCH:106
2024-01-23 01:00:30,857 MainThread INFO: Time Consumed:1.910158634185791s
2024-01-23 01:00:30,857 MainThread INFO: Total Frames:16800s
  1%|          | 107/10000 [01:58<5:02:25,  1.83s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11964.48201
Train_Epoch_Reward                14965.07503
Running_Training_Average_Rewards  12954.50068
Explore_Time                      0.00084
Train___Time                      0.08719
Eval____Time                      1.81937
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12280.00790
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.31026     1.08692    93.53123     90.38658
alpha_0                           0.94799      0.00013    0.94818      0.94780
Alpha_loss                        -0.35898     0.00098    -0.35752     -0.36047
Training/policy_loss              -2.62178     0.00286    -2.61856     -2.62631
Training/qf1_loss                 17276.17598  896.32081  18756.81641  16114.31543
Training/qf2_loss                 17424.35234  897.46648  18906.73242  16260.96094
Training/pf_norm                  0.12976      0.03244    0.17428      0.08703
Training/qf1_norm                 604.85326    6.08992    612.24371    596.64807
Training/qf2_norm                 225.96616    2.59780    228.93242    221.40787
log_std/mean                      -0.12742     0.00008    -0.12734     -0.12755
log_probs/mean                    -2.73380     0.00303    -2.73068     -2.73830
mean/mean                         0.00060      0.00013    0.00072      0.00038
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01976609230041504
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70546
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [17100]
collect time 0.001024484634399414
inner_dict_sum {'sac_diff0': 0.0002067089080810547, 'sac_diff1': 0.006783485412597656, 'sac_diff2': 0.007898092269897461, 'sac_diff3': 0.009985685348510742, 'sac_diff4': 0.006984710693359375, 'sac_diff5': 0.03194475173950195, 'sac_diff6': 0.0003809928894042969, 'all': 0.06418442726135254}
diff5_list [0.006641387939453125, 0.00632166862487793, 0.006236076354980469, 0.006398677825927734, 0.006346940994262695]
time3 0
time4 0.06493020057678223
time5 0.06498336791992188
time7 4.76837158203125e-07
gen_weight_change tensor(-25.4003)
policy weight change tensor(38.4543, grad_fn=<SumBackward0>)
time8 0.002060413360595703
train_time 0.07629013061523438
eval time 1.6696064472198486
epoch last part time 7.62939453125e-06
2024-01-23 01:00:32,629 MainThread INFO: EPOCH:107
2024-01-23 01:00:32,629 MainThread INFO: Time Consumed:1.749474048614502s
2024-01-23 01:00:32,629 MainThread INFO: Total Frames:16950s
  1%|          | 108/10000 [02:00<4:59:22,  1.82s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11997.52376
Train_Epoch_Reward                32249.22543
Running_Training_Average_Rewards  13184.07003
Explore_Time                      0.00102
Train___Time                      0.07629
Eval____Time                      1.66961
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12219.56437
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.10149     1.90643     98.53763     93.41715
alpha_0                           0.94752      0.00013     0.94771      0.94733
Alpha_loss                        -0.36240     0.00098     -0.36075     -0.36371
Training/policy_loss              -2.62639     0.00606     -2.62150     -2.63822
Training/qf1_loss                 18349.18711  1144.23481  20361.78320  17160.49219
Training/qf2_loss                 18513.77617  1146.67652  20529.45703  17322.22852
Training/pf_norm                  0.12927      0.00854     0.14027      0.11714
Training/qf1_norm                 678.42999    12.05038    695.60291    666.67035
Training/qf2_norm                 241.05959    4.60876     249.31425    236.89627
log_std/mean                      -0.14342     0.00015     -0.14318     -0.14359
log_probs/mean                    -2.73472     0.00635     -2.72901     -2.74693
mean/mean                         0.00095      0.00006     0.00104      0.00090
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01964855194091797
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70546
epoch first part time 2.86102294921875e-06
replay_buffer._size: [17250]
collect time 0.0010190010070800781
inner_dict_sum {'sac_diff0': 0.00019550323486328125, 'sac_diff1': 0.007129192352294922, 'sac_diff2': 0.008348941802978516, 'sac_diff3': 0.011183977127075195, 'sac_diff4': 0.007420539855957031, 'sac_diff5': 0.03368544578552246, 'sac_diff6': 0.0004048347473144531, 'all': 0.06836843490600586}
diff5_list [0.007219791412353516, 0.00675511360168457, 0.006307125091552734, 0.006335020065307617, 0.0070683956146240234]
time3 0
time4 0.06916356086730957
time5 0.06920957565307617
time7 4.76837158203125e-07
gen_weight_change tensor(-25.4003)
policy weight change tensor(38.3854, grad_fn=<SumBackward0>)
time8 0.0018587112426757812
train_time 0.08055496215820312
eval time 1.9280190467834473
epoch last part time 6.9141387939453125e-06
2024-01-23 01:00:34,665 MainThread INFO: EPOCH:108
2024-01-23 01:00:34,665 MainThread INFO: Time Consumed:2.012122869491577s
2024-01-23 01:00:34,665 MainThread INFO: Total Frames:17100s
  1%|          | 109/10000 [02:02<5:10:10,  1.88s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12021.19526
Train_Epoch_Reward                21530.40785
Running_Training_Average_Rewards  13196.20051
Explore_Time                      0.00101
Train___Time                      0.08055
Eval____Time                      1.92802
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12134.51784
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.86918     2.65920     98.97289     91.43882
alpha_0                           0.94704      0.00013     0.94723      0.94685
Alpha_loss                        -0.36553     0.00090     -0.36443     -0.36683
Training/policy_loss              -2.61520     0.00615     -2.60694     -2.62404
Training/qf1_loss                 18382.91641  1547.34495  21216.45703  16692.74023
Training/qf2_loss                 18546.64727  1551.73768  21388.76953  16855.16797
Training/pf_norm                  0.13618      0.02193     0.16925      0.10430
Training/qf1_norm                 654.64608    19.44397    689.33069    634.82983
Training/qf2_norm                 225.98819    6.18866     237.87480    220.35764
log_std/mean                      -0.13763     0.00004     -0.13756     -0.13769
log_probs/mean                    -2.73017     0.00636     -2.72171     -2.73950
mean/mean                         0.00077      0.00004     0.00084      0.00074
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01915144920349121
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70546
epoch first part time 2.86102294921875e-06
replay_buffer._size: [17400]
collect time 0.0008740425109863281
inner_dict_sum {'sac_diff0': 0.0002200603485107422, 'sac_diff1': 0.0073893070220947266, 'sac_diff2': 0.008983373641967773, 'sac_diff3': 0.010992288589477539, 'sac_diff4': 0.007743358612060547, 'sac_diff5': 0.03490948677062988, 'sac_diff6': 0.00041937828063964844, 'all': 0.07065725326538086}
diff5_list [0.006871461868286133, 0.00839376449584961, 0.0071141719818115234, 0.006338596343994141, 0.0061914920806884766]
time3 0
time4 0.07152295112609863
time5 0.07158994674682617
time7 7.152557373046875e-07
gen_weight_change tensor(-25.4003)
policy weight change tensor(38.3611, grad_fn=<SumBackward0>)
time8 0.0018558502197265625
train_time 0.08266377449035645
eval time 1.7467386722564697
epoch last part time 7.3909759521484375e-06
2024-01-23 01:00:36,520 MainThread INFO: EPOCH:109
2024-01-23 01:00:36,520 MainThread INFO: Time Consumed:1.8328197002410889s
2024-01-23 01:00:36,521 MainThread INFO: Total Frames:17250s
  1%|          | 110/10000 [02:04<5:08:52,  1.87s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12042.45531
Train_Epoch_Reward                6734.55364
Running_Training_Average_Rewards  12744.46897
Explore_Time                      0.00087
Train___Time                      0.08266
Eval____Time                      1.74674
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12084.94605
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.62192     2.71469     96.32066     88.51249
alpha_0                           0.94657      0.00013     0.94676      0.94638
Alpha_loss                        -0.36940     0.00126     -0.36786     -0.37148
Training/policy_loss              -2.62163     0.00612     -2.61677     -2.63369
Training/qf1_loss                 17969.55234  1648.21464  20499.36328  15373.04492
Training/qf2_loss                 18138.99551  1652.06761  20670.61328  15531.98926
Training/pf_norm                  0.14535      0.01961     0.17451      0.11854
Training/qf1_norm                 647.51017    19.24731    658.25604    609.11261
Training/qf2_norm                 220.83708    6.17846     226.90891    209.19896
log_std/mean                      -0.12628     0.00004     -0.12623     -0.12635
log_probs/mean                    -2.73924     0.00676     -2.73382     -2.75255
mean/mean                         0.00128      0.00005     0.00138      0.00125
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019611835479736328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70546
epoch first part time 3.337860107421875e-06
replay_buffer._size: [17550]
collect time 0.000942230224609375
inside mustsac before update, task 0, sumup 70546
inside mustsac after update, task 0, sumup 70908
inner_dict_sum {'sac_diff0': 0.0002014636993408203, 'sac_diff1': 0.006958961486816406, 'sac_diff2': 0.00838613510131836, 'sac_diff3': 0.01006317138671875, 'sac_diff4': 0.007260560989379883, 'sac_diff5': 0.0518343448638916, 'sac_diff6': 0.0004200935363769531, 'all': 0.08512473106384277}
diff5_list [0.011285781860351562, 0.009847640991210938, 0.00971364974975586, 0.010065793991088867, 0.010921478271484375]
time3 0.0008721351623535156
time4 0.08605694770812988
time5 0.08611083030700684
time7 0.32392096519470215
gen_weight_change tensor(-25.5430)
policy weight change tensor(38.3545, grad_fn=<SumBackward0>)
time8 0.0029664039611816406
train_time 0.4310317039489746
eval time 1.4519915580749512
epoch last part time 6.9141387939453125e-06
2024-01-23 01:00:38,430 MainThread INFO: EPOCH:110
2024-01-23 01:00:38,430 MainThread INFO: Time Consumed:1.8864407539367676s
2024-01-23 01:00:38,430 MainThread INFO: Total Frames:17400s
  1%|          | 111/10000 [02:06<5:10:43,  1.89s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12064.98593
Train_Epoch_Reward                15392.98271
Running_Training_Average_Rewards  12864.86549
Explore_Time                      0.00094
Train___Time                      0.43103
Eval____Time                      1.45199
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12120.02412
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.99864     2.38280     97.58372     90.86327
alpha_0                           0.94609      0.00013     0.94628      0.94590
Alpha_loss                        -0.37249     0.00093     -0.37139     -0.37390
Training/policy_loss              -2.61685     0.00737     -2.60832     -2.62574
Training/qf1_loss                 18229.07715  1487.86518  20610.75391  16156.75488
Training/qf2_loss                 18396.94902  1490.69873  20784.18750  16323.55176
Training/pf_norm                  0.11550      0.02182     0.14564      0.08971
Training/qf1_norm                 661.47452    25.93271    700.76532    626.89001
Training/qf2_norm                 223.82988    7.58833     236.21254    215.34753
log_std/mean                      -0.13353     0.00341     -0.12913     -0.13760
log_probs/mean                    -2.73409     0.00621     -2.72643     -2.74143
mean/mean                         -0.00040     0.00039     0.00031      -0.00088
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018868684768676758
epoch last part time3 0.0030052661895751953
inside rlalgo, task 0, sumup 70908
epoch first part time 2.86102294921875e-06
replay_buffer._size: [17700]
collect time 0.0008852481842041016
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.0070078372955322266, 'sac_diff2': 0.009017229080200195, 'sac_diff3': 0.011294841766357422, 'sac_diff4': 0.007513999938964844, 'sac_diff5': 0.03394341468811035, 'sac_diff6': 0.0004355907440185547, 'all': 0.06943392753601074}
diff5_list [0.007010221481323242, 0.006273031234741211, 0.006704807281494141, 0.0074808597564697266, 0.006474494934082031]
time3 0
time4 0.07031583786010742
time5 0.07036972045898438
time7 7.152557373046875e-07
gen_weight_change tensor(-25.5430)
policy weight change tensor(38.3653, grad_fn=<SumBackward0>)
time8 0.0019583702087402344
train_time 0.08145475387573242
eval time 1.9126648902893066
epoch last part time 1.1682510375976562e-05
2024-01-23 01:00:40,452 MainThread INFO: EPOCH:111
2024-01-23 01:00:40,453 MainThread INFO: Time Consumed:1.9976975917816162s
2024-01-23 01:00:40,453 MainThread INFO: Total Frames:17550s
  1%|          | 112/10000 [02:08<5:17:20,  1.93s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12059.08513
Train_Epoch_Reward                21840.17234
Running_Training_Average_Rewards  12742.51207
Explore_Time                      0.00088
Train___Time                      0.08145
Eval____Time                      1.91266
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11901.86237
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.57779     2.16994     98.09324     92.18434
alpha_0                           0.94562      0.00013     0.94581      0.94543
Alpha_loss                        -0.37610     0.00100     -0.37518     -0.37803
Training/policy_loss              -2.62253     0.00909     -2.60696     -2.63180
Training/qf1_loss                 18776.19570  1292.67107  20626.50391  16879.70508
Training/qf2_loss                 18941.76562  1298.22933  20801.40234  17039.59570
Training/pf_norm                  0.12706      0.02416     0.16441      0.09171
Training/qf1_norm                 649.69368    20.08418    681.88562    627.17163
Training/qf2_norm                 230.50450    5.11108     238.76570    224.94527
log_std/mean                      -0.12661     0.00010     -0.12652     -0.12678
log_probs/mean                    -2.73834     0.00953     -2.72215     -2.74871
mean/mean                         0.00109      0.00003     0.00114      0.00104
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018926143646240234
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70908
epoch first part time 2.86102294921875e-06
replay_buffer._size: [17850]
collect time 0.0009691715240478516
inner_dict_sum {'sac_diff0': 0.00022292137145996094, 'sac_diff1': 0.0072002410888671875, 'sac_diff2': 0.008815526962280273, 'sac_diff3': 0.010975360870361328, 'sac_diff4': 0.00774383544921875, 'sac_diff5': 0.03599667549133301, 'sac_diff6': 0.00044083595275878906, 'all': 0.0713953971862793}
diff5_list [0.0065534114837646484, 0.006201982498168945, 0.007173299789428711, 0.00805211067199707, 0.008015871047973633]
time3 0
time4 0.07225227355957031
time5 0.07230854034423828
time7 9.5367431640625e-07
gen_weight_change tensor(-25.5430)
policy weight change tensor(38.4096, grad_fn=<SumBackward0>)
time8 0.002130270004272461
train_time 0.08373332023620605
eval time 1.8536512851715088
epoch last part time 9.298324584960938e-06
2024-01-23 01:00:42,416 MainThread INFO: EPOCH:112
2024-01-23 01:00:42,416 MainThread INFO: Time Consumed:1.941164255142212s
2024-01-23 01:00:42,417 MainThread INFO: Total Frames:17700s
  1%|          | 113/10000 [02:10<5:19:27,  1.94s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12055.13519
Train_Epoch_Reward                11632.95042
Running_Training_Average_Rewards  12951.84297
Explore_Time                      0.00096
Train___Time                      0.08373
Eval____Time                      1.85365
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11981.04856
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.57731     2.17222     95.66689     89.66013
alpha_0                           0.94515      0.00013     0.94534      0.94496
Alpha_loss                        -0.37928     0.00092     -0.37778     -0.38052
Training/policy_loss              -2.61932     0.00388     -2.61442     -2.62474
Training/qf1_loss                 17832.69414  1305.94112  20124.23438  16357.91406
Training/qf2_loss                 17993.78594  1308.28921  20288.97266  16517.13672
Training/pf_norm                  0.13529      0.04012     0.20611      0.10143
Training/qf1_norm                 646.37521    14.00220    663.39697    630.95465
Training/qf2_norm                 228.07117    5.18049     235.44267    221.18805
log_std/mean                      -0.12895     0.00007     -0.12886     -0.12904
log_probs/mean                    -2.73480     0.00398     -2.72990     -2.74052
mean/mean                         -0.00145     0.00007     -0.00133     -0.00153
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.024007797241210938
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70908
epoch first part time 3.814697265625e-06
replay_buffer._size: [18000]
collect time 0.0009450912475585938
inner_dict_sum {'sac_diff0': 0.00022029876708984375, 'sac_diff1': 0.007346391677856445, 'sac_diff2': 0.008795738220214844, 'sac_diff3': 0.010411500930786133, 'sac_diff4': 0.007344484329223633, 'sac_diff5': 0.03199410438537598, 'sac_diff6': 0.0003979206085205078, 'all': 0.06651043891906738}
diff5_list [0.00688934326171875, 0.0063822269439697266, 0.006086111068725586, 0.006147146224975586, 0.006489276885986328]
time3 0
time4 0.06730151176452637
time5 0.06735587120056152
time7 7.152557373046875e-07
gen_weight_change tensor(-25.5430)
policy weight change tensor(38.4338, grad_fn=<SumBackward0>)
time8 0.001965045928955078
train_time 0.0793607234954834
eval time 1.7952678203582764
epoch last part time 9.059906005859375e-06
2024-01-23 01:00:44,322 MainThread INFO: EPOCH:113
2024-01-23 01:00:44,323 MainThread INFO: Time Consumed:1.8782567977905273s
2024-01-23 01:00:44,323 MainThread INFO: Total Frames:17850s
  1%|          | 114/10000 [02:11<5:17:48,  1.93s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12056.99263
Train_Epoch_Reward                1937.36486
Running_Training_Average_Rewards  12694.58451
Explore_Time                      0.00094
Train___Time                      0.07936
Eval____Time                      1.79527
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12030.64324
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.85347     3.08817     97.69336     88.34216
alpha_0                           0.94467      0.00013     0.94486      0.94448
Alpha_loss                        -0.38273     0.00052     -0.38216     -0.38358
Training/policy_loss              -2.61763     0.00877     -2.60877     -2.63349
Training/qf1_loss                 17807.47148  1581.65237  19486.14844  14857.68750
Training/qf2_loss                 17985.45625  1587.90348  19674.69922  15025.17969
Training/pf_norm                  0.14951      0.01315     0.16507      0.13028
Training/qf1_norm                 710.68816    25.53533    748.05817    668.84851
Training/qf2_norm                 232.73031    7.36865     241.87105    219.59871
log_std/mean                      -0.13000     0.00002     -0.12996     -0.13003
log_probs/mean                    -2.73624     0.00892     -2.72741     -2.75248
mean/mean                         -0.00198     0.00011     -0.00184     -0.00213
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02405524253845215
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70908
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [18150]
collect time 0.0010466575622558594
inner_dict_sum {'sac_diff0': 0.0001990795135498047, 'sac_diff1': 0.007136821746826172, 'sac_diff2': 0.008836746215820312, 'sac_diff3': 0.010717630386352539, 'sac_diff4': 0.007311105728149414, 'sac_diff5': 0.03284955024719238, 'sac_diff6': 0.0003962516784667969, 'all': 0.06744718551635742}
diff5_list [0.006956815719604492, 0.006699323654174805, 0.006331920623779297, 0.006154298782348633, 0.006707191467285156]
time3 0
time4 0.06824898719787598
time5 0.06830024719238281
time7 7.152557373046875e-07
gen_weight_change tensor(-25.5430)
policy weight change tensor(38.4870, grad_fn=<SumBackward0>)
time8 0.002085447311401367
train_time 0.0804145336151123
eval time 1.8299891948699951
epoch last part time 9.775161743164062e-06
2024-01-23 01:00:46,264 MainThread INFO: EPOCH:114
2024-01-23 01:00:46,264 MainThread INFO: Time Consumed:1.9141666889190674s
2024-01-23 01:00:46,265 MainThread INFO: Total Frames:18000s
  1%|          | 115/10000 [02:13<5:18:18,  1.93s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12069.34281
Train_Epoch_Reward                15821.09718
Running_Training_Average_Rewards  13172.06092
Explore_Time                      0.00104
Train___Time                      0.08041
Eval____Time                      1.82999
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12084.40806
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.81146     1.43204     94.38061     90.47131
alpha_0                           0.94420      0.00013     0.94439      0.94401
Alpha_loss                        -0.38608     0.00118     -0.38437     -0.38766
Training/policy_loss              -2.61526     0.00569     -2.60704     -2.62119
Training/qf1_loss                 17689.12480  1161.18910  19182.20312  16121.72949
Training/qf2_loss                 17870.32969  1164.35016  19365.57812  16299.10156
Training/pf_norm                  0.11488      0.02076     0.14529      0.08537
Training/qf1_norm                 679.06461    12.86927    698.88824    664.52051
Training/qf2_norm                 226.69287    3.36384     230.38805    221.22150
log_std/mean                      -0.13288     0.00012     -0.13272     -0.13306
log_probs/mean                    -2.73576     0.00622     -2.72705     -2.74231
mean/mean                         -0.00068     0.00006     -0.00059     -0.00076
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021865129470825195
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70908
epoch first part time 4.291534423828125e-06
replay_buffer._size: [18300]
collect time 0.0009992122650146484
inside mustsac before update, task 0, sumup 70908
inside mustsac after update, task 0, sumup 70922
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.0073435306549072266, 'sac_diff2': 0.008914470672607422, 'sac_diff3': 0.01074671745300293, 'sac_diff4': 0.008239030838012695, 'sac_diff5': 0.051035404205322266, 'sac_diff6': 0.0004229545593261719, 'all': 0.08691620826721191}
diff5_list [0.01102137565612793, 0.009785652160644531, 0.009618520736694336, 0.010277986526489258, 0.010331869125366211]
time3 0.0008738040924072266
time4 0.08787131309509277
time5 0.08793210983276367
time7 0.3309898376464844
gen_weight_change tensor(-25.3014)
policy weight change tensor(38.4832, grad_fn=<SumBackward0>)
time8 0.0023784637451171875
train_time 0.4402925968170166
eval time 1.4544308185577393
epoch last part time 8.58306884765625e-06
2024-01-23 01:00:48,188 MainThread INFO: EPOCH:115
2024-01-23 01:00:48,188 MainThread INFO: Time Consumed:1.8984043598175049s
2024-01-23 01:00:48,189 MainThread INFO: Total Frames:18150s
  1%|          | 116/10000 [02:15<5:17:52,  1.93s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12100.06124
Train_Epoch_Reward                5937.18562
Running_Training_Average_Rewards  13019.14378
Explore_Time                      0.00099
Train___Time                      0.44029
Eval____Time                      1.45443
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12163.58990
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.97221     1.78213    93.95751     88.68202
alpha_0                           0.94373      0.00013    0.94392      0.94354
Alpha_loss                        -0.38923     0.00082    -0.38815     -0.39035
Training/policy_loss              -2.61054     0.00322    -2.60658     -2.61604
Training/qf1_loss                 17087.60820  780.80616  17859.95703  15607.50586
Training/qf2_loss                 17276.25840  771.40171  18035.53125  15816.27832
Training/pf_norm                  0.12804      0.02033    0.16421      0.10318
Training/qf1_norm                 711.75922    53.67032   774.51001    629.26917
Training/qf2_norm                 221.58447    5.99330    228.46010    210.94211
log_std/mean                      -0.12917     0.00717    -0.11912     -0.13941
log_probs/mean                    -2.73180     0.00283    -2.72792     -2.73646
mean/mean                         -0.00044     0.00064    0.00039      -0.00157
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02209329605102539
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70922
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [18450]
collect time 0.0010399818420410156
inner_dict_sum {'sac_diff0': 0.0001952648162841797, 'sac_diff1': 0.006762981414794922, 'sac_diff2': 0.007531166076660156, 'sac_diff3': 0.009097814559936523, 'sac_diff4': 0.006043195724487305, 'sac_diff5': 0.031148910522460938, 'sac_diff6': 0.000392913818359375, 'all': 0.0611722469329834}
diff5_list [0.006632328033447266, 0.005918264389038086, 0.006336688995361328, 0.006170988082885742, 0.006090641021728516]
time3 0
time4 0.06191897392272949
time5 0.06195998191833496
time7 7.152557373046875e-07
gen_weight_change tensor(-25.3014)
policy weight change tensor(38.5504, grad_fn=<SumBackward0>)
time8 0.0019047260284423828
train_time 0.07352495193481445
eval time 2.1006553173065186
epoch last part time 8.344650268554688e-06
2024-01-23 01:00:50,391 MainThread INFO: EPOCH:116
2024-01-23 01:00:50,392 MainThread INFO: Time Consumed:2.1778860092163086s
2024-01-23 01:00:50,392 MainThread INFO: Total Frames:18300s
  1%|          | 117/10000 [02:18<5:31:25,  2.01s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12100.91229
Train_Epoch_Reward                8490.05475
Running_Training_Average_Rewards  12179.46858
Explore_Time                      0.00103
Train___Time                      0.07352
Eval____Time                      2.10066
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12288.51837
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.12056     1.99579     94.72679     89.56230
alpha_0                           0.94325      0.00013     0.94344      0.94306
Alpha_loss                        -0.39292     0.00101     -0.39141     -0.39410
Training/policy_loss              -2.61702     0.00297     -2.61372     -2.62131
Training/qf1_loss                 17770.29648  1317.53227  19819.96680  15671.13867
Training/qf2_loss                 17961.91406  1319.80654  20014.83789  15859.06055
Training/pf_norm                  0.10140      0.02512     0.13512      0.07790
Training/qf1_norm                 715.29722    11.11595    727.99701    699.46332
Training/qf2_norm                 230.30575    4.76574     236.43796    224.14378
log_std/mean                      -0.13489     0.00011     -0.13475     -0.13507
log_probs/mean                    -2.73730     0.00320     -2.73436     -2.74184
mean/mean                         0.00041      0.00010     0.00054      0.00025
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.022856950759887695
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70922
epoch first part time 3.337860107421875e-06
replay_buffer._size: [18600]
collect time 0.001020669937133789
inner_dict_sum {'sac_diff0': 0.00020074844360351562, 'sac_diff1': 0.006767749786376953, 'sac_diff2': 0.007679939270019531, 'sac_diff3': 0.010311365127563477, 'sac_diff4': 0.0069217681884765625, 'sac_diff5': 0.03145194053649902, 'sac_diff6': 0.00038170814514160156, 'all': 0.06371521949768066}
diff5_list [0.006930828094482422, 0.006140708923339844, 0.006270170211791992, 0.006087064743041992, 0.0060231685638427734]
time3 0
time4 0.06450176239013672
time5 0.06454896926879883
time7 7.152557373046875e-07
gen_weight_change tensor(-25.3014)
policy weight change tensor(38.5464, grad_fn=<SumBackward0>)
time8 0.0043299198150634766
train_time 0.07890582084655762
eval time 1.851344347000122
epoch last part time 2.3126602172851562e-05
2024-01-23 01:00:52,353 MainThread INFO: EPOCH:117
2024-01-23 01:00:52,353 MainThread INFO: Time Consumed:1.934840202331543s
2024-01-23 01:00:52,354 MainThread INFO: Total Frames:18450s
  1%|          | 118/10000 [02:19<5:28:46,  2.00s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12112.21537
Train_Epoch_Reward                9519.93687
Running_Training_Average_Rewards  12382.08132
Explore_Time                      0.00102
Train___Time                      0.07891
Eval____Time                      1.85134
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12332.59522
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.86314     0.84192    92.93065     90.54311
alpha_0                           0.94278      0.00013    0.94297      0.94259
Alpha_loss                        -0.39649     0.00116    -0.39513     -0.39848
Training/policy_loss              -2.61678     0.00643    -2.60765     -2.62643
Training/qf1_loss                 17072.71523  422.99602  17868.60938  16691.05664
Training/qf2_loss                 17284.67383  425.69586  18083.34375  16898.05664
Training/pf_norm                  0.10743      0.02666    0.14876      0.07645
Training/qf1_norm                 755.04736    13.01985   773.60089    739.49188
Training/qf2_norm                 221.00221    1.98539    223.48869    217.90500
log_std/mean                      -0.13983     0.00005    -0.13974     -0.13987
log_probs/mean                    -2.74069     0.00695    -2.73128     -2.75150
mean/mean                         0.00012      0.00010    0.00028      0.00001
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.021162748336791992
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70922
epoch first part time 7.867813110351562e-06
replay_buffer._size: [18750]
collect time 0.0012044906616210938
inner_dict_sum {'sac_diff0': 0.0001990795135498047, 'sac_diff1': 0.0075418949127197266, 'sac_diff2': 0.009931325912475586, 'sac_diff3': 0.010759830474853516, 'sac_diff4': 0.007548809051513672, 'sac_diff5': 0.032500267028808594, 'sac_diff6': 0.0003993511199951172, 'all': 0.06888055801391602}
diff5_list [0.0074956417083740234, 0.006181240081787109, 0.006094217300415039, 0.0060846805572509766, 0.006644487380981445]
time3 0
time4 0.06963872909545898
time5 0.06968498229980469
time7 9.5367431640625e-07
gen_weight_change tensor(-25.3014)
policy weight change tensor(38.4264, grad_fn=<SumBackward0>)
time8 0.0021026134490966797
train_time 0.08264303207397461
eval time 1.7599139213562012
epoch last part time 7.152557373046875e-06
2024-01-23 01:00:54,224 MainThread INFO: EPOCH:118
2024-01-23 01:00:54,224 MainThread INFO: Time Consumed:1.8462638854980469s
2024-01-23 01:00:54,225 MainThread INFO: Total Frames:18600s
  1%|          | 119/10000 [02:21<5:22:34,  1.96s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12129.98531
Train_Epoch_Reward                12087.35439
Running_Training_Average_Rewards  12737.00566
Explore_Time                      0.00118
Train___Time                      0.08264
Eval____Time                      1.75991
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12312.21722
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.92866     1.84530     92.68549     87.58585
alpha_0                           0.94231      0.00013     0.94250      0.94212
Alpha_loss                        -0.39958     0.00090     -0.39876     -0.40089
Training/policy_loss              -2.61244     0.00730     -2.59944     -2.62159
Training/qf1_loss                 16794.21895  1073.92863  17709.19141  14877.18457
Training/qf2_loss                 16976.87656  1077.84879  17893.94141  15051.82617
Training/pf_norm                  0.14824      0.01685     0.17572      0.12689
Training/qf1_norm                 667.99270    13.92393    678.18250    640.52368
Training/qf2_norm                 223.14684    4.33591     227.16910    215.26624
log_std/mean                      -0.14379     0.00028     -0.14336     -0.14414
log_probs/mean                    -2.73577     0.00760     -2.72197     -2.74493
mean/mean                         -0.00012     0.00016     0.00011      -0.00034
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020723581314086914
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70922
epoch first part time 3.337860107421875e-06
replay_buffer._size: [18900]
collect time 0.0010426044464111328
inner_dict_sum {'sac_diff0': 0.00021529197692871094, 'sac_diff1': 0.00673675537109375, 'sac_diff2': 0.007905721664428711, 'sac_diff3': 0.010112524032592773, 'sac_diff4': 0.006947040557861328, 'sac_diff5': 0.03163337707519531, 'sac_diff6': 0.0003974437713623047, 'all': 0.06394815444946289}
diff5_list [0.006634235382080078, 0.006558656692504883, 0.006282329559326172, 0.006048917770385742, 0.0061092376708984375]
time3 0
time4 0.06467819213867188
time5 0.06472063064575195
time7 4.76837158203125e-07
gen_weight_change tensor(-25.3014)
policy weight change tensor(38.3301, grad_fn=<SumBackward0>)
time8 0.0018038749694824219
train_time 0.07608985900878906
eval time 1.8662610054016113
epoch last part time 6.9141387939453125e-06
2024-01-23 01:00:56,195 MainThread INFO: EPOCH:119
2024-01-23 01:00:56,195 MainThread INFO: Time Consumed:1.9458565711975098s
2024-01-23 01:00:56,195 MainThread INFO: Total Frames:18750s
  1%|          | 120/10000 [02:23<5:23:03,  1.96s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12147.41513
Train_Epoch_Reward                19398.92702
Running_Training_Average_Rewards  13265.67840
Explore_Time                      0.00104
Train___Time                      0.07609
Eval____Time                      1.86626
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12259.24426
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.57635     2.62732     96.82796     89.43174
alpha_0                           0.94184      0.00013     0.94203      0.94165
Alpha_loss                        -0.40273     0.00114     -0.40116     -0.40382
Training/policy_loss              -2.60469     0.00902     -2.59087     -2.61566
Training/qf1_loss                 17413.36289  1404.22622  19519.88477  15650.80469
Training/qf2_loss                 17635.62871  1410.76079  19752.15820  15862.70605
Training/pf_norm                  0.13267      0.03432     0.18549      0.08270
Training/qf1_norm                 791.70709    23.38202    827.80316    758.32697
Training/qf2_norm                 220.16927    6.04313     229.95660    212.87418
log_std/mean                      -0.12894     0.00014     -0.12877     -0.12917
log_probs/mean                    -2.73198     0.00958     -2.71705     -2.74361
mean/mean                         -0.00076     0.00010     -0.00063     -0.00091
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01925349235534668
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70922
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [19050]
collect time 0.0008521080017089844
inside mustsac before update, task 0, sumup 70922
inside mustsac after update, task 0, sumup 70700
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.00719904899597168, 'sac_diff2': 0.008898258209228516, 'sac_diff3': 0.01064300537109375, 'sac_diff4': 0.0071010589599609375, 'sac_diff5': 0.0520169734954834, 'sac_diff6': 0.0004093647003173828, 'all': 0.08648014068603516}
diff5_list [0.011343240737915039, 0.009826421737670898, 0.012481451034545898, 0.009322881698608398, 0.009042978286743164]
time3 0.0008711814880371094
time4 0.0873575210571289
time5 0.08740735054016113
time7 0.3410298824310303
gen_weight_change tensor(-25.0128)
policy weight change tensor(38.3689, grad_fn=<SumBackward0>)
time8 0.002862691879272461
train_time 0.4511253833770752
eval time 1.4260179996490479
epoch last part time 1.0251998901367188e-05
2024-01-23 01:00:58,098 MainThread INFO: EPOCH:120
2024-01-23 01:00:58,098 MainThread INFO: Time Consumed:1.8804981708526611s
2024-01-23 01:00:58,099 MainThread INFO: Total Frames:18900s
  1%|          | 121/10000 [02:25<5:20:18,  1.95s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12156.01792
Train_Epoch_Reward                6583.49901
Running_Training_Average_Rewards  13292.97320
Explore_Time                      0.00085
Train___Time                      0.45113
Eval____Time                      1.42602
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12206.05196
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.29176     0.94817    91.30247     88.64067
alpha_0                           0.94137      0.00013    0.94155      0.94118
Alpha_loss                        -0.40628     0.00106    -0.40444     -0.40759
Training/policy_loss              -2.60936     0.00540    -2.60094     -2.61738
Training/qf1_loss                 16217.66973  598.12902  16928.65234  15210.19727
Training/qf2_loss                 16431.06426  607.73772  17159.13477  15404.02051
Training/pf_norm                  0.14576      0.01932    0.18383      0.13145
Training/qf1_norm                 758.93124    33.02786   792.64423    701.90607
Training/qf2_norm                 220.16013    4.09000    224.58035    212.56851
log_std/mean                      -0.13212     0.00813    -0.12363     -0.14284
log_probs/mean                    -2.73492     0.00514    -2.72678     -2.74106
mean/mean                         -0.00122     0.00064    -0.00060     -0.00247
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01946401596069336
epoch last part time3 0.0030884742736816406
inside rlalgo, task 0, sumup 70700
epoch first part time 7.3909759521484375e-06
replay_buffer._size: [19200]
collect time 0.0009353160858154297
inner_dict_sum {'sac_diff0': 0.00019693374633789062, 'sac_diff1': 0.00686955451965332, 'sac_diff2': 0.007529735565185547, 'sac_diff3': 0.00965571403503418, 'sac_diff4': 0.0066869258880615234, 'sac_diff5': 0.03082871437072754, 'sac_diff6': 0.00040221214294433594, 'all': 0.062169790267944336}
diff5_list [0.0065953731536865234, 0.006102561950683594, 0.005959987640380859, 0.005873441696166992, 0.00629734992980957]
time3 0
time4 0.06290006637573242
time5 0.06294512748718262
time7 9.5367431640625e-07
gen_weight_change tensor(-25.0128)
policy weight change tensor(38.3066, grad_fn=<SumBackward0>)
time8 0.0019407272338867188
train_time 0.07394814491271973
eval time 2.097282648086548
epoch last part time 1.430511474609375e-05
2024-01-23 01:01:00,299 MainThread INFO: EPOCH:121
2024-01-23 01:01:00,300 MainThread INFO: Time Consumed:2.1753273010253906s
2024-01-23 01:01:00,300 MainThread INFO: Total Frames:19050s
  1%|          | 122/10000 [02:27<5:32:43,  2.02s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12188.87630
Train_Epoch_Reward                11001.14289
Running_Training_Average_Rewards  13427.99299
Explore_Time                      0.00092
Train___Time                      0.07395
Eval____Time                      2.09728
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12230.44618
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.38212     2.99174     95.63676     87.25596
alpha_0                           0.94089      0.00013     0.94108      0.94071
Alpha_loss                        -0.40932     0.00101     -0.40748     -0.41037
Training/policy_loss              -2.60257     0.00660     -2.59536     -2.61436
Training/qf1_loss                 17389.51504  1575.82698  18877.34961  14523.98145
Training/qf2_loss                 17605.84844  1581.35900  19101.03906  14731.98633
Training/pf_norm                  0.14227      0.04376     0.19388      0.07587
Training/qf1_norm                 778.30095    23.63719    805.15326    743.94415
Training/qf2_norm                 222.18598    6.91334     229.80469    210.35281
log_std/mean                      -0.14211     0.00010     -0.14194     -0.14223
log_probs/mean                    -2.72960     0.00696     -2.72141     -2.74185
mean/mean                         -0.00244     0.00012     -0.00226     -0.00257
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018999814987182617
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70700
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [19350]
collect time 0.0009183883666992188
inner_dict_sum {'sac_diff0': 0.0002079010009765625, 'sac_diff1': 0.0071108341217041016, 'sac_diff2': 0.008514165878295898, 'sac_diff3': 0.010882377624511719, 'sac_diff4': 0.007387876510620117, 'sac_diff5': 0.03387618064880371, 'sac_diff6': 0.0003972053527832031, 'all': 0.06837654113769531}
diff5_list [0.008253097534179688, 0.006932735443115234, 0.006284236907958984, 0.00619816780090332, 0.006207942962646484]
time3 0
time4 0.06920361518859863
time5 0.0692591667175293
time7 4.76837158203125e-07
gen_weight_change tensor(-25.0128)
policy weight change tensor(38.2535, grad_fn=<SumBackward0>)
time8 0.0018773078918457031
train_time 0.0803079605102539
eval time 1.877096176147461
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:02,283 MainThread INFO: EPOCH:122
2024-01-23 01:01:02,283 MainThread INFO: Time Consumed:1.9608385562896729s
2024-01-23 01:01:02,283 MainThread INFO: Total Frames:19200s
  1%|          | 123/10000 [02:29<5:30:49,  2.01s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12208.74426
Train_Epoch_Reward                11221.08786
Running_Training_Average_Rewards  13259.88722
Explore_Time                      0.00091
Train___Time                      0.08031
Eval____Time                      1.87710
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12179.72816
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.73778     2.80224     95.12337     87.55965
alpha_0                           0.94042      0.00013     0.94061      0.94023
Alpha_loss                        -0.41294     0.00098     -0.41144     -0.41434
Training/policy_loss              -2.60327     0.00171     -2.60125     -2.60588
Training/qf1_loss                 17165.73242  1431.43169  18943.52539  15121.89844
Training/qf2_loss                 17394.00020  1438.39307  19178.01562  15341.76270
Training/pf_norm                  0.12785      0.01467     0.15175      0.10899
Training/qf1_norm                 798.45226    24.91901    832.52863    766.96265
Training/qf2_norm                 222.44420    6.57297     230.40366    212.69164
log_std/mean                      -0.12792     0.00008     -0.12784     -0.12804
log_probs/mean                    -2.73354     0.00184     -2.73116     -2.73605
mean/mean                         -0.00052     0.00016     -0.00028     -0.00068
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019071578979492188
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70700
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [19500]
collect time 0.0013058185577392578
inner_dict_sum {'sac_diff0': 0.00019788742065429688, 'sac_diff1': 0.007028818130493164, 'sac_diff2': 0.00848245620727539, 'sac_diff3': 0.010961771011352539, 'sac_diff4': 0.007175445556640625, 'sac_diff5': 0.03420257568359375, 'sac_diff6': 0.0004134178161621094, 'all': 0.06846237182617188}
diff5_list [0.006827116012573242, 0.0061032772064208984, 0.006049156188964844, 0.007317066192626953, 0.007905960083007812]
time3 0
time4 0.06928539276123047
time5 0.06934094429016113
time7 7.152557373046875e-07
gen_weight_change tensor(-25.0128)
policy weight change tensor(38.2097, grad_fn=<SumBackward0>)
time8 0.0022530555725097656
train_time 0.08193278312683105
eval time 2.07143497467041
epoch last part time 1.2636184692382812e-05
2024-01-23 01:01:04,463 MainThread INFO: EPOCH:123
2024-01-23 01:01:04,463 MainThread INFO: Time Consumed:2.157841205596924s
2024-01-23 01:01:04,464 MainThread INFO: Total Frames:19350s
  1%|          | 124/10000 [02:32<5:39:13,  2.06s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12221.47540
Train_Epoch_Reward                19314.41768
Running_Training_Average_Rewards  13615.19539
Explore_Time                      0.00130
Train___Time                      0.08193
Eval____Time                      2.07143
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12157.95472
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.11896     2.09078     95.31291     89.91115
alpha_0                           0.93995      0.00013     0.94014      0.93976
Alpha_loss                        -0.41632     0.00107     -0.41511     -0.41781
Training/policy_loss              -2.60425     0.00406     -2.59638     -2.60753
Training/qf1_loss                 17107.61367  1079.09055  18592.67773  15374.93164
Training/qf2_loss                 17322.89004  1081.85096  18813.39453  15586.62207
Training/pf_norm                  0.19108      0.02096     0.21673      0.16654
Training/qf1_norm                 775.67097    12.72245    797.59540    760.67944
Training/qf2_norm                 224.59423    4.90333     232.15727    219.41248
log_std/mean                      -0.12583     0.00003     -0.12579     -0.12588
log_probs/mean                    -2.73369     0.00442     -2.72503     -2.73719
mean/mean                         -0.00138     0.00014     -0.00123     -0.00158
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019136428833007812
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70700
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [19650]
collect time 0.0008821487426757812
inner_dict_sum {'sac_diff0': 0.0002148151397705078, 'sac_diff1': 0.006619453430175781, 'sac_diff2': 0.007807016372680664, 'sac_diff3': 0.009912967681884766, 'sac_diff4': 0.0067539215087890625, 'sac_diff5': 0.030315399169921875, 'sac_diff6': 0.00037384033203125, 'all': 0.061997413635253906}
diff5_list [0.006448984146118164, 0.005952119827270508, 0.0060291290283203125, 0.00601506233215332, 0.00587010383605957]
time3 0
time4 0.06272029876708984
time5 0.06276226043701172
time7 7.152557373046875e-07
gen_weight_change tensor(-25.0128)
policy weight change tensor(38.2308, grad_fn=<SumBackward0>)
time8 0.0019450187683105469
train_time 0.0740821361541748
eval time 2.0201237201690674
epoch last part time 5.9604644775390625e-06
2024-01-23 01:01:06,583 MainThread INFO: EPOCH:124
2024-01-23 01:01:06,583 MainThread INFO: Time Consumed:2.097543239593506s
2024-01-23 01:01:06,584 MainThread INFO: Total Frames:19500s
  1%|▏         | 125/10000 [02:34<5:42:08,  2.08s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12230.36410
Train_Epoch_Reward                8195.16324
Running_Training_Average_Rewards  13603.81007
Explore_Time                      0.00088
Train___Time                      0.07408
Eval____Time                      2.02012
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12173.29503
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.73359     2.03440     93.06011     87.35127
alpha_0                           0.93948      0.00013     0.93967      0.93929
Alpha_loss                        -0.41946     0.00078     -0.41839     -0.42065
Training/policy_loss              -2.60076     0.00584     -2.59092     -2.60706
Training/qf1_loss                 16284.72871  1108.88789  17705.25977  14312.05469
Training/qf2_loss                 16533.00977  1111.76360  17955.94727  14553.21289
Training/pf_norm                  0.13595      0.03820     0.17618      0.07253
Training/qf1_norm                 859.70897    17.97809    884.99817    832.70660
Training/qf2_norm                 223.20552    4.80129     228.66197    215.19768
log_std/mean                      -0.11938     0.00011     -0.11926     -0.11957
log_probs/mean                    -2.73008     0.00598     -2.71986     -2.73680
mean/mean                         0.00104      0.00011     0.00118      0.00088
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01981067657470703
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70700
epoch first part time 3.337860107421875e-06
replay_buffer._size: [19800]
collect time 0.0009508132934570312
inside mustsac before update, task 0, sumup 70700
inside mustsac after update, task 0, sumup 70173
inner_dict_sum {'sac_diff0': 0.00019931793212890625, 'sac_diff1': 0.0069544315338134766, 'sac_diff2': 0.008468866348266602, 'sac_diff3': 0.011041641235351562, 'sac_diff4': 0.007726907730102539, 'sac_diff5': 0.051575422286987305, 'sac_diff6': 0.0004105567932128906, 'all': 0.08637714385986328}
diff5_list [0.01044917106628418, 0.010610342025756836, 0.009889364242553711, 0.010022878646850586, 0.010603666305541992]
time3 0.0008652210235595703
time4 0.08724784851074219
time5 0.08730268478393555
time7 0.42461657524108887
gen_weight_change tensor(-24.8167)
policy weight change tensor(38.2673, grad_fn=<SumBackward0>)
time8 0.0020568370819091797
train_time 0.5326254367828369
eval time 1.5918564796447754
epoch last part time 7.152557373046875e-06
2024-01-23 01:01:08,734 MainThread INFO: EPOCH:125
2024-01-23 01:01:08,735 MainThread INFO: Time Consumed:2.127899169921875s
2024-01-23 01:01:08,735 MainThread INFO: Total Frames:19650s
  1%|▏         | 126/10000 [02:36<5:45:37,  2.10s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12230.52732
Train_Epoch_Reward                21014.55732
Running_Training_Average_Rewards  13758.95755
Explore_Time                      0.00095
Train___Time                      0.53263
Eval____Time                      1.59186
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12165.22206
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.93000     2.11935     96.96809     91.10118
alpha_0                           0.93901      0.00013     0.93920      0.93882
Alpha_loss                        -0.42304     0.00067     -0.42238     -0.42430
Training/policy_loss              -2.60386     0.00639     -2.59613     -2.61483
Training/qf1_loss                 18069.28027  1251.06991  19291.74609  16000.80176
Training/qf2_loss                 18334.92734  1242.12808  19539.69336  16275.60352
Training/pf_norm                  0.11906      0.01831     0.13573      0.08537
Training/qf1_norm                 901.71055    66.83565    1019.96155   823.20459
Training/qf2_norm                 231.55122    8.61567     241.60815    217.68961
log_std/mean                      -0.13619     0.00566     -0.12693     -0.14457
log_probs/mean                    -2.73340     0.00820     -2.72180     -2.74748
mean/mean                         -0.00035     0.00089     0.00123      -0.00137
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01870560646057129
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70173
epoch first part time 2.86102294921875e-06
replay_buffer._size: [19950]
collect time 0.0008883476257324219
inner_dict_sum {'sac_diff0': 0.00023651123046875, 'sac_diff1': 0.007690906524658203, 'sac_diff2': 0.009267568588256836, 'sac_diff3': 0.011133909225463867, 'sac_diff4': 0.008403301239013672, 'sac_diff5': 0.03883194923400879, 'sac_diff6': 0.00044846534729003906, 'all': 0.07601261138916016}
diff5_list [0.0065577030181884766, 0.006315469741821289, 0.013056039810180664, 0.006511211395263672, 0.0063915252685546875]
time3 0
time4 0.07694721221923828
time5 0.07700586318969727
time7 4.76837158203125e-07
gen_weight_change tensor(-24.8167)
policy weight change tensor(38.3852, grad_fn=<SumBackward0>)
time8 0.0022416114807128906
train_time 0.08898615837097168
eval time 2.296948194503784
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:11,146 MainThread INFO: EPOCH:126
2024-01-23 01:01:11,147 MainThread INFO: Time Consumed:2.3895740509033203s
2024-01-23 01:01:11,147 MainThread INFO: Total Frames:19800s
  1%|▏         | 127/10000 [02:38<6:01:00,  2.19s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12226.26073
Train_Epoch_Reward                29005.23802
Running_Training_Average_Rewards  14577.12818
Explore_Time                      0.00088
Train___Time                      0.08899
Eval____Time                      2.29695
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12245.85244
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.85840     3.27766     97.57613     88.60169
alpha_0                           0.93854      0.00013     0.93873      0.93835
Alpha_loss                        -0.42658     0.00102     -0.42524     -0.42812
Training/policy_loss              -2.60246     0.00590     -2.59285     -2.61013
Training/qf1_loss                 17493.21426  1544.54000  19606.26367  15072.17676
Training/qf2_loss                 17780.83398  1553.11456  19905.45312  15348.30664
Training/pf_norm                  0.12483      0.03065     0.18409      0.10276
Training/qf1_norm                 964.63530    31.17572    1003.81714   920.91736
Training/qf2_norm                 226.37891    7.63790     235.11749    214.12328
log_std/mean                      -0.12023     0.00022     -0.11995     -0.12058
log_probs/mean                    -2.73609     0.00625     -2.72558     -2.74371
mean/mean                         -0.00100     0.00003     -0.00097     -0.00106
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01976490020751953
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70173
epoch first part time 4.76837158203125e-06
replay_buffer._size: [20100]
collect time 0.0010018348693847656
inner_dict_sum {'sac_diff0': 0.00022339820861816406, 'sac_diff1': 0.0068645477294921875, 'sac_diff2': 0.007548332214355469, 'sac_diff3': 0.00960850715637207, 'sac_diff4': 0.006699085235595703, 'sac_diff5': 0.0319516658782959, 'sac_diff6': 0.00037384033203125, 'all': 0.06326937675476074}
diff5_list [0.006603717803955078, 0.006154537200927734, 0.006711483001708984, 0.006339073181152344, 0.006142854690551758]
time3 0
time4 0.06401920318603516
time5 0.06406331062316895
time7 4.76837158203125e-07
gen_weight_change tensor(-24.8167)
policy weight change tensor(38.4212, grad_fn=<SumBackward0>)
time8 0.0018072128295898438
train_time 0.0751042366027832
eval time 2.0298922061920166
epoch last part time 6.9141387939453125e-06
2024-01-23 01:01:13,278 MainThread INFO: EPOCH:127
2024-01-23 01:01:13,278 MainThread INFO: Time Consumed:2.1085643768310547s
2024-01-23 01:01:13,278 MainThread INFO: Total Frames:19950s
  1%|▏         | 128/10000 [02:41<6:09:34,  2.25s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12218.53486
Train_Epoch_Reward                9814.20509
Running_Training_Average_Rewards  14748.90619
Explore_Time                      0.00100
Train___Time                      0.07510
Eval____Time                      2.02989
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12255.33654
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.16847     1.23061    93.66065     90.02391
alpha_0                           0.93807      0.00013    0.93826      0.93788
Alpha_loss                        -0.42955     0.00116    -0.42832     -0.43153
Training/policy_loss              -2.59559     0.00662    -2.58643     -2.60430
Training/qf1_loss                 16742.33594  824.25726  18026.43164  15557.22168
Training/qf2_loss                 17007.47832  824.87069  18294.84961  15828.45605
Training/pf_norm                  0.12210      0.02647    0.14935      0.08121
Training/qf1_norm                 908.59783    16.83427   924.58069    881.81598
Training/qf2_norm                 222.09010    2.86542    225.59439    217.16397
log_std/mean                      -0.13472     0.00004    -0.13466     -0.13477
log_probs/mean                    -2.72977     0.00715    -2.72027     -2.73960
mean/mean                         -0.00082     0.00003    -0.00078     -0.00086
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.2558012008666992
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70173
epoch first part time 2.86102294921875e-06
replay_buffer._size: [20250]
collect time 0.004097938537597656
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.007838010787963867, 'sac_diff2': 0.009361505508422852, 'sac_diff3': 0.012030601501464844, 'sac_diff4': 0.00788116455078125, 'sac_diff5': 0.03583788871765137, 'sac_diff6': 0.00044655799865722656, 'all': 0.07362079620361328}
diff5_list [0.007729768753051758, 0.008196592330932617, 0.007080078125, 0.006453275680541992, 0.006378173828125]
time3 0
time4 0.07451653480529785
time5 0.07457447052001953
time7 7.152557373046875e-07
gen_weight_change tensor(-24.8167)
policy weight change tensor(38.4097, grad_fn=<SumBackward0>)
time8 0.0020537376403808594
train_time 0.08602762222290039
eval time 1.7170157432556152
epoch last part time 6.67572021484375e-06
2024-01-23 01:01:15,378 MainThread INFO: EPOCH:128
2024-01-23 01:01:15,378 MainThread INFO: Time Consumed:1.8096702098846436s
2024-01-23 01:01:15,379 MainThread INFO: Total Frames:20100s
  1%|▏         | 129/10000 [02:42<5:50:37,  2.13s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12215.23618
Train_Epoch_Reward                5794.43416
Running_Training_Average_Rewards  13418.09728
Explore_Time                      0.00409
Train___Time                      0.08603
Eval____Time                      1.71702
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12279.23041
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.94109     2.43567     94.93594     87.48203
alpha_0                           0.93760      0.00013     0.93779      0.93741
Alpha_loss                        -0.43294     0.00076     -0.43156     -0.43389
Training/policy_loss              -2.59667     0.00579     -2.59037     -2.60614
Training/qf1_loss                 16659.88223  1048.61641  18223.52539  15610.70312
Training/qf2_loss                 16929.52344  1054.51689  18502.73047  15874.83594
Training/pf_norm                  0.12876      0.02624     0.15891      0.08186
Training/qf1_norm                 868.14978    20.27540    901.54657    845.87592
Training/qf2_norm                 222.20391    5.74811     231.66423    214.08640
log_std/mean                      -0.13505     0.00003     -0.13500     -0.13509
log_probs/mean                    -2.73005     0.00591     -2.72388     -2.73988
mean/mean                         -0.00105     0.00004     -0.00100     -0.00110
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018878459930419922
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70173
epoch first part time 2.86102294921875e-06
replay_buffer._size: [20400]
collect time 0.0009620189666748047
inner_dict_sum {'sac_diff0': 0.00021219253540039062, 'sac_diff1': 0.006676673889160156, 'sac_diff2': 0.00799870491027832, 'sac_diff3': 0.010243415832519531, 'sac_diff4': 0.006696224212646484, 'sac_diff5': 0.03162074089050293, 'sac_diff6': 0.00037932395935058594, 'all': 0.0638272762298584}
diff5_list [0.006663799285888672, 0.006241798400878906, 0.006240367889404297, 0.006444454193115234, 0.00603032112121582]
time3 0
time4 0.06458425521850586
time5 0.06462717056274414
time7 4.76837158203125e-07
gen_weight_change tensor(-24.8167)
policy weight change tensor(38.3741, grad_fn=<SumBackward0>)
time8 0.001959085464477539
train_time 0.07575511932373047
eval time 1.9341118335723877
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:17,414 MainThread INFO: EPOCH:129
2024-01-23 01:01:17,414 MainThread INFO: Time Consumed:2.013439893722534s
2024-01-23 01:01:17,414 MainThread INFO: Total Frames:20250s
  1%|▏         | 130/10000 [02:45<5:45:53,  2.10s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12211.10437
Train_Epoch_Reward                12498.72862
Running_Training_Average_Rewards  13494.94106
Explore_Time                      0.00096
Train___Time                      0.07576
Eval____Time                      1.93411
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12217.92614
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.44539     1.11503    94.23241     90.83643
alpha_0                           0.93713      0.00013    0.93732      0.93695
Alpha_loss                        -0.43658     0.00101    -0.43525     -0.43805
Training/policy_loss              -2.60195     0.00166    -2.59891     -2.60340
Training/qf1_loss                 16992.62852  533.77847  17748.40234  16404.97266
Training/qf2_loss                 17275.50117  536.90250  18041.15625  16690.80469
Training/pf_norm                  0.13845      0.04685    0.22543      0.09244
Training/qf1_norm                 927.76886    16.23541   955.52789    911.40881
Training/qf2_norm                 225.67273    2.65256    229.94353    221.89479
log_std/mean                      -0.12687     0.00005    -0.12680     -0.12695
log_probs/mean                    -2.73422     0.00190    -2.73097     -2.73610
mean/mean                         -0.00097     0.00011    -0.00077     -0.00107
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018733978271484375
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70173
epoch first part time 3.337860107421875e-06
replay_buffer._size: [20550]
collect time 0.0008552074432373047
inside mustsac before update, task 0, sumup 70173
inside mustsac after update, task 0, sumup 71750
inner_dict_sum {'sac_diff0': 0.00019979476928710938, 'sac_diff1': 0.0067403316497802734, 'sac_diff2': 0.008226156234741211, 'sac_diff3': 0.010846853256225586, 'sac_diff4': 0.00733184814453125, 'sac_diff5': 0.05099201202392578, 'sac_diff6': 0.00041413307189941406, 'all': 0.08475112915039062}
diff5_list [0.01275014877319336, 0.009744882583618164, 0.009621620178222656, 0.009486198425292969, 0.009389162063598633]
time3 0.000873565673828125
time4 0.08560729026794434
time5 0.08566045761108398
time7 0.4991261959075928
gen_weight_change tensor(-24.5730)
policy weight change tensor(38.3710, grad_fn=<SumBackward0>)
time8 0.0029511451721191406
train_time 0.606163740158081
eval time 1.6272177696228027
epoch last part time 7.3909759521484375e-06
2024-01-23 01:01:19,673 MainThread INFO: EPOCH:130
2024-01-23 01:01:19,673 MainThread INFO: Time Consumed:2.2367103099823s
2024-01-23 01:01:19,674 MainThread INFO: Total Frames:20400s
  1%|▏         | 131/10000 [02:47<5:53:43,  2.15s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12205.48439
Train_Epoch_Reward                8652.14954
Running_Training_Average_Rewards  13560.36674
Explore_Time                      0.00085
Train___Time                      0.60616
Eval____Time                      1.62722
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12149.85221
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.91680     1.37526    92.68870     89.35000
alpha_0                           0.93666      0.00013    0.93685      0.93648
Alpha_loss                        -0.44032     0.00057    -0.43974     -0.44118
Training/policy_loss              -2.60558     0.00651    -2.59718     -2.61546
Training/qf1_loss                 15802.18633  651.78424  16615.12695  14735.84082
Training/qf2_loss                 16080.11035  651.88097  16871.49414  15013.42773
Training/pf_norm                  0.13328      0.03678    0.17191      0.07544
Training/qf1_norm                 922.13672    48.84512   1012.73254   880.48218
Training/qf2_norm                 225.68719    4.77771    232.22746    221.49391
log_std/mean                      -0.13580     0.00616    -0.12504     -0.14125
log_probs/mean                    -2.73992     0.00659    -2.73244     -2.75167
mean/mean                         -0.00008     0.00124    0.00172      -0.00186
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018584251403808594
epoch last part time3 0.003180265426635742
inside rlalgo, task 0, sumup 71750
epoch first part time 3.337860107421875e-06
replay_buffer._size: [20700]
collect time 0.0010745525360107422
inner_dict_sum {'sac_diff0': 0.00021266937255859375, 'sac_diff1': 0.0066986083984375, 'sac_diff2': 0.007745027542114258, 'sac_diff3': 0.01000070571899414, 'sac_diff4': 0.006642580032348633, 'sac_diff5': 0.0303957462310791, 'sac_diff6': 0.0003883838653564453, 'all': 0.06208372116088867}
diff5_list [0.006599903106689453, 0.006060123443603516, 0.0059964656829833984, 0.00600433349609375, 0.005734920501708984]
time3 0
time4 0.06279301643371582
time5 0.06283450126647949
time7 4.76837158203125e-07
gen_weight_change tensor(-24.5730)
policy weight change tensor(38.3658, grad_fn=<SumBackward0>)
time8 0.0019080638885498047
train_time 0.07396197319030762
eval time 1.9505829811096191
epoch last part time 1.239776611328125e-05
2024-01-23 01:01:21,727 MainThread INFO: EPOCH:131
2024-01-23 01:01:21,727 MainThread INFO: Time Consumed:2.0286686420440674s
2024-01-23 01:01:21,728 MainThread INFO: Total Frames:20550s
  1%|▏         | 132/10000 [02:49<5:48:47,  2.12s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12211.75945
Train_Epoch_Reward                11036.37766
Running_Training_Average_Rewards  13718.04674
Explore_Time                      0.00107
Train___Time                      0.07396
Eval____Time                      1.95058
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12293.19681
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.28463     1.90196    95.47102     89.84773
alpha_0                           0.93619      0.00013    0.93638      0.93601
Alpha_loss                        -0.44354     0.00111    -0.44180     -0.44476
Training/policy_loss              -2.59952     0.00359    -2.59448     -2.60462
Training/qf1_loss                 17631.56016  666.54240  18727.83398  16967.26172
Training/qf2_loss                 17910.56055  673.43070  19021.70703  17242.77734
Training/pf_norm                  0.13514      0.02000    0.17013      0.11611
Training/qf1_norm                 936.05474    23.62096   980.84204    910.86285
Training/qf2_norm                 224.09892    4.41897    231.54738    218.46449
log_std/mean                      -0.12750     0.00002    -0.12747     -0.12753
log_probs/mean                    -2.73751     0.00403    -2.73158     -2.74325
mean/mean                         -0.00024     0.00015    -0.00004     -0.00049
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01910400390625
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71750
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [20850]
collect time 0.0009179115295410156
inner_dict_sum {'sac_diff0': 0.0002238750457763672, 'sac_diff1': 0.007540464401245117, 'sac_diff2': 0.007675647735595703, 'sac_diff3': 0.010254144668579102, 'sac_diff4': 0.006613254547119141, 'sac_diff5': 0.03378009796142578, 'sac_diff6': 0.0004017353057861328, 'all': 0.06648921966552734}
diff5_list [0.00668025016784668, 0.008397102355957031, 0.006505727767944336, 0.006181955337524414, 0.00601506233215332]
time3 0
time4 0.0672910213470459
time5 0.06734752655029297
time7 9.5367431640625e-07
gen_weight_change tensor(-24.5730)
policy weight change tensor(38.4736, grad_fn=<SumBackward0>)
time8 0.001848459243774414
train_time 0.07900810241699219
eval time 1.968824863433838
epoch last part time 9.298324584960938e-06
2024-01-23 01:01:23,801 MainThread INFO: EPOCH:132
2024-01-23 01:01:23,801 MainThread INFO: Time Consumed:2.0512781143188477s
2024-01-23 01:01:23,802 MainThread INFO: Total Frames:20700s
  1%|▏         | 133/10000 [02:51<5:46:30,  2.11s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12219.73281
Train_Epoch_Reward                15442.17636
Running_Training_Average_Rewards  13372.20496
Explore_Time                      0.00091
Train___Time                      0.07901
Eval____Time                      1.96882
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12259.46169
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.04184     1.07908    94.92157     91.77242
alpha_0                           0.93573      0.00013    0.93591      0.93554
Alpha_loss                        -0.44649     0.00110    -0.44501     -0.44808
Training/policy_loss              -2.59450     0.00225    -2.59149     -2.59739
Training/qf1_loss                 17074.17676  725.31167  18346.88867  16270.21582
Training/qf2_loss                 17334.97109  725.31980  18608.94336  16531.79688
Training/pf_norm                  0.11390      0.03086    0.16590      0.08042
Training/qf1_norm                 860.81317    5.98741    869.66931    851.68896
Training/qf2_norm                 226.76763    2.48436    231.04601    223.78896
log_std/mean                      -0.12412     0.00021    -0.12385     -0.12445
log_probs/mean                    -2.73111     0.00271    -2.72790     -2.73475
mean/mean                         -0.00022     0.00002    -0.00021     -0.00025
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02015519142150879
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71750
epoch first part time 3.337860107421875e-06
replay_buffer._size: [21000]
collect time 0.0009827613830566406
inner_dict_sum {'sac_diff0': 0.0001976490020751953, 'sac_diff1': 0.006954193115234375, 'sac_diff2': 0.0074307918548583984, 'sac_diff3': 0.010144233703613281, 'sac_diff4': 0.006464719772338867, 'sac_diff5': 0.0319821834564209, 'sac_diff6': 0.00037741661071777344, 'all': 0.06355118751525879}
diff5_list [0.007598161697387695, 0.006213665008544922, 0.006160259246826172, 0.0061604976654052734, 0.005849599838256836]
time3 0
time4 0.06428003311157227
time5 0.06432032585144043
time7 4.76837158203125e-07
gen_weight_change tensor(-24.5730)
policy weight change tensor(38.4505, grad_fn=<SumBackward0>)
time8 0.0017809867858886719
train_time 0.0751793384552002
eval time 0.7614505290985107
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:24,665 MainThread INFO: EPOCH:133
2024-01-23 01:01:24,666 MainThread INFO: Time Consumed:0.839989185333252s
2024-01-23 01:01:24,666 MainThread INFO: Total Frames:20850s
  1%|▏         | 134/10000 [02:52<4:45:04,  1.73s/it]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12217.70425
Train_Epoch_Reward                9637.28247
Running_Training_Average_Rewards  13023.83221
Explore_Time                      0.00097
Train___Time                      0.07518
Eval____Time                      0.76145
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12137.66915
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.26451     0.85085    93.39075     91.25089
alpha_0                           0.93526      0.00013    0.93545      0.93507
Alpha_loss                        -0.45037     0.00119    -0.44836     -0.45156
Training/policy_loss              -2.59717     0.00533    -2.58835     -2.60316
Training/qf1_loss                 16687.47148  634.42511  17665.68359  15850.30664
Training/qf2_loss                 16975.67051  635.37153  17956.44727  16139.58105
Training/pf_norm                  0.12648      0.04110    0.17907      0.07194
Training/qf1_norm                 934.21582    7.52358    942.84949    921.34241
Training/qf2_norm                 228.93191    2.11156    231.64636    226.50427
log_std/mean                      -0.14624     0.00022    -0.14591     -0.14646
log_probs/mean                    -2.73874     0.00587    -2.72877     -2.74515
mean/mean                         0.00016      0.00001    0.00018      0.00015
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01840806007385254
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71750
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [21150]
collect time 0.0008394718170166016
inner_dict_sum {'sac_diff0': 0.0002033710479736328, 'sac_diff1': 0.00639796257019043, 'sac_diff2': 0.0072460174560546875, 'sac_diff3': 0.009598731994628906, 'sac_diff4': 0.0063893795013427734, 'sac_diff5': 0.031198978424072266, 'sac_diff6': 0.000377655029296875, 'all': 0.06141209602355957}
diff5_list [0.006598949432373047, 0.006184816360473633, 0.0059397220611572266, 0.006159067153930664, 0.006316423416137695]
time3 0
time4 0.06213665008544922
time5 0.06217789649963379
time7 7.152557373046875e-07
gen_weight_change tensor(-24.5730)
policy weight change tensor(38.4264, grad_fn=<SumBackward0>)
time8 0.0018019676208496094
train_time 0.07265329360961914
eval time 0.16452932357788086
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:24,928 MainThread INFO: EPOCH:134
2024-01-23 01:01:24,928 MainThread INFO: Time Consumed:0.2402968406677246s
2024-01-23 01:01:24,928 MainThread INFO: Total Frames:21000s
  1%|▏         | 135/10000 [02:52<3:32:28,  1.29s/it]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12205.82882
Train_Epoch_Reward                3320.98179
Running_Training_Average_Rewards  12909.79927
Explore_Time                      0.00083
Train___Time                      0.07265
Eval____Time                      0.16453
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12054.54076
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.79411     1.93140     96.46759     91.14410
alpha_0                           0.93479      0.00013     0.93498      0.93460
Alpha_loss                        -0.45336     0.00087     -0.45201     -0.45438
Training/policy_loss              -2.59353     0.00782     -2.58460     -2.60536
Training/qf1_loss                 19979.33125  1326.79432  21384.61914  17453.11523
Training/qf2_loss                 20293.16523  1331.18168  21699.54883  17757.17578
Training/pf_norm                  0.12967      0.03992     0.18130      0.06578
Training/qf1_norm                 981.51189    16.58559    991.91595    948.50134
Training/qf2_norm                 232.53853    4.57929     236.46219    223.86673
log_std/mean                      -0.12768     0.00003     -0.12765     -0.12774
log_probs/mean                    -2.73297     0.00819     -2.72395     -2.74534
mean/mean                         -0.00033     0.00007     -0.00021     -0.00042
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018433332443237305
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71750
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [21300]
collect time 0.0009860992431640625
inside mustsac before update, task 0, sumup 71750
inside mustsac after update, task 0, sumup 70145
inner_dict_sum {'sac_diff0': 0.0001888275146484375, 'sac_diff1': 0.006296634674072266, 'sac_diff2': 0.00698542594909668, 'sac_diff3': 0.00938272476196289, 'sac_diff4': 0.006295680999755859, 'sac_diff5': 0.04733633995056152, 'sac_diff6': 0.0003840923309326172, 'all': 0.07686972618103027}
diff5_list [0.009960174560546875, 0.00935673713684082, 0.009222030639648438, 0.00915837287902832, 0.00963902473449707]
time3 0.0008370876312255859
time4 0.07763814926147461
time5 0.07768368721008301
time7 0.009253740310668945
gen_weight_change tensor(-24.3205)
policy weight change tensor(38.4150, grad_fn=<SumBackward0>)
time8 0.0018880367279052734
train_time 0.10639715194702148
eval time 0.13157439231872559
epoch last part time 4.5299530029296875e-06
2024-01-23 01:01:25,191 MainThread INFO: EPOCH:135
2024-01-23 01:01:25,191 MainThread INFO: Time Consumed:0.2412090301513672s
2024-01-23 01:01:25,191 MainThread INFO: Total Frames:21150s
  1%|▏         | 136/10000 [02:52<2:41:42,  1.02it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12201.96918
Train_Epoch_Reward                67577.30768
Running_Training_Average_Rewards  14921.53458
Explore_Time                      0.00098
Train___Time                      0.10640
Eval____Time                      0.13157
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12126.62567
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.95857     1.54249     97.09053     92.88211
alpha_0                           0.93432      0.00013     0.93451      0.93413
Alpha_loss                        -0.45691     0.00082     -0.45591     -0.45803
Training/policy_loss              -2.59580     0.00657     -2.58745     -2.60619
Training/qf1_loss                 20267.97656  1089.71206  22167.02344  18846.30859
Training/qf2_loss                 20597.21289  1080.90617  22482.68750  19195.16602
Training/pf_norm                  0.14686      0.04131     0.20683      0.08783
Training/qf1_norm                 1042.24922   42.17700    1110.59375   1003.76892
Training/qf2_norm                 232.65278    8.13168     243.08809    218.68730
log_std/mean                      -0.12664     0.01143     -0.11498     -0.14407
log_probs/mean                    -2.73572     0.00672     -2.72789     -2.74431
mean/mean                         0.00086      0.00075     0.00178      -0.00010
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01872849464416504
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70145
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [21450]
collect time 0.0008091926574707031
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.007013797760009766, 'sac_diff2': 0.011295080184936523, 'sac_diff3': 0.012608528137207031, 'sac_diff4': 0.007637739181518555, 'sac_diff5': 0.03355526924133301, 'sac_diff6': 0.0003790855407714844, 'all': 0.07270026206970215}
diff5_list [0.008502721786499023, 0.006476879119873047, 0.006197452545166016, 0.005934476852416992, 0.00644373893737793]
time3 0
time4 0.07347559928894043
time5 0.07353591918945312
time7 7.152557373046875e-07
gen_weight_change tensor(-24.3205)
policy weight change tensor(38.4813, grad_fn=<SumBackward0>)
time8 0.002382516860961914
train_time 0.08534359931945801
eval time 0.15215635299682617
epoch last part time 4.5299530029296875e-06
2024-01-23 01:01:25,454 MainThread INFO: EPOCH:136
2024-01-23 01:01:25,454 MainThread INFO: Time Consumed:0.2408435344696045s
2024-01-23 01:01:25,455 MainThread INFO: Total Frames:21300s
  1%|▏         | 137/10000 [02:53<2:06:14,  1.30it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12119.27839
Train_Epoch_Reward                11356.49027
Running_Training_Average_Rewards  14801.24842
Explore_Time                      0.00080
Train___Time                      0.08534
Eval____Time                      0.15216
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11418.94448
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.41502     3.81885     100.12575    90.23415
alpha_0                           0.93385      0.00013     0.93404      0.93367
Alpha_loss                        -0.45986     0.00060     -0.45896     -0.46055
Training/policy_loss              -2.59183     0.01020     -2.57954     -2.60671
Training/qf1_loss                 20002.31719  2457.78133  22581.94922  16077.45117
Training/qf2_loss                 20325.59063  2469.52398  22915.96680  16385.70898
Training/pf_norm                  0.12719      0.03954     0.17650      0.06433
Training/qf1_norm                 1043.03964   44.96936    1106.32129   991.84680
Training/qf2_norm                 240.60014    9.28532     252.02289    228.02547
log_std/mean                      -0.12626     0.00014     -0.12611     -0.12648
log_probs/mean                    -2.72942     0.01062     -2.71626     -2.74480
mean/mean                         0.00060      0.00007     0.00071      0.00051
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020705223083496094
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70145
epoch first part time 2.09808349609375e-05
replay_buffer._size: [21600]
collect time 0.0009720325469970703
inner_dict_sum {'sac_diff0': 0.00019741058349609375, 'sac_diff1': 0.007150173187255859, 'sac_diff2': 0.008054018020629883, 'sac_diff3': 0.010118722915649414, 'sac_diff4': 0.006833314895629883, 'sac_diff5': 0.0318906307220459, 'sac_diff6': 0.0003960132598876953, 'all': 0.06464028358459473}
diff5_list [0.006951332092285156, 0.006173610687255859, 0.006458282470703125, 0.00608515739440918, 0.006222248077392578]
time3 0
time4 0.06539034843444824
time5 0.06543684005737305
time7 7.152557373046875e-07
gen_weight_change tensor(-24.3205)
policy weight change tensor(38.5097, grad_fn=<SumBackward0>)
time8 0.0018846988677978516
train_time 0.07654428482055664
eval time 0.1587200164794922
epoch last part time 4.5299530029296875e-06
2024-01-23 01:01:25,716 MainThread INFO: EPOCH:137
2024-01-23 01:01:25,717 MainThread INFO: Time Consumed:0.23848271369934082s
2024-01-23 01:01:25,717 MainThread INFO: Total Frames:21450s
  1%|▏         | 138/10000 [02:53<1:41:12,  1.62it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12051.16527
Train_Epoch_Reward                4665.36464
Running_Training_Average_Rewards  13881.78640
Explore_Time                      0.00097
Train___Time                      0.07654
Eval____Time                      0.15872
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11574.20541
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.75267     2.43824     97.31062     90.60606
alpha_0                           0.93339      0.00013     0.93357      0.93320
Alpha_loss                        -0.46345     0.00072     -0.46227     -0.46447
Training/policy_loss              -2.59244     0.00471     -2.58654     -2.59886
Training/qf1_loss                 18262.99531  1786.82919  20955.69922  15750.95508
Training/qf2_loss                 18580.27969  1792.67336  21283.33984  16061.24023
Training/pf_norm                  0.11443      0.01832     0.13642      0.08393
Training/qf1_norm                 994.40054    21.44801    1031.75000   974.73438
Training/qf2_norm                 232.97388    5.83909     243.88983    227.81763
log_std/mean                      -0.14363     0.00006     -0.14354     -0.14370
log_probs/mean                    -2.73268     0.00473     -2.72662     -2.73928
mean/mean                         0.00122      0.00008     0.00133      0.00112
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018324613571166992
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70145
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [21750]
collect time 0.0008313655853271484
inner_dict_sum {'sac_diff0': 0.00020647048950195312, 'sac_diff1': 0.006528615951538086, 'sac_diff2': 0.007849454879760742, 'sac_diff3': 0.010366439819335938, 'sac_diff4': 0.006846904754638672, 'sac_diff5': 0.03191423416137695, 'sac_diff6': 0.00037932395935058594, 'all': 0.06409144401550293}
diff5_list [0.0067005157470703125, 0.006276130676269531, 0.00651097297668457, 0.006173372268676758, 0.006253242492675781]
time3 0
time4 0.06483864784240723
time5 0.06488370895385742
time7 7.152557373046875e-07
gen_weight_change tensor(-24.3205)
policy weight change tensor(38.5364, grad_fn=<SumBackward0>)
time8 0.0019953250885009766
train_time 0.07567763328552246
eval time 0.16518950462341309
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:25,982 MainThread INFO: EPOCH:138
2024-01-23 01:01:25,982 MainThread INFO: Time Consumed:0.24398303031921387s
2024-01-23 01:01:25,983 MainThread INFO: Total Frames:21600s
  1%|▏         | 139/10000 [02:53<1:23:57,  1.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11987.14026
Train_Epoch_Reward                15045.98025
Running_Training_Average_Rewards  13665.63881
Explore_Time                      0.00083
Train___Time                      0.07568
Eval____Time                      0.16519
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11638.98031
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.46967     2.62579     97.37267     90.20951
alpha_0                           0.93292      0.00013     0.93311      0.93273
Alpha_loss                        -0.46705     0.00112     -0.46558     -0.46898
Training/policy_loss              -2.59306     0.00454     -2.58791     -2.60029
Training/qf1_loss                 18990.43477  1803.57155  21749.86914  16185.45508
Training/qf2_loss                 19330.84023  1809.24541  22093.80664  16510.66016
Training/pf_norm                  0.10100      0.04184     0.15045      0.04321
Training/qf1_norm                 1017.08876   26.58392    1050.88525   971.37244
Training/qf2_norm                 234.12746    6.21907     240.99649    224.00929
log_std/mean                      -0.13639     0.00006     -0.13630     -0.13646
log_probs/mean                    -2.73599     0.00499     -2.73075     -2.74429
mean/mean                         0.00126      0.00016     0.00146      0.00104
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018727779388427734
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70145
epoch first part time 2.86102294921875e-06
replay_buffer._size: [21900]
collect time 0.0008776187896728516
inner_dict_sum {'sac_diff0': 0.0002117156982421875, 'sac_diff1': 0.00715184211730957, 'sac_diff2': 0.008288145065307617, 'sac_diff3': 0.010498046875, 'sac_diff4': 0.007100343704223633, 'sac_diff5': 0.03315114974975586, 'sac_diff6': 0.00038623809814453125, 'all': 0.0667874813079834}
diff5_list [0.006788015365600586, 0.006447315216064453, 0.006692171096801758, 0.006691455841064453, 0.006532192230224609]
time3 0
time4 0.06752276420593262
time5 0.06756782531738281
time7 9.5367431640625e-07
gen_weight_change tensor(-24.3205)
policy weight change tensor(38.5055, grad_fn=<SumBackward0>)
time8 0.0018854141235351562
train_time 0.07825994491577148
eval time 0.1638338565826416
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:26,250 MainThread INFO: EPOCH:139
2024-01-23 01:01:26,250 MainThread INFO: Time Consumed:0.24519681930541992s
2024-01-23 01:01:26,250 MainThread INFO: Total Frames:21750s
  1%|▏         | 140/10000 [02:53<1:11:56,  2.28it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11931.80287
Train_Epoch_Reward                11008.63902
Running_Training_Average_Rewards  13808.10832
Explore_Time                      0.00087
Train___Time                      0.07826
Eval____Time                      0.16383
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11664.55224
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.54583     2.26193     94.61778     88.56930
alpha_0                           0.93245      0.00013     0.93264      0.93227
Alpha_loss                        -0.47014     0.00084     -0.46912     -0.47111
Training/policy_loss              -2.58877     0.00474     -2.58323     -2.59458
Training/qf1_loss                 16930.51543  1681.63466  19711.29492  15463.95605
Training/qf2_loss                 17288.98242  1690.96860  20087.97656  15815.40625
Training/pf_norm                  0.12432      0.02785     0.15728      0.07506
Training/qf1_norm                 1077.80759   29.62741    1131.02197   1044.94531
Training/qf2_norm                 225.22638    5.47497     235.07637    220.36073
log_std/mean                      -0.13530     0.00009     -0.13517     -0.13540
log_probs/mean                    -2.73194     0.00488     -2.72654     -2.73816
mean/mean                         0.00026      0.00002     0.00027      0.00023
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018688678741455078
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70145
epoch first part time 2.86102294921875e-06
replay_buffer._size: [22050]
collect time 0.0008232593536376953
inside mustsac before update, task 0, sumup 70145
inside mustsac after update, task 0, sumup 71337
inner_dict_sum {'sac_diff0': 0.00020575523376464844, 'sac_diff1': 0.008013725280761719, 'sac_diff2': 0.008177518844604492, 'sac_diff3': 0.010706186294555664, 'sac_diff4': 0.007318258285522461, 'sac_diff5': 0.051187992095947266, 'sac_diff6': 0.00040435791015625, 'all': 0.0860137939453125}
diff5_list [0.011613130569458008, 0.010507345199584961, 0.00968790054321289, 0.00978541374206543, 0.009594202041625977]
time3 0.0008769035339355469
time4 0.08687114715576172
time5 0.08692765235900879
time7 0.009439468383789062
gen_weight_change tensor(-24.2695)
policy weight change tensor(38.5399, grad_fn=<SumBackward0>)
time8 0.0026259422302246094
train_time 0.11688756942749023
eval time 0.12078142166137695
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:26,512 MainThread INFO: EPOCH:140
2024-01-23 01:01:26,513 MainThread INFO: Time Consumed:0.24067282676696777s
2024-01-23 01:01:26,513 MainThread INFO: Total Frames:21900s
  1%|▏         | 141/10000 [02:54<1:03:28,  2.59it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11892.27542
Train_Epoch_Reward                3175.45961
Running_Training_Average_Rewards  13400.85755
Explore_Time                      0.00082
Train___Time                      0.11689
Eval____Time                      0.12078
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11754.57765
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.03340     3.14273     94.88280     86.91127
alpha_0                           0.93199      0.00013     0.93217      0.93180
Alpha_loss                        -0.47386     0.00125     -0.47201     -0.47565
Training/policy_loss              -2.59207     0.00822     -2.57949     -2.60320
Training/qf1_loss                 16993.77461  1571.72739  18132.28516  13906.25977
Training/qf2_loss                 17350.13203  1574.40573  18513.06055  14263.11523
Training/pf_norm                  0.15927      0.04178     0.23820      0.11429
Training/qf1_norm                 1084.73840   51.77876    1177.71094   1034.10095
Training/qf2_norm                 224.41574    10.34470    235.48349    208.13110
log_std/mean                      -0.13077     0.00683     -0.12359     -0.14153
log_probs/mean                    -2.73698     0.01045     -2.72011     -2.75017
mean/mean                         0.00149      0.00094     0.00290      0.00029
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018710613250732422
epoch last part time3 0.0032300949096679688
inside rlalgo, task 0, sumup 71337
epoch first part time 3.337860107421875e-06
replay_buffer._size: [22200]
collect time 0.0009183883666992188
inner_dict_sum {'sac_diff0': 0.00019788742065429688, 'sac_diff1': 0.007066965103149414, 'sac_diff2': 0.008453369140625, 'sac_diff3': 0.010310649871826172, 'sac_diff4': 0.007127285003662109, 'sac_diff5': 0.031708478927612305, 'sac_diff6': 0.0003809928894042969, 'all': 0.0652456283569336}
diff5_list [0.006958961486816406, 0.006256580352783203, 0.006434917449951172, 0.006323099136352539, 0.005734920501708984]
time3 0
time4 0.06602978706359863
time5 0.06607818603515625
time7 7.152557373046875e-07
gen_weight_change tensor(-24.2695)
policy weight change tensor(38.4879, grad_fn=<SumBackward0>)
time8 0.0018773078918457031
train_time 0.07749629020690918
eval time 0.1555161476135254
epoch last part time 5.9604644775390625e-06
2024-01-23 01:01:26,774 MainThread INFO: EPOCH:141
2024-01-23 01:01:26,774 MainThread INFO: Time Consumed:0.23621273040771484s
2024-01-23 01:01:26,774 MainThread INFO: Total Frames:22050s
  1%|▏         | 142/10000 [02:54<57:18,  2.87it/s]  --------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11885.38154
Train_Epoch_Reward                10822.20393
Running_Training_Average_Rewards  13033.59194
Explore_Time                      0.00091
Train___Time                      0.07750
Eval____Time                      0.15552
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12224.25799
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.82786     3.85535     96.49669     87.20206
alpha_0                           0.93152      0.00013     0.93171      0.93133
Alpha_loss                        -0.47672     0.00080     -0.47562     -0.47798
Training/policy_loss              -2.58539     0.00659     -2.57829     -2.59618
Training/qf1_loss                 18132.57676  2554.66047  22384.62500  15401.86230
Training/qf2_loss                 18514.99258  2571.06368  22785.60156  15763.88867
Training/pf_norm                  0.12568      0.03694     0.19347      0.08870
Training/qf1_norm                 1138.44783   53.32516    1194.74292   1069.44812
Training/qf2_norm                 229.29684    9.32081     240.55766    217.97527
log_std/mean                      -0.14249     0.00010     -0.14231     -0.14258
log_probs/mean                    -2.72966     0.00684     -2.72205     -2.74095
mean/mean                         -0.00012     0.00004     -0.00005     -0.00016
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0220491886138916
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71337
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [22350]
collect time 0.0009391307830810547
inner_dict_sum {'sac_diff0': 0.00019812583923339844, 'sac_diff1': 0.007244586944580078, 'sac_diff2': 0.008691549301147461, 'sac_diff3': 0.01079416275024414, 'sac_diff4': 0.007350921630859375, 'sac_diff5': 0.033640146255493164, 'sac_diff6': 0.0004000663757324219, 'all': 0.06831955909729004}
diff5_list [0.006491184234619141, 0.006447553634643555, 0.007834196090698242, 0.006966114044189453, 0.0059010982513427734]
time3 0
time4 0.06909823417663574
time5 0.06914448738098145
time7 7.152557373046875e-07
gen_weight_change tensor(-24.2695)
policy weight change tensor(38.4870, grad_fn=<SumBackward0>)
time8 0.0019342899322509766
train_time 0.08044219017028809
eval time 0.15613460540771484
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:27,039 MainThread INFO: EPOCH:142
2024-01-23 01:01:27,039 MainThread INFO: Time Consumed:0.23977899551391602s
2024-01-23 01:01:27,040 MainThread INFO: Total Frames:22200s
  1%|▏         | 143/10000 [02:54<53:14,  3.09it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11883.59461
Train_Epoch_Reward                8262.18649
Running_Training_Average_Rewards  12921.23314
Explore_Time                      0.00093
Train___Time                      0.08044
Eval____Time                      0.15613
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12241.59240
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.19793     2.17004     94.82988     88.50991
alpha_0                           0.93105      0.00013     0.93124      0.93087
Alpha_loss                        -0.48040     0.00089     -0.47930     -0.48163
Training/policy_loss              -2.59003     0.00207     -2.58799     -2.59379
Training/qf1_loss                 18709.51680  1003.98627  19793.80664  17162.90234
Training/qf2_loss                 19050.93906  1005.85213  20144.40430  17508.97266
Training/pf_norm                  0.12574      0.02321     0.16698      0.10135
Training/qf1_norm                 1036.95117   29.51947    1065.37463   991.18170
Training/qf2_norm                 234.52816    5.31902     240.97707    225.52072
log_std/mean                      -0.12487     0.00005     -0.12482     -0.12496
log_probs/mean                    -2.73400     0.00199     -2.73197     -2.73747
mean/mean                         0.00169      0.00006     0.00179      0.00162
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02281665802001953
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71337
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [22500]
collect time 0.0010263919830322266
inner_dict_sum {'sac_diff0': 0.00019168853759765625, 'sac_diff1': 0.006630897521972656, 'sac_diff2': 0.008192062377929688, 'sac_diff3': 0.01081991195678711, 'sac_diff4': 0.007050514221191406, 'sac_diff5': 0.03125, 'sac_diff6': 0.00037860870361328125, 'all': 0.0645136833190918}
diff5_list [0.006365299224853516, 0.0060577392578125, 0.006260871887207031, 0.006477832794189453, 0.0060882568359375]
time3 0
time4 0.06530427932739258
time5 0.06536030769348145
time7 1.1920928955078125e-06
gen_weight_change tensor(-24.2695)
policy weight change tensor(38.4815, grad_fn=<SumBackward0>)
time8 0.0020165443420410156
train_time 0.07708144187927246
eval time 0.15917444229125977
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:27,305 MainThread INFO: EPOCH:143
2024-01-23 01:01:27,306 MainThread INFO: Time Consumed:0.23961806297302246s
2024-01-23 01:01:27,306 MainThread INFO: Total Frames:22350s
  1%|▏         | 144/10000 [02:54<50:18,  3.27it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11898.42150
Train_Epoch_Reward                4395.72301
Running_Training_Average_Rewards  13003.17842
Explore_Time                      0.00102
Train___Time                      0.07708
Eval____Time                      0.15917
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12285.93809
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.94155     2.01263     95.80737     90.69435
alpha_0                           0.93059      0.00013     0.93077      0.93040
Alpha_loss                        -0.48383     0.00081     -0.48276     -0.48526
Training/policy_loss              -2.58632     0.00525     -2.57767     -2.59257
Training/qf1_loss                 18265.66328  1179.81913  19878.28711  16812.95703
Training/qf2_loss                 18647.47070  1185.18262  20274.47070  17184.45898
Training/pf_norm                  0.12349      0.03228     0.16903      0.07450
Training/qf1_norm                 1136.74595   31.61438    1177.37231   1103.29175
Training/qf2_norm                 227.67780    4.77491     234.48878    222.48096
log_std/mean                      -0.13681     0.00007     -0.13670     -0.13689
log_probs/mean                    -2.73482     0.00542     -2.72579     -2.74127
mean/mean                         0.00094      0.00005     0.00103      0.00086
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021272659301757812
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71337
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [22650]
collect time 0.0009734630584716797
inner_dict_sum {'sac_diff0': 0.00019121170043945312, 'sac_diff1': 0.007035493850708008, 'sac_diff2': 0.008066415786743164, 'sac_diff3': 0.010310888290405273, 'sac_diff4': 0.007048368453979492, 'sac_diff5': 0.03278374671936035, 'sac_diff6': 0.0003857612609863281, 'all': 0.06582188606262207}
diff5_list [0.006389617919921875, 0.006122112274169922, 0.006129026412963867, 0.00707554817199707, 0.007067441940307617]
time3 0
time4 0.06657242774963379
time5 0.0666208267211914
time7 7.152557373046875e-07
gen_weight_change tensor(-24.2695)
policy weight change tensor(38.5264, grad_fn=<SumBackward0>)
time8 0.0065762996673583984
train_time 0.08303356170654297
eval time 0.16760563850402832
epoch last part time 9.5367431640625e-06
2024-01-23 01:01:27,585 MainThread INFO: EPOCH:144
2024-01-23 01:01:27,585 MainThread INFO: Time Consumed:0.25429654121398926s
2024-01-23 01:01:27,585 MainThread INFO: Total Frames:22500s
  1%|▏         | 145/10000 [02:55<48:52,  3.36it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11922.23802
Train_Epoch_Reward                21792.04279
Running_Training_Average_Rewards  13202.20994
Explore_Time                      0.00097
Train___Time                      0.08303
Eval____Time                      0.16761
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12292.70592
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.56828     3.27087     96.64326     87.42740
alpha_0                           0.93012      0.00013     0.93031      0.92993
Alpha_loss                        -0.48734     0.00100     -0.48564     -0.48840
Training/policy_loss              -2.59438     0.00563     -2.58546     -2.60034
Training/qf1_loss                 19178.32773  2282.26419  21390.15039  15064.55469
Training/qf2_loss                 19559.41348  2291.50825  21768.74805  15426.30176
Training/pf_norm                  0.11144      0.01408     0.13579      0.09771
Training/qf1_norm                 1152.49089   36.00622    1197.15784   1088.13489
Training/qf2_norm                 242.50178    8.17329     250.28246    227.14377
log_std/mean                      -0.12644     0.00009     -0.12636     -0.12660
log_probs/mean                    -2.73676     0.00600     -2.72770     -2.74318
mean/mean                         0.00069      0.00006     0.00077      0.00060
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019497394561767578
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71337
epoch first part time 5.4836273193359375e-06
replay_buffer._size: [22800]
collect time 0.0009665489196777344
inside mustsac before update, task 0, sumup 71337
inside mustsac after update, task 0, sumup 71466
inner_dict_sum {'sac_diff0': 0.0002048015594482422, 'sac_diff1': 0.00781393051147461, 'sac_diff2': 0.010334968566894531, 'sac_diff3': 0.011420488357543945, 'sac_diff4': 0.008459329605102539, 'sac_diff5': 0.05132722854614258, 'sac_diff6': 0.0003972053527832031, 'all': 0.08995795249938965}
diff5_list [0.012373685836791992, 0.009828567504882812, 0.009698867797851562, 0.009808540344238281, 0.00961756706237793]
time3 0.0008833408355712891
time4 0.09079432487487793
time5 0.09084606170654297
time7 0.01023101806640625
gen_weight_change tensor(-24.3076)
policy weight change tensor(38.5499, grad_fn=<SumBackward0>)
time8 0.0022592544555664062
train_time 0.12293243408203125
eval time 0.12359428405761719
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:27,858 MainThread INFO: EPOCH:145
2024-01-23 01:01:27,858 MainThread INFO: Time Consumed:0.24990272521972656s
2024-01-23 01:01:27,858 MainThread INFO: Total Frames:22650s
  1%|▏         | 146/10000 [02:55<47:54,  3.43it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11934.41204
Train_Epoch_Reward                12256.76160
Running_Training_Average_Rewards  13412.86247
Explore_Time                      0.00096
Train___Time                      0.12293
Eval____Time                      0.12359
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12248.36595
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.84657     1.47538    93.49404     90.12277
alpha_0                           0.92966      0.00013    0.92984      0.92947
Alpha_loss                        -0.49023     0.00107    -0.48872     -0.49168
Training/policy_loss              -2.58143     0.00730    -2.57490     -2.59461
Training/qf1_loss                 17608.24297  867.35827  18887.91797  16603.53906
Training/qf2_loss                 18002.43359  864.05389  19281.13086  17004.12109
Training/pf_norm                  0.11462      0.02516    0.16116      0.09110
Training/qf1_norm                 1158.56545   47.74431   1224.45251   1075.34705
Training/qf2_norm                 226.90962    2.57880    229.14088    222.87500
log_std/mean                      -0.13082     0.00663    -0.12500     -0.14338
log_probs/mean                    -2.73020     0.00656    -2.72126     -2.74083
mean/mean                         0.00151      0.00088    0.00247      0.00025
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.024341106414794922
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71466
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [22950]
collect time 0.0009341239929199219
inner_dict_sum {'sac_diff0': 0.0001900196075439453, 'sac_diff1': 0.006863117218017578, 'sac_diff2': 0.008357048034667969, 'sac_diff3': 0.010251998901367188, 'sac_diff4': 0.006825447082519531, 'sac_diff5': 0.030898094177246094, 'sac_diff6': 0.0003750324249267578, 'all': 0.06376075744628906}
diff5_list [0.006667613983154297, 0.006255626678466797, 0.006193637847900391, 0.0059545040130615234, 0.005826711654663086]
time3 0
time4 0.06452393531799316
time5 0.0645747184753418
time7 1.1920928955078125e-06
gen_weight_change tensor(-24.3076)
policy weight change tensor(38.6599, grad_fn=<SumBackward0>)
time8 0.0018532276153564453
train_time 0.0759897232055664
eval time 0.18354225158691406
epoch last part time 8.821487426757812e-06
2024-01-23 01:01:28,148 MainThread INFO: EPOCH:146
2024-01-23 01:01:28,149 MainThread INFO: Time Consumed:0.2630879878997803s
2024-01-23 01:01:28,149 MainThread INFO: Total Frames:22800s
  1%|▏         | 147/10000 [02:55<47:47,  3.44it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12026.53044
Train_Epoch_Reward                4701.55446
Running_Training_Average_Rewards  13286.57913
Explore_Time                      0.00093
Train___Time                      0.07599
Eval____Time                      0.18354
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12340.12842
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.84921     1.70804     93.82786     88.86642
alpha_0                           0.92919      0.00013     0.92938      0.92900
Alpha_loss                        -0.49374     0.00091     -0.49277     -0.49526
Training/policy_loss              -2.58275     0.00324     -2.57889     -2.58810
Training/qf1_loss                 17933.55703  1042.46993  18964.28320  16033.71094
Training/qf2_loss                 18338.78320  1048.65712  19369.33203  16422.68359
Training/pf_norm                  0.12261      0.04066     0.19275      0.08002
Training/qf1_norm                 1176.57485   24.18797    1200.33020   1131.44824
Training/qf2_norm                 228.70153    4.05143     233.29594    221.58810
log_std/mean                      -0.12793     0.00016     -0.12774     -0.12820
log_probs/mean                    -2.73202     0.00336     -2.72811     -2.73723
mean/mean                         0.00043      0.00008     0.00049      0.00027
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.023329496383666992
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71466
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [23100]
collect time 0.0009655952453613281
inner_dict_sum {'sac_diff0': 0.0001976490020751953, 'sac_diff1': 0.008418798446655273, 'sac_diff2': 0.008324146270751953, 'sac_diff3': 0.010133028030395508, 'sac_diff4': 0.007101774215698242, 'sac_diff5': 0.03049182891845703, 'sac_diff6': 0.0003726482391357422, 'all': 0.06503987312316895}
diff5_list [0.006647825241088867, 0.005946159362792969, 0.006027936935424805, 0.006030559539794922, 0.005839347839355469]
time3 0
time4 0.06581902503967285
time5 0.06587338447570801
time7 4.76837158203125e-07
gen_weight_change tensor(-24.3076)
policy weight change tensor(38.7201, grad_fn=<SumBackward0>)
time8 0.002006053924560547
train_time 0.07788538932800293
eval time 0.16516375541687012
epoch last part time 1.1682510375976562e-05
2024-01-23 01:01:28,422 MainThread INFO: EPOCH:147
2024-01-23 01:01:28,422 MainThread INFO: Time Consumed:0.24658823013305664s
2024-01-23 01:01:28,422 MainThread INFO: Total Frames:22950s
  1%|▏         | 148/10000 [02:56<46:43,  3.51it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12088.30307
Train_Epoch_Reward                7895.12932
Running_Training_Average_Rewards  13232.41887
Explore_Time                      0.00096
Train___Time                      0.07789
Eval____Time                      0.16516
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12191.93176
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.88335     2.08345     94.20879     88.29913
alpha_0                           0.92873      0.00013     0.92891      0.92854
Alpha_loss                        -0.49716     0.00123     -0.49588     -0.49917
Training/policy_loss              -2.58099     0.00554     -2.57369     -2.58881
Training/qf1_loss                 17887.69844  1291.58553  19936.07031  16507.54688
Training/qf2_loss                 18307.46797  1291.61031  20358.89453  16929.43945
Training/pf_norm                  0.12084      0.02741     0.17116      0.09374
Training/qf1_norm                 1176.78218   30.94605    1226.10461   1133.14783
Training/qf2_norm                 227.47139    5.04560     235.53430    221.13638
log_std/mean                      -0.14470     0.00008     -0.14455     -0.14478
log_probs/mean                    -2.73269     0.00613     -2.72457     -2.74167
mean/mean                         0.00201      0.00010     0.00212      0.00186
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01930522918701172
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71466
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [23250]
collect time 0.0010271072387695312
inner_dict_sum {'sac_diff0': 0.0002002716064453125, 'sac_diff1': 0.006889820098876953, 'sac_diff2': 0.007965564727783203, 'sac_diff3': 0.010215997695922852, 'sac_diff4': 0.007028102874755859, 'sac_diff5': 0.03158926963806152, 'sac_diff6': 0.00038361549377441406, 'all': 0.06427264213562012}
diff5_list [0.0066950321197509766, 0.006278514862060547, 0.006227016448974609, 0.006239175796508789, 0.0061495304107666016]
time3 0
time4 0.06502938270568848
time5 0.06508016586303711
time7 4.76837158203125e-07
gen_weight_change tensor(-24.3076)
policy weight change tensor(38.7946, grad_fn=<SumBackward0>)
time8 0.0019664764404296875
train_time 0.07715439796447754
eval time 0.16436004638671875
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:28,690 MainThread INFO: EPOCH:148
2024-01-23 01:01:28,690 MainThread INFO: Time Consumed:0.24482512474060059s
2024-01-23 01:01:28,690 MainThread INFO: Total Frames:23100s
  1%|▏         | 149/10000 [02:56<46:04,  3.56it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12131.87577
Train_Epoch_Reward                5897.34182
Running_Training_Average_Rewards  13026.08512
Explore_Time                      0.00100
Train___Time                      0.07715
Eval____Time                      0.16436
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12074.70732
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.72770     1.21318     93.09621     89.74870
alpha_0                           0.92826      0.00013     0.92845      0.92807
Alpha_loss                        -0.50060     0.00124     -0.49910     -0.50241
Training/policy_loss              -2.58319     0.00706     -2.57821     -2.59710
Training/qf1_loss                 17863.97891  1187.48679  19801.43359  16195.70117
Training/qf2_loss                 18232.83047  1189.30289  20171.79688  16561.98242
Training/pf_norm                  0.12855      0.02731     0.16247      0.08595
Training/qf1_norm                 1050.82273   10.87222    1067.77954   1038.68665
Training/qf2_norm                 230.43665    2.90454     233.74915    225.74141
log_std/mean                      -0.11961     0.00017     -0.11942     -0.11987
log_probs/mean                    -2.73367     0.00771     -2.72882     -2.74895
mean/mean                         0.00091      0.00005     0.00097      0.00084
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.022800922393798828
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71466
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [23400]
collect time 0.0008993148803710938
inner_dict_sum {'sac_diff0': 0.0001914501190185547, 'sac_diff1': 0.006566762924194336, 'sac_diff2': 0.008215188980102539, 'sac_diff3': 0.009747028350830078, 'sac_diff4': 0.006767988204956055, 'sac_diff5': 0.03136920928955078, 'sac_diff6': 0.0003752708435058594, 'all': 0.0632328987121582}
diff5_list [0.006742715835571289, 0.006224393844604492, 0.006132364273071289, 0.006114959716796875, 0.006154775619506836]
time3 0
time4 0.06397867202758789
time5 0.06402969360351562
time7 4.76837158203125e-07
gen_weight_change tensor(-24.3076)
policy weight change tensor(38.8357, grad_fn=<SumBackward0>)
time8 0.0018930435180664062
train_time 0.07541298866271973
eval time 0.16845202445983887
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:28,963 MainThread INFO: EPOCH:149
2024-01-23 01:01:28,964 MainThread INFO: Time Consumed:0.24738717079162598s
2024-01-23 01:01:28,964 MainThread INFO: Total Frames:23250s
  2%|▏         | 150/10000 [02:56<45:41,  3.59it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12166.67022
Train_Epoch_Reward                28655.41440
Running_Training_Average_Rewards  13334.63470
Explore_Time                      0.00089
Train___Time                      0.07541
Eval____Time                      0.16845
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12012.49675
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.83016     1.69314     93.92847     89.05379
alpha_0                           0.92780      0.00013     0.92798      0.92761
Alpha_loss                        -0.50403     0.00098     -0.50225     -0.50499
Training/policy_loss              -2.58239     0.00613     -2.57552     -2.59256
Training/qf1_loss                 16507.16426  1165.72913  18409.73633  14816.37012
Training/qf2_loss                 16959.95586  1174.00708  18876.96875  15255.10156
Training/pf_norm                  0.11278      0.02977     0.14859      0.07411
Training/qf1_norm                 1282.18811   26.04907    1323.95642   1246.87280
Training/qf2_norm                 225.53567    4.06933     232.95526    221.15065
log_std/mean                      -0.13603     0.00002     -0.13600     -0.13607
log_probs/mean                    -2.73450     0.00653     -2.72763     -2.74520
mean/mean                         0.00115      0.00014     0.00136      0.00096
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.022200345993041992
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71466
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [23550]
collect time 0.0008885860443115234
inside mustsac before update, task 0, sumup 71466
inside mustsac after update, task 0, sumup 71075
inner_dict_sum {'sac_diff0': 0.00020051002502441406, 'sac_diff1': 0.0068852901458740234, 'sac_diff2': 0.00859689712524414, 'sac_diff3': 0.010367631912231445, 'sac_diff4': 0.007705211639404297, 'sac_diff5': 0.05103802680969238, 'sac_diff6': 0.0004105567932128906, 'all': 0.0852041244506836}
diff5_list [0.011077165603637695, 0.009958982467651367, 0.009952306747436523, 0.010403156280517578, 0.009646415710449219]
time3 0.0008740425109863281
time4 0.0861063003540039
time5 0.08616518974304199
time7 0.009301424026489258
gen_weight_change tensor(-24.1379)
policy weight change tensor(38.7930, grad_fn=<SumBackward0>)
time8 0.002655506134033203
train_time 0.1167762279510498
eval time 0.11371636390686035
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:29,222 MainThread INFO: EPOCH:150
2024-01-23 01:01:29,223 MainThread INFO: Time Consumed:0.2335803508758545s
2024-01-23 01:01:29,223 MainThread INFO: Total Frames:23400s
  2%|▏         | 151/10000 [02:56<44:49,  3.66it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12195.79826
Train_Epoch_Reward                14167.71564
Running_Training_Average_Rewards  13587.44192
Explore_Time                      0.00088
Train___Time                      0.11678
Eval____Time                      0.11372
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12045.85801
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.15120     3.55885     95.13499     84.66531
alpha_0                           0.92733      0.00013     0.92752      0.92715
Alpha_loss                        -0.50737     0.00115     -0.50570     -0.50859
Training/policy_loss              -2.58387     0.00601     -2.57768     -2.59398
Training/qf1_loss                 16513.12344  1415.30281  18306.99609  14263.20312
Training/qf2_loss                 16950.19414  1402.32720  18695.21484  14710.17188
Training/pf_norm                  0.11946      0.01224     0.13821      0.10247
Training/qf1_norm                 1263.17087   78.14801    1382.06262   1136.24219
Training/qf2_norm                 230.68040    6.02346     239.16209    220.56635
log_std/mean                      -0.13393     0.00323     -0.12999     -0.13835
log_probs/mean                    -2.73409     0.00542     -2.72854     -2.74361
mean/mean                         0.00120      0.00079     0.00231      0.00003
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021289348602294922
epoch last part time3 0.0025806427001953125
inside rlalgo, task 0, sumup 71075
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [23700]
collect time 0.0009729862213134766
inner_dict_sum {'sac_diff0': 0.00020241737365722656, 'sac_diff1': 0.011717796325683594, 'sac_diff2': 0.010708332061767578, 'sac_diff3': 0.011376142501831055, 'sac_diff4': 0.007930994033813477, 'sac_diff5': 0.03337454795837402, 'sac_diff6': 0.0003914833068847656, 'all': 0.07570171356201172}
diff5_list [0.00809478759765625, 0.006371974945068359, 0.006341695785522461, 0.006360530853271484, 0.006205558776855469]
time3 0
time4 0.07649540901184082
time5 0.07656383514404297
time7 4.76837158203125e-07
gen_weight_change tensor(-24.1379)
policy weight change tensor(38.7883, grad_fn=<SumBackward0>)
time8 0.0023550987243652344
train_time 0.08933305740356445
eval time 0.14173197746276855
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:29,484 MainThread INFO: EPOCH:151
2024-01-23 01:01:29,485 MainThread INFO: Time Consumed:0.23467373847961426s
2024-01-23 01:01:29,485 MainThread INFO: Total Frames:23550s
  2%|▏         | 152/10000 [02:57<44:10,  3.72it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12231.78074
Train_Epoch_Reward                1513.31775
Running_Training_Average_Rewards  13271.18108
Explore_Time                      0.00097
Train___Time                      0.08933
Eval____Time                      0.14173
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12584.08283
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.75441     1.34782    91.31664     87.61031
alpha_0                           0.92687      0.00013    0.92705      0.92668
Alpha_loss                        -0.51071     0.00073    -0.50992     -0.51169
Training/policy_loss              -2.57802     0.01007    -2.56246     -2.59029
Training/qf1_loss                 15967.84141  522.89622  16826.91797  15358.88477
Training/qf2_loss                 16446.54063  522.09132  17301.21094  15843.94043
Training/pf_norm                  0.15172      0.01266    0.16110      0.12805
Training/qf1_norm                 1329.10779   18.25701   1351.01147   1304.40845
Training/qf2_norm                 219.25829    3.22237    222.96420    214.15811
log_std/mean                      -0.13919     0.00005    -0.13912     -0.13924
log_probs/mean                    -2.73367     0.01060    -2.71716     -2.74635
mean/mean                         0.00230      0.00003    0.00234      0.00225
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.022405147552490234
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71075
epoch first part time 5.245208740234375e-06
replay_buffer._size: [23850]
collect time 0.0009391307830810547
inner_dict_sum {'sac_diff0': 0.0002009868621826172, 'sac_diff1': 0.006844043731689453, 'sac_diff2': 0.008216619491577148, 'sac_diff3': 0.010155439376831055, 'sac_diff4': 0.007020235061645508, 'sac_diff5': 0.03158426284790039, 'sac_diff6': 0.0003838539123535156, 'all': 0.06440544128417969}
diff5_list [0.006700992584228516, 0.006366729736328125, 0.006097316741943359, 0.006253957748413086, 0.006165266036987305]
time3 0
time4 0.06516647338867188
time5 0.06521773338317871
time7 7.152557373046875e-07
gen_weight_change tensor(-24.1379)
policy weight change tensor(38.7540, grad_fn=<SumBackward0>)
time8 0.0019741058349609375
train_time 0.07726597785949707
eval time 0.15130186080932617
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:29,742 MainThread INFO: EPOCH:152
2024-01-23 01:01:29,742 MainThread INFO: Time Consumed:0.23183584213256836s
2024-01-23 01:01:29,742 MainThread INFO: Total Frames:23700s
  2%|▏         | 153/10000 [02:57<43:33,  3.77it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12266.28897
Train_Epoch_Reward                1840.45043
Running_Training_Average_Rewards  12958.49317
Explore_Time                      0.00093
Train___Time                      0.07727
Eval____Time                      0.15130
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12586.67461
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.05622     2.81373     94.26830     86.09090
alpha_0                           0.92640      0.00013     0.92659      0.92622
Alpha_loss                        -0.51403     0.00108     -0.51273     -0.51536
Training/policy_loss              -2.58169     0.00501     -2.57389     -2.58939
Training/qf1_loss                 16887.50078  2325.99831  19424.85156  13782.55273
Training/qf2_loss                 17329.54609  2337.25145  19882.69141  14201.66406
Training/pf_norm                  0.13125      0.02233     0.16756      0.10648
Training/qf1_norm                 1276.07014   41.05300    1335.71924   1211.82605
Training/qf2_norm                 231.39305    6.99850     241.93124    221.53532
log_std/mean                      -0.13239     0.00009     -0.13226     -0.13253
log_probs/mean                    -2.73302     0.00543     -2.72460     -2.74160
mean/mean                         0.00146      0.00004     0.00151      0.00138
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0211336612701416
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71075
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [24000]
collect time 0.0009136199951171875
inner_dict_sum {'sac_diff0': 0.0001983642578125, 'sac_diff1': 0.006633281707763672, 'sac_diff2': 0.007977962493896484, 'sac_diff3': 0.009911298751831055, 'sac_diff4': 0.0067596435546875, 'sac_diff5': 0.030426025390625, 'sac_diff6': 0.00037980079650878906, 'all': 0.062286376953125}
diff5_list [0.0064809322357177734, 0.006113290786743164, 0.005928754806518555, 0.00587773323059082, 0.0060253143310546875]
time3 0
time4 0.06301999092102051
time5 0.06307196617126465
time7 7.152557373046875e-07
gen_weight_change tensor(-24.1379)
policy weight change tensor(38.7483, grad_fn=<SumBackward0>)
time8 0.0019845962524414062
train_time 0.07448458671569824
eval time 0.1549365520477295
epoch last part time 5.4836273193359375e-06
2024-01-23 01:01:29,999 MainThread INFO: EPOCH:153
2024-01-23 01:01:29,999 MainThread INFO: Time Consumed:0.2327122688293457s
2024-01-23 01:01:29,999 MainThread INFO: Total Frames:23850s
  2%|▏         | 154/10000 [02:57<43:09,  3.80it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12296.11480
Train_Epoch_Reward                11855.15216
Running_Training_Average_Rewards  12709.85099
Explore_Time                      0.00091
Train___Time                      0.07448
Eval____Time                      0.15494
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12584.19646
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.48508     3.92156     98.01218     88.28472
alpha_0                           0.92594      0.00013     0.92613      0.92575
Alpha_loss                        -0.51761     0.00109     -0.51621     -0.51952
Training/policy_loss              -2.58289     0.00453     -2.57575     -2.58924
Training/qf1_loss                 19225.46055  1882.23922  21479.02148  16529.49805
Training/qf2_loss                 19706.82461  1901.69666  21987.12305  16986.15234
Training/pf_norm                  0.14139      0.00742     0.15572      0.13453
Training/qf1_norm                 1331.19033   55.33230    1403.06702   1264.84412
Training/qf2_norm                 239.78727    9.68182     251.00201    226.93182
log_std/mean                      -0.12294     0.00004     -0.12289     -0.12300
log_probs/mean                    -2.73569     0.00493     -2.72826     -2.74308
mean/mean                         0.00175      0.00005     0.00182      0.00167
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02155327796936035
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71075
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [24150]
collect time 0.0009214878082275391
inner_dict_sum {'sac_diff0': 0.00019741058349609375, 'sac_diff1': 0.0068874359130859375, 'sac_diff2': 0.008328676223754883, 'sac_diff3': 0.010078907012939453, 'sac_diff4': 0.007016658782958984, 'sac_diff5': 0.03134608268737793, 'sac_diff6': 0.00039958953857421875, 'all': 0.0642547607421875}
diff5_list [0.0066165924072265625, 0.006253957748413086, 0.006268501281738281, 0.006201982498168945, 0.006005048751831055]
time3 0
time4 0.06501913070678711
time5 0.06507110595703125
time7 7.152557373046875e-07
gen_weight_change tensor(-24.1379)
policy weight change tensor(38.7662, grad_fn=<SumBackward0>)
time8 0.0019686222076416016
train_time 0.07664608955383301
eval time 0.15491151809692383
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:30,258 MainThread INFO: EPOCH:154
2024-01-23 01:01:30,259 MainThread INFO: Time Consumed:0.23481464385986328s
2024-01-23 01:01:30,259 MainThread INFO: Total Frames:24000s
  2%|▏         | 155/10000 [02:57<42:59,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12322.05092
Train_Epoch_Reward                4949.27754
Running_Training_Average_Rewards  12601.65480
Explore_Time                      0.00092
Train___Time                      0.07665
Eval____Time                      0.15491
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12552.06708
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.37164     2.83050     93.12114     84.66361
alpha_0                           0.92548      0.00013     0.92566      0.92529
Alpha_loss                        -0.52025     0.00117     -0.51859     -0.52159
Training/policy_loss              -2.56435     0.00331     -2.56102     -2.56978
Training/qf1_loss                 16426.06758  1384.88544  17809.12891  13854.03906
Training/qf2_loss                 16830.96211  1398.23186  18224.37305  14237.61719
Training/pf_norm                  0.12616      0.02427     0.15673      0.08720
Training/qf1_norm                 1145.57031   41.75770    1206.98706   1084.83740
Training/qf2_norm                 212.04470    6.48512     220.67590    201.31084
log_std/mean                      -0.13643     0.00001     -0.13641     -0.13646
log_probs/mean                    -2.72630     0.00383     -2.72218     -2.73244
mean/mean                         0.00216      0.00003     0.00220      0.00213
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021463632583618164
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71075
epoch first part time 3.337860107421875e-06
replay_buffer._size: [24300]
collect time 0.0008416175842285156
inside mustsac before update, task 0, sumup 71075
inside mustsac after update, task 0, sumup 70340
inner_dict_sum {'sac_diff0': 0.000213623046875, 'sac_diff1': 0.007090091705322266, 'sac_diff2': 0.008425235748291016, 'sac_diff3': 0.010563850402832031, 'sac_diff4': 0.0073850154876708984, 'sac_diff5': 0.0522456169128418, 'sac_diff6': 0.00045180320739746094, 'all': 0.08637523651123047}
diff5_list [0.01154184341430664, 0.009565353393554688, 0.011907339096069336, 0.009666204452514648, 0.009564876556396484]
time3 0.0008757114410400391
time4 0.08729267120361328
time5 0.08738088607788086
time7 0.009826898574829102
gen_weight_change tensor(-23.8936)
policy weight change tensor(38.6950, grad_fn=<SumBackward0>)
time8 0.0020034313201904297
train_time 0.11818671226501465
eval time 0.11282086372375488
epoch last part time 4.291534423828125e-06
2024-01-23 01:01:30,517 MainThread INFO: EPOCH:155
2024-01-23 01:01:30,518 MainThread INFO: Time Consumed:0.23409581184387207s
2024-01-23 01:01:30,518 MainThread INFO: Total Frames:24150s
  2%|▏         | 156/10000 [02:58<42:53,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12357.41726
Train_Epoch_Reward                10556.72729
Running_Training_Average_Rewards  12253.06046
Explore_Time                      0.00084
Train___Time                      0.11819
Eval____Time                      0.11282
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12602.02934
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.59392     2.10134     93.76186     87.32056
alpha_0                           0.92501      0.00013     0.92520      0.92483
Alpha_loss                        -0.52399     0.00080     -0.52312     -0.52513
Training/policy_loss              -2.57481     0.00302     -2.57124     -2.57870
Training/qf1_loss                 17617.19434  1923.48445  21084.41797  15982.58887
Training/qf2_loss                 18093.31445  1924.20724  21564.54883  16473.03516
Training/pf_norm                  0.13981      0.01999     0.16349      0.10642
Training/qf1_norm                 1303.26379   30.56891    1338.94165   1251.04895
Training/qf2_norm                 226.58179    4.39654     230.69498    219.82220
log_std/mean                      -0.14046     0.00641     -0.13170     -0.15022
log_probs/mean                    -2.73106     0.00319     -2.72836     -2.73721
mean/mean                         0.00129      0.00074     0.00275      0.00070
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.022538423538208008
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70340
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [24450]
collect time 0.000823974609375
inner_dict_sum {'sac_diff0': 0.00018835067749023438, 'sac_diff1': 0.006670475006103516, 'sac_diff2': 0.008099079132080078, 'sac_diff3': 0.009915828704833984, 'sac_diff4': 0.007046222686767578, 'sac_diff5': 0.03146243095397949, 'sac_diff6': 0.0003883838653564453, 'all': 0.06377077102661133}
diff5_list [0.006706953048706055, 0.0061092376708984375, 0.00611114501953125, 0.0061876773834228516, 0.0063474178314208984]
time3 0
time4 0.06451892852783203
time5 0.06456565856933594
time7 7.152557373046875e-07
gen_weight_change tensor(-23.8936)
policy weight change tensor(38.7766, grad_fn=<SumBackward0>)
time8 0.0019767284393310547
train_time 0.07603979110717773
eval time 0.15621352195739746
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:30,779 MainThread INFO: EPOCH:156
2024-01-23 01:01:30,779 MainThread INFO: Time Consumed:0.23537349700927734s
2024-01-23 01:01:30,779 MainThread INFO: Total Frames:24300s
  2%|▏         | 157/10000 [02:58<42:51,  3.83it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12331.30093
Train_Epoch_Reward                2604.05526
Running_Training_Average_Rewards  11373.02104
Explore_Time                      0.00082
Train___Time                      0.07604
Eval____Time                      0.15621
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12078.96515
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.88967     1.69694    90.59090     85.55782
alpha_0                           0.92455      0.00013    0.92474      0.92437
Alpha_loss                        -0.52735     0.00111    -0.52573     -0.52868
Training/policy_loss              -2.57811     0.00280    -2.57526     -2.58243
Training/qf1_loss                 17010.39180  968.13807  18704.65430  15866.01367
Training/qf2_loss                 17493.08066  975.29123  19195.95898  16338.86230
Training/pf_norm                  0.15974      0.01803    0.18473      0.12909
Training/qf1_norm                 1340.81279   24.27167   1370.42749   1310.37891
Training/qf2_norm                 237.10966    4.36476    243.96315    231.06902
log_std/mean                      -0.12331     0.00017    -0.12313     -0.12357
log_probs/mean                    -2.73098     0.00321    -2.72747     -2.73561
mean/mean                         0.00142      0.00005    0.00149      0.00136
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02226400375366211
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70340
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [24600]
collect time 0.0007774829864501953
inner_dict_sum {'sac_diff0': 0.00019168853759765625, 'sac_diff1': 0.00635075569152832, 'sac_diff2': 0.008001327514648438, 'sac_diff3': 0.009878396987915039, 'sac_diff4': 0.006914377212524414, 'sac_diff5': 0.03120732307434082, 'sac_diff6': 0.0003745555877685547, 'all': 0.06291842460632324}
diff5_list [0.006584882736206055, 0.006170034408569336, 0.006040334701538086, 0.0062181949615478516, 0.006193876266479492]
time3 0
time4 0.0636599063873291
time5 0.06370401382446289
time7 7.152557373046875e-07
gen_weight_change tensor(-23.8936)
policy weight change tensor(38.7842, grad_fn=<SumBackward0>)
time8 0.0019342899322509766
train_time 0.07514619827270508
eval time 0.1555490493774414
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:31,038 MainThread INFO: EPOCH:157
2024-01-23 01:01:31,038 MainThread INFO: Time Consumed:0.23378419876098633s
2024-01-23 01:01:31,038 MainThread INFO: Total Frames:24450s
  2%|▏         | 158/10000 [02:58<42:48,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12332.67908
Train_Epoch_Reward                3418.32357
Running_Training_Average_Rewards  11159.82498
Explore_Time                      0.00077
Train___Time                      0.07515
Eval____Time                      0.15555
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12205.71322
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.23393     2.05977     90.55873     85.69124
alpha_0                           0.92409      0.00013     0.92427      0.92390
Alpha_loss                        -0.53102     0.00130     -0.52905     -0.53289
Training/policy_loss              -2.57755     0.00528     -2.57073     -2.58320
Training/qf1_loss                 16849.24883  1418.91402  19412.16602  15319.22461
Training/qf2_loss                 17302.57949  1424.76706  19871.89453  15766.22168
Training/pf_norm                  0.13378      0.02477     0.17237      0.10085
Training/qf1_norm                 1246.37354   25.27617    1279.09705   1213.38501
Training/qf2_norm                 227.08557    5.03477     232.65967    220.83946
log_std/mean                      -0.14268     0.00010     -0.14249     -0.14276
log_probs/mean                    -2.73480     0.00594     -2.72687     -2.74146
mean/mean                         0.00147      0.00010     0.00162      0.00134
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02328038215637207
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70340
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [24750]
collect time 0.0008347034454345703
inner_dict_sum {'sac_diff0': 0.00019431114196777344, 'sac_diff1': 0.006736278533935547, 'sac_diff2': 0.008235692977905273, 'sac_diff3': 0.010095596313476562, 'sac_diff4': 0.006853342056274414, 'sac_diff5': 0.03080892562866211, 'sac_diff6': 0.0003757476806640625, 'all': 0.06329989433288574}
diff5_list [0.006563663482666016, 0.006072998046875, 0.006066322326660156, 0.005965232849121094, 0.006140708923339844]
time3 0
time4 0.06406092643737793
time5 0.06410861015319824
time7 7.152557373046875e-07
gen_weight_change tensor(-23.8936)
policy weight change tensor(38.6909, grad_fn=<SumBackward0>)
time8 0.0020589828491210938
train_time 0.07574963569641113
eval time 0.1556541919708252
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:31,299 MainThread INFO: EPOCH:158
2024-01-23 01:01:31,299 MainThread INFO: Time Consumed:0.23464488983154297s
2024-01-23 01:01:31,300 MainThread INFO: Total Frames:24600s
  2%|▏         | 159/10000 [02:58<42:52,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12361.32862
Train_Epoch_Reward                7466.69513
Running_Training_Average_Rewards  11215.56702
Explore_Time                      0.00083
Train___Time                      0.07575
Eval____Time                      0.15565
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12361.20276
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.95831     1.82598     92.89455     87.39552
alpha_0                           0.92363      0.00013     0.92381      0.92344
Alpha_loss                        -0.53399     0.00104     -0.53279     -0.53525
Training/policy_loss              -2.57658     0.00320     -2.57125     -2.58046
Training/qf1_loss                 17262.06328  1556.52773  19930.93359  15734.65625
Training/qf2_loss                 17820.36875  1565.44410  20506.16602  16273.53711
Training/pf_norm                  0.11882      0.02305     0.14684      0.08854
Training/qf1_norm                 1510.37729   33.88302    1557.53101   1459.90015
Training/qf2_norm                 240.56295    4.67280     248.05734    233.96335
log_std/mean                      -0.13681     0.00020     -0.13650     -0.13706
log_probs/mean                    -2.72979     0.00351     -2.72375     -2.73426
mean/mean                         0.00139      0.00002     0.00143      0.00137
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02426910400390625
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70340
epoch first part time 2.86102294921875e-06
replay_buffer._size: [24900]
collect time 0.0007991790771484375
inner_dict_sum {'sac_diff0': 0.00019812583923339844, 'sac_diff1': 0.00673675537109375, 'sac_diff2': 0.008282661437988281, 'sac_diff3': 0.009947538375854492, 'sac_diff4': 0.006958484649658203, 'sac_diff5': 0.03077554702758789, 'sac_diff6': 0.0003745555877685547, 'all': 0.06327366828918457}
diff5_list [0.006681203842163086, 0.006029605865478516, 0.006099224090576172, 0.0059549808502197266, 0.006010532379150391]
time3 0
time4 0.06402873992919922
time5 0.06407642364501953
time7 9.5367431640625e-07
gen_weight_change tensor(-23.8936)
policy weight change tensor(38.5243, grad_fn=<SumBackward0>)
time8 0.0019490718841552734
train_time 0.07567453384399414
eval time 0.14969682693481445
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:31,555 MainThread INFO: EPOCH:159
2024-01-23 01:01:31,556 MainThread INFO: Time Consumed:0.22846603393554688s
2024-01-23 01:01:31,556 MainThread INFO: Total Frames:24750s
  2%|▏         | 160/10000 [02:59<42:30,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12409.56027
Train_Epoch_Reward                5332.94817
Running_Training_Average_Rewards  10976.70767
Explore_Time                      0.00079
Train___Time                      0.07567
Eval____Time                      0.14970
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12494.81319
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.63616     2.46150     92.08663     84.75787
alpha_0                           0.92316      0.00013     0.92335      0.92298
Alpha_loss                        -0.53805     0.00129     -0.53644     -0.53958
Training/policy_loss              -2.57920     0.00512     -2.57432     -2.58879
Training/qf1_loss                 16453.64531  1503.47660  17850.96875  13527.22852
Training/qf2_loss                 16979.92383  1514.36978  18393.84961  14033.67773
Training/pf_norm                  0.13052      0.01899     0.15417      0.10309
Training/qf1_norm                 1404.17253   36.56044    1452.23694   1347.64526
Training/qf2_norm                 226.22704    6.01703     234.66684    216.74702
log_std/mean                      -0.13677     0.00030     -0.13631     -0.13715
log_probs/mean                    -2.73848     0.00580     -2.73291     -2.74917
mean/mean                         0.00202      0.00006     0.00208      0.00192
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.022242307662963867
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70340
epoch first part time 3.337860107421875e-06
replay_buffer._size: [25050]
collect time 0.0008363723754882812
inside mustsac before update, task 0, sumup 70340
inside mustsac after update, task 0, sumup 70867
inner_dict_sum {'sac_diff0': 0.00019741058349609375, 'sac_diff1': 0.0068509578704833984, 'sac_diff2': 0.008512735366821289, 'sac_diff3': 0.010619163513183594, 'sac_diff4': 0.0076215267181396484, 'sac_diff5': 0.050524234771728516, 'sac_diff6': 0.0004146099090576172, 'all': 0.08474063873291016}
diff5_list [0.010738611221313477, 0.01004791259765625, 0.010016441345214844, 0.009927749633789062, 0.009793519973754883]
time3 0.0008826255798339844
time4 0.08562207221984863
time5 0.08567571640014648
time7 0.00920248031616211
gen_weight_change tensor(-23.6637)
policy weight change tensor(38.4162, grad_fn=<SumBackward0>)
time8 0.0032694339752197266
train_time 0.11747384071350098
eval time 0.11419367790222168
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:31,816 MainThread INFO: EPOCH:160
2024-01-23 01:01:31,816 MainThread INFO: Time Consumed:0.2347095012664795s
2024-01-23 01:01:31,816 MainThread INFO: Total Frames:24900s
  2%|▏         | 161/10000 [02:59<42:30,  3.86it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12462.73610
Train_Epoch_Reward                20549.27724
Running_Training_Average_Rewards  11373.27859
Explore_Time                      0.00083
Train___Time                      0.11747
Eval____Time                      0.11419
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12577.61634
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.71085     1.72465    90.90395     86.09947
alpha_0                           0.92270      0.00013    0.92289      0.92252
Alpha_loss                        -0.54110     0.00071    -0.54037     -0.54224
Training/policy_loss              -2.57425     0.00302    -2.57083     -2.57977
Training/qf1_loss                 15377.95762  701.08451  16685.63672  14682.47363
Training/qf2_loss                 15881.78203  695.00130  17182.96680  15222.11523
Training/pf_norm                  0.15018      0.02318    0.17174      0.10677
Training/qf1_norm                 1325.31665   70.04688   1381.95276   1195.22229
Training/qf2_norm                 222.23235    5.91028    233.00986    216.15370
log_std/mean                      -0.13465     0.00598    -0.12455     -0.14034
log_probs/mean                    -2.73438     0.00399    -2.73162     -2.74208
mean/mean                         0.00132      0.00082    0.00237      0.00014
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01813817024230957
epoch last part time3 0.0027692317962646484
inside rlalgo, task 0, sumup 70867
epoch first part time 3.337860107421875e-06
replay_buffer._size: [25200]
collect time 0.0009071826934814453
inner_dict_sum {'sac_diff0': 0.000194549560546875, 'sac_diff1': 0.007115364074707031, 'sac_diff2': 0.008603334426879883, 'sac_diff3': 0.010732889175415039, 'sac_diff4': 0.0069882869720458984, 'sac_diff5': 0.031882286071777344, 'sac_diff6': 0.0003783702850341797, 'all': 0.06589508056640625}
diff5_list [0.006737947463989258, 0.006338834762573242, 0.0063931941986083984, 0.006287574768066406, 0.006124734878540039]
time3 0
time4 0.06663012504577637
time5 0.06667494773864746
time7 4.76837158203125e-07
gen_weight_change tensor(-23.6637)
policy weight change tensor(38.3385, grad_fn=<SumBackward0>)
time8 0.0019593238830566406
train_time 0.07831954956054688
eval time 0.1528334617614746
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:32,074 MainThread INFO: EPOCH:161
2024-01-23 01:01:32,075 MainThread INFO: Time Consumed:0.23436236381530762s
2024-01-23 01:01:32,075 MainThread INFO: Total Frames:25050s
  2%|▏         | 162/10000 [02:59<42:19,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12462.85942
Train_Epoch_Reward                8650.61917
Running_Training_Average_Rewards  11293.75331
Explore_Time                      0.00090
Train___Time                      0.07832
Eval____Time                      0.15283
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12585.31606
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.98358     2.32244     93.43402     87.07013
alpha_0                           0.92224      0.00013     0.92243      0.92206
Alpha_loss                        -0.54412     0.00101     -0.54242     -0.54550
Training/policy_loss              -2.56876     0.00776     -2.55809     -2.58197
Training/qf1_loss                 17571.04648  1481.30297  19233.51172  14784.66406
Training/qf2_loss                 18101.01680  1494.22918  19779.64453  15291.34570
Training/pf_norm                  0.13143      0.05122     0.22319      0.06594
Training/qf1_norm                 1422.33013   36.66453    1460.46545   1359.18347
Training/qf2_norm                 225.61754    5.57788     231.46059    216.22754
log_std/mean                      -0.12342     0.00008     -0.12331     -0.12353
log_probs/mean                    -2.73008     0.00831     -2.71882     -2.74416
mean/mean                         0.00152      0.00010     0.00164      0.00137
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018068790435791016
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70867
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [25350]
collect time 0.0008466243743896484
inner_dict_sum {'sac_diff0': 0.00018978118896484375, 'sac_diff1': 0.006580829620361328, 'sac_diff2': 0.007780790328979492, 'sac_diff3': 0.00986170768737793, 'sac_diff4': 0.006571531295776367, 'sac_diff5': 0.03079056739807129, 'sac_diff6': 0.0003719329833984375, 'all': 0.06214714050292969}
diff5_list [0.006460666656494141, 0.005938291549682617, 0.006090879440307617, 0.006333827972412109, 0.005966901779174805]
time3 0
time4 0.06287550926208496
time5 0.06291794776916504
time7 4.76837158203125e-07
gen_weight_change tensor(-23.6637)
policy weight change tensor(38.3201, grad_fn=<SumBackward0>)
time8 0.0018877983093261719
train_time 0.07381129264831543
eval time 0.15877723693847656
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:32,332 MainThread INFO: EPOCH:162
2024-01-23 01:01:32,332 MainThread INFO: Time Consumed:0.23570799827575684s
2024-01-23 01:01:32,332 MainThread INFO: Total Frames:25200s
  2%|▏         | 163/10000 [02:59<42:17,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12464.64485
Train_Epoch_Reward                8686.84528
Running_Training_Average_Rewards  11068.57561
Explore_Time                      0.00084
Train___Time                      0.07381
Eval____Time                      0.15878
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12604.52891
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.44017     2.70735    93.45025     86.25381
alpha_0                           0.92178      0.00013    0.92196      0.92159
Alpha_loss                        -0.54765     0.00094    -0.54630     -0.54922
Training/policy_loss              -2.57044     0.00365    -2.56358     -2.57435
Training/qf1_loss                 17552.35234  770.67770  18506.14062  16249.75586
Training/qf2_loss                 18122.75664  782.19322  19074.88477  16804.65234
Training/pf_norm                  0.12330      0.02987    0.17350      0.08676
Training/qf1_norm                 1533.41282   54.79166   1599.01355   1453.83691
Training/qf2_norm                 232.05406    6.67017    239.44301    221.79088
log_std/mean                      -0.13575     0.00007    -0.13567     -0.13583
log_probs/mean                    -2.73202     0.00387    -2.72485     -2.73599
mean/mean                         0.00213      0.00009    0.00227      0.00203
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01854085922241211
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70867
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [25500]
collect time 0.0008068084716796875
inner_dict_sum {'sac_diff0': 0.00022220611572265625, 'sac_diff1': 0.0065233707427978516, 'sac_diff2': 0.0075719356536865234, 'sac_diff3': 0.009735345840454102, 'sac_diff4': 0.006724119186401367, 'sac_diff5': 0.031167268753051758, 'sac_diff6': 0.0003883838653564453, 'all': 0.0623326301574707}
diff5_list [0.006338834762573242, 0.006336212158203125, 0.006368875503540039, 0.006079673767089844, 0.006043672561645508]
time3 0
time4 0.06306767463684082
time5 0.0631113052368164
time7 4.76837158203125e-07
gen_weight_change tensor(-23.6637)
policy weight change tensor(38.2767, grad_fn=<SumBackward0>)
time8 0.0018243789672851562
train_time 0.07366228103637695
eval time 0.16034317016601562
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:32,591 MainThread INFO: EPOCH:163
2024-01-23 01:01:32,591 MainThread INFO: Time Consumed:0.2371079921722412s
2024-01-23 01:01:32,591 MainThread INFO: Total Frames:25350s
  2%|▏         | 164/10000 [03:00<42:19,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12472.26585
Train_Epoch_Reward                36464.55776
Running_Training_Average_Rewards  11962.81812
Explore_Time                      0.00080
Train___Time                      0.07366
Eval____Time                      0.16034
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12660.40648
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.50409     2.09764     93.30886     87.78604
alpha_0                           0.92132      0.00013     0.92150      0.92113
Alpha_loss                        -0.55140     0.00104     -0.55033     -0.55284
Training/policy_loss              -2.57143     0.00578     -2.56286     -2.57971
Training/qf1_loss                 16662.59805  1456.60202  19469.97070  15470.38281
Training/qf2_loss                 17233.10332  1467.75728  20064.53516  16048.65332
Training/pf_norm                  0.10626      0.02497     0.14644      0.07346
Training/qf1_norm                 1529.34409   38.03079    1595.24817   1494.49292
Training/qf2_norm                 225.17648    5.09710     234.41164    221.05453
log_std/mean                      -0.14425     0.00008     -0.14411     -0.14434
log_probs/mean                    -2.73677     0.00622     -2.72746     -2.74602
mean/mean                         0.00186      0.00005     0.00195      0.00181
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01833033561706543
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70867
epoch first part time 3.337860107421875e-06
replay_buffer._size: [25650]
collect time 0.0008406639099121094
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.006473064422607422, 'sac_diff2': 0.007661581039428711, 'sac_diff3': 0.009711742401123047, 'sac_diff4': 0.006676197052001953, 'sac_diff5': 0.030698060989379883, 'sac_diff6': 0.0003724098205566406, 'all': 0.06181812286376953}
diff5_list [0.006087064743041992, 0.006158351898193359, 0.0060272216796875, 0.006171703338623047, 0.006253719329833984]
time3 0
time4 0.06255578994750977
time5 0.06260061264038086
time7 4.76837158203125e-07
gen_weight_change tensor(-23.6637)
policy weight change tensor(38.2211, grad_fn=<SumBackward0>)
time8 0.0017745494842529297
train_time 0.072998046875
eval time 0.15716290473937988
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:32,846 MainThread INFO: EPOCH:164
2024-01-23 01:01:32,846 MainThread INFO: Time Consumed:0.2332746982574463s
2024-01-23 01:01:32,846 MainThread INFO: Total Frames:25500s
  2%|▏         | 165/10000 [03:00<42:13,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12486.66573
Train_Epoch_Reward                3830.66296
Running_Training_Average_Rewards  11979.80749
Explore_Time                      0.00084
Train___Time                      0.07300
Eval____Time                      0.15716
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12696.06586
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.22362     2.57676     92.93976     86.36491
alpha_0                           0.92086      0.00013     0.92104      0.92067
Alpha_loss                        -0.55418     0.00124     -0.55275     -0.55612
Training/policy_loss              -2.56278     0.00388     -2.55677     -2.56895
Training/qf1_loss                 17102.46387  1745.22556  19767.78711  14815.42676
Training/qf2_loss                 17730.43516  1750.83675  20409.72656  15433.64062
Training/pf_norm                  0.13131      0.03803     0.19604      0.08920
Training/qf1_norm                 1616.38862   52.10407    1681.54944   1533.50427
Training/qf2_norm                 228.42385    6.36339     235.10217    218.86182
log_std/mean                      -0.12349     0.00006     -0.12342     -0.12357
log_probs/mean                    -2.72948     0.00448     -2.72269     -2.73672
mean/mean                         0.00139      0.00005     0.00147      0.00131
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019130945205688477
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70867
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [25800]
collect time 0.000911712646484375
inside mustsac before update, task 0, sumup 70867
inside mustsac after update, task 0, sumup 69922
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.007425069808959961, 'sac_diff2': 0.008678197860717773, 'sac_diff3': 0.011042356491088867, 'sac_diff4': 0.007355928421020508, 'sac_diff5': 0.0519719123840332, 'sac_diff6': 0.00042510032653808594, 'all': 0.08710837364196777}
diff5_list [0.010692119598388672, 0.009463310241699219, 0.009647130966186523, 0.011329412460327148, 0.01083993911743164]
time3 0.0008771419525146484
time4 0.08800888061523438
time5 0.08806371688842773
time7 0.008911609649658203
gen_weight_change tensor(-23.5817)
policy weight change tensor(38.1465, grad_fn=<SumBackward0>)
time8 0.0018658638000488281
train_time 0.11762452125549316
eval time 0.11992144584655762
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:33,109 MainThread INFO: EPOCH:165
2024-01-23 01:01:33,110 MainThread INFO: Time Consumed:0.24077510833740234s
2024-01-23 01:01:33,110 MainThread INFO: Total Frames:25650s
  2%|▏         | 166/10000 [03:00<46:01,  3.56it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12497.43930
Train_Epoch_Reward                8587.13494
Running_Training_Average_Rewards  10013.46840
Explore_Time                      0.00091
Train___Time                      0.11762
Eval____Time                      0.11992
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12709.76498
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.55823     2.75393     94.10889     87.53904
alpha_0                           0.92040      0.00013     0.92058      0.92021
Alpha_loss                        -0.55792     0.00109     -0.55625     -0.55914
Training/policy_loss              -2.57208     0.00417     -2.56755     -2.57814
Training/qf1_loss                 17889.30195  2298.31450  21527.77930  15199.81738
Training/qf2_loss                 18469.20410  2313.76498  22131.76953  15775.78613
Training/pf_norm                  0.13483      0.02264     0.17945      0.11767
Training/qf1_norm                 1497.20522   56.60054    1556.99915   1394.93713
Training/qf2_norm                 235.53627    6.10434     243.36977    226.84309
log_std/mean                      -0.12899     0.00624     -0.12369     -0.14073
log_probs/mean                    -2.73400     0.00351     -2.73013     -2.74004
mean/mean                         0.00078      0.00059     0.00150      -0.00014
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.09077787399291992
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69922
epoch first part time 3.337860107421875e-06
replay_buffer._size: [25950]
collect time 0.0008471012115478516
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.006780385971069336, 'sac_diff2': 0.00832366943359375, 'sac_diff3': 0.010977745056152344, 'sac_diff4': 0.007209062576293945, 'sac_diff5': 0.03176236152648926, 'sac_diff6': 0.00039458274841308594, 'all': 0.0656728744506836}
diff5_list [0.006678104400634766, 0.006393909454345703, 0.0063419342041015625, 0.006134033203125, 0.0062143802642822266]
time3 0
time4 0.06647276878356934
time5 0.06652688980102539
time7 4.76837158203125e-07
gen_weight_change tensor(-23.5817)
policy weight change tensor(38.1001, grad_fn=<SumBackward0>)
time8 0.0019040107727050781
train_time 0.07744812965393066
eval time 0.10499072074890137
epoch last part time 5.4836273193359375e-06
2024-01-23 01:01:33,389 MainThread INFO: EPOCH:166
2024-01-23 01:01:33,389 MainThread INFO: Time Consumed:0.18554210662841797s
2024-01-23 01:01:33,390 MainThread INFO: Total Frames:25800s
  2%|▏         | 167/10000 [03:01<42:25,  3.86it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12540.38454
Train_Epoch_Reward                22388.54821
Running_Training_Average_Rewards  10381.20366
Explore_Time                      0.00084
Train___Time                      0.07745
Eval____Time                      0.10499
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12508.41757
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.40221     0.91126    90.76070     88.09340
alpha_0                           0.91994      0.00013    0.92012      0.91975
Alpha_loss                        -0.56164     0.00132    -0.56018     -0.56366
Training/policy_loss              -2.57324     0.00496    -2.56508     -2.58008
Training/qf1_loss                 16819.08125  884.80516  17882.41211  15728.04102
Training/qf2_loss                 17388.31484  892.46264  18457.78125  16288.00195
Training/pf_norm                  0.12910      0.03891    0.17583      0.07208
Training/qf1_norm                 1479.69697   19.09097   1503.47534   1455.63245
Training/qf2_norm                 227.02108    2.31321    230.53285    223.77426
log_std/mean                      -0.13140     0.00007    -0.13129     -0.13151
log_probs/mean                    -2.73824     0.00562    -2.72915     -2.74619
mean/mean                         0.00017      0.00008    0.00025      0.00007
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018696308135986328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69922
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [26100]
collect time 0.0008747577667236328
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.006598234176635742, 'sac_diff2': 0.007398128509521484, 'sac_diff3': 0.009711503982543945, 'sac_diff4': 0.006621837615966797, 'sac_diff5': 0.030576229095458984, 'sac_diff6': 0.0003826618194580078, 'all': 0.061493635177612305}
diff5_list [0.006354331970214844, 0.006016731262207031, 0.005987644195556641, 0.0062313079833984375, 0.005986213684082031]
time3 0
time4 0.06222057342529297
time5 0.06226468086242676
time7 4.76837158203125e-07
gen_weight_change tensor(-23.5817)
policy weight change tensor(38.0916, grad_fn=<SumBackward0>)
time8 0.0018928050994873047
train_time 0.07283568382263184
eval time 0.1612093448638916
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:33,648 MainThread INFO: EPOCH:167
2024-01-23 01:01:33,649 MainThread INFO: Time Consumed:0.23721718788146973s
2024-01-23 01:01:33,649 MainThread INFO: Total Frames:25950s
  2%|▏         | 168/10000 [03:01<42:26,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12570.26727
Train_Epoch_Reward                17889.36731
Running_Training_Average_Rewards  10822.00375
Explore_Time                      0.00087
Train___Time                      0.07284
Eval____Time                      0.16121
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12504.54057
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.30650     1.19811     90.76169     87.62529
alpha_0                           0.91948      0.00013     0.91966      0.91929
Alpha_loss                        -0.56441     0.00120     -0.56240     -0.56563
Training/policy_loss              -2.57082     0.00574     -2.56399     -2.58133
Training/qf1_loss                 16887.42617  1052.69189  17841.36523  15052.09180
Training/qf2_loss                 17444.86328  1061.00250  18407.47266  15595.56445
Training/pf_norm                  0.13713      0.02595     0.18065      0.10561
Training/qf1_norm                 1436.65698   21.55549    1459.68640   1402.94250
Training/qf2_norm                 237.77749    3.00866     241.46304    233.62386
log_std/mean                      -0.12194     0.00005     -0.12189     -0.12203
log_probs/mean                    -2.73108     0.00632     -2.72312     -2.74250
mean/mean                         0.00031      0.00009     0.00047      0.00021
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018720149993896484
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69922
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [26250]
collect time 0.0008645057678222656
inner_dict_sum {'sac_diff0': 0.0002827644348144531, 'sac_diff1': 0.008265256881713867, 'sac_diff2': 0.009648323059082031, 'sac_diff3': 0.012242794036865234, 'sac_diff4': 0.008358478546142578, 'sac_diff5': 0.038582801818847656, 'sac_diff6': 0.0005085468292236328, 'all': 0.07788896560668945}
diff5_list [0.00799417495727539, 0.007576704025268555, 0.007787227630615234, 0.0075168609619140625, 0.007707834243774414]
time3 0
time4 0.07865762710571289
time5 0.078704833984375
time7 4.76837158203125e-07
gen_weight_change tensor(-23.5817)
policy weight change tensor(38.1324, grad_fn=<SumBackward0>)
time8 0.0032911300659179688
train_time 0.09542250633239746
eval time 0.13710904121398926
epoch last part time 8.58306884765625e-06
2024-01-23 01:01:33,906 MainThread INFO: EPOCH:168
2024-01-23 01:01:33,907 MainThread INFO: Time Consumed:0.23600006103515625s
2024-01-23 01:01:33,907 MainThread INFO: Total Frames:26100s
  2%|▏         | 169/10000 [03:01<42:26,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12587.99630
Train_Epoch_Reward                9702.93185
Running_Training_Average_Rewards  10643.90214
Explore_Time                      0.00086
Train___Time                      0.09542
Eval____Time                      0.13711
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12538.49307
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.65869     3.98502     94.77393     83.05667
alpha_0                           0.91902      0.00013     0.91920      0.91883
Alpha_loss                        -0.56785     0.00097     -0.56673     -0.56906
Training/policy_loss              -2.56617     0.00478     -2.56150     -2.57231
Training/qf1_loss                 16482.72305  1605.28740  18119.69922  13421.26758
Training/qf2_loss                 17020.48281  1628.56100  18681.35938  13916.48242
Training/pf_norm                  0.15046      0.02420     0.17997      0.11060
Training/qf1_norm                 1412.49126   64.09057    1482.25806   1301.91626
Training/qf2_norm                 229.59154    9.77728     242.01872    213.37822
log_std/mean                      -0.12763     0.00008     -0.12753     -0.12774
log_probs/mean                    -2.73193     0.00510     -2.72656     -2.73818
mean/mean                         0.00263      0.00005     0.00272      0.00256
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020124435424804688
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69922
epoch first part time 4.76837158203125e-06
replay_buffer._size: [26400]
collect time 0.0009374618530273438
inner_dict_sum {'sac_diff0': 0.0002512931823730469, 'sac_diff1': 0.009814262390136719, 'sac_diff2': 0.012228965759277344, 'sac_diff3': 0.014299631118774414, 'sac_diff4': 0.009657859802246094, 'sac_diff5': 0.04187488555908203, 'sac_diff6': 0.0005323886871337891, 'all': 0.08865928649902344}
diff5_list [0.00903010368347168, 0.009229183197021484, 0.008065223693847656, 0.007718801498413086, 0.007831573486328125]
time3 0
time4 0.08947134017944336
time5 0.0895237922668457
time7 7.152557373046875e-07
gen_weight_change tensor(-23.5817)
policy weight change tensor(38.0091, grad_fn=<SumBackward0>)
time8 0.002087831497192383
train_time 0.10207319259643555
eval time 0.1604478359222412
epoch last part time 7.62939453125e-06
2024-01-23 01:01:34,196 MainThread INFO: EPOCH:169
2024-01-23 01:01:34,197 MainThread INFO: Time Consumed:0.2661008834838867s
2024-01-23 01:01:34,197 MainThread INFO: Total Frames:26250s
  2%|▏         | 170/10000 [03:01<44:05,  3.72it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12588.50594
Train_Epoch_Reward                6095.98331
Running_Training_Average_Rewards  10480.14695
Explore_Time                      0.00093
Train___Time                      0.10207
Eval____Time                      0.16045
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12499.90959
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.48777     1.38767     89.74944     85.93379
alpha_0                           0.91856      0.00013     0.91874      0.91837
Alpha_loss                        -0.57101     0.00128     -0.56943     -0.57316
Training/policy_loss              -2.55623     0.00409     -2.55347     -2.56434
Training/qf1_loss                 15099.34160  1305.34449  17571.81641  13786.77148
Training/qf2_loss                 15722.24258  1312.58121  18207.20312  14391.86230
Training/pf_norm                  0.14226      0.01768     0.16397      0.11706
Training/qf1_norm                 1534.75547   26.97317    1569.24316   1495.46985
Training/qf2_norm                 216.70374    3.35435     222.13635    212.82913
log_std/mean                      -0.15230     0.00043     -0.15162     -0.15281
log_probs/mean                    -2.72946     0.00475     -2.72619     -2.73885
mean/mean                         -0.00015     0.00006     -0.00006     -0.00024
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0226438045501709
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69922
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [26550]
collect time 0.001043081283569336
inside mustsac before update, task 0, sumup 69922
inside mustsac after update, task 0, sumup 70804
inner_dict_sum {'sac_diff0': 0.00026917457580566406, 'sac_diff1': 0.009264945983886719, 'sac_diff2': 0.010830163955688477, 'sac_diff3': 0.013539791107177734, 'sac_diff4': 0.009566545486450195, 'sac_diff5': 0.06516265869140625, 'sac_diff6': 0.0005457401275634766, 'all': 0.10917901992797852}
diff5_list [0.015424251556396484, 0.012507915496826172, 0.01256108283996582, 0.012211799621582031, 0.012457609176635742]
time3 0.0012099742889404297
time4 0.11014723777770996
time5 0.11021995544433594
time7 0.010001897811889648
gen_weight_change tensor(-23.7045)
policy weight change tensor(38.0359, grad_fn=<SumBackward0>)
time8 0.003019571304321289
train_time 0.14470410346984863
eval time 0.09314632415771484
epoch last part time 7.867813110351562e-06
2024-01-23 01:01:34,464 MainThread INFO: EPOCH:170
2024-01-23 01:01:34,465 MainThread INFO: Time Consumed:0.24132513999938965s
2024-01-23 01:01:34,465 MainThread INFO: Total Frames:26400s
  2%|▏         | 171/10000 [03:02<44:08,  3.71it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12576.56960
Train_Epoch_Reward                14118.98504
Running_Training_Average_Rewards  10844.93113
Explore_Time                      0.00104
Train___Time                      0.14470
Eval____Time                      0.09315
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12458.25291
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.50547     3.19637     93.18935     84.60668
alpha_0                           0.91810      0.00013     0.91828      0.91791
Alpha_loss                        -0.57483     0.00094     -0.57392     -0.57623
Training/policy_loss              -2.56779     0.00747     -2.55457     -2.57413
Training/qf1_loss                 17012.19492  1144.67936  18356.63672  15186.37305
Training/qf2_loss                 17647.66289  1132.67453  18969.57031  15859.51367
Training/pf_norm                  0.14646      0.02782     0.19160      0.10878
Training/qf1_norm                 1567.06411   73.81051    1665.04187   1458.04858
Training/qf2_norm                 232.19660    12.18788    246.54576    218.11949
log_std/mean                      -0.13245     0.00614     -0.12183     -0.13884
log_probs/mean                    -2.73470     0.00528     -2.72668     -2.74260
mean/mean                         0.00122      0.00124     0.00270      -0.00041
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02158832550048828
epoch last part time3 0.002977132797241211
inside rlalgo, task 0, sumup 70804
epoch first part time 3.814697265625e-06
replay_buffer._size: [26700]
collect time 0.0009162425994873047
inner_dict_sum {'sac_diff0': 0.00023412704467773438, 'sac_diff1': 0.0076406002044677734, 'sac_diff2': 0.008783102035522461, 'sac_diff3': 0.011076927185058594, 'sac_diff4': 0.007356166839599609, 'sac_diff5': 0.0341496467590332, 'sac_diff6': 0.000400543212890625, 'all': 0.06964111328125}
diff5_list [0.008019685745239258, 0.0069773197174072266, 0.006387233734130859, 0.006550788879394531, 0.006214618682861328]
time3 0
time4 0.07041645050048828
time5 0.07047271728515625
time7 9.5367431640625e-07
gen_weight_change tensor(-23.7045)
policy weight change tensor(37.9247, grad_fn=<SumBackward0>)
time8 0.002008199691772461
train_time 0.08195734024047852
eval time 0.149003267288208
epoch last part time 7.867813110351562e-06
2024-01-23 01:01:34,727 MainThread INFO: EPOCH:171
2024-01-23 01:01:34,727 MainThread INFO: Time Consumed:0.23441076278686523s
2024-01-23 01:01:34,727 MainThread INFO: Total Frames:26550s
  2%|▏         | 172/10000 [03:02<43:34,  3.76it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12615.99565
Train_Epoch_Reward                16965.64301
Running_Training_Average_Rewards  11049.71243
Explore_Time                      0.00091
Train___Time                      0.08196
Eval____Time                      0.14900
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12979.57659
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.57503     2.97253     95.24986     88.00815
alpha_0                           0.91764      0.00013     0.91782      0.91745
Alpha_loss                        -0.57800     0.00085     -0.57666     -0.57910
Training/policy_loss              -2.56675     0.00690     -2.56143     -2.57988
Training/qf1_loss                 18536.01270  2246.58943  21412.49414  15698.33887
Training/qf2_loss                 19263.55547  2269.09224  22160.81250  16397.73438
Training/pf_norm                  0.09749      0.02589     0.13201      0.06717
Training/qf1_norm                 1757.97427   59.62700    1834.31018   1688.64795
Training/qf2_norm                 238.03167    7.43490     247.14661    229.00015
log_std/mean                      -0.13656     0.00012     -0.13645     -0.13675
log_probs/mean                    -2.73240     0.00732     -2.72663     -2.74643
mean/mean                         0.00119      0.00012     0.00134      0.00102
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0201873779296875
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70804
epoch first part time 3.814697265625e-06
replay_buffer._size: [26850]
collect time 0.0010259151458740234
inner_dict_sum {'sac_diff0': 0.00022649765014648438, 'sac_diff1': 0.007298707962036133, 'sac_diff2': 0.008764505386352539, 'sac_diff3': 0.011822700500488281, 'sac_diff4': 0.007330894470214844, 'sac_diff5': 0.032579660415649414, 'sac_diff6': 0.00038814544677734375, 'all': 0.06841111183166504}
diff5_list [0.0066986083984375, 0.006574153900146484, 0.006646871566772461, 0.006451606750488281, 0.0062084197998046875]
time3 0
time4 0.06918191909790039
time5 0.06924009323120117
time7 7.152557373046875e-07
gen_weight_change tensor(-23.7045)
policy weight change tensor(37.8023, grad_fn=<SumBackward0>)
time8 0.0019598007202148438
train_time 0.08049392700195312
eval time 0.15033674240112305
epoch last part time 9.5367431640625e-06
2024-01-23 01:01:34,985 MainThread INFO: EPOCH:172
2024-01-23 01:01:34,986 MainThread INFO: Time Consumed:0.23439335823059082s
2024-01-23 01:01:34,986 MainThread INFO: Total Frames:26700s
  2%|▏         | 173/10000 [03:02<43:11,  3.79it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12654.57524
Train_Epoch_Reward                11919.73895
Running_Training_Average_Rewards  11171.63085
Explore_Time                      0.00101
Train___Time                      0.08049
Eval____Time                      0.15034
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12990.32476
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.64479     2.68918     91.68552     84.12841
alpha_0                           0.91718      0.00013     0.91736      0.91699
Alpha_loss                        -0.58176     0.00119     -0.57986     -0.58328
Training/policy_loss              -2.57222     0.00454     -2.56683     -2.57999
Training/qf1_loss                 16147.88437  1711.68391  18983.03320  13711.34668
Training/qf2_loss                 16800.64414  1726.04287  19653.69922  14339.34082
Training/pf_norm                  0.10655      0.01778     0.13160      0.07757
Training/qf1_norm                 1564.09343   40.78035    1608.03369   1497.72363
Training/qf2_norm                 232.54069    6.81950     240.14996    221.03819
log_std/mean                      -0.14139     0.00027     -0.14096     -0.14173
log_probs/mean                    -2.73690     0.00509     -2.73048     -2.74537
mean/mean                         0.00092      0.00003     0.00097      0.00089
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020247220993041992
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70804
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [27000]
collect time 0.0010116100311279297
inner_dict_sum {'sac_diff0': 0.0002391338348388672, 'sac_diff1': 0.0069255828857421875, 'sac_diff2': 0.008417367935180664, 'sac_diff3': 0.010900735855102539, 'sac_diff4': 0.006997585296630859, 'sac_diff5': 0.0319523811340332, 'sac_diff6': 0.000385284423828125, 'all': 0.06581807136535645}
diff5_list [0.0066530704498291016, 0.006392717361450195, 0.006267547607421875, 0.006234645843505859, 0.006404399871826172]
time3 0
time4 0.0665898323059082
time5 0.06664133071899414
time7 7.152557373046875e-07
gen_weight_change tensor(-23.7045)
policy weight change tensor(37.7865, grad_fn=<SumBackward0>)
time8 0.0018978118896484375
train_time 0.07770943641662598
eval time 0.15327215194702148
epoch last part time 8.58306884765625e-06
2024-01-23 01:01:35,244 MainThread INFO: EPOCH:173
2024-01-23 01:01:35,244 MainThread INFO: Time Consumed:0.23454499244689941s
2024-01-23 01:01:35,245 MainThread INFO: Total Frames:26850s
  2%|▏         | 174/10000 [03:02<42:56,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12688.43404
Train_Epoch_Reward                12211.89118
Running_Training_Average_Rewards  11432.16978
Explore_Time                      0.00100
Train___Time                      0.07771
Eval____Time                      0.15327
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12998.99449
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.26080     2.45425     92.30177     85.32260
alpha_0                           0.91672      0.00013     0.91690      0.91654
Alpha_loss                        -0.58473     0.00122     -0.58300     -0.58638
Training/policy_loss              -2.56148     0.00523     -2.55696     -2.57146
Training/qf1_loss                 17358.51641  1856.63653  19630.05078  13991.39648
Training/qf2_loss                 18043.99922  1874.23972  20322.37695  14639.70312
Training/pf_norm                  0.10779      0.03188     0.16283      0.06684
Training/qf1_norm                 1674.52654   49.63571    1734.56482   1587.95496
Training/qf2_norm                 230.76806    6.16329     238.36203    220.84958
log_std/mean                      -0.12155     0.00008     -0.12146     -0.12169
log_probs/mean                    -2.73232     0.00583     -2.72785     -2.74349
mean/mean                         0.00113      0.00003     0.00117      0.00110
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020038127899169922
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70804
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [27150]
collect time 0.0009675025939941406
inner_dict_sum {'sac_diff0': 0.0002231597900390625, 'sac_diff1': 0.0069162845611572266, 'sac_diff2': 0.008095979690551758, 'sac_diff3': 0.010537385940551758, 'sac_diff4': 0.007076263427734375, 'sac_diff5': 0.033393144607543945, 'sac_diff6': 0.00038695335388183594, 'all': 0.06662917137145996}
diff5_list [0.00671696662902832, 0.007090091705322266, 0.006741762161254883, 0.0061910152435302734, 0.006653308868408203]
time3 0
time4 0.06741189956665039
time5 0.06746339797973633
time7 7.152557373046875e-07
gen_weight_change tensor(-23.7045)
policy weight change tensor(37.9071, grad_fn=<SumBackward0>)
time8 0.002065896987915039
train_time 0.07853960990905762
eval time 0.15507006645202637
epoch last part time 8.344650268554688e-06
2024-01-23 01:01:35,505 MainThread INFO: EPOCH:174
2024-01-23 01:01:35,505 MainThread INFO: Time Consumed:0.23708891868591309s
2024-01-23 01:01:35,505 MainThread INFO: Total Frames:27000s
  2%|▏         | 175/10000 [03:03<42:46,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12712.69232
Train_Epoch_Reward                4265.00295
Running_Training_Average_Rewards  10847.93512
Explore_Time                      0.00096
Train___Time                      0.07854
Eval____Time                      0.15507
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12938.64873
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.04230     1.71188     90.42086     85.37569
alpha_0                           0.91626      0.00013     0.91644      0.91608
Alpha_loss                        -0.58738     0.00102     -0.58629     -0.58906
Training/policy_loss              -2.55547     0.00315     -2.55157     -2.55873
Training/qf1_loss                 16124.83242  1923.47841  18385.41602  13289.67969
Training/qf2_loss                 16815.50449  1935.73822  19081.73438  13954.33105
Training/pf_norm                  0.15155      0.03066     0.18085      0.09715
Training/qf1_norm                 1716.12351   35.59937    1752.05225   1653.61072
Training/qf2_norm                 238.28852    4.40956     244.34679    231.37268
log_std/mean                      -0.12072     0.00020     -0.12049     -0.12105
log_probs/mean                    -2.72403     0.00344     -2.71983     -2.72790
mean/mean                         0.00179      0.00007     0.00186      0.00170
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018477201461791992
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70804
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [27300]
collect time 0.0009837150573730469
inside mustsac before update, task 0, sumup 70804
inside mustsac after update, task 0, sumup 70580
inner_dict_sum {'sac_diff0': 0.00020313262939453125, 'sac_diff1': 0.007146120071411133, 'sac_diff2': 0.008458375930786133, 'sac_diff3': 0.010787487030029297, 'sac_diff4': 0.00754237174987793, 'sac_diff5': 0.05057024955749512, 'sac_diff6': 0.0004324913024902344, 'all': 0.08514022827148438}
diff5_list [0.010340213775634766, 0.009831666946411133, 0.010777711868286133, 0.010024547576904297, 0.009596109390258789]
time3 0.0008881092071533203
time4 0.08606600761413574
time5 0.08612728118896484
time7 0.009168624877929688
gen_weight_change tensor(-23.8356)
policy weight change tensor(37.9460, grad_fn=<SumBackward0>)
time8 0.0019769668579101562
train_time 0.11579561233520508
eval time 0.11456441879272461
epoch last part time 7.867813110351562e-06
2024-01-23 01:01:35,761 MainThread INFO: EPOCH:175
2024-01-23 01:01:35,761 MainThread INFO: Time Consumed:0.23379087448120117s
2024-01-23 01:01:35,761 MainThread INFO: Total Frames:27150s
  2%|▏         | 176/10000 [03:03<42:29,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12723.14482
Train_Epoch_Reward                9650.76692
Running_Training_Average_Rewards  10761.06863
Explore_Time                      0.00098
Train___Time                      0.11580
Eval____Time                      0.11456
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12814.28998
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.75162     1.75359     92.81806     88.28359
alpha_0                           0.91580      0.00013     0.91599      0.91562
Alpha_loss                        -0.59149     0.00078     -0.59063     -0.59269
Training/policy_loss              -2.55869     0.00553     -2.55293     -2.56917
Training/qf1_loss                 17832.22070  1432.21879  20414.41211  16512.23633
Training/qf2_loss                 18580.61328  1396.71660  21052.65430  17260.04297
Training/pf_norm                  0.14451      0.04129     0.19263      0.08686
Training/qf1_norm                 1807.74543   134.42386   1990.20239   1572.50671
Training/qf2_norm                 230.62267    9.41231     244.41199    216.33186
log_std/mean                      -0.12964     0.00330     -0.12552     -0.13557
log_probs/mean                    -2.73258     0.00395     -2.72729     -2.73812
mean/mean                         0.00055      0.00054     0.00117      -0.00014
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018088817596435547
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70580
epoch first part time 2.86102294921875e-06
replay_buffer._size: [27450]
collect time 0.0009706020355224609
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.007967710494995117, 'sac_diff2': 0.009554386138916016, 'sac_diff3': 0.01201772689819336, 'sac_diff4': 0.008291244506835938, 'sac_diff5': 0.037798166275024414, 'sac_diff6': 0.00044798851013183594, 'all': 0.07629156112670898}
diff5_list [0.007414102554321289, 0.0070993900299072266, 0.007771730422973633, 0.007660388946533203, 0.007852554321289062]
time3 0
time4 0.07718491554260254
time5 0.07727861404418945
time7 9.5367431640625e-07
gen_weight_change tensor(-23.8356)
policy weight change tensor(38.1074, grad_fn=<SumBackward0>)
time8 0.0022034645080566406
train_time 0.08900642395019531
eval time 0.1707303524017334
epoch last part time 8.344650268554688e-06
2024-01-23 01:01:36,046 MainThread INFO: EPOCH:176
2024-01-23 01:01:36,046 MainThread INFO: Time Consumed:0.2631971836090088s
2024-01-23 01:01:36,046 MainThread INFO: Total Frames:27300s
  2%|▏         | 177/10000 [03:03<43:45,  3.74it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12667.51719
Train_Epoch_Reward                13713.27951
Running_Training_Average_Rewards  11061.45947
Explore_Time                      0.00097
Train___Time                      0.08901
Eval____Time                      0.17073
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11952.14118
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.35677     4.84586     93.82555     81.11076
alpha_0                           0.91534      0.00013     0.91553      0.91516
Alpha_loss                        -0.59558     0.00128     -0.59370     -0.59717
Training/policy_loss              -2.56450     0.00969     -2.55222     -2.58098
Training/qf1_loss                 15321.20332  2381.52041  19001.21484  12321.98926
Training/qf2_loss                 16012.16289  2419.47404  19742.51172  12956.94531
Training/pf_norm                  0.11568      0.01775     0.14987      0.10082
Training/qf1_norm                 1672.96458   95.55872    1798.02979   1539.30713
Training/qf2_norm                 224.33907    12.02732    240.34010    208.74898
log_std/mean                      -0.12875     0.00027     -0.12836     -0.12911
log_probs/mean                    -2.74071     0.01054     -2.72703     -2.75869
mean/mean                         0.00151      0.00010     0.00164      0.00138
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018653392791748047
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70580
epoch first part time 3.337860107421875e-06
replay_buffer._size: [27600]
collect time 0.0008788108825683594
inner_dict_sum {'sac_diff0': 0.00020956993103027344, 'sac_diff1': 0.007339954376220703, 'sac_diff2': 0.008155345916748047, 'sac_diff3': 0.010565042495727539, 'sac_diff4': 0.006822824478149414, 'sac_diff5': 0.03228759765625, 'sac_diff6': 0.0003848075866699219, 'all': 0.0657651424407959}
diff5_list [0.007474422454833984, 0.006475210189819336, 0.00640416145324707, 0.005945444107055664, 0.005988359451293945]
time3 0
time4 0.06653833389282227
time5 0.06658411026000977
time7 4.76837158203125e-07
gen_weight_change tensor(-23.8356)
policy weight change tensor(38.1470, grad_fn=<SumBackward0>)
time8 0.0019266605377197266
train_time 0.08063626289367676
eval time 0.14817452430725098
epoch last part time 1.1205673217773438e-05
2024-01-23 01:01:36,300 MainThread INFO: EPOCH:177
2024-01-23 01:01:36,301 MainThread INFO: Time Consumed:0.23249077796936035s
2024-01-23 01:01:36,301 MainThread INFO: Total Frames:27450s
  2%|▏         | 178/10000 [03:03<43:12,  3.79it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12593.28715
Train_Epoch_Reward                1653.00422
Running_Training_Average_Rewards  10853.38863
Explore_Time                      0.00087
Train___Time                      0.08064
Eval____Time                      0.14817
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11762.24020
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.44482     1.99241     90.28539     85.35672
alpha_0                           0.91489      0.00013     0.91507      0.91470
Alpha_loss                        -0.59870     0.00082     -0.59741     -0.59985
Training/policy_loss              -2.55969     0.00647     -2.55332     -2.57164
Training/qf1_loss                 15555.56562  984.79025   16870.02539  14443.03125
Training/qf2_loss                 16265.06152  1002.54300  17588.58203  15128.32324
Training/pf_norm                  0.10531      0.03041     0.14645      0.05503
Training/qf1_norm                 1694.35376   46.73564    1768.87292   1641.29797
Training/qf2_norm                 222.61888    4.94928     229.65726    217.43785
log_std/mean                      -0.13552     0.00005     -0.13543     -0.13555
log_probs/mean                    -2.73783     0.00686     -2.73116     -2.75063
mean/mean                         0.00086      0.00013     0.00104      0.00068
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020138025283813477
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70580
epoch first part time 4.76837158203125e-06
replay_buffer._size: [27750]
collect time 0.001064300537109375
inner_dict_sum {'sac_diff0': 0.00020933151245117188, 'sac_diff1': 0.007587909698486328, 'sac_diff2': 0.009824991226196289, 'sac_diff3': 0.01116800308227539, 'sac_diff4': 0.0077114105224609375, 'sac_diff5': 0.03381061553955078, 'sac_diff6': 0.00039839744567871094, 'all': 0.07071065902709961}
diff5_list [0.00800633430480957, 0.006551504135131836, 0.0060651302337646484, 0.005997180938720703, 0.0071904659271240234]
time3 0
time4 0.0715034008026123
time5 0.07155895233154297
time7 7.152557373046875e-07
gen_weight_change tensor(-23.8356)
policy weight change tensor(38.1465, grad_fn=<SumBackward0>)
time8 0.00223541259765625
train_time 0.0837252140045166
eval time 0.1414048671722412
epoch last part time 7.3909759521484375e-06
2024-01-23 01:01:36,553 MainThread INFO: EPOCH:178
2024-01-23 01:01:36,553 MainThread INFO: Time Consumed:0.2286512851715088s
2024-01-23 01:01:36,553 MainThread INFO: Total Frames:27600s
  2%|▏         | 179/10000 [03:04<42:34,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12504.37164
Train_Epoch_Reward                3016.13441
Running_Training_Average_Rewards  10757.34839
Explore_Time                      0.00106
Train___Time                      0.08373
Eval____Time                      0.14140
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11649.33796
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.26259     3.02224     92.34047     83.90833
alpha_0                           0.91443      0.00013     0.91461      0.91424
Alpha_loss                        -0.60137     0.00110     -0.59980     -0.60267
Training/policy_loss              -2.55374     0.00356     -2.54982     -2.55993
Training/qf1_loss                 16121.78730  2201.99926  20067.05078  13762.20312
Training/qf2_loss                 16947.99297  2231.20152  20941.54492  14561.53711
Training/pf_norm                  0.12759      0.03625     0.18028      0.07546
Training/qf1_norm                 1935.74968   73.30706    2044.79822   1844.01990
Training/qf2_norm                 227.06075    7.50104     237.20207    216.37669
log_std/mean                      -0.13438     0.00004     -0.13435     -0.13445
log_probs/mean                    -2.72995     0.00398     -2.72617     -2.73699
mean/mean                         0.00009      0.00007     0.00021      0.00004
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018569231033325195
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70580
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [27900]
collect time 0.0009701251983642578
inner_dict_sum {'sac_diff0': 0.00022220611572265625, 'sac_diff1': 0.0075092315673828125, 'sac_diff2': 0.008169412612915039, 'sac_diff3': 0.010489225387573242, 'sac_diff4': 0.007088184356689453, 'sac_diff5': 0.033621788024902344, 'sac_diff6': 0.0004162788391113281, 'all': 0.06751632690429688}
diff5_list [0.00725102424621582, 0.006474494934082031, 0.006521701812744141, 0.00719451904296875, 0.0061800479888916016]
time3 0
time4 0.06836962699890137
time5 0.06842708587646484
time7 7.152557373046875e-07
gen_weight_change tensor(-23.8356)
policy weight change tensor(38.1339, grad_fn=<SumBackward0>)
time8 0.0019512176513671875
train_time 0.07956671714782715
eval time 0.14737629890441895
epoch last part time 8.344650268554688e-06
2024-01-23 01:01:36,806 MainThread INFO: EPOCH:179
2024-01-23 01:01:36,806 MainThread INFO: Time Consumed:0.2303786277770996s
2024-01-23 01:01:36,806 MainThread INFO: Total Frames:27750s
  2%|▏         | 180/10000 [03:04<42:17,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12406.52614
Train_Epoch_Reward                9212.10915
Running_Training_Average_Rewards  10109.23821
Explore_Time                      0.00096
Train___Time                      0.07957
Eval____Time                      0.14738
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11521.45465
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       86.89707     1.64305     88.21846     83.65630
alpha_0                           0.91397      0.00013     0.91415      0.91379
Alpha_loss                        -0.60481     0.00096     -0.60357     -0.60623
Training/policy_loss              -2.55214     0.00429     -2.54668     -2.55736
Training/qf1_loss                 15373.28867  1178.35249  17501.57422  13992.15918
Training/qf2_loss                 16148.35371  1186.54491  18279.09570  14733.72168
Training/pf_norm                  0.14917      0.02778     0.18635      0.10150
Training/qf1_norm                 1824.68838   39.21262    1854.77869   1747.25146
Training/qf2_norm                 225.41070    4.08570     228.66772    217.35033
log_std/mean                      -0.13246     0.00004     -0.13239     -0.13250
log_probs/mean                    -2.73076     0.00460     -2.72454     -2.73589
mean/mean                         -0.00018     0.00013     -0.00006     -0.00040
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019948959350585938
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70580
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [28050]
collect time 0.0011196136474609375
inside mustsac before update, task 0, sumup 70580
inside mustsac after update, task 0, sumup 71127
inner_dict_sum {'sac_diff0': 0.00021028518676757812, 'sac_diff1': 0.007447957992553711, 'sac_diff2': 0.00834345817565918, 'sac_diff3': 0.01069784164428711, 'sac_diff4': 0.00696563720703125, 'sac_diff5': 0.052898406982421875, 'sac_diff6': 0.0004322528839111328, 'all': 0.08699584007263184}
diff5_list [0.011329889297485352, 0.010289430618286133, 0.010075807571411133, 0.00988006591796875, 0.011323213577270508]
time3 0.0008842945098876953
time4 0.08795404434204102
time5 0.08802008628845215
time7 0.00984954833984375
gen_weight_change tensor(-23.9425)
policy weight change tensor(38.1837, grad_fn=<SumBackward0>)
time8 0.0027954578399658203
train_time 0.1190805435180664
eval time 0.11953425407409668
epoch last part time 8.106231689453125e-06
2024-01-23 01:01:37,072 MainThread INFO: EPOCH:180
2024-01-23 01:01:37,072 MainThread INFO: Time Consumed:0.24215102195739746s
2024-01-23 01:01:37,072 MainThread INFO: Total Frames:27900s
  2%|▏         | 181/10000 [03:04<42:46,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12306.23969
Train_Epoch_Reward                6711.92509
Running_Training_Average_Rewards  9860.71186
Explore_Time                      0.00111
Train___Time                      0.11908
Eval____Time                      0.11953
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11455.38840
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.07740     1.95750     89.78259     83.80807
alpha_0                           0.91351      0.00013     0.91370      0.91333
Alpha_loss                        -0.60852     0.00081     -0.60753     -0.60947
Training/policy_loss              -2.56077     0.00898     -2.55316     -2.57328
Training/qf1_loss                 15438.57090  1251.73618  17611.28906  13900.27637
Training/qf2_loss                 16208.55488  1296.01897  18424.51172  14529.64844
Training/pf_norm                  0.12079      0.03454     0.18184      0.08042
Training/qf1_norm                 1830.07405   124.01102   1960.09534   1614.32080
Training/qf2_norm                 227.47683    6.96575     234.87515    214.99622
log_std/mean                      -0.13190     0.00473     -0.12247     -0.13480
log_probs/mean                    -2.73461     0.00769     -2.72478     -2.74514
mean/mean                         0.00045      0.00111     0.00198      -0.00061
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019452333450317383
epoch last part time3 0.002835512161254883
inside rlalgo, task 0, sumup 71127
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [28200]
collect time 0.0008902549743652344
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.007113456726074219, 'sac_diff2': 0.008435964584350586, 'sac_diff3': 0.010630130767822266, 'sac_diff4': 0.00700688362121582, 'sac_diff5': 0.03244352340698242, 'sac_diff6': 0.00040030479431152344, 'all': 0.06624460220336914}
diff5_list [0.006529808044433594, 0.006287574768066406, 0.006538867950439453, 0.0066645145416259766, 0.006422758102416992]
time3 0
time4 0.06709814071655273
time5 0.06715059280395508
time7 4.76837158203125e-07
gen_weight_change tensor(-23.9425)
policy weight change tensor(38.1772, grad_fn=<SumBackward0>)
time8 0.001959085464477539
train_time 0.07809090614318848
eval time 0.15105795860290527
epoch last part time 8.344650268554688e-06
2024-01-23 01:01:37,330 MainThread INFO: EPOCH:181
2024-01-23 01:01:37,330 MainThread INFO: Time Consumed:0.23251056671142578s
2024-01-23 01:01:37,330 MainThread INFO: Total Frames:28050s
  2%|▏         | 182/10000 [03:04<42:25,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12187.47928
Train_Epoch_Reward                30449.80994
Running_Training_Average_Rewards  10825.26160
Explore_Time                      0.00088
Train___Time                      0.07809
Eval____Time                      0.15106
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11791.97243
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.63275     2.87653     94.53899     86.54523
alpha_0                           0.91306      0.00013     0.91324      0.91287
Alpha_loss                        -0.61169     0.00069     -0.61068     -0.61263
Training/policy_loss              -2.55509     0.00904     -2.54500     -2.57158
Training/qf1_loss                 16696.68516  2013.25335  20505.15625  14719.05176
Training/qf2_loss                 17457.05820  2039.72161  21312.29297  15446.43555
Training/pf_norm                  0.13177      0.01808     0.15025      0.09741
Training/qf1_norm                 1848.18660   67.10925    1961.07312   1772.55029
Training/qf2_norm                 232.57468    7.21487     244.85333    224.75629
log_std/mean                      -0.12777     0.00006     -0.12771     -0.12785
log_probs/mean                    -2.73234     0.00961     -2.72130     -2.74985
mean/mean                         0.00096      0.00006     0.00105      0.00090
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018747329711914062
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71127
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [28350]
collect time 0.000972747802734375
inner_dict_sum {'sac_diff0': 0.0002048015594482422, 'sac_diff1': 0.007389545440673828, 'sac_diff2': 0.00871586799621582, 'sac_diff3': 0.011228322982788086, 'sac_diff4': 0.007460594177246094, 'sac_diff5': 0.03389453887939453, 'sac_diff6': 0.0004096031188964844, 'all': 0.06930327415466309}
diff5_list [0.0068814754486083984, 0.007676839828491211, 0.006695747375488281, 0.006124019622802734, 0.006516456604003906]
time3 0
time4 0.07012486457824707
time5 0.07018160820007324
time7 4.76837158203125e-07
gen_weight_change tensor(-23.9425)
policy weight change tensor(38.2130, grad_fn=<SumBackward0>)
time8 0.00196075439453125
train_time 0.08163881301879883
eval time 0.15413188934326172
epoch last part time 8.344650268554688e-06
2024-01-23 01:01:37,591 MainThread INFO: EPOCH:182
2024-01-23 01:01:37,592 MainThread INFO: Time Consumed:0.23920392990112305s
2024-01-23 01:01:37,592 MainThread INFO: Total Frames:28200s
  2%|▏         | 183/10000 [03:05<42:34,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12067.97349
Train_Epoch_Reward                2492.09959
Running_Training_Average_Rewards  10846.98324
Explore_Time                      0.00097
Train___Time                      0.08164
Eval____Time                      0.15413
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11795.26686
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.88816     1.90654     92.46505     87.07467
alpha_0                           0.91260      0.00013     0.91278      0.91242
Alpha_loss                        -0.61497     0.00099     -0.61404     -0.61623
Training/policy_loss              -2.55636     0.00454     -2.55004     -2.56201
Training/qf1_loss                 17119.46309  2132.10891  21125.19141  15158.24805
Training/qf2_loss                 17926.30234  2145.64758  21951.28320  15938.65625
Training/pf_norm                  0.11404      0.02486     0.14246      0.08256
Training/qf1_norm                 1874.85139   39.68443    1924.00391   1812.27747
Training/qf2_norm                 232.61264    4.72984     238.96687    225.61333
log_std/mean                      -0.12947     0.00002     -0.12946     -0.12952
log_probs/mean                    -2.73137     0.00489     -2.72447     -2.73783
mean/mean                         -0.00104     0.00004     -0.00100     -0.00110
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019364356994628906
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71127
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [28500]
collect time 0.000988006591796875
inner_dict_sum {'sac_diff0': 0.0002346038818359375, 'sac_diff1': 0.009299278259277344, 'sac_diff2': 0.008527994155883789, 'sac_diff3': 0.011178255081176758, 'sac_diff4': 0.007304191589355469, 'sac_diff5': 0.03394770622253418, 'sac_diff6': 0.00039958953857421875, 'all': 0.0708916187286377}
diff5_list [0.006777524948120117, 0.006742238998413086, 0.006591796875, 0.006501436233520508, 0.007334709167480469]
time3 0
time4 0.07166218757629395
time5 0.0717315673828125
time7 9.5367431640625e-07
gen_weight_change tensor(-23.9425)
policy weight change tensor(38.2501, grad_fn=<SumBackward0>)
time8 0.0019490718841552734
train_time 0.0833287239074707
eval time 0.14330124855041504
epoch last part time 9.775161743164062e-06
2024-01-23 01:01:37,845 MainThread INFO: EPOCH:183
2024-01-23 01:01:37,845 MainThread INFO: Time Consumed:0.2301018238067627s
2024-01-23 01:01:37,846 MainThread INFO: Total Frames:28350s
  2%|▏         | 184/10000 [03:05<42:14,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11945.62082
Train_Epoch_Reward                19404.61904
Running_Training_Average_Rewards  11098.63213
Explore_Time                      0.00098
Train___Time                      0.08333
Eval____Time                      0.14330
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11775.46782
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.63223     2.03976     89.47058     83.67051
alpha_0                           0.91214      0.00013     0.91233      0.91196
Alpha_loss                        -0.61861     0.00122     -0.61727     -0.62081
Training/policy_loss              -2.55326     0.00444     -2.54765     -2.56115
Training/qf1_loss                 15837.81367  1750.60338  18345.15039  12855.48145
Training/qf2_loss                 16714.87969  1765.40891  19230.48438  13696.99414
Training/pf_norm                  0.11325      0.01996     0.13575      0.08152
Training/qf1_norm                 2003.28403   43.83456    2030.22437   1916.10120
Training/qf2_norm                 224.89809    5.00720     229.27502    215.13815
log_std/mean                      -0.13682     0.00004     -0.13675     -0.13686
log_probs/mean                    -2.73441     0.00502     -2.72854     -2.74360
mean/mean                         0.00008      0.00011     0.00020      -0.00009
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019329309463500977
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71127
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [28650]
collect time 0.0009250640869140625
inner_dict_sum {'sac_diff0': 0.00022149085998535156, 'sac_diff1': 0.007142782211303711, 'sac_diff2': 0.008353471755981445, 'sac_diff3': 0.010427713394165039, 'sac_diff4': 0.0069332122802734375, 'sac_diff5': 0.033335208892822266, 'sac_diff6': 0.00039386749267578125, 'all': 0.06680774688720703}
diff5_list [0.007004737854003906, 0.006372928619384766, 0.006558895111083984, 0.006684064865112305, 0.006714582443237305]
time3 0
time4 0.06756234169006348
time5 0.06760692596435547
time7 7.152557373046875e-07
gen_weight_change tensor(-23.9425)
policy weight change tensor(38.2825, grad_fn=<SumBackward0>)
time8 0.0019321441650390625
train_time 0.0787663459777832
eval time 0.15671324729919434
epoch last part time 6.9141387939453125e-06
2024-01-23 01:01:38,107 MainThread INFO: EPOCH:184
2024-01-23 01:01:38,107 MainThread INFO: Time Consumed:0.238816499710083s
2024-01-23 01:01:38,107 MainThread INFO: Total Frames:28500s
  2%|▏         | 185/10000 [03:05<42:22,  3.86it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11826.44977
Train_Epoch_Reward                2124.52065
Running_Training_Average_Rewards  11004.47357
Explore_Time                      0.00090
Train___Time                      0.07877
Eval____Time                      0.15671
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11746.93826
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       85.45791     1.68911    87.02881     82.45893
alpha_0                           0.91169      0.00013    0.91187      0.91150
Alpha_loss                        -0.62198     0.00104    -0.62066     -0.62356
Training/policy_loss              -2.55969     0.00376    -2.55392     -2.56507
Training/qf1_loss                 14633.07617  961.56288  15880.40137  12941.34668
Training/qf2_loss                 15412.22520  979.05135  16674.36914  13684.89355
Training/pf_norm                  0.10657      0.04026    0.15913      0.05894
Training/qf1_norm                 1801.83206   41.22371   1837.37170   1723.56519
Training/qf2_norm                 231.71251    4.51360    235.90308    223.69443
log_std/mean                      -0.13413     0.00008    -0.13400     -0.13424
log_probs/mean                    -2.73436     0.00413    -2.72829     -2.74028
mean/mean                         -0.00064     0.00017    -0.00040     -0.00085
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018314123153686523
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71127
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [28800]
collect time 0.0008265972137451172
inside mustsac before update, task 0, sumup 71127
inside mustsac after update, task 0, sumup 70889
inner_dict_sum {'sac_diff0': 0.0002155303955078125, 'sac_diff1': 0.00698399543762207, 'sac_diff2': 0.008582115173339844, 'sac_diff3': 0.010769128799438477, 'sac_diff4': 0.007381439208984375, 'sac_diff5': 0.05061507225036621, 'sac_diff6': 0.0004398822784423828, 'all': 0.08498716354370117}
diff5_list [0.010872125625610352, 0.009518861770629883, 0.010303020477294922, 0.010382890701293945, 0.00953817367553711]
time3 0.0009095668792724609
time4 0.08591890335083008
time5 0.08597183227539062
time7 0.008952140808105469
gen_weight_change tensor(-23.8965)
policy weight change tensor(38.3229, grad_fn=<SumBackward0>)
time8 0.0018761157989501953
train_time 0.11507701873779297
eval time 0.11568355560302734
epoch last part time 6.67572021484375e-06
2024-01-23 01:01:38,363 MainThread INFO: EPOCH:185
2024-01-23 01:01:38,363 MainThread INFO: Time Consumed:0.233978271484375s
2024-01-23 01:01:38,363 MainThread INFO: Total Frames:28650s
  2%|▏         | 186/10000 [03:05<42:13,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11711.52484
Train_Epoch_Reward                5569.10529
Running_Training_Average_Rewards  10838.21950
Explore_Time                      0.00082
Train___Time                      0.11508
Eval____Time                      0.11568
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11665.04064
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       86.49138     1.63672     88.32219     84.34824
alpha_0                           0.91123      0.00013     0.91141      0.91105
Alpha_loss                        -0.62569     0.00089     -0.62421     -0.62681
Training/policy_loss              -2.55960     0.00938     -2.54568     -2.57473
Training/qf1_loss                 14894.94043  1080.72694  16668.98828  13481.04980
Training/qf2_loss                 15682.57812  1095.97422  17486.73828  14210.61719
Training/pf_norm                  0.12092      0.03015     0.15417      0.07608
Training/qf1_norm                 1848.55686   72.45832    1934.30005   1734.69177
Training/qf2_norm                 228.45094    6.76967     236.82971    217.61032
log_std/mean                      -0.12896     0.00203     -0.12659     -0.13195
log_probs/mean                    -2.73802     0.00719     -2.72939     -2.75114
mean/mean                         -0.00177     0.00113     -0.00038     -0.00381
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018615007400512695
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70889
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [28950]
collect time 0.0008614063262939453
inner_dict_sum {'sac_diff0': 0.00022268295288085938, 'sac_diff1': 0.007028102874755859, 'sac_diff2': 0.008589506149291992, 'sac_diff3': 0.010753631591796875, 'sac_diff4': 0.007550716400146484, 'sac_diff5': 0.03301811218261719, 'sac_diff6': 0.00040340423583984375, 'all': 0.0675661563873291}
diff5_list [0.006814718246459961, 0.006617069244384766, 0.0071086883544921875, 0.006216287612915039, 0.006261348724365234]
time3 0
time4 0.06840252876281738
time5 0.06845974922180176
time7 7.152557373046875e-07
gen_weight_change tensor(-23.8965)
policy weight change tensor(38.2044, grad_fn=<SumBackward0>)
time8 0.0019378662109375
train_time 0.07953429222106934
eval time 0.14838624000549316
epoch last part time 5.9604644775390625e-06
2024-01-23 01:01:38,616 MainThread INFO: EPOCH:186
2024-01-23 01:01:38,617 MainThread INFO: Time Consumed:0.2311861515045166s
2024-01-23 01:01:38,617 MainThread INFO: Total Frames:28800s
  2%|▏         | 187/10000 [03:06<41:59,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11701.58218
Train_Epoch_Reward                28157.21455
Running_Training_Average_Rewards  11689.99148
Explore_Time                      0.00086
Train___Time                      0.07953
Eval____Time                      0.14839
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11852.71462
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.53192     1.33499     90.54970     86.54945
alpha_0                           0.91077      0.00013     0.91096      0.91059
Alpha_loss                        -0.62868     0.00099     -0.62714     -0.62996
Training/policy_loss              -2.55535     0.00985     -2.54033     -2.56567
Training/qf1_loss                 16491.99688  1221.25010  18496.63672  14773.09375
Training/qf2_loss                 17304.09082  1229.24982  19327.07812  15583.37988
Training/pf_norm                  0.13436      0.02816     0.15944      0.08496
Training/qf1_norm                 1930.97024   26.49784    1976.13245   1892.97620
Training/qf2_norm                 234.82316    3.39203     239.99638    229.78964
log_std/mean                      -0.14367     0.00032     -0.14318     -0.14403
log_probs/mean                    -2.73388     0.01063     -2.71739     -2.74496
mean/mean                         -0.00320     0.00009     -0.00304     -0.00328
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018869876861572266
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70889
epoch first part time 2.86102294921875e-06
replay_buffer._size: [29100]
collect time 0.0008385181427001953
inner_dict_sum {'sac_diff0': 0.00023221969604492188, 'sac_diff1': 0.007422208786010742, 'sac_diff2': 0.00842595100402832, 'sac_diff3': 0.010386943817138672, 'sac_diff4': 0.007123708724975586, 'sac_diff5': 0.03354239463806152, 'sac_diff6': 0.0004119873046875, 'all': 0.06754541397094727}
diff5_list [0.007842063903808594, 0.007013559341430664, 0.006333589553833008, 0.006329536437988281, 0.0060236454010009766]
time3 0
time4 0.06835126876831055
time5 0.06839871406555176
time7 7.152557373046875e-07
gen_weight_change tensor(-23.8965)
policy weight change tensor(38.0555, grad_fn=<SumBackward0>)
time8 0.0019230842590332031
train_time 0.07943034172058105
eval time 0.15779876708984375
epoch last part time 7.152557373046875e-06
2024-01-23 01:01:38,879 MainThread INFO: EPOCH:187
2024-01-23 01:01:38,879 MainThread INFO: Time Consumed:0.24048137664794922s
2024-01-23 01:01:38,879 MainThread INFO: Total Frames:28950s
  2%|▏         | 188/10000 [03:06<42:16,  3.87it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11704.65862
Train_Epoch_Reward                15332.30723
Running_Training_Average_Rewards  12087.12427
Explore_Time                      0.00083
Train___Time                      0.07943
Eval____Time                      0.15780
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11793.00461
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.72896     1.72809    89.83377     85.69096
alpha_0                           0.91032      0.00013    0.91050      0.91014
Alpha_loss                        -0.63221     0.00129    -0.63021     -0.63331
Training/policy_loss              -2.55178     0.00538    -2.54604     -2.56071
Training/qf1_loss                 15664.35801  896.33094  17254.83984  14719.10449
Training/qf2_loss                 16507.95195  899.83910  18110.51953  15580.81836
Training/pf_norm                  0.14945      0.02004    0.17276      0.12267
Training/qf1_norm                 1856.34800   31.71796   1897.61584   1819.00757
Training/qf2_norm                 226.71679    4.22431    231.88757    221.72890
log_std/mean                      -0.13661     0.00028    -0.13622     -0.13701
log_probs/mean                    -2.73560     0.00605    -2.72869     -2.74538
mean/mean                         -0.00241     0.00010    -0.00224     -0.00250
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018532276153564453
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70889
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [29250]
collect time 0.0008535385131835938
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.007157087326049805, 'sac_diff2': 0.00823974609375, 'sac_diff3': 0.010941028594970703, 'sac_diff4': 0.007026195526123047, 'sac_diff5': 0.03210186958312988, 'sac_diff6': 0.0003917217254638672, 'all': 0.06606817245483398}
diff5_list [0.007569074630737305, 0.0064275264739990234, 0.0062258243560791016, 0.0060961246490478516, 0.0057833194732666016]
time3 0
time4 0.06685376167297363
time5 0.06690454483032227
time7 4.76837158203125e-07
gen_weight_change tensor(-23.8965)
policy weight change tensor(37.9091, grad_fn=<SumBackward0>)
time8 0.0018877983093261719
train_time 0.07775735855102539
eval time 0.15988779067993164
epoch last part time 6.67572021484375e-06
2024-01-23 01:01:39,142 MainThread INFO: EPOCH:188
2024-01-23 01:01:39,142 MainThread INFO: Time Consumed:0.2409203052520752s
2024-01-23 01:01:39,142 MainThread INFO: Total Frames:29100s
  2%|▏         | 189/10000 [03:06<42:29,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11720.74586
Train_Epoch_Reward                10400.05084
Running_Training_Average_Rewards  12184.90279
Explore_Time                      0.00085
Train___Time                      0.07776
Eval____Time                      0.15989
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11810.21036
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.58011     3.63135     94.36659     83.20471
alpha_0                           0.90986      0.00013     0.91005      0.90968
Alpha_loss                        -0.63534     0.00119     -0.63359     -0.63659
Training/policy_loss              -2.54739     0.00943     -2.53776     -2.56543
Training/qf1_loss                 16089.57148  2252.48592  19878.15234  13303.75781
Training/qf2_loss                 16935.24102  2282.92065  20771.64453  14106.79492
Training/pf_norm                  0.09535      0.02915     0.14729      0.06676
Training/qf1_norm                 1939.88169   75.46939    2058.99414   1832.52246
Training/qf2_norm                 228.05140    8.89163     242.10016    214.80573
log_std/mean                      -0.13451     0.00026     -0.13414     -0.13485
log_probs/mean                    -2.73307     0.01027     -2.72246     -2.75262
mean/mean                         -0.00142     0.00011     -0.00125     -0.00155
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01857447624206543
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70889
epoch first part time 2.86102294921875e-06
replay_buffer._size: [29400]
collect time 0.000865936279296875
inner_dict_sum {'sac_diff0': 0.00019812583923339844, 'sac_diff1': 0.0069658756256103516, 'sac_diff2': 0.008202552795410156, 'sac_diff3': 0.00996851921081543, 'sac_diff4': 0.006818294525146484, 'sac_diff5': 0.032486915588378906, 'sac_diff6': 0.00039696693420410156, 'all': 0.06503725051879883}
diff5_list [0.006523847579956055, 0.006242275238037109, 0.006634950637817383, 0.006184101104736328, 0.006901741027832031]
time3 0
time4 0.06583428382873535
time5 0.06588006019592285
time7 7.152557373046875e-07
gen_weight_change tensor(-23.8965)
policy weight change tensor(37.7841, grad_fn=<SumBackward0>)
time8 0.0021750926971435547
train_time 0.07702302932739258
eval time 0.15389728546142578
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:39,398 MainThread INFO: EPOCH:189
2024-01-23 01:01:39,399 MainThread INFO: Time Consumed:0.23413705825805664s
2024-01-23 01:01:39,399 MainThread INFO: Total Frames:29250s
  2%|▏         | 190/10000 [03:07<42:18,  3.86it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11752.57662
Train_Epoch_Reward                12234.67069
Running_Training_Average_Rewards  12414.96021
Explore_Time                      0.00086
Train___Time                      0.07702
Eval____Time                      0.15390
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11839.76225
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.43169     1.36413    90.00392     85.92717
alpha_0                           0.90941      0.00013    0.90959      0.90923
Alpha_loss                        -0.63899     0.00091    -0.63796     -0.64050
Training/policy_loss              -2.55145     0.00301    -2.54629     -2.55499
Training/qf1_loss                 15828.24688  905.97528  16934.48438  14252.08887
Training/qf2_loss                 16721.89180  908.02684  17816.13672  15130.06055
Training/pf_norm                  0.13877      0.02190    0.16847      0.11223
Training/qf1_norm                 2031.39382   42.83096   2113.11523   1994.96692
Training/qf2_norm                 227.68049    3.43477    234.14397    223.82942
log_std/mean                      -0.13373     0.00022    -0.13345     -0.13404
log_probs/mean                    -2.73600     0.00316    -2.73060     -2.73935
mean/mean                         -0.00307     0.00006    -0.00297     -0.00313
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018709421157836914
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70889
epoch first part time 3.337860107421875e-06
replay_buffer._size: [29550]
collect time 0.000820159912109375
inside mustsac before update, task 0, sumup 70889
inside mustsac after update, task 0, sumup 70803
inner_dict_sum {'sac_diff0': 0.00021576881408691406, 'sac_diff1': 0.0071239471435546875, 'sac_diff2': 0.008377790451049805, 'sac_diff3': 0.010851383209228516, 'sac_diff4': 0.008038043975830078, 'sac_diff5': 0.05176854133605957, 'sac_diff6': 0.00042557716369628906, 'all': 0.08680105209350586}
diff5_list [0.01063084602355957, 0.010815858840942383, 0.010320186614990234, 0.010025978088378906, 0.009975671768188477]
time3 0.000881195068359375
time4 0.08771443367004395
time5 0.08776974678039551
time7 0.009073495864868164
gen_weight_change tensor(-23.7394)
policy weight change tensor(37.7813, grad_fn=<SumBackward0>)
time8 0.0026390552520751953
train_time 0.11825251579284668
eval time 0.10805439949035645
epoch last part time 6.67572021484375e-06
2024-01-23 01:01:39,650 MainThread INFO: EPOCH:190
2024-01-23 01:01:39,650 MainThread INFO: Time Consumed:0.22953510284423828s
2024-01-23 01:01:39,651 MainThread INFO: Total Frames:29400s
  2%|▏         | 191/10000 [03:07<42:08,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11791.06792
Train_Epoch_Reward                8234.20349
Running_Training_Average_Rewards  12004.45775
Explore_Time                      0.00082
Train___Time                      0.11825
Eval____Time                      0.10805
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11840.30134
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.75335     2.39671    91.38607     84.30014
alpha_0                           0.90895      0.00013    0.90913      0.90877
Alpha_loss                        -0.64219     0.00100    -0.64103     -0.64395
Training/policy_loss              -2.55040     0.00573    -2.54149     -2.55723
Training/qf1_loss                 15522.99297  869.23052  16480.42188  13920.43555
Training/qf2_loss                 16415.41367  931.97363  17436.60742  14689.36914
Training/pf_norm                  0.12742      0.02156    0.16474      0.10635
Training/qf1_norm                 1995.59211   122.76202  2152.58179   1780.38049
Training/qf2_norm                 225.93593    8.84598    237.91481    215.64308
log_std/mean                      -0.13312     0.00487    -0.12759     -0.14022
log_probs/mean                    -2.73424     0.00299    -2.73022     -2.73849
mean/mean                         -0.00073     0.00050    -0.00004     -0.00146
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019311189651489258
epoch last part time3 0.0028066635131835938
inside rlalgo, task 0, sumup 70803
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [29700]
collect time 0.0008769035339355469
inner_dict_sum {'sac_diff0': 0.00020432472229003906, 'sac_diff1': 0.008087158203125, 'sac_diff2': 0.009640693664550781, 'sac_diff3': 0.012678861618041992, 'sac_diff4': 0.008783817291259766, 'sac_diff5': 0.04033684730529785, 'sac_diff6': 0.0004611015319824219, 'all': 0.08019280433654785}
diff5_list [0.008073568344116211, 0.008077621459960938, 0.008066415786743164, 0.008017778396606445, 0.008101463317871094]
time3 0
time4 0.08110451698303223
time5 0.08115768432617188
time7 4.76837158203125e-07
gen_weight_change tensor(-23.7394)
policy weight change tensor(37.7460, grad_fn=<SumBackward0>)
time8 0.0021414756774902344
train_time 0.09290528297424316
eval time 0.1914839744567871
epoch last part time 8.58306884765625e-06
2024-01-23 01:01:39,963 MainThread INFO: EPOCH:191
2024-01-23 01:01:39,963 MainThread INFO: Time Consumed:0.2877364158630371s
2024-01-23 01:01:39,964 MainThread INFO: Total Frames:29550s
  2%|▏         | 192/10000 [03:07<44:40,  3.66it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11784.06621
Train_Epoch_Reward                13680.77316
Running_Training_Average_Rewards  12172.12955
Explore_Time                      0.00087
Train___Time                      0.09291
Eval____Time                      0.19148
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11721.95533
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.37756     1.54763     90.24686     85.67134
alpha_0                           0.90850      0.00013     0.90868      0.90832
Alpha_loss                        -0.64541     0.00122     -0.64387     -0.64670
Training/policy_loss              -2.55076     0.00587     -2.54271     -2.56099
Training/qf1_loss                 16316.02578  1008.88562  17706.79297  14955.00293
Training/qf2_loss                 17148.42168  1025.76187  18567.33789  15777.93750
Training/pf_norm                  0.11543      0.01553     0.13469      0.09665
Training/qf1_norm                 1921.52461   41.47681    1980.45471   1860.81836
Training/qf2_norm                 236.26613    4.00507     241.27339    229.38222
log_std/mean                      -0.12500     0.00004     -0.12495     -0.12505
log_probs/mean                    -2.73271     0.00649     -2.72360     -2.74391
mean/mean                         0.00022      0.00019     0.00051      -0.00003
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018589258193969727
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70803
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [29850]
collect time 0.0009531974792480469
inner_dict_sum {'sac_diff0': 0.00034427642822265625, 'sac_diff1': 0.008668184280395508, 'sac_diff2': 0.009769201278686523, 'sac_diff3': 0.010912656784057617, 'sac_diff4': 0.0077288150787353516, 'sac_diff5': 0.03164482116699219, 'sac_diff6': 0.0003783702850341797, 'all': 0.06944632530212402}
diff5_list [0.006535530090332031, 0.006856203079223633, 0.0060138702392578125, 0.006090879440307617, 0.006148338317871094]
time3 0
time4 0.0702211856842041
time5 0.07027745246887207
time7 7.152557373046875e-07
gen_weight_change tensor(-23.7394)
policy weight change tensor(37.7509, grad_fn=<SumBackward0>)
time8 0.0023577213287353516
train_time 0.0850067138671875
eval time 0.15950989723205566
epoch last part time 6.9141387939453125e-06
2024-01-23 01:01:40,234 MainThread INFO: EPOCH:192
2024-01-23 01:01:40,234 MainThread INFO: Time Consumed:0.24823427200317383s
2024-01-23 01:01:40,234 MainThread INFO: Total Frames:29700s
  2%|▏         | 193/10000 [03:07<44:34,  3.67it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11778.40300
Train_Epoch_Reward                6817.45360
Running_Training_Average_Rewards  12109.81650
Explore_Time                      0.00095
Train___Time                      0.08501
Eval____Time                      0.15951
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11738.63475
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       86.45778     3.74992     90.65780     81.07994
alpha_0                           0.90804      0.00013     0.90823      0.90786
Alpha_loss                        -0.64901     0.00071     -0.64779     -0.64981
Training/policy_loss              -2.55012     0.00332     -2.54455     -2.55447
Training/qf1_loss                 14543.93203  2059.84585  16923.62109  12053.18359
Training/qf2_loss                 15460.65215  2094.39482  17887.28906  12938.10645
Training/pf_norm                  0.13326      0.03667     0.17393      0.07377
Training/qf1_norm                 2082.40171   86.78791    2192.41895   1960.42151
Training/qf2_norm                 227.33803    9.45115     238.06552    213.79996
log_std/mean                      -0.12870     0.00003     -0.12864     -0.12874
log_probs/mean                    -2.73502     0.00334     -2.72942     -2.73955
mean/mean                         0.00147      0.00004     0.00152      0.00141
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019845008850097656
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70803
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [30000]
collect time 0.001035451889038086
inner_dict_sum {'sac_diff0': 0.0002086162567138672, 'sac_diff1': 0.007045269012451172, 'sac_diff2': 0.007844209671020508, 'sac_diff3': 0.010186910629272461, 'sac_diff4': 0.006917476654052734, 'sac_diff5': 0.032437801361083984, 'sac_diff6': 0.0003867149353027344, 'all': 0.06502699851989746}
diff5_list [0.0068509578704833984, 0.006255149841308594, 0.006688594818115234, 0.006420135498046875, 0.006222963333129883]
time3 0
time4 0.06579828262329102
time5 0.0658416748046875
time7 7.152557373046875e-07
gen_weight_change tensor(-23.7394)
policy weight change tensor(37.7562, grad_fn=<SumBackward0>)
time8 0.00197601318359375
train_time 0.07725119590759277
eval time 0.15233445167541504
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:40,490 MainThread INFO: EPOCH:193
2024-01-23 01:01:40,490 MainThread INFO: Time Consumed:0.23291349411010742s
2024-01-23 01:01:40,490 MainThread INFO: Total Frames:29850s
  2%|▏         | 194/10000 [03:08<43:43,  3.74it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11773.28611
Train_Epoch_Reward                7702.42712
Running_Training_Average_Rewards  11151.07881
Explore_Time                      0.00103
Train___Time                      0.07725
Eval____Time                      0.15233
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11724.29894
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       85.71798     1.13263    87.58723     84.30801
alpha_0                           0.90759      0.00013    0.90777      0.90741
Alpha_loss                        -0.65236     0.00072    -0.65133     -0.65316
Training/policy_loss              -2.54995     0.00322    -2.54441     -2.55346
Training/qf1_loss                 13973.97070  743.00747  15404.77344  13312.33398
Training/qf2_loss                 14971.55703  752.16666  16419.88672  14287.91211
Training/pf_norm                  0.09900      0.01962    0.12250      0.06495
Training/qf1_norm                 2145.99600   34.31986   2193.53027   2100.94165
Training/qf2_norm                 227.43505    2.90915    232.26173    223.88287
log_std/mean                      -0.13016     0.00004    -0.13011     -0.13022
log_probs/mean                    -2.73480     0.00324    -2.72921     -2.73813
mean/mean                         -0.00044     0.00002    -0.00041     -0.00047
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01873040199279785
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70803
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [30150]
collect time 0.0009369850158691406
inner_dict_sum {'sac_diff0': 0.00021219253540039062, 'sac_diff1': 0.00691986083984375, 'sac_diff2': 0.008268356323242188, 'sac_diff3': 0.010370254516601562, 'sac_diff4': 0.006914377212524414, 'sac_diff5': 0.03187084197998047, 'sac_diff6': 0.0004062652587890625, 'all': 0.06496214866638184}
diff5_list [0.0065572261810302734, 0.006036043167114258, 0.006650209426879883, 0.006443500518798828, 0.0061838626861572266]
time3 0
time4 0.06579899787902832
time5 0.06585001945495605
time7 7.152557373046875e-07
gen_weight_change tensor(-23.7394)
policy weight change tensor(37.8172, grad_fn=<SumBackward0>)
time8 0.001863718032836914
train_time 0.07676291465759277
eval time 0.15191984176635742
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:40,744 MainThread INFO: EPOCH:194
2024-01-23 01:01:40,745 MainThread INFO: Time Consumed:0.23203778266906738s
2024-01-23 01:01:40,745 MainThread INFO: Total Frames:30000s
  2%|▏         | 195/10000 [03:08<43:06,  3.79it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11777.56968
Train_Epoch_Reward                18381.79552
Running_Training_Average_Rewards  11636.11656
Explore_Time                      0.00093
Train___Time                      0.07676
Eval____Time                      0.15192
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11789.77391
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.96367     3.60861     95.07217     83.88216
alpha_0                           0.90713      0.00013     0.90732      0.90695
Alpha_loss                        -0.65640     0.00148     -0.65460     -0.65841
Training/policy_loss              -2.55372     0.00992     -2.53700     -2.56593
Training/qf1_loss                 16121.13691  2815.70506  20606.91016  12014.85156
Training/qf2_loss                 17156.74805  2855.53684  21709.74609  12990.23242
Training/pf_norm                  0.08917      0.01825     0.11921      0.06746
Training/qf1_norm                 2199.58164   93.09746    2351.33301   2067.55835
Training/qf2_norm                 232.52699    9.10418     247.92738    219.75935
log_std/mean                      -0.11881     0.00009     -0.11870     -0.11897
log_probs/mean                    -2.74173     0.01096     -2.72327     -2.75546
mean/mean                         0.00089      0.00003     0.00092      0.00085
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019062519073486328
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70803
epoch first part time 2.86102294921875e-06
replay_buffer._size: [30300]
collect time 0.0009999275207519531
inside mustsac before update, task 0, sumup 70803
inside mustsac after update, task 0, sumup 71274
inner_dict_sum {'sac_diff0': 0.00022029876708984375, 'sac_diff1': 0.006837368011474609, 'sac_diff2': 0.007757663726806641, 'sac_diff3': 0.010454893112182617, 'sac_diff4': 0.007264614105224609, 'sac_diff5': 0.04939985275268555, 'sac_diff6': 0.000392913818359375, 'all': 0.08232760429382324}
diff5_list [0.01118159294128418, 0.009673833847045898, 0.009717702865600586, 0.009448766708374023, 0.00937795639038086]
time3 0.0008502006530761719
time4 0.08314013481140137
time5 0.0831906795501709
time7 0.009091377258300781
gen_weight_change tensor(-23.5454)
policy weight change tensor(37.8081, grad_fn=<SumBackward0>)
time8 0.001901388168334961
train_time 0.11198258399963379
eval time 0.11540365219116211
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:40,998 MainThread INFO: EPOCH:195
2024-01-23 01:01:40,998 MainThread INFO: Time Consumed:0.23078584671020508s
2024-01-23 01:01:40,999 MainThread INFO: Total Frames:30150s
  2%|▏         | 196/10000 [03:08<42:33,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11800.27268
Train_Epoch_Reward                16519.95798
Running_Training_Average_Rewards  11900.54399
Explore_Time                      0.00099
Train___Time                      0.11198
Eval____Time                      0.11540
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11892.07065
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       85.95919     2.71317     88.72906     81.35779
alpha_0                           0.90668      0.00013     0.90686      0.90650
Alpha_loss                        -0.65913     0.00099     -0.65776     -0.66054
Training/policy_loss              -2.54885     0.00669     -2.53815     -2.55834
Training/qf1_loss                 13865.90273  1239.53123  15238.57617  11551.38965
Training/qf2_loss                 14787.09922  1293.92160  16265.20801  12409.05762
Training/pf_norm                  0.12426      0.01622     0.15204      0.10760
Training/qf1_norm                 2039.96736   124.28482   2226.08984   1932.02856
Training/qf2_norm                 225.67733    7.77337     235.44637    212.06947
log_std/mean                      -0.12744     0.00633     -0.11821     -0.13512
log_probs/mean                    -2.73517     0.00514     -2.72702     -2.74325
mean/mean                         -0.00008     0.00045     0.00073      -0.00067
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018296480178833008
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71274
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [30450]
collect time 0.0009446144104003906
inner_dict_sum {'sac_diff0': 0.00022530555725097656, 'sac_diff1': 0.006763935089111328, 'sac_diff2': 0.007747650146484375, 'sac_diff3': 0.010093212127685547, 'sac_diff4': 0.0067729949951171875, 'sac_diff5': 0.03302001953125, 'sac_diff6': 0.0003960132598876953, 'all': 0.06501913070678711}
diff5_list [0.006977558135986328, 0.0059740543365478516, 0.005963563919067383, 0.0066068172454833984, 0.007498025894165039]
time3 0
time4 0.06583976745605469
time5 0.0658872127532959
time7 7.152557373046875e-07
gen_weight_change tensor(-23.5454)
policy weight change tensor(37.9051, grad_fn=<SumBackward0>)
time8 0.0019505023956298828
train_time 0.07684135437011719
eval time 0.15465497970581055
epoch last part time 6.9141387939453125e-06
2024-01-23 01:01:41,255 MainThread INFO: EPOCH:196
2024-01-23 01:01:41,255 MainThread INFO: Time Consumed:0.23482799530029297s
2024-01-23 01:01:41,255 MainThread INFO: Total Frames:30300s
  2%|▏         | 197/10000 [03:08<42:23,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11821.22752
Train_Epoch_Reward                13621.92032
Running_Training_Average_Rewards  11608.32306
Explore_Time                      0.00094
Train___Time                      0.07684
Eval____Time                      0.15465
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12062.26304
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.90883     1.96987     89.56993     84.28850
alpha_0                           0.90623      0.00013     0.90641      0.90605
Alpha_loss                        -0.66221     0.00131     -0.66052     -0.66397
Training/policy_loss              -2.54490     0.00477     -2.53573     -2.54883
Training/qf1_loss                 16255.09316  1058.75782  17820.11328  14901.92188
Training/qf2_loss                 17348.07949  1070.48098  18928.70703  15987.23145
Training/pf_norm                  0.12740      0.01537     0.15022      0.10280
Training/qf1_norm                 2329.62642   47.34726    2369.09741   2244.23486
Training/qf2_norm                 234.89561    5.05074     239.16496    225.57976
log_std/mean                      -0.12969     0.00014     -0.12951     -0.12990
log_probs/mean                    -2.73219     0.00545     -2.72180     -2.73680
mean/mean                         0.00108      0.00009     0.00121      0.00095
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018347501754760742
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71274
epoch first part time 2.86102294921875e-06
replay_buffer._size: [30600]
collect time 0.0009431838989257812
inner_dict_sum {'sac_diff0': 0.000213623046875, 'sac_diff1': 0.007016181945800781, 'sac_diff2': 0.008476495742797852, 'sac_diff3': 0.011463165283203125, 'sac_diff4': 0.0073812007904052734, 'sac_diff5': 0.03328442573547363, 'sac_diff6': 0.00039577484130859375, 'all': 0.06823086738586426}
diff5_list [0.006621837615966797, 0.00688934326171875, 0.006323337554931641, 0.007147073745727539, 0.006302833557128906]
time3 0
time4 0.06903767585754395
time5 0.06908631324768066
time7 9.5367431640625e-07
gen_weight_change tensor(-23.5454)
policy weight change tensor(38.0302, grad_fn=<SumBackward0>)
time8 0.0019123554229736328
train_time 0.08007001876831055
eval time 0.14893770217895508
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:41,509 MainThread INFO: EPOCH:197
2024-01-23 01:01:41,510 MainThread INFO: Time Consumed:0.23238205909729004s
2024-01-23 01:01:41,510 MainThread INFO: Total Frames:30450s
  2%|▏         | 198/10000 [03:09<42:09,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11852.10305
Train_Epoch_Reward                10403.98780
Running_Training_Average_Rewards  11358.81041
Explore_Time                      0.00094
Train___Time                      0.08007
Eval____Time                      0.14894
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12101.75997
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.26584     3.30497     90.25475     81.12837
alpha_0                           0.90577      0.00013     0.90596      0.90559
Alpha_loss                        -0.66645     0.00090     -0.66519     -0.66750
Training/policy_loss              -2.54509     0.00256     -2.54152     -2.54826
Training/qf1_loss                 15086.62520  1669.35477  16604.76367  12291.74023
Training/qf2_loss                 16070.08027  1702.92692  17622.06836  13214.87695
Training/pf_norm                  0.13027      0.04432     0.20808      0.08796
Training/qf1_norm                 2136.41943   81.14062    2217.21411   1993.40479
Training/qf2_norm                 226.43073    8.27101     233.89313    211.10405
log_std/mean                      -0.12972     0.00023     -0.12941     -0.13004
log_probs/mean                    -2.74093     0.00268     -2.73752     -2.74475
mean/mean                         -0.00070     0.00008     -0.00060     -0.00079
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019037485122680664
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71274
epoch first part time 2.86102294921875e-06
replay_buffer._size: [30750]
collect time 0.0009357929229736328
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.007227420806884766, 'sac_diff2': 0.008430004119873047, 'sac_diff3': 0.010729551315307617, 'sac_diff4': 0.0069997310638427734, 'sac_diff5': 0.03224301338195801, 'sac_diff6': 0.00041937828063964844, 'all': 0.06626343727111816}
diff5_list [0.006462574005126953, 0.007169246673583984, 0.006531715393066406, 0.006087064743041992, 0.005992412567138672]
time3 0
time4 0.06710028648376465
time5 0.06715154647827148
time7 4.76837158203125e-07
gen_weight_change tensor(-23.5454)
policy weight change tensor(38.1348, grad_fn=<SumBackward0>)
time8 0.0018515586853027344
train_time 0.07819795608520508
eval time 0.149766206741333
epoch last part time 6.9141387939453125e-06
2024-01-23 01:01:41,763 MainThread INFO: EPOCH:198
2024-01-23 01:01:41,764 MainThread INFO: Time Consumed:0.23128604888916016s
2024-01-23 01:01:41,764 MainThread INFO: Total Frames:30600s
  2%|▏         | 199/10000 [03:09<41:58,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11882.63367
Train_Epoch_Reward                12263.26300
Running_Training_Average_Rewards  11444.15479
Explore_Time                      0.00093
Train___Time                      0.07820
Eval____Time                      0.14977
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12115.51649
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.67428     2.06876    91.32526     85.19650
alpha_0                           0.90532      0.00013    0.90550      0.90514
Alpha_loss                        -0.66895     0.00114    -0.66718     -0.67054
Training/policy_loss              -2.54166     0.00641    -2.53517     -2.54974
Training/qf1_loss                 15411.17168  895.63041  16398.13477  14040.35742
Training/qf2_loss                 16394.03574  921.31871  17422.56641  14974.68555
Training/pf_norm                  0.11493      0.01948    0.13665      0.09107
Training/qf1_norm                 2136.68247   60.18879   2221.19287   2035.27417
Training/qf2_norm                 234.65724    5.31464    241.42836    225.69089
log_std/mean                      -0.12862     0.00015    -0.12841     -0.12884
log_probs/mean                    -2.73212     0.00705    -2.72550     -2.74133
mean/mean                         -0.00110     0.00008    -0.00097     -0.00120
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019413232803344727
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71274
epoch first part time 1.3828277587890625e-05
replay_buffer._size: [30900]
collect time 0.0010650157928466797
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.0065343379974365234, 'sac_diff2': 0.007818937301635742, 'sac_diff3': 0.010264396667480469, 'sac_diff4': 0.006823062896728516, 'sac_diff5': 0.0320277214050293, 'sac_diff6': 0.000400543212890625, 'all': 0.06408190727233887}
diff5_list [0.006346464157104492, 0.0059506893157958984, 0.0058650970458984375, 0.006274223327636719, 0.00759124755859375]
time3 0
time4 0.06486392021179199
time5 0.06491255760192871
time7 7.152557373046875e-07
gen_weight_change tensor(-23.5454)
policy weight change tensor(38.2550, grad_fn=<SumBackward0>)
time8 0.0019350051879882812
train_time 0.07581043243408203
eval time 0.14968538284301758
epoch last part time 5.9604644775390625e-06
2024-01-23 01:01:42,015 MainThread INFO: EPOCH:199
2024-01-23 01:01:42,016 MainThread INFO: Time Consumed:0.22894048690795898s
2024-01-23 01:01:42,016 MainThread INFO: Total Frames:30750s
  2%|▏         | 200/10000 [03:09<41:41,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11909.00389
Train_Epoch_Reward                21495.00987
Running_Training_Average_Rewards  11957.45567
Explore_Time                      0.00106
Train___Time                      0.07581
Eval____Time                      0.14969
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12103.46443
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.42014     1.36286    88.86203     85.03642
alpha_0                           0.90487      0.00013    0.90505      0.90469
Alpha_loss                        -0.67271     0.00112    -0.67085     -0.67423
Training/policy_loss              -2.54600     0.00551    -2.53940     -2.55344
Training/qf1_loss                 14781.96445  836.87230  16165.77637  13776.14062
Training/qf2_loss                 15931.64434  838.98799  17306.19141  14905.35449
Training/pf_norm                  0.12061      0.01804    0.13712      0.08650
Training/qf1_norm                 2408.80645   31.72359   2442.14258   2354.92920
Training/qf2_norm                 231.25818    3.48199    235.00032    225.19855
log_std/mean                      -0.11925     0.00016    -0.11907     -0.11951
log_probs/mean                    -2.73606     0.00607    -2.72932     -2.74458
mean/mean                         0.00031      0.00002    0.00033      0.00027
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018584012985229492
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71274
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [31050]
collect time 0.0009465217590332031
inside mustsac before update, task 0, sumup 71274
inside mustsac after update, task 0, sumup 70535
inner_dict_sum {'sac_diff0': 0.00021696090698242188, 'sac_diff1': 0.008032798767089844, 'sac_diff2': 0.009532690048217773, 'sac_diff3': 0.012067317962646484, 'sac_diff4': 0.008301496505737305, 'sac_diff5': 0.05714869499206543, 'sac_diff6': 0.0004425048828125, 'all': 0.09574246406555176}
diff5_list [0.010219812393188477, 0.012660026550292969, 0.011495113372802734, 0.011385440826416016, 0.011388301849365234]
time3 0.0009427070617675781
time4 0.09670639038085938
time5 0.09676194190979004
time7 0.009058713912963867
gen_weight_change tensor(-23.2489)
policy weight change tensor(38.1889, grad_fn=<SumBackward0>)
time8 0.002772808074951172
train_time 0.12791824340820312
eval time 0.12141895294189453
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:42,290 MainThread INFO: EPOCH:200
2024-01-23 01:01:42,291 MainThread INFO: Time Consumed:0.2526412010192871s
2024-01-23 01:01:42,291 MainThread INFO: Total Frames:30900s
  2%|▏         | 201/10000 [03:09<42:49,  3.81it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11934.01942
Train_Epoch_Reward                16140.18557
Running_Training_Average_Rewards  12024.82902
Explore_Time                      0.00094
Train___Time                      0.12792
Eval____Time                      0.12142
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12090.45672
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.14282     1.64322    88.81869     84.75307
alpha_0                           0.90441      0.00013    0.90460      0.90423
Alpha_loss                        -0.67615     0.00145    -0.67421     -0.67851
Training/policy_loss              -2.54747     0.00524    -2.54244     -2.55705
Training/qf1_loss                 14267.38008  884.24870  15267.77148  12914.13477
Training/qf2_loss                 15335.48047  909.42958  16427.53125  13984.82617
Training/pf_norm                  0.15039      0.02262    0.17644      0.11337
Training/qf1_norm                 2266.84043   104.52607  2425.74292   2151.09839
Training/qf2_norm                 232.90463    7.94149    242.64546    219.38779
log_std/mean                      -0.14119     0.00648    -0.12831     -0.14576
log_probs/mean                    -2.73672     0.00534    -2.73090     -2.74679
mean/mean                         0.00008      0.00073    0.00150      -0.00048
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018889427185058594
epoch last part time3 0.0031311511993408203
inside rlalgo, task 0, sumup 70535
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [31200]
collect time 0.0009744167327880859
inner_dict_sum {'sac_diff0': 0.00022912025451660156, 'sac_diff1': 0.007568836212158203, 'sac_diff2': 0.008791446685791016, 'sac_diff3': 0.011227130889892578, 'sac_diff4': 0.007791996002197266, 'sac_diff5': 0.036545515060424805, 'sac_diff6': 0.0004367828369140625, 'all': 0.07259082794189453}
diff5_list [0.009310722351074219, 0.007033824920654297, 0.006789684295654297, 0.007081031799316406, 0.006330251693725586]
time3 0
time4 0.07338166236877441
time5 0.0734400749206543
time7 7.152557373046875e-07
gen_weight_change tensor(-23.2489)
policy weight change tensor(38.3630, grad_fn=<SumBackward0>)
time8 0.0019750595092773438
train_time 0.0848548412322998
eval time 0.15088868141174316
epoch last part time 1.8596649169921875e-05
2024-01-23 01:01:42,555 MainThread INFO: EPOCH:201
2024-01-23 01:01:42,555 MainThread INFO: Time Consumed:0.23919677734375s
2024-01-23 01:01:42,556 MainThread INFO: Total Frames:31050s
  2%|▏         | 202/10000 [03:10<42:46,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11988.50959
Train_Epoch_Reward                10498.18124
Running_Training_Average_Rewards  11809.24696
Explore_Time                      0.00097
Train___Time                      0.08485
Eval____Time                      0.15089
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12266.85700
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.61209     1.59850     89.36550     85.06852
alpha_0                           0.90396      0.00013     0.90414      0.90378
Alpha_loss                        -0.67967     0.00131     -0.67850     -0.68168
Training/policy_loss              -2.54847     0.00498     -2.54094     -2.55389
Training/qf1_loss                 15120.73145  1406.12638  17118.53516  13493.85645
Training/qf2_loss                 16130.01680  1420.89635  18130.86719  14498.46094
Training/pf_norm                  0.11397      0.01925     0.14915      0.09586
Training/qf1_norm                 2197.65586   47.47315    2245.64478   2113.86597
Training/qf2_norm                 233.32804    4.16167     237.74284    226.64917
log_std/mean                      -0.12548     0.00029     -0.12511     -0.12593
log_probs/mean                    -2.73825     0.00568     -2.72990     -2.74476
mean/mean                         0.00020      0.00011     0.00034      0.00006
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01848626136779785
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70535
epoch first part time 2.384185791015625e-06
replay_buffer._size: [31350]
collect time 0.00089263916015625
inner_dict_sum {'sac_diff0': 0.00021266937255859375, 'sac_diff1': 0.0076944828033447266, 'sac_diff2': 0.008870363235473633, 'sac_diff3': 0.010781049728393555, 'sac_diff4': 0.007433176040649414, 'sac_diff5': 0.032913923263549805, 'sac_diff6': 0.00040650367736816406, 'all': 0.06831216812133789}
diff5_list [0.006546974182128906, 0.006119728088378906, 0.006239414215087891, 0.007072925567626953, 0.0069348812103271484]
time3 0
time4 0.06918597221374512
time5 0.06923651695251465
time7 7.152557373046875e-07
gen_weight_change tensor(-23.2489)
policy weight change tensor(38.4858, grad_fn=<SumBackward0>)
time8 0.0018987655639648438
train_time 0.08017683029174805
eval time 0.15463614463806152
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:42,815 MainThread INFO: EPOCH:202
2024-01-23 01:01:42,816 MainThread INFO: Time Consumed:0.23804998397827148s
2024-01-23 01:01:42,816 MainThread INFO: Total Frames:31200s
  2%|▏         | 203/10000 [03:10<42:41,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12036.60765
Train_Epoch_Reward                17265.00562
Running_Training_Average_Rewards  11987.42252
Explore_Time                      0.00088
Train___Time                      0.08018
Eval____Time                      0.15464
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12219.61531
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.63119     0.95062     88.58940     85.86085
alpha_0                           0.90351      0.00013     0.90369      0.90333
Alpha_loss                        -0.68280     0.00125     -0.68126     -0.68501
Training/policy_loss              -2.54899     0.00433     -2.54270     -2.55619
Training/qf1_loss                 15108.38477  1160.16806  16290.53320  13113.03418
Training/qf2_loss                 16159.57598  1172.03366  17361.00586  14143.01367
Training/pf_norm                  0.10293      0.03818     0.16103      0.06222
Training/qf1_norm                 2224.31431   26.82449    2258.23950   2178.09961
Training/qf2_norm                 238.32421    2.44077     240.78212    233.77783
log_std/mean                      -0.13487     0.00017     -0.13463     -0.13513
log_probs/mean                    -2.73582     0.00495     -2.72913     -2.74433
mean/mean                         0.00115      0.00007     0.00124      0.00106
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01855945587158203
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70535
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [31500]
collect time 0.0010035037994384766
inner_dict_sum {'sac_diff0': 0.00023031234741210938, 'sac_diff1': 0.0072040557861328125, 'sac_diff2': 0.008380651473999023, 'sac_diff3': 0.010761737823486328, 'sac_diff4': 0.007101535797119141, 'sac_diff5': 0.032747507095336914, 'sac_diff6': 0.00040912628173828125, 'all': 0.06683492660522461}
diff5_list [0.006964206695556641, 0.0070438385009765625, 0.006518125534057617, 0.0061109066009521484, 0.006110429763793945]
time3 0
time4 0.06766104698181152
time5 0.06771302223205566
time7 4.76837158203125e-07
gen_weight_change tensor(-23.2489)
policy weight change tensor(38.5676, grad_fn=<SumBackward0>)
time8 0.0019652843475341797
train_time 0.0788271427154541
eval time 0.15456342697143555
epoch last part time 7.152557373046875e-06
2024-01-23 01:01:43,074 MainThread INFO: EPOCH:203
2024-01-23 01:01:43,075 MainThread INFO: Time Consumed:0.23677659034729004s
2024-01-23 01:01:43,075 MainThread INFO: Total Frames:31350s
  2%|▏         | 204/10000 [03:10<42:38,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12083.93187
Train_Epoch_Reward                20320.39631
Running_Training_Average_Rewards  12257.70602
Explore_Time                      0.00100
Train___Time                      0.07883
Eval____Time                      0.15456
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12197.54122
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.61591     1.81477     91.97314     86.60300
alpha_0                           0.90306      0.00013     0.90324      0.90288
Alpha_loss                        -0.68561     0.00127     -0.68414     -0.68775
Training/policy_loss              -2.54391     0.00432     -2.53864     -2.55038
Training/qf1_loss                 15428.34922  1812.18019  18705.47266  13502.21875
Training/qf2_loss                 16602.52207  1836.88318  19924.77148  14638.31250
Training/pf_norm                  0.12136      0.03748     0.19438      0.09373
Training/qf1_norm                 2457.21841   55.65318    2555.60327   2382.36743
Training/qf2_norm                 242.12444    4.75469     250.86519    236.69942
log_std/mean                      -0.13883     0.00013     -0.13864     -0.13900
log_probs/mean                    -2.73036     0.00496     -2.72481     -2.73807
mean/mean                         -0.00091     0.00002     -0.00088     -0.00095
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019765138626098633
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70535
epoch first part time 3.337860107421875e-06
replay_buffer._size: [31650]
collect time 0.0009667873382568359
inner_dict_sum {'sac_diff0': 0.00023698806762695312, 'sac_diff1': 0.007355690002441406, 'sac_diff2': 0.008585453033447266, 'sac_diff3': 0.01119375228881836, 'sac_diff4': 0.007353067398071289, 'sac_diff5': 0.034256935119628906, 'sac_diff6': 0.00040221214294433594, 'all': 0.06938409805297852}
diff5_list [0.0068361759185791016, 0.006582975387573242, 0.007681846618652344, 0.006997108459472656, 0.0061588287353515625]
time3 0
time4 0.07020330429077148
time5 0.0702512264251709
time7 7.152557373046875e-07
gen_weight_change tensor(-23.2489)
policy weight change tensor(38.6419, grad_fn=<SumBackward0>)
time8 0.0019083023071289062
train_time 0.08141016960144043
eval time 0.15734338760375977
epoch last part time 2.0265579223632812e-05
2024-01-23 01:01:43,341 MainThread INFO: EPOCH:204
2024-01-23 01:01:43,341 MainThread INFO: Time Consumed:0.2422161102294922s
2024-01-23 01:01:43,341 MainThread INFO: Total Frames:31500s
  2%|▏         | 205/10000 [03:10<42:50,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12126.69924
Train_Epoch_Reward                7730.00376
Running_Training_Average_Rewards  12373.20605
Explore_Time                      0.00096
Train___Time                      0.08141
Eval____Time                      0.15734
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12217.44758
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.15691     2.72563     90.12144     82.81686
alpha_0                           0.90261      0.00013     0.90279      0.90243
Alpha_loss                        -0.68922     0.00136     -0.68749     -0.69152
Training/policy_loss              -2.54104     0.00407     -2.53806     -2.54894
Training/qf1_loss                 14932.20410  1630.63611  16742.59766  12297.80957
Training/qf2_loss                 16221.28047  1660.99019  18054.27930  13518.64551
Training/pf_norm                  0.11221      0.03824     0.17049      0.05678
Training/qf1_norm                 2604.17417   77.18446    2656.75171   2451.58643
Training/qf2_norm                 234.82762    7.02204     239.95309    221.07123
log_std/mean                      -0.12127     0.00013     -0.12110     -0.12146
log_probs/mean                    -2.73263     0.00480     -2.72891     -2.74195
mean/mean                         0.00112      0.00005     0.00118      0.00103
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01929783821105957
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70535
epoch first part time 3.337860107421875e-06
replay_buffer._size: [31800]
collect time 0.000873565673828125
inside mustsac before update, task 0, sumup 70535
inside mustsac after update, task 0, sumup 70382
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.007382392883300781, 'sac_diff2': 0.009206295013427734, 'sac_diff3': 0.011681795120239258, 'sac_diff4': 0.00814962387084961, 'sac_diff5': 0.05333971977233887, 'sac_diff6': 0.00041985511779785156, 'all': 0.0903925895690918}
diff5_list [0.010529518127441406, 0.010469436645507812, 0.009793996810913086, 0.012284040451049805, 0.010262727737426758]
time3 0.0008840560913085938
time4 0.09128189086914062
time5 0.09133505821228027
time7 0.008921146392822266
gen_weight_change tensor(-22.9774)
policy weight change tensor(38.5571, grad_fn=<SumBackward0>)
time8 0.002029895782470703
train_time 0.1205604076385498
eval time 0.11276102066040039
epoch last part time 6.67572021484375e-06
2024-01-23 01:01:43,600 MainThread INFO: EPOCH:205
2024-01-23 01:01:43,600 MainThread INFO: Time Consumed:0.236619234085083s
2024-01-23 01:01:43,601 MainThread INFO: Total Frames:31650s
  2%|▏         | 206/10000 [03:11<42:39,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12160.12398
Train_Epoch_Reward                33347.90704
Running_Training_Average_Rewards  13163.11072
Explore_Time                      0.00087
Train___Time                      0.12056
Eval____Time                      0.11276
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12226.31806
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.00022     2.67258     93.42765     86.10512
alpha_0                           0.90215      0.00013     0.90233      0.90197
Alpha_loss                        -0.69288     0.00118     -0.69142     -0.69501
Training/policy_loss              -2.53892     0.00464     -2.53231     -2.54425
Training/qf1_loss                 16292.67539  1730.36567  19098.33594  14431.10645
Training/qf2_loss                 17459.27383  1707.42768  20186.52148  15617.34863
Training/pf_norm                  0.15248      0.05090     0.24883      0.09696
Training/qf1_norm                 2398.18540   61.13664    2503.36353   2335.82520
Training/qf2_norm                 234.16052    6.94198     244.80379    223.94441
log_std/mean                      -0.13525     0.00625     -0.12564     -0.14261
log_probs/mean                    -2.73552     0.00509     -2.72972     -2.74313
mean/mean                         0.00035      0.00105     0.00163      -0.00155
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01833200454711914
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70382
epoch first part time 2.86102294921875e-06
replay_buffer._size: [31950]
collect time 0.000789642333984375
inner_dict_sum {'sac_diff0': 0.00021576881408691406, 'sac_diff1': 0.00722193717956543, 'sac_diff2': 0.008656740188598633, 'sac_diff3': 0.010724782943725586, 'sac_diff4': 0.007502317428588867, 'sac_diff5': 0.03399491310119629, 'sac_diff6': 0.00041294097900390625, 'all': 0.06872940063476562}
diff5_list [0.006966829299926758, 0.006233692169189453, 0.0065135955810546875, 0.007572174072265625, 0.006708621978759766]
time3 0
time4 0.06957340240478516
time5 0.0696256160736084
time7 9.5367431640625e-07
gen_weight_change tensor(-22.9774)
policy weight change tensor(38.6467, grad_fn=<SumBackward0>)
time8 0.002023935317993164
train_time 0.0807490348815918
eval time 0.1529552936553955
epoch last part time 6.67572021484375e-06
2024-01-23 01:01:43,859 MainThread INFO: EPOCH:206
2024-01-23 01:01:43,859 MainThread INFO: Time Consumed:0.23690199851989746s
2024-01-23 01:01:43,859 MainThread INFO: Total Frames:31800s
  2%|▏         | 207/10000 [03:11<42:33,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12201.17255
Train_Epoch_Reward                5857.88321
Running_Training_Average_Rewards  12901.26418
Explore_Time                      0.00079
Train___Time                      0.08075
Eval____Time                      0.15296
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12472.74875
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.44261     2.65957     91.82118     84.26129
alpha_0                           0.90170      0.00013     0.90188      0.90152
Alpha_loss                        -0.69634     0.00084     -0.69498     -0.69732
Training/policy_loss              -2.53908     0.00570     -2.53181     -2.54701
Training/qf1_loss                 16018.80566  2267.27172  18784.02930  12335.98926
Training/qf2_loss                 17179.18047  2299.84223  19969.06445  13417.91602
Training/pf_norm                  0.11156      0.03658     0.15999      0.05735
Training/qf1_norm                 2358.44058   80.48616    2415.09717   2200.45776
Training/qf2_norm                 236.29616    6.80910     242.50237    223.05765
log_std/mean                      -0.12640     0.00013     -0.12622     -0.12659
log_probs/mean                    -2.73640     0.00612     -2.72888     -2.74465
mean/mean                         0.00015      0.00010     0.00027      0.00001
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019058942794799805
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70382
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [32100]
collect time 0.0009250640869140625
inner_dict_sum {'sac_diff0': 0.00020885467529296875, 'sac_diff1': 0.00809025764465332, 'sac_diff2': 0.009624004364013672, 'sac_diff3': 0.012895584106445312, 'sac_diff4': 0.008857488632202148, 'sac_diff5': 0.04003548622131348, 'sac_diff6': 0.0004394054412841797, 'all': 0.08015108108520508}
diff5_list [0.008287668228149414, 0.00800776481628418, 0.008156776428222656, 0.008033275604248047, 0.00755000114440918]
time3 0
time4 0.08102989196777344
time5 0.08108210563659668
time7 7.152557373046875e-07
gen_weight_change tensor(-22.9774)
policy weight change tensor(38.7018, grad_fn=<SumBackward0>)
time8 0.001947641372680664
train_time 0.09241008758544922
eval time 0.15155267715454102
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:44,129 MainThread INFO: EPOCH:207
2024-01-23 01:01:44,129 MainThread INFO: Time Consumed:0.24715375900268555s
2024-01-23 01:01:44,129 MainThread INFO: Total Frames:31950s
  2%|▏         | 208/10000 [03:11<42:58,  3.80it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12234.95800
Train_Epoch_Reward                35787.26257
Running_Training_Average_Rewards  14039.07279
Explore_Time                      0.00092
Train___Time                      0.09241
Eval____Time                      0.15155
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12439.61445
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.27830     1.10027     90.90549     87.88236
alpha_0                           0.90125      0.00013     0.90143      0.90107
Alpha_loss                        -0.69954     0.00160     -0.69745     -0.70206
Training/policy_loss              -2.53465     0.00550     -2.52868     -2.54431
Training/qf1_loss                 15499.14121  1496.33213  17159.10938  13215.27637
Training/qf2_loss                 16662.80254  1510.22345  18333.44336  14352.54883
Training/pf_norm                  0.13870      0.02793     0.17762      0.09507
Training/qf1_norm                 2456.77417   31.43258    2494.88159   2407.77783
Training/qf2_norm                 232.18140    2.81449     236.28996    228.72826
log_std/mean                      -0.14149     0.00007     -0.14138     -0.14156
log_probs/mean                    -2.73475     0.00646     -2.72756     -2.74603
mean/mean                         0.00088      0.00007     0.00098      0.00080
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018590927124023438
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70382
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [32250]
collect time 0.0009417533874511719
inner_dict_sum {'sac_diff0': 0.0002090930938720703, 'sac_diff1': 0.006994009017944336, 'sac_diff2': 0.008011579513549805, 'sac_diff3': 0.010848045349121094, 'sac_diff4': 0.007050514221191406, 'sac_diff5': 0.03277230262756348, 'sac_diff6': 0.0003962516784667969, 'all': 0.06628179550170898}
diff5_list [0.007850408554077148, 0.006806373596191406, 0.0060918331146240234, 0.006044626235961914, 0.005979061126708984]
time3 0
time4 0.06702756881713867
time5 0.06707310676574707
time7 4.76837158203125e-07
gen_weight_change tensor(-22.9774)
policy weight change tensor(38.7704, grad_fn=<SumBackward0>)
time8 0.0018389225006103516
train_time 0.07788515090942383
eval time 0.1557173728942871
epoch last part time 4.291534423828125e-06
2024-01-23 01:01:44,388 MainThread INFO: EPOCH:208
2024-01-23 01:01:44,388 MainThread INFO: Time Consumed:0.23681855201721191s
2024-01-23 01:01:44,388 MainThread INFO: Total Frames:32100s
  2%|▏         | 209/10000 [03:12<42:46,  3.81it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12261.73146
Train_Epoch_Reward                3670.47973
Running_Training_Average_Rewards  14060.88430
Explore_Time                      0.00094
Train___Time                      0.07789
Eval____Time                      0.15572
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12383.25105
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.57389     1.01904    91.64018     88.67893
alpha_0                           0.90080      0.00013    0.90098      0.90062
Alpha_loss                        -0.70259     0.00133    -0.70103     -0.70490
Training/policy_loss              -2.52991     0.00445    -2.52421     -2.53772
Training/qf1_loss                 16551.85488  683.77207  17159.70508  15321.99512
Training/qf2_loss                 17908.39687  678.96528  18513.60547  16692.62109
Training/pf_norm                  0.12170      0.04533    0.18660      0.08011
Training/qf1_norm                 2748.15898   23.44886   2773.47510   2707.27051
Training/qf2_norm                 233.40020    2.53552    236.07953    228.68262
log_std/mean                      -0.12589     0.00018    -0.12573     -0.12619
log_probs/mean                    -2.73168     0.00514    -2.72532     -2.74086
mean/mean                         0.00110      0.00003    0.00114      0.00106
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01898360252380371
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70382
epoch first part time 2.86102294921875e-06
replay_buffer._size: [32400]
collect time 0.0009946823120117188
inner_dict_sum {'sac_diff0': 0.0002067089080810547, 'sac_diff1': 0.006663799285888672, 'sac_diff2': 0.007537841796875, 'sac_diff3': 0.009840965270996094, 'sac_diff4': 0.006710529327392578, 'sac_diff5': 0.031006336212158203, 'sac_diff6': 0.00037169456481933594, 'all': 0.06233787536621094}
diff5_list [0.006467103958129883, 0.006113290786743164, 0.0060749053955078125, 0.006147146224975586, 0.006203889846801758]
time3 0
time4 0.0630640983581543
time5 0.06310796737670898
time7 9.5367431640625e-07
gen_weight_change tensor(-22.9774)
policy weight change tensor(38.7938, grad_fn=<SumBackward0>)
time8 0.0018622875213623047
train_time 0.07376694679260254
eval time 0.1477806568145752
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:44,635 MainThread INFO: EPOCH:209
2024-01-23 01:01:44,635 MainThread INFO: Time Consumed:0.22475790977478027s
2024-01-23 01:01:44,636 MainThread INFO: Total Frames:32250s
  2%|▏         | 210/10000 [03:12<42:02,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12287.77897
Train_Epoch_Reward                13494.35047
Running_Training_Average_Rewards  14203.62568
Explore_Time                      0.00099
Train___Time                      0.07377
Eval____Time                      0.14778
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12363.93950
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.63822     0.91238     90.17870     87.47849
alpha_0                           0.90035      0.00013     0.90053      0.90017
Alpha_loss                        -0.70575     0.00082     -0.70487     -0.70706
Training/policy_loss              -2.53116     0.00392     -2.52482     -2.53560
Training/qf1_loss                 14809.13262  1230.25947  16760.44531  12980.42773
Training/qf2_loss                 16171.65977  1241.35539  18147.73438  14325.71289
Training/pf_norm                  0.14593      0.02570     0.17409      0.10867
Training/qf1_norm                 2743.26670   31.77299    2796.13379   2716.59399
Training/qf2_norm                 234.47737    2.30602     238.37875    231.58479
log_std/mean                      -0.13690     0.00007     -0.13678     -0.13698
log_probs/mean                    -2.72970     0.00414     -2.72295     -2.73411
mean/mean                         0.00235      0.00006     0.00244      0.00226
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018653392791748047
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70382
epoch first part time 2.384185791015625e-06
replay_buffer._size: [32550]
collect time 0.0008087158203125
inside mustsac before update, task 0, sumup 70382
inside mustsac after update, task 0, sumup 70808
inner_dict_sum {'sac_diff0': 0.00019478797912597656, 'sac_diff1': 0.006514072418212891, 'sac_diff2': 0.008071660995483398, 'sac_diff3': 0.01031637191772461, 'sac_diff4': 0.007304191589355469, 'sac_diff5': 0.05065774917602539, 'sac_diff6': 0.00040912628173828125, 'all': 0.08346796035766602}
diff5_list [0.010358572006225586, 0.009931564331054688, 0.010250329971313477, 0.009788036346435547, 0.010329246520996094]
time3 0.0008258819580078125
time4 0.08429217338562012
time5 0.08434128761291504
time7 0.008819818496704102
gen_weight_change tensor(-22.6931)
policy weight change tensor(38.6623, grad_fn=<SumBackward0>)
time8 0.0028107166290283203
train_time 0.11398601531982422
eval time 0.10635566711425781
epoch last part time 4.0531158447265625e-06
2024-01-23 01:01:44,881 MainThread INFO: EPOCH:210
2024-01-23 01:01:44,881 MainThread INFO: Time Consumed:0.22332119941711426s
2024-01-23 01:01:44,881 MainThread INFO: Total Frames:32400s
  2%|▏         | 211/10000 [03:12<41:33,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12316.78350
Train_Epoch_Reward                13779.09740
Running_Training_Average_Rewards  14439.19809
Explore_Time                      0.00080
Train___Time                      0.11399
Eval____Time                      0.10636
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12380.50204
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.55841     2.33341     93.67999     86.93478
alpha_0                           0.89990      0.00013     0.90008      0.89972
Alpha_loss                        -0.70983     0.00112     -0.70840     -0.71141
Training/policy_loss              -2.53913     0.00766     -2.53374     -2.55436
Training/qf1_loss                 15910.61914  1778.05336  18119.56445  13749.89844
Training/qf2_loss                 17197.91016  1791.78207  19543.43164  15030.69727
Training/pf_norm                  0.13083      0.01595     0.15064      0.10857
Training/qf1_norm                 2610.58413   120.71792   2829.85425   2503.48560
Training/qf2_norm                 240.67908    4.31880     245.35948    234.72452
log_std/mean                      -0.13541     0.00505     -0.12768     -0.14241
log_probs/mean                    -2.73641     0.00467     -2.73163     -2.74501
mean/mean                         0.00121      0.00096     0.00233      -0.00004
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018671274185180664
epoch last part time3 0.002460002899169922
inside rlalgo, task 0, sumup 70808
epoch first part time 2.86102294921875e-06
replay_buffer._size: [32700]
collect time 0.0008263587951660156
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.006850719451904297, 'sac_diff2': 0.007985353469848633, 'sac_diff3': 0.010198831558227539, 'sac_diff4': 0.006999015808105469, 'sac_diff5': 0.031958580017089844, 'sac_diff6': 0.00038504600524902344, 'all': 0.06458258628845215}
diff5_list [0.006643772125244141, 0.006359577178955078, 0.006249666213989258, 0.0065648555755615234, 0.006140708923339844]
time3 0
time4 0.06532979011535645
time5 0.06537604331970215
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6931)
policy weight change tensor(38.7002, grad_fn=<SumBackward0>)
time8 0.0018150806427001953
train_time 0.07623553276062012
eval time 0.14284062385559082
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:45,127 MainThread INFO: EPOCH:211
2024-01-23 01:01:45,127 MainThread INFO: Time Consumed:0.22211599349975586s
2024-01-23 01:01:45,127 MainThread INFO: Total Frames:32550s
  2%|▏         | 212/10000 [03:12<40:59,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12321.06717
Train_Epoch_Reward                20716.17945
Running_Training_Average_Rewards  14114.74374
Explore_Time                      0.00082
Train___Time                      0.07624
Eval____Time                      0.14284
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12309.69378
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.66485     1.21947    89.11123     86.25011
alpha_0                           0.89945      0.00013    0.89963      0.89927
Alpha_loss                        -0.71315     0.00134    -0.71126     -0.71475
Training/policy_loss              -2.53520     0.00513    -2.52512     -2.53954
Training/qf1_loss                 14389.37070  926.21919  15592.20020  13170.91406
Training/qf2_loss                 15673.65859  934.08429  16867.52539  14433.63574
Training/pf_norm                  0.11390      0.04215    0.16444      0.05673
Training/qf1_norm                 2552.01006   33.24358   2594.70142   2507.56860
Training/qf2_norm                 232.85080    3.05917    236.35013    229.24898
log_std/mean                      -0.13029     0.00013    -0.13015     -0.13048
log_probs/mean                    -2.73595     0.00586    -2.72445     -2.74103
mean/mean                         0.00031      0.00003    0.00035      0.00028
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018038034439086914
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70808
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [32850]
collect time 0.0008225440979003906
inner_dict_sum {'sac_diff0': 0.00022530555725097656, 'sac_diff1': 0.006814479827880859, 'sac_diff2': 0.008124351501464844, 'sac_diff3': 0.010141611099243164, 'sac_diff4': 0.007113218307495117, 'sac_diff5': 0.032756805419921875, 'sac_diff6': 0.0003864765167236328, 'all': 0.06556224822998047}
diff5_list [0.006361722946166992, 0.006529808044433594, 0.007161617279052734, 0.006581544876098633, 0.006122112274169922]
time3 0
time4 0.06632375717163086
time5 0.06636953353881836
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6931)
policy weight change tensor(38.6849, grad_fn=<SumBackward0>)
time8 0.001790761947631836
train_time 0.07701754570007324
eval time 0.1515488624572754
epoch last part time 4.5299530029296875e-06
2024-01-23 01:01:45,380 MainThread INFO: EPOCH:212
2024-01-23 01:01:45,380 MainThread INFO: Time Consumed:0.23160386085510254s
2024-01-23 01:01:45,380 MainThread INFO: Total Frames:32700s
  2%|▏         | 213/10000 [03:12<41:05,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12327.40562
Train_Epoch_Reward                8011.99915
Running_Training_Average_Rewards  14298.74039
Explore_Time                      0.00082
Train___Time                      0.07702
Eval____Time                      0.15155
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12282.99976
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.66803     3.19832     93.50294     84.51021
alpha_0                           0.89900      0.00013     0.89918      0.89882
Alpha_loss                        -0.71625     0.00080     -0.71531     -0.71739
Training/policy_loss              -2.54519     0.00298     -2.54224     -2.54958
Training/qf1_loss                 14843.20781  1816.27093  17720.62891  12485.68945
Training/qf2_loss                 16195.87051  1865.28443  19161.86523  13794.38281
Training/pf_norm                  0.12877      0.03322     0.17105      0.07751
Training/qf1_norm                 2670.03198   105.07255   2843.70923   2543.90332
Training/qf2_norm                 255.18590    8.85966     268.65430    243.59659
log_std/mean                      -0.12778     0.00010     -0.12763     -0.12793
log_probs/mean                    -2.73338     0.00306     -2.73041     -2.73772
mean/mean                         -0.00022     0.00002     -0.00020     -0.00025
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018280982971191406
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70808
epoch first part time 2.86102294921875e-06
replay_buffer._size: [33000]
collect time 0.0008270740509033203
inner_dict_sum {'sac_diff0': 0.00022029876708984375, 'sac_diff1': 0.00657343864440918, 'sac_diff2': 0.007846593856811523, 'sac_diff3': 0.010063886642456055, 'sac_diff4': 0.0066721439361572266, 'sac_diff5': 0.030839204788208008, 'sac_diff6': 0.0003757476806640625, 'all': 0.0625913143157959}
diff5_list [0.006562232971191406, 0.006151676177978516, 0.00601649284362793, 0.005986928939819336, 0.00612187385559082]
time3 0
time4 0.06333708763122559
time5 0.06338286399841309
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6931)
policy weight change tensor(38.6875, grad_fn=<SumBackward0>)
time8 0.001895904541015625
train_time 0.0743105411529541
eval time 0.14823293685913086
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:45,627 MainThread INFO: EPOCH:213
2024-01-23 01:01:45,628 MainThread INFO: Time Consumed:0.22560405731201172s
2024-01-23 01:01:45,628 MainThread INFO: Total Frames:32850s
  2%|▏         | 214/10000 [03:13<40:52,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12328.47430
Train_Epoch_Reward                4155.45449
Running_Training_Average_Rewards  13790.43490
Explore_Time                      0.00082
Train___Time                      0.07431
Eval____Time                      0.14823
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12208.22799
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.50544     1.72311    91.84692     86.65532
alpha_0                           0.89855      0.00013    0.89873      0.89837
Alpha_loss                        -0.71879     0.00095    -0.71792     -0.72055
Training/policy_loss              -2.52719     0.00398    -2.52042     -2.53189
Training/qf1_loss                 15488.00410  733.46822  16353.47168  14478.26562
Training/qf2_loss                 16830.43496  758.78376  17737.62891  15803.83105
Training/pf_norm                  0.10615      0.02464    0.14249      0.06872
Training/qf1_norm                 2611.49819   59.58422   2691.73389   2522.15186
Training/qf2_norm                 244.58465    4.54146    250.74634    237.09419
log_std/mean                      -0.13200     0.00003    -0.13195     -0.13205
log_probs/mean                    -2.72568     0.00435    -2.71838     -2.73045
mean/mean                         0.00115      0.00002    0.00117      0.00111
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018410682678222656
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70808
epoch first part time 2.86102294921875e-06
replay_buffer._size: [33150]
collect time 0.0008084774017333984
inner_dict_sum {'sac_diff0': 0.00025177001953125, 'sac_diff1': 0.00678253173828125, 'sac_diff2': 0.007985115051269531, 'sac_diff3': 0.010233402252197266, 'sac_diff4': 0.006837129592895508, 'sac_diff5': 0.0317838191986084, 'sac_diff6': 0.00038909912109375, 'all': 0.06426286697387695}
diff5_list [0.006561756134033203, 0.0064544677734375, 0.006101131439208984, 0.006562232971191406, 0.006104230880737305]
time3 0
time4 0.06499028205871582
time5 0.06503534317016602
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6931)
policy weight change tensor(38.5760, grad_fn=<SumBackward0>)
time8 0.00191497802734375
train_time 0.07623004913330078
eval time 0.1430647373199463
epoch last part time 4.291534423828125e-06
2024-01-23 01:01:45,872 MainThread INFO: EPOCH:214
2024-01-23 01:01:45,872 MainThread INFO: Time Consumed:0.22234559059143066s
2024-01-23 01:01:45,872 MainThread INFO: Total Frames:33000s
  2%|▏         | 215/10000 [03:13<40:34,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12327.01832
Train_Epoch_Reward                26841.38829
Running_Training_Average_Rewards  14614.33049
Explore_Time                      0.00080
Train___Time                      0.07623
Eval____Time                      0.14306
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12202.88784
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.12626     1.41744    89.64434     85.96457
alpha_0                           0.89810      0.00013    0.89828      0.89792
Alpha_loss                        -0.72274     0.00112    -0.72077     -0.72377
Training/policy_loss              -2.53096     0.00523    -2.52621     -2.54008
Training/qf1_loss                 14006.13379  699.16873  14865.42676  12728.95312
Training/qf2_loss                 15415.36289  715.25994  16291.99512  14107.81055
Training/pf_norm                  0.13846      0.02779    0.18702      0.11116
Training/qf1_norm                 2701.95620   39.16079   2741.84766   2636.63647
Training/qf2_norm                 241.15795    3.70733    245.22884    235.53189
log_std/mean                      -0.15091     0.00033    -0.15043     -0.15132
log_probs/mean                    -2.73117     0.00578    -2.72534     -2.74104
mean/mean                         0.00062      0.00006    0.00068      0.00052
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01862192153930664
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70808
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [33300]
collect time 0.0007998943328857422
inside mustsac before update, task 0, sumup 70808
inside mustsac after update, task 0, sumup 70940
inner_dict_sum {'sac_diff0': 0.00022268295288085938, 'sac_diff1': 0.007277488708496094, 'sac_diff2': 0.009309768676757812, 'sac_diff3': 0.011088848114013672, 'sac_diff4': 0.008510828018188477, 'sac_diff5': 0.056420087814331055, 'sac_diff6': 0.00045490264892578125, 'all': 0.09328460693359375}
diff5_list [0.01177525520324707, 0.014518022537231445, 0.010888338088989258, 0.00971674919128418, 0.009521722793579102]
time3 0.0009009838104248047
time4 0.0943152904510498
time5 0.09437894821166992
time7 0.0101165771484375
gen_weight_change tensor(-22.4724)
policy weight change tensor(38.4810, grad_fn=<SumBackward0>)
time8 0.002058744430541992
train_time 0.12588787078857422
eval time 0.0990145206451416
epoch last part time 4.5299530029296875e-06
2024-01-23 01:01:46,122 MainThread INFO: EPOCH:215
2024-01-23 01:01:46,122 MainThread INFO: Time Consumed:0.2278883457183838s
2024-01-23 01:01:46,122 MainThread INFO: Total Frames:33150s
  2%|▏         | 216/10000 [03:13<40:39,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12316.90203
Train_Epoch_Reward                14615.23342
Running_Training_Average_Rewards  14915.86810
Explore_Time                      0.00080
Train___Time                      0.12589
Eval____Time                      0.09901
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12125.15512
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.61893     1.91367     89.57286     84.12754
alpha_0                           0.89765      0.00013     0.89783      0.89747
Alpha_loss                        -0.72657     0.00126     -0.72445     -0.72828
Training/policy_loss              -2.53295     0.00758     -2.52394     -2.54316
Training/qf1_loss                 14235.54707  1178.90474  15612.00977  12233.35840
Training/qf2_loss                 15603.39180  1256.11071  17135.98828  13458.47461
Training/pf_norm                  0.11054      0.01606     0.12948      0.08595
Training/qf1_norm                 2724.25933   211.44128   2998.21777   2461.79663
Training/qf2_norm                 233.40424    8.23699     246.17375    221.87886
log_std/mean                      -0.12810     0.00289     -0.12441     -0.13202
log_probs/mean                    -2.73546     0.00468     -2.72821     -2.74189
mean/mean                         0.00001      0.00101     0.00169      -0.00098
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019071102142333984
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [33450]
collect time 0.0008373260498046875
inner_dict_sum {'sac_diff0': 0.0002429485321044922, 'sac_diff1': 0.006888628005981445, 'sac_diff2': 0.00812530517578125, 'sac_diff3': 0.010367393493652344, 'sac_diff4': 0.007307529449462891, 'sac_diff5': 0.03310751914978027, 'sac_diff6': 0.0004086494445800781, 'all': 0.06644797325134277}
diff5_list [0.0067598819732666016, 0.006293773651123047, 0.0067446231842041016, 0.007148027420043945, 0.006161212921142578]
time3 0
time4 0.06720829010009766
time5 0.06725645065307617
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4724)
policy weight change tensor(38.3134, grad_fn=<SumBackward0>)
time8 0.0020084381103515625
train_time 0.07817649841308594
eval time 0.1579296588897705
epoch last part time 4.5299530029296875e-06
2024-01-23 01:01:46,384 MainThread INFO: EPOCH:216
2024-01-23 01:01:46,384 MainThread INFO: Time Consumed:0.23939871788024902s
2024-01-23 01:01:46,384 MainThread INFO: Total Frames:33300s
  2%|▏         | 217/10000 [03:13<41:12,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12274.12709
Train_Epoch_Reward                2914.85949
Running_Training_Average_Rewards  14074.45626
Explore_Time                      0.00083
Train___Time                      0.07818
Eval____Time                      0.15793
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12044.99941
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.59935     1.59538     95.35381     91.18314
alpha_0                           0.89720      0.00013     0.89738      0.89702
Alpha_loss                        -0.72979     0.00061     -0.72918     -0.73056
Training/policy_loss              -2.53407     0.00605     -2.52617     -2.54080
Training/qf1_loss                 16943.53535  1442.43241  18240.33984  14932.28125
Training/qf2_loss                 18453.69180  1459.05283  19758.46289  16408.19922
Training/pf_norm                  0.13039      0.02581     0.16903      0.09685
Training/qf1_norm                 3006.25391   60.37071    3112.87915   2943.61743
Training/qf2_norm                 252.03877    4.20114     259.23331    248.25948
log_std/mean                      -0.14495     0.00040     -0.14429     -0.14541
log_probs/mean                    -2.73407     0.00647     -2.72551     -2.74116
mean/mean                         0.00115      0.00006     0.00120      0.00105
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018263816833496094
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 2.384185791015625e-06
replay_buffer._size: [33600]
collect time 0.0008263587951660156
inner_dict_sum {'sac_diff0': 0.0002009868621826172, 'sac_diff1': 0.00655674934387207, 'sac_diff2': 0.007703065872192383, 'sac_diff3': 0.009768009185791016, 'sac_diff4': 0.006825685501098633, 'sac_diff5': 0.030887603759765625, 'sac_diff6': 0.0003750324249267578, 'all': 0.0623171329498291}
diff5_list [0.006407976150512695, 0.006284236907958984, 0.0059053897857666016, 0.006170988082885742, 0.0061190128326416016]
time3 0
time4 0.06307077407836914
time5 0.06311535835266113
time7 9.5367431640625e-07
gen_weight_change tensor(-22.4724)
policy weight change tensor(38.1623, grad_fn=<SumBackward0>)
time8 0.0018768310546875
train_time 0.07395172119140625
eval time 0.1616053581237793
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:46,644 MainThread INFO: EPOCH:217
2024-01-23 01:01:46,644 MainThread INFO: Time Consumed:0.2386162281036377s
2024-01-23 01:01:46,644 MainThread INFO: Total Frames:33450s
  2%|▏         | 218/10000 [03:14<41:34,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12229.08170
Train_Epoch_Reward                24682.40835
Running_Training_Average_Rewards  14386.12630
Explore_Time                      0.00082
Train___Time                      0.07395
Eval____Time                      0.16161
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11989.16046
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.79835     2.16894    91.58231     84.84383
alpha_0                           0.89675      0.00013    0.89693      0.89657
Alpha_loss                        -0.73274     0.00116    -0.73119     -0.73443
Training/policy_loss              -2.53362     0.00304    -2.53029     -2.53829
Training/qf1_loss                 14160.19727  792.13953  14910.77539  12631.33398
Training/qf2_loss                 15630.15547  818.02690  16431.50000  14056.70801
Training/pf_norm                  0.14243      0.03415    0.19367      0.10421
Training/qf1_norm                 2864.92935   68.31447   2978.88208   2768.63867
Training/qf2_norm                 240.90908    5.73975    250.92073    233.08043
log_std/mean                      -0.12826     0.00018    -0.12801     -0.12855
log_probs/mean                    -2.73019     0.00350    -2.72678     -2.73537
mean/mean                         0.00041      0.00001    0.00042      0.00040
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018271684646606445
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [33750]
collect time 0.0008771419525146484
inner_dict_sum {'sac_diff0': 0.00022482872009277344, 'sac_diff1': 0.006557941436767578, 'sac_diff2': 0.007687091827392578, 'sac_diff3': 0.010093927383422852, 'sac_diff4': 0.007071733474731445, 'sac_diff5': 0.03202557563781738, 'sac_diff6': 0.00039315223693847656, 'all': 0.06405425071716309}
diff5_list [0.0068929195404052734, 0.006445884704589844, 0.0065844058990478516, 0.005971670150756836, 0.006130695343017578]
time3 0
time4 0.0648043155670166
time5 0.06484770774841309
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4724)
policy weight change tensor(37.9996, grad_fn=<SumBackward0>)
time8 0.0019540786743164062
train_time 0.07564806938171387
eval time 0.16159677505493164
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:46,906 MainThread INFO: EPOCH:218
2024-01-23 01:01:46,906 MainThread INFO: Time Consumed:0.24042320251464844s
2024-01-23 01:01:46,907 MainThread INFO: Total Frames:33600s
  2%|▏         | 219/10000 [03:14<41:56,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12193.20944
Train_Epoch_Reward                17645.14820
Running_Training_Average_Rewards  14627.62954
Explore_Time                      0.00087
Train___Time                      0.07565
Eval____Time                      0.16160
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12024.52853
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.65003     1.46476    92.47254     88.48212
alpha_0                           0.89631      0.00013    0.89648      0.89613
Alpha_loss                        -0.73618     0.00120    -0.73450     -0.73739
Training/policy_loss              -2.53303     0.00489    -2.52797     -2.54043
Training/qf1_loss                 15143.33770  938.71654  16911.26367  14118.29492
Training/qf2_loss                 16580.23828  962.66523  18400.97461  15548.76562
Training/pf_norm                  0.12258      0.02398    0.15691      0.09610
Training/qf1_norm                 2762.06406   52.47327   2863.76782   2712.43628
Training/qf2_norm                 246.74425    3.90971    254.29700    243.66722
log_std/mean                      -0.13595     0.00029    -0.13552     -0.13635
log_probs/mean                    -2.73088     0.00551    -2.72579     -2.73916
mean/mean                         0.00035      0.00007    0.00047      0.00024
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018707990646362305
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 2.86102294921875e-06
replay_buffer._size: [33900]
collect time 0.0009136199951171875
inner_dict_sum {'sac_diff0': 0.00022459030151367188, 'sac_diff1': 0.006699085235595703, 'sac_diff2': 0.007769584655761719, 'sac_diff3': 0.009940147399902344, 'sac_diff4': 0.006619453430175781, 'sac_diff5': 0.03128838539123535, 'sac_diff6': 0.0003781318664550781, 'all': 0.06291937828063965}
diff5_list [0.006562471389770508, 0.0061550140380859375, 0.006407499313354492, 0.0060727596282958984, 0.006090641021728516]
time3 0
time4 0.06365680694580078
time5 0.06369924545288086
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4724)
policy weight change tensor(37.8900, grad_fn=<SumBackward0>)
time8 0.0019571781158447266
train_time 0.07448053359985352
eval time 0.16504573822021484
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:47,171 MainThread INFO: EPOCH:219
2024-01-23 01:01:47,171 MainThread INFO: Time Consumed:0.24285149574279785s
2024-01-23 01:01:47,172 MainThread INFO: Total Frames:33750s
  2%|▏         | 220/10000 [03:14<42:20,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12167.75841
Train_Epoch_Reward                1925.43534
Running_Training_Average_Rewards  14283.98836
Explore_Time                      0.00091
Train___Time                      0.07448
Eval____Time                      0.16505
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12109.42921
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.88347     3.07173     95.06189     87.19453
alpha_0                           0.89586      0.00013     0.89604      0.89568
Alpha_loss                        -0.74063     0.00122     -0.73865     -0.74238
Training/policy_loss              -2.53452     0.00935     -2.52415     -2.54880
Training/qf1_loss                 15765.32031  1867.71959  18338.73047  13662.74414
Training/qf2_loss                 17218.42930  1908.53646  19813.56250  15054.43945
Training/pf_norm                  0.10032      0.03766     0.14096      0.05130
Training/qf1_norm                 2850.90498   99.13868    2981.86377   2725.28076
Training/qf2_norm                 245.77271    7.99308     256.72678    236.15599
log_std/mean                      -0.13184     0.00016     -0.13161     -0.13206
log_probs/mean                    -2.74069     0.01030     -2.72887     -2.75662
mean/mean                         0.00053      0.00003     0.00057      0.00049
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019191503524780273
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [34050]
collect time 0.0009348392486572266
inside mustsac before update, task 0, sumup 70940
inside mustsac after update, task 0, sumup 70935
inner_dict_sum {'sac_diff0': 0.0002009868621826172, 'sac_diff1': 0.007120609283447266, 'sac_diff2': 0.008892059326171875, 'sac_diff3': 0.011584043502807617, 'sac_diff4': 0.007786989212036133, 'sac_diff5': 0.052011728286743164, 'sac_diff6': 0.0004401206970214844, 'all': 0.08803653717041016}
diff5_list [0.011392354965209961, 0.010895967483520508, 0.009600400924682617, 0.010230779647827148, 0.00989222526550293]
time3 0.0008895397186279297
time4 0.08892703056335449
time5 0.08897805213928223
time7 0.009276866912841797
gen_weight_change tensor(-22.3898)
policy weight change tensor(37.8313, grad_fn=<SumBackward0>)
time8 0.0028128623962402344
train_time 0.11977362632751465
eval time 0.12328743934631348
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:47,440 MainThread INFO: EPOCH:220
2024-01-23 01:01:47,440 MainThread INFO: Time Consumed:0.24624252319335938s
2024-01-23 01:01:47,441 MainThread INFO: Total Frames:33900s
  2%|▏         | 221/10000 [03:15<42:53,  3.80it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12143.96505
Train_Epoch_Reward                17485.13564
Running_Training_Average_Rewards  14592.35277
Explore_Time                      0.00093
Train___Time                      0.11977
Eval____Time                      0.12329
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12142.56837
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.30980     1.37279     91.07392     86.86669
alpha_0                           0.89541      0.00013     0.89559      0.89523
Alpha_loss                        -0.74329     0.00128     -0.74132     -0.74514
Training/policy_loss              -2.53192     0.00821     -2.51693     -2.53845
Training/qf1_loss                 14746.40937  1163.61247  16114.87012  12624.95410
Training/qf2_loss                 16251.98906  1168.36943  17672.45898  14159.90332
Training/pf_norm                  0.14814      0.01590     0.17015      0.12329
Training/qf1_norm                 2879.14819   77.96972    2992.60693   2753.87036
Training/qf2_norm                 244.44154    4.45578     249.61511    238.29836
log_std/mean                      -0.13133     0.01012     -0.11952     -0.14704
log_probs/mean                    -2.73425     0.00672     -2.72401     -2.74118
mean/mean                         0.00014      0.00077     0.00121      -0.00092
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018375158309936523
epoch last part time3 0.0025963783264160156
inside rlalgo, task 0, sumup 70935
epoch first part time 2.86102294921875e-06
replay_buffer._size: [34200]
collect time 0.0008933544158935547
inner_dict_sum {'sac_diff0': 0.00020933151245117188, 'sac_diff1': 0.006754398345947266, 'sac_diff2': 0.007871150970458984, 'sac_diff3': 0.009911298751831055, 'sac_diff4': 0.007117748260498047, 'sac_diff5': 0.03251481056213379, 'sac_diff6': 0.0003840923309326172, 'all': 0.06476283073425293}
diff5_list [0.006921529769897461, 0.006978750228881836, 0.006401538848876953, 0.006066083908081055, 0.006146907806396484]
time3 0
time4 0.06554293632507324
time5 0.06558942794799805
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3898)
policy weight change tensor(37.7565, grad_fn=<SumBackward0>)
time8 0.0019092559814453125
train_time 0.07623600959777832
eval time 0.15598106384277344
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:47,700 MainThread INFO: EPOCH:221
2024-01-23 01:01:47,700 MainThread INFO: Time Consumed:0.2353520393371582s
2024-01-23 01:01:47,700 MainThread INFO: Total Frames:34050s
  2%|▏         | 222/10000 [03:15<42:35,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12125.82328
Train_Epoch_Reward                15635.21003
Running_Training_Average_Rewards  14657.50067
Explore_Time                      0.00089
Train___Time                      0.07624
Eval____Time                      0.15598
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12128.27608
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.66700     2.07766     94.77070     89.21467
alpha_0                           0.89496      0.00013     0.89514      0.89478
Alpha_loss                        -0.74685     0.00104     -0.74514     -0.74823
Training/policy_loss              -2.52916     0.00178     -2.52671     -2.53128
Training/qf1_loss                 15364.08203  1233.14165  17555.19922  14185.72656
Training/qf2_loss                 16846.80059  1262.32268  19112.86719  15672.98438
Training/pf_norm                  0.09915      0.01339     0.12245      0.08276
Training/qf1_norm                 2845.07773   75.58263    2990.99243   2782.80249
Training/qf2_norm                 244.80458    5.41272     255.50008    240.98834
log_std/mean                      -0.12503     0.00013     -0.12485     -0.12522
log_probs/mean                    -2.73598     0.00205     -2.73270     -2.73808
mean/mean                         0.00053      0.00004     0.00058      0.00047
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018628597259521484
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70935
epoch first part time 2.86102294921875e-06
replay_buffer._size: [34350]
collect time 0.0009138584136962891
inner_dict_sum {'sac_diff0': 0.0002357959747314453, 'sac_diff1': 0.006770610809326172, 'sac_diff2': 0.007944345474243164, 'sac_diff3': 0.010175943374633789, 'sac_diff4': 0.006680965423583984, 'sac_diff5': 0.031221389770507812, 'sac_diff6': 0.0003752708435058594, 'all': 0.06340432167053223}
diff5_list [0.006499290466308594, 0.006307840347290039, 0.006159305572509766, 0.0062923431396484375, 0.0059626102447509766]
time3 0
time4 0.06414937973022461
time5 0.06420207023620605
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3898)
policy weight change tensor(37.7088, grad_fn=<SumBackward0>)
time8 0.001786947250366211
train_time 0.07493352890014648
eval time 0.17167448997497559
epoch last part time 5.4836273193359375e-06
2024-01-23 01:01:47,972 MainThread INFO: EPOCH:222
2024-01-23 01:01:47,972 MainThread INFO: Time Consumed:0.2499246597290039s
2024-01-23 01:01:47,972 MainThread INFO: Total Frames:34200s
  2%|▏         | 223/10000 [03:15<43:06,  3.78it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12115.20704
Train_Epoch_Reward                9445.33989
Running_Training_Average_Rewards  14745.09688
Explore_Time                      0.00091
Train___Time                      0.07493
Eval____Time                      0.17167
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12176.83743
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.48066     2.61993     92.47617     84.88939
alpha_0                           0.89451      0.00013     0.89469      0.89433
Alpha_loss                        -0.75000     0.00142     -0.74808     -0.75171
Training/policy_loss              -2.52540     0.00468     -2.51945     -2.53250
Training/qf1_loss                 15640.60703  1009.64661  16521.68750  13696.33301
Training/qf2_loss                 17267.43789  1055.86360  18187.21680  15243.55469
Training/pf_norm                  0.12388      0.01532     0.14459      0.10566
Training/qf1_norm                 2982.96836   97.91930    3109.78931   2823.82104
Training/qf2_norm                 236.88140    6.71438     244.60146    225.16936
log_std/mean                      -0.13354     0.00009     -0.13340     -0.13363
log_probs/mean                    -2.73401     0.00547     -2.72737     -2.74225
mean/mean                         0.00048      0.00002     0.00051      0.00046
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018422365188598633
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70935
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [34500]
collect time 0.0009400844573974609
inner_dict_sum {'sac_diff0': 0.00022745132446289062, 'sac_diff1': 0.006467580795288086, 'sac_diff2': 0.007721662521362305, 'sac_diff3': 0.009905338287353516, 'sac_diff4': 0.006704092025756836, 'sac_diff5': 0.03127241134643555, 'sac_diff6': 0.0004181861877441406, 'all': 0.06271672248840332}
diff5_list [0.006376981735229492, 0.006353139877319336, 0.006215572357177734, 0.006273508071899414, 0.00605320930480957]
time3 0
time4 0.06346297264099121
time5 0.06351447105407715
time7 9.5367431640625e-07
gen_weight_change tensor(-22.3898)
policy weight change tensor(37.6814, grad_fn=<SumBackward0>)
time8 0.0018258094787597656
train_time 0.07425880432128906
eval time 0.17469477653503418
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:48,246 MainThread INFO: EPOCH:223
2024-01-23 01:01:48,246 MainThread INFO: Time Consumed:0.2522573471069336s
2024-01-23 01:01:48,247 MainThread INFO: Total Frames:34350s
  2%|▏         | 224/10000 [03:15<43:37,  3.74it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12120.79270
Train_Epoch_Reward                12210.40692
Running_Training_Average_Rewards  14895.36287
Explore_Time                      0.00094
Train___Time                      0.07426
Eval____Time                      0.17469
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12264.08454
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.47096     2.52986     90.33797     83.80855
alpha_0                           0.89407      0.00013     0.89424      0.89389
Alpha_loss                        -0.75328     0.00114     -0.75182     -0.75489
Training/policy_loss              -2.53874     0.00607     -2.52957     -2.54845
Training/qf1_loss                 14417.97031  1560.39882  16391.43555  12316.45117
Training/qf2_loss                 15795.46563  1601.06289  17810.75000  13626.82324
Training/pf_norm                  0.11149      0.02408     0.14271      0.09143
Training/qf1_norm                 2730.90654   95.66847    2843.16772   2596.90259
Training/qf2_norm                 255.71739    7.15094     263.73883    245.46793
log_std/mean                      -0.12472     0.00004     -0.12467     -0.12478
log_probs/mean                    -2.73322     0.00672     -2.72296     -2.74379
mean/mean                         -0.00003     0.00007     0.00006      -0.00013
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01942920684814453
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70935
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [34650]
collect time 0.0009245872497558594
inner_dict_sum {'sac_diff0': 0.0002319812774658203, 'sac_diff1': 0.007139682769775391, 'sac_diff2': 0.008398294448852539, 'sac_diff3': 0.010650396347045898, 'sac_diff4': 0.006986141204833984, 'sac_diff5': 0.03235435485839844, 'sac_diff6': 0.0003905296325683594, 'all': 0.06615138053894043}
diff5_list [0.006700038909912109, 0.00664520263671875, 0.006600141525268555, 0.006298542022705078, 0.006110429763793945]
time3 0
time4 0.06691813468933105
time5 0.06696462631225586
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3898)
policy weight change tensor(37.7395, grad_fn=<SumBackward0>)
time8 0.001837015151977539
train_time 0.07783627510070801
eval time 0.1510779857635498
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:48,501 MainThread INFO: EPOCH:224
2024-01-23 01:01:48,502 MainThread INFO: Time Consumed:0.23209166526794434s
2024-01-23 01:01:48,502 MainThread INFO: Total Frames:34500s
  2%|▏         | 225/10000 [03:16<42:56,  3.79it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12132.94025
Train_Epoch_Reward                10648.77460
Running_Training_Average_Rewards  14637.59550
Explore_Time                      0.00092
Train___Time                      0.07784
Eval____Time                      0.15108
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12324.36335
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       86.61176     3.12872     91.80993     82.08985
alpha_0                           0.89362      0.00013     0.89380      0.89344
Alpha_loss                        -0.75666     0.00136     -0.75408     -0.75793
Training/policy_loss              -2.52670     0.00537     -2.51736     -2.53376
Training/qf1_loss                 14190.49336  2095.66644  17120.39453  10816.35449
Training/qf2_loss                 15553.93730  2138.18018  18567.08203  12117.20410
Training/pf_norm                  0.11452      0.03164     0.14334      0.05402
Training/qf1_norm                 2588.19053   97.54925    2754.42993   2455.37744
Training/qf2_norm                 237.27297    8.27982     251.05995    225.35631
log_std/mean                      -0.11921     0.00015     -0.11901     -0.11944
log_probs/mean                    -2.73333     0.00615     -2.72230     -2.74099
mean/mean                         0.00206      0.00007     0.00213      0.00196
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018023967742919922
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70935
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [34800]
collect time 0.000888824462890625
inside mustsac before update, task 0, sumup 70935
inside mustsac after update, task 0, sumup 70869
inner_dict_sum {'sac_diff0': 0.0002028942108154297, 'sac_diff1': 0.006825923919677734, 'sac_diff2': 0.008337974548339844, 'sac_diff3': 0.010175704956054688, 'sac_diff4': 0.007096767425537109, 'sac_diff5': 0.04840874671936035, 'sac_diff6': 0.0003974437713623047, 'all': 0.08144545555114746}
diff5_list [0.009973526000976562, 0.009477615356445312, 0.009638547897338867, 0.009665966033935547, 0.009653091430664062]
time3 0.0008628368377685547
time4 0.08227777481079102
time5 0.08232331275939941
time7 0.009343147277832031
gen_weight_change tensor(-22.4251)
policy weight change tensor(37.6799, grad_fn=<SumBackward0>)
time8 0.0019257068634033203
train_time 0.1116018295288086
eval time 0.11890840530395508
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:48,756 MainThread INFO: EPOCH:225
2024-01-23 01:01:48,757 MainThread INFO: Time Consumed:0.2336139678955078s
2024-01-23 01:01:48,757 MainThread INFO: Total Frames:34650s
  2%|▏         | 226/10000 [03:16<42:32,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12151.60645
Train_Epoch_Reward                8287.32750
Running_Training_Average_Rewards  14363.17449
Explore_Time                      0.00088
Train___Time                      0.11160
Eval____Time                      0.11891
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12311.81712
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.70698     2.41186     91.36703     84.79570
alpha_0                           0.89317      0.00013     0.89335      0.89299
Alpha_loss                        -0.75978     0.00143     -0.75824     -0.76221
Training/policy_loss              -2.52266     0.01048     -2.50633     -2.53696
Training/qf1_loss                 14709.44727  1070.00457  16024.92969  13545.42188
Training/qf2_loss                 16318.48262  1027.80942  17590.29297  15181.98633
Training/pf_norm                  0.13208      0.03862     0.19465      0.08415
Training/qf1_norm                 2973.54150   70.36068    3036.89160   2848.92114
Training/qf2_norm                 238.55425    6.19731     246.55632    229.00601
log_std/mean                      -0.12997     0.00520     -0.12283     -0.13874
log_probs/mean                    -2.73110     0.00813     -2.72342     -2.74666
mean/mean                         0.00073      0.00087     0.00192      -0.00066
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0185849666595459
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70869
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [34950]
collect time 0.0009167194366455078
inner_dict_sum {'sac_diff0': 0.00021219253540039062, 'sac_diff1': 0.0068399906158447266, 'sac_diff2': 0.008409500122070312, 'sac_diff3': 0.011592864990234375, 'sac_diff4': 0.007787227630615234, 'sac_diff5': 0.03366684913635254, 'sac_diff6': 0.0003898143768310547, 'all': 0.06889843940734863}
diff5_list [0.006610393524169922, 0.006338357925415039, 0.0062444210052490234, 0.007482051849365234, 0.00699162483215332]
time3 0
time4 0.06967282295227051
time5 0.06972050666809082
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4251)
policy weight change tensor(37.7622, grad_fn=<SumBackward0>)
time8 0.0018830299377441406
train_time 0.08074402809143066
eval time 0.19142770767211914
epoch last part time 6.9141387939453125e-06
2024-01-23 01:01:49,054 MainThread INFO: EPOCH:226
2024-01-23 01:01:49,054 MainThread INFO: Time Consumed:0.2754826545715332s
2024-01-23 01:01:49,055 MainThread INFO: Total Frames:34800s
  2%|▏         | 227/10000 [03:16<44:20,  3.67it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12229.85360
Train_Epoch_Reward                24481.34831
Running_Training_Average_Rewards  14725.15542
Explore_Time                      0.00091
Train___Time                      0.08074
Eval____Time                      0.19143
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12827.47089
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.61333     1.97461     91.74270     86.55680
alpha_0                           0.89273      0.00013     0.89290      0.89255
Alpha_loss                        -0.76327     0.00098     -0.76202     -0.76462
Training/policy_loss              -2.52482     0.00313     -2.52051     -2.52987
Training/qf1_loss                 14401.46758  1212.79441  15816.84961  12734.22852
Training/qf2_loss                 16001.78203  1240.93132  17436.99023  14334.92676
Training/pf_norm                  0.12183      0.05037     0.20025      0.04946
Training/qf1_norm                 3025.61284   79.32813    3151.56787   2929.61011
Training/qf2_norm                 243.14353    5.30164     251.50774    237.55505
log_std/mean                      -0.13155     0.00017     -0.13132     -0.13178
log_probs/mean                    -2.73214     0.00342     -2.72707     -2.73725
mean/mean                         -0.00091     0.00007     -0.00083     -0.00103
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018817901611328125
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70869
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [35100]
collect time 0.0010302066802978516
inner_dict_sum {'sac_diff0': 0.00022125244140625, 'sac_diff1': 0.007355928421020508, 'sac_diff2': 0.008504390716552734, 'sac_diff3': 0.011007308959960938, 'sac_diff4': 0.007537126541137695, 'sac_diff5': 0.03306722640991211, 'sac_diff6': 0.00039649009704589844, 'all': 0.06808972358703613}
diff5_list [0.0070912837982177734, 0.007154703140258789, 0.006265163421630859, 0.006262540817260742, 0.006293535232543945]
time3 0
time4 0.06886982917785645
time5 0.06891918182373047
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4251)
policy weight change tensor(37.8896, grad_fn=<SumBackward0>)
time8 0.0019104480743408203
train_time 0.08025026321411133
eval time 0.19301819801330566
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:49,353 MainThread INFO: EPOCH:227
2024-01-23 01:01:49,354 MainThread INFO: Time Consumed:0.2766897678375244s
2024-01-23 01:01:49,354 MainThread INFO: Total Frames:34950s
  2%|▏         | 228/10000 [03:16<45:38,  3.57it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12315.48988
Train_Epoch_Reward                8668.43730
Running_Training_Average_Rewards  14667.30374
Explore_Time                      0.00103
Train___Time                      0.08025
Eval____Time                      0.19302
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12845.52329
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.29308     2.11682    92.77165     86.96645
alpha_0                           0.89228      0.00013    0.89246      0.89210
Alpha_loss                        -0.76646     0.00062    -0.76572     -0.76744
Training/policy_loss              -2.51598     0.00470    -2.50852     -2.52120
Training/qf1_loss                 14635.05000  841.01484  15375.94043  13092.49414
Training/qf2_loss                 16336.03574  855.66747  17035.58008  14749.23145
Training/pf_norm                  0.12599      0.03626    0.16687      0.08338
Training/qf1_norm                 3172.05869   76.42742   3285.66455   3085.39429
Training/qf2_norm                 236.67280    5.38764    245.46364    230.69670
log_std/mean                      -0.12521     0.00023    -0.12496     -0.12559
log_probs/mean                    -2.73064     0.00494    -2.72257     -2.73592
mean/mean                         0.00088      0.00005    0.00096      0.00083
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018448829650878906
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70869
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [35250]
collect time 0.0009326934814453125
inner_dict_sum {'sac_diff0': 0.00022482872009277344, 'sac_diff1': 0.0068247318267822266, 'sac_diff2': 0.008081912994384766, 'sac_diff3': 0.009900808334350586, 'sac_diff4': 0.00688624382019043, 'sac_diff5': 0.03258466720581055, 'sac_diff6': 0.0003826618194580078, 'all': 0.06488585472106934}
diff5_list [0.006714582443237305, 0.006371259689331055, 0.006239652633666992, 0.006584882736206055, 0.006674289703369141]
time3 0
time4 0.06563210487365723
time5 0.06567597389221191
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4251)
policy weight change tensor(38.0555, grad_fn=<SumBackward0>)
time8 0.0019469261169433594
train_time 0.0765995979309082
eval time 0.15956354141235352
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:49,615 MainThread INFO: EPOCH:228
2024-01-23 01:01:49,616 MainThread INFO: Time Consumed:0.2395923137664795s
2024-01-23 01:01:49,616 MainThread INFO: Total Frames:35100s
  2%|▏         | 229/10000 [03:17<44:45,  3.64it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12401.52781
Train_Epoch_Reward                17930.40353
Running_Training_Average_Rewards  14856.20842
Explore_Time                      0.00093
Train___Time                      0.07660
Eval____Time                      0.15956
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12884.90787
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.09851     2.55927     92.68770     85.29158
alpha_0                           0.89183      0.00013     0.89201      0.89165
Alpha_loss                        -0.77057     0.00144     -0.76781     -0.77177
Training/policy_loss              -2.53152     0.00655     -2.52111     -2.54154
Training/qf1_loss                 15175.66133  2456.02204  19900.88867  12867.61133
Training/qf2_loss                 16912.84063  2496.99524  21704.64453  14535.01074
Training/pf_norm                  0.12429      0.04134     0.17455      0.07126
Training/qf1_norm                 3180.37632   101.21940   3315.94434   3039.79565
Training/qf2_norm                 246.48225    6.81243     256.06763    236.53676
log_std/mean                      -0.13369     0.00030     -0.13323     -0.13408
log_probs/mean                    -2.73707     0.00751     -2.72473     -2.74814
mean/mean                         0.00020      0.00004     0.00024      0.00014
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019065380096435547
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70869
epoch first part time 3.337860107421875e-06
replay_buffer._size: [35400]
collect time 0.0010409355163574219
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.008057355880737305, 'sac_diff2': 0.009247064590454102, 'sac_diff3': 0.012138128280639648, 'sac_diff4': 0.007875680923461914, 'sac_diff5': 0.0355532169342041, 'sac_diff6': 0.0004210472106933594, 'all': 0.07350683212280273}
diff5_list [0.007092952728271484, 0.006655216217041016, 0.007196903228759766, 0.007485866546630859, 0.0071222782135009766]
time3 0
time4 0.07437825202941895
time5 0.0744318962097168
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4251)
policy weight change tensor(38.0873, grad_fn=<SumBackward0>)
time8 0.002029895782470703
train_time 0.08753800392150879
eval time 0.14711809158325195
epoch last part time 1.2874603271484375e-05
2024-01-23 01:01:49,876 MainThread INFO: EPOCH:229
2024-01-23 01:01:49,877 MainThread INFO: Time Consumed:0.238175630569458s
2024-01-23 01:01:49,877 MainThread INFO: Total Frames:35250s
  2%|▏         | 230/10000 [03:17<44:04,  3.69it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12482.65954
Train_Epoch_Reward                18534.41058
Running_Training_Average_Rewards  14757.52178
Explore_Time                      0.00104
Train___Time                      0.08754
Eval____Time                      0.14712
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12920.74650
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.46496     2.79249     95.26312     86.58846
alpha_0                           0.89139      0.00013     0.89156      0.89121
Alpha_loss                        -0.77420     0.00172     -0.77250     -0.77654
Training/policy_loss              -2.53444     0.00685     -2.52679     -2.54298
Training/qf1_loss                 16130.94453  1866.66481  19519.98047  14261.91113
Training/qf2_loss                 17945.75039  1919.84438  21423.10547  15975.43750
Training/pf_norm                  0.10665      0.03211     0.14528      0.05970
Training/qf1_norm                 3312.36924   114.75694   3477.76172   3117.19141
Training/qf2_norm                 257.68064    7.59158     268.04712    244.44107
log_std/mean                      -0.13471     0.00002     -0.13468     -0.13473
log_probs/mean                    -2.73934     0.00796     -2.73044     -2.74920
mean/mean                         0.00169      0.00015     0.00191      0.00150
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018988847732543945
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70869
epoch first part time 6.67572021484375e-06
replay_buffer._size: [35550]
collect time 0.0009796619415283203
inside mustsac before update, task 0, sumup 70869
inside mustsac after update, task 0, sumup 70504
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.007225751876831055, 'sac_diff2': 0.008688926696777344, 'sac_diff3': 0.011240482330322266, 'sac_diff4': 0.007835626602172852, 'sac_diff5': 0.05061197280883789, 'sac_diff6': 0.0004177093505859375, 'all': 0.08623099327087402}
diff5_list [0.010808706283569336, 0.01010751724243164, 0.009935140609741211, 0.009621858596801758, 0.010138750076293945]
time3 0.0008728504180908203
time4 0.08714914321899414
time5 0.08720541000366211
time7 0.009247064590454102
gen_weight_change tensor(-22.3358)
policy weight change tensor(38.0741, grad_fn=<SumBackward0>)
time8 0.002836465835571289
train_time 0.11786198616027832
eval time 0.11854982376098633
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:50,139 MainThread INFO: EPOCH:230
2024-01-23 01:01:50,139 MainThread INFO: Time Consumed:0.23966622352600098s
2024-01-23 01:01:50,139 MainThread INFO: Total Frames:35400s
  2%|▏         | 231/10000 [03:17<43:48,  3.72it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12565.70745
Train_Epoch_Reward                2633.87548
Running_Training_Average_Rewards  14307.31144
Explore_Time                      0.00097
Train___Time                      0.11786
Eval____Time                      0.11855
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12973.04745
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.24223     2.24097    91.14362     84.29685
alpha_0                           0.89094      0.00013    0.89112      0.89076
Alpha_loss                        -0.77700     0.00102    -0.77592     -0.77857
Training/policy_loss              -2.52359     0.00486    -2.51881     -2.53286
Training/qf1_loss                 14591.50859  845.08946  15745.31250  13336.07227
Training/qf2_loss                 16305.27051  815.96226  17347.20703  15007.63770
Training/pf_norm                  0.11936      0.03721    0.16809      0.07209
Training/qf1_norm                 3119.87847   138.11151  3347.45752   2929.47412
Training/qf2_norm                 240.98689    9.30096    252.23177    225.56793
log_std/mean                      -0.12854     0.00548    -0.12287     -0.13801
log_probs/mean                    -2.73439     0.00351    -2.72742     -2.73670
mean/mean                         0.00111      0.00053    0.00202      0.00051
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018990516662597656
epoch last part time3 0.002963542938232422
inside rlalgo, task 0, sumup 70504
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [35700]
collect time 0.0010159015655517578
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.007367849349975586, 'sac_diff2': 0.008584737777709961, 'sac_diff3': 0.01110219955444336, 'sac_diff4': 0.0072858333587646484, 'sac_diff5': 0.03303265571594238, 'sac_diff6': 0.00040149688720703125, 'all': 0.06798458099365234}
diff5_list [0.0067882537841796875, 0.0069010257720947266, 0.006479024887084961, 0.006161689758300781, 0.0067026615142822266]
time3 0
time4 0.06878519058227539
time5 0.06883573532104492
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3358)
policy weight change tensor(38.0870, grad_fn=<SumBackward0>)
time8 0.002012968063354492
train_time 0.08028411865234375
eval time 0.14894390106201172
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:50,397 MainThread INFO: EPOCH:231
2024-01-23 01:01:50,397 MainThread INFO: Time Consumed:0.23264002799987793s
2024-01-23 01:01:50,397 MainThread INFO: Total Frames:35550s
  2%|▏         | 232/10000 [03:18<43:04,  3.78it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12595.37563
Train_Epoch_Reward                12699.22486
Running_Training_Average_Rewards  14380.67956
Explore_Time                      0.00101
Train___Time                      0.08028
Eval____Time                      0.14894
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12424.95788
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       86.74633     2.32126     89.37251     84.14257
alpha_0                           0.89049      0.00013     0.89067      0.89032
Alpha_loss                        -0.77989     0.00047     -0.77921     -0.78046
Training/policy_loss              -2.52538     0.00665     -2.51824     -2.53511
Training/qf1_loss                 13853.50547  1220.68841  15592.81250  12206.12793
Training/qf2_loss                 15364.44570  1254.61705  17145.41602  13684.16699
Training/pf_norm                  0.13357      0.01649     0.15736      0.11673
Training/qf1_norm                 2860.39409   85.07193    2961.12109   2749.04248
Training/qf2_norm                 249.79991    6.54689     257.20984    242.35951
log_std/mean                      -0.12771     0.00004     -0.12767     -0.12777
log_probs/mean                    -2.73031     0.00711     -2.72284     -2.74097
mean/mean                         0.00011      0.00004     0.00015      0.00007
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01825428009033203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70504
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [35850]
collect time 0.0009751319885253906
inner_dict_sum {'sac_diff0': 0.0002200603485107422, 'sac_diff1': 0.0069615840911865234, 'sac_diff2': 0.008283138275146484, 'sac_diff3': 0.01009511947631836, 'sac_diff4': 0.007123708724975586, 'sac_diff5': 0.03223586082458496, 'sac_diff6': 0.00039267539978027344, 'all': 0.06531214714050293}
diff5_list [0.006895303726196289, 0.006567955017089844, 0.006257534027099609, 0.006039857864379883, 0.006475210189819336]
time3 0
time4 0.06608986854553223
time5 0.06613588333129883
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3358)
policy weight change tensor(38.1424, grad_fn=<SumBackward0>)
time8 0.001966238021850586
train_time 0.07737398147583008
eval time 0.14220404624938965
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:50,641 MainThread INFO: EPOCH:232
2024-01-23 01:01:50,642 MainThread INFO: Time Consumed:0.22284579277038574s
2024-01-23 01:01:50,642 MainThread INFO: Total Frames:35700s
  2%|▏         | 233/10000 [03:18<42:06,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12620.47978
Train_Epoch_Reward                7633.58032
Running_Training_Average_Rewards  14059.63205
Explore_Time                      0.00097
Train___Time                      0.07737
Eval____Time                      0.14220
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12427.87892
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.16232     2.18533     92.40427     86.42915
alpha_0                           0.89005      0.00013     0.89023      0.88987
Alpha_loss                        -0.78382     0.00113     -0.78213     -0.78514
Training/policy_loss              -2.52160     0.00421     -2.51720     -2.52958
Training/qf1_loss                 15581.60293  1439.18748  17080.26953  13349.28320
Training/qf2_loss                 17295.32812  1477.17466  18856.82031  14994.29883
Training/pf_norm                  0.11003      0.02038     0.14022      0.08339
Training/qf1_norm                 3062.29761   83.02287    3181.76343   2941.67993
Training/qf2_norm                 247.49299    5.90693     256.18158    239.99135
log_std/mean                      -0.13184     0.00007     -0.13177     -0.13195
log_probs/mean                    -2.73513     0.00475     -2.73046     -2.74412
mean/mean                         0.00233      0.00008     0.00244      0.00222
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018338918685913086
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70504
epoch first part time 2.384185791015625e-06
replay_buffer._size: [36000]
collect time 0.0008404254913330078
inner_dict_sum {'sac_diff0': 0.00022649765014648438, 'sac_diff1': 0.006639003753662109, 'sac_diff2': 0.007700681686401367, 'sac_diff3': 0.010010719299316406, 'sac_diff4': 0.00670170783996582, 'sac_diff5': 0.03135108947753906, 'sac_diff6': 0.0003781318664550781, 'all': 0.06300783157348633}
diff5_list [0.006429195404052734, 0.006180286407470703, 0.0062177181243896484, 0.0062749385833740234, 0.006248950958251953]
time3 0
time4 0.06373977661132812
time5 0.06378483772277832
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3358)
policy weight change tensor(38.1545, grad_fn=<SumBackward0>)
time8 0.0019183158874511719
train_time 0.07464790344238281
eval time 0.14387989044189453
epoch last part time 4.5299530029296875e-06
2024-01-23 01:01:50,885 MainThread INFO: EPOCH:233
2024-01-23 01:01:50,885 MainThread INFO: Time Consumed:0.22163081169128418s
2024-01-23 01:01:50,885 MainThread INFO: Total Frames:35850s
  2%|▏         | 234/10000 [03:18<41:21,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12640.94173
Train_Epoch_Reward                7307.60132
Running_Training_Average_Rewards  13625.87222
Explore_Time                      0.00084
Train___Time                      0.07465
Eval____Time                      0.14388
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12468.70402
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.07720     2.37491     89.98744     83.80399
alpha_0                           0.88960      0.00013     0.88978      0.88943
Alpha_loss                        -0.78738     0.00166     -0.78542     -0.78927
Training/policy_loss              -2.53313     0.00584     -2.52394     -2.53998
Training/qf1_loss                 13471.59160  1524.35704  16292.02148  12156.67969
Training/qf2_loss                 15117.39512  1563.09227  17992.95312  13746.32520
Training/pf_norm                  0.12272      0.01898     0.14122      0.08752
Training/qf1_norm                 3014.20947   92.92000    3115.85742   2898.10400
Training/qf2_norm                 254.81734    6.73358     263.07159    245.64241
log_std/mean                      -0.13799     0.00009     -0.13787     -0.13810
log_probs/mean                    -2.73675     0.00687     -2.72612     -2.74468
mean/mean                         0.00072      0.00002     0.00075      0.00068
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018204450607299805
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70504
epoch first part time 2.86102294921875e-06
replay_buffer._size: [36150]
collect time 0.0008196830749511719
inner_dict_sum {'sac_diff0': 0.00021600723266601562, 'sac_diff1': 0.006595134735107422, 'sac_diff2': 0.00785970687866211, 'sac_diff3': 0.010111808776855469, 'sac_diff4': 0.006693601608276367, 'sac_diff5': 0.03136181831359863, 'sac_diff6': 0.0003962516784667969, 'all': 0.06323432922363281}
diff5_list [0.006746530532836914, 0.0062046051025390625, 0.006006002426147461, 0.006308078765869141, 0.006096601486206055]
time3 0
time4 0.06399345397949219
time5 0.06403970718383789
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3358)
policy weight change tensor(38.1243, grad_fn=<SumBackward0>)
time8 0.0019061565399169922
train_time 0.07508969306945801
eval time 0.14372038841247559
epoch last part time 4.291534423828125e-06
2024-01-23 01:01:51,128 MainThread INFO: EPOCH:234
2024-01-23 01:01:51,128 MainThread INFO: Time Consumed:0.22183918952941895s
2024-01-23 01:01:51,129 MainThread INFO: Total Frames:36000s
  2%|▏         | 235/10000 [03:18<40:49,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12663.86044
Train_Epoch_Reward                6687.66788
Running_Training_Average_Rewards  13591.12769
Explore_Time                      0.00082
Train___Time                      0.07509
Eval____Time                      0.14372
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12553.55049
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.96749     1.86786     90.97481     85.89897
alpha_0                           0.88916      0.00013     0.88934      0.88898
Alpha_loss                        -0.79045     0.00104     -0.78877     -0.79181
Training/policy_loss              -2.51872     0.00465     -2.51079     -2.52370
Training/qf1_loss                 14346.49180  1204.97803  16371.39355  12717.86914
Training/qf2_loss                 16086.69512  1229.36263  18152.92188  14411.37012
Training/pf_norm                  0.10590      0.02431     0.14329      0.07898
Training/qf1_norm                 3092.59136   70.13361    3172.80273   2993.79199
Training/qf2_norm                 244.80190    4.97033     250.08115    236.78865
log_std/mean                      -0.13761     0.00008     -0.13750     -0.13773
log_probs/mean                    -2.73415     0.00514     -2.72578     -2.73999
mean/mean                         -0.00023     0.00003     -0.00018     -0.00027
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018203020095825195
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70504
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [36300]
collect time 0.0008008480072021484
inside mustsac before update, task 0, sumup 70504
inside mustsac after update, task 0, sumup 70689
inner_dict_sum {'sac_diff0': 0.00019860267639160156, 'sac_diff1': 0.006619453430175781, 'sac_diff2': 0.007932186126708984, 'sac_diff3': 0.010283946990966797, 'sac_diff4': 0.007005929946899414, 'sac_diff5': 0.04884195327758789, 'sac_diff6': 0.0003986358642578125, 'all': 0.08128070831298828}
diff5_list [0.010099649429321289, 0.009866714477539062, 0.009582996368408203, 0.009363889694213867, 0.009928703308105469]
time3 0.0008451938629150391
time4 0.0821065902709961
time5 0.08215880393981934
time7 0.009002685546875
gen_weight_change tensor(-22.2759)
policy weight change tensor(38.1486, grad_fn=<SumBackward0>)
time8 0.0019257068634033203
train_time 0.1109623908996582
eval time 0.12027120590209961
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:51,384 MainThread INFO: EPOCH:235
2024-01-23 01:01:51,385 MainThread INFO: Time Consumed:0.2343144416809082s
2024-01-23 01:01:51,385 MainThread INFO: Total Frames:36150s
  2%|▏         | 236/10000 [03:18<41:05,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12693.80751
Train_Epoch_Reward                22251.58088
Running_Training_Average_Rewards  13221.25015
Explore_Time                      0.00080
Train___Time                      0.11096
Eval____Time                      0.12027
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12611.28783
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       86.92845     1.80514    90.41009     85.21995
alpha_0                           0.88871      0.00013    0.88889      0.88854
Alpha_loss                        -0.79384     0.00072    -0.79258     -0.79478
Training/policy_loss              -2.52108     0.00465    -2.51206     -2.52506
Training/qf1_loss                 13670.77461  380.20412  14002.80762  12971.96094
Training/qf2_loss                 15462.95176  418.91263  15925.13086  14695.67676
Training/pf_norm                  0.12783      0.03611    0.17942      0.08146
Training/qf1_norm                 3163.63267   174.97202  3439.53906   2904.23779
Training/qf2_norm                 241.49467    4.09569    245.14566    234.35522
log_std/mean                      -0.12893     0.00460    -0.12298     -0.13513
log_probs/mean                    -2.73436     0.00658    -2.72212     -2.74204
mean/mean                         0.00141      0.00075    0.00235      0.00027
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01845717430114746
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70689
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [36450]
collect time 0.0009176731109619141
inner_dict_sum {'sac_diff0': 0.00023651123046875, 'sac_diff1': 0.006604194641113281, 'sac_diff2': 0.007926702499389648, 'sac_diff3': 0.009911775588989258, 'sac_diff4': 0.006667137145996094, 'sac_diff5': 0.03162789344787598, 'sac_diff6': 0.0003991127014160156, 'all': 0.06337332725524902}
diff5_list [0.006476640701293945, 0.006289005279541016, 0.006301164627075195, 0.006387472152709961, 0.006173610687255859]
time3 0
time4 0.06414341926574707
time5 0.06418871879577637
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2759)
policy weight change tensor(38.0777, grad_fn=<SumBackward0>)
time8 0.0019445419311523438
train_time 0.07497835159301758
eval time 0.16326689720153809
epoch last part time 4.291534423828125e-06
2024-01-23 01:01:51,648 MainThread INFO: EPOCH:236
2024-01-23 01:01:51,648 MainThread INFO: Time Consumed:0.24144291877746582s
2024-01-23 01:01:51,648 MainThread INFO: Total Frames:36300s
  2%|▏         | 237/10000 [03:19<41:35,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12606.11258
Train_Epoch_Reward                6495.97337
Running_Training_Average_Rewards  13242.51983
Explore_Time                      0.00091
Train___Time                      0.07498
Eval____Time                      0.16327
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11950.52155
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       86.90006     2.16921    91.07108     84.81412
alpha_0                           0.88827      0.00013    0.88845      0.88809
Alpha_loss                        -0.79725     0.00097    -0.79594     -0.79825
Training/policy_loss              -2.51753     0.00718    -2.50958     -2.53033
Training/qf1_loss                 13748.78125  489.51910  14225.30664  12898.63086
Training/qf2_loss                 15455.66953  510.70860  16008.15918  14601.98633
Training/pf_norm                  0.12853      0.04928    0.20720      0.06049
Training/qf1_norm                 3083.44805   93.21898   3260.12378   2992.60742
Training/qf2_norm                 237.54745    5.78737    248.70345    232.19704
log_std/mean                      -0.13587     0.00015    -0.13563     -0.13604
log_probs/mean                    -2.73468     0.00790    -2.72580     -2.74884
mean/mean                         0.00063      0.00015    0.00088      0.00050
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018092870712280273
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70689
epoch first part time 2.86102294921875e-06
replay_buffer._size: [36600]
collect time 0.0008890628814697266
inner_dict_sum {'sac_diff0': 0.00021505355834960938, 'sac_diff1': 0.006500959396362305, 'sac_diff2': 0.007669925689697266, 'sac_diff3': 0.009743452072143555, 'sac_diff4': 0.0067217350006103516, 'sac_diff5': 0.03096914291381836, 'sac_diff6': 0.00038242340087890625, 'all': 0.06220269203186035}
diff5_list [0.006413936614990234, 0.0061626434326171875, 0.0062978267669677734, 0.006128549575805664, 0.0059661865234375]
time3 0
time4 0.06297969818115234
time5 0.06302428245544434
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2759)
policy weight change tensor(38.0770, grad_fn=<SumBackward0>)
time8 0.0019104480743408203
train_time 0.07368159294128418
eval time 0.16194987297058105
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:51,908 MainThread INFO: EPOCH:237
2024-01-23 01:01:51,908 MainThread INFO: Time Consumed:0.23883676528930664s
2024-01-23 01:01:51,908 MainThread INFO: Total Frames:36450s
  2%|▏         | 238/10000 [03:19<41:50,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12518.19568
Train_Epoch_Reward                11336.98170
Running_Training_Average_Rewards  12427.51046
Explore_Time                      0.00089
Train___Time                      0.07368
Eval____Time                      0.16195
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11966.35431
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       86.00736     1.87071    88.69614     83.83276
alpha_0                           0.88783      0.00013    0.88800      0.88765
Alpha_loss                        -0.79950     0.00116    -0.79751     -0.80081
Training/policy_loss              -2.51292     0.00367    -2.50861     -2.51909
Training/qf1_loss                 12575.54141  693.89874  13515.32031  11820.08008
Training/qf2_loss                 14546.39199  710.29159  15556.96875  13749.18457
Training/pf_norm                  0.13227      0.02473    0.16678      0.09217
Training/qf1_norm                 3356.71567   81.08993   3481.07764   3260.44775
Training/qf2_norm                 242.90281    5.05492    250.27660    236.98192
log_std/mean                      -0.11963     0.00007    -0.11957     -0.11975
log_probs/mean                    -2.72528     0.00419    -2.71986     -2.73223
mean/mean                         0.00079      0.00016    0.00100      0.00056
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018445730209350586
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70689
epoch first part time 2.384185791015625e-06
replay_buffer._size: [36750]
collect time 0.0009369850158691406
inner_dict_sum {'sac_diff0': 0.00022935867309570312, 'sac_diff1': 0.006533384323120117, 'sac_diff2': 0.0077784061431884766, 'sac_diff3': 0.010113239288330078, 'sac_diff4': 0.006968021392822266, 'sac_diff5': 0.0321505069732666, 'sac_diff6': 0.0003833770751953125, 'all': 0.06415629386901855}
diff5_list [0.006669282913208008, 0.006268739700317383, 0.006495475769042969, 0.006367683410644531, 0.006349325180053711]
time3 0
time4 0.06492042541503906
time5 0.06496429443359375
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2759)
policy weight change tensor(38.0908, grad_fn=<SumBackward0>)
time8 0.0019655227661132812
train_time 0.07569026947021484
eval time 0.16084051132202148
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:52,169 MainThread INFO: EPOCH:238
2024-01-23 01:01:52,170 MainThread INFO: Time Consumed:0.2397298812866211s
2024-01-23 01:01:52,170 MainThread INFO: Total Frames:36600s
  2%|▏         | 239/10000 [03:19<42:04,  3.87it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12425.23909
Train_Epoch_Reward                15965.12215
Running_Training_Average_Rewards  12837.33188
Explore_Time                      0.00093
Train___Time                      0.07569
Eval____Time                      0.16084
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11955.34190
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.84682     0.68326    89.70782     87.92413
alpha_0                           0.88738      0.00013    0.88756      0.88720
Alpha_loss                        -0.80429     0.00176    -0.80245     -0.80753
Training/policy_loss              -2.52242     0.00845    -2.51296     -2.53595
Training/qf1_loss                 13002.41309  389.38029  13494.29102  12575.46680
Training/qf2_loss                 15267.90020  405.61165  15771.28418  14807.62695
Training/pf_norm                  0.14574      0.01969    0.18109      0.12175
Training/qf1_norm                 3734.47378   41.08379   3788.02295   3682.12549
Training/qf2_norm                 250.17798    1.90056    252.64455    247.71057
log_std/mean                      -0.13566     0.00004    -0.13561     -0.13571
log_probs/mean                    -2.73720     0.00968    -2.72653     -2.75303
mean/mean                         0.00306      0.00002    0.00308      0.00302
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018726587295532227
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70689
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [36900]
collect time 0.0009205341339111328
inner_dict_sum {'sac_diff0': 0.00022363662719726562, 'sac_diff1': 0.00641322135925293, 'sac_diff2': 0.0077075958251953125, 'sac_diff3': 0.009838104248046875, 'sac_diff4': 0.0067212581634521484, 'sac_diff5': 0.03131723403930664, 'sac_diff6': 0.0003902912139892578, 'all': 0.06261134147644043}
diff5_list [0.006319284439086914, 0.006002902984619141, 0.006082057952880859, 0.0068209171295166016, 0.006092071533203125]
time3 0
time4 0.0633690357208252
time5 0.06341743469238281
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2759)
policy weight change tensor(38.1764, grad_fn=<SumBackward0>)
time8 0.0019147396087646484
train_time 0.07472825050354004
eval time 0.1626894474029541
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:52,432 MainThread INFO: EPOCH:239
2024-01-23 01:01:52,433 MainThread INFO: Time Consumed:0.24062371253967285s
2024-01-23 01:01:52,433 MainThread INFO: Total Frames:36750s
  2%|▏         | 240/10000 [03:20<42:14,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12326.28145
Train_Epoch_Reward                3363.43947
Running_Training_Average_Rewards  12499.63484
Explore_Time                      0.00092
Train___Time                      0.07473
Eval____Time                      0.16269
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11931.17018
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.28887     2.19658     90.29344     85.28124
alpha_0                           0.88694      0.00013     0.88712      0.88676
Alpha_loss                        -0.80746     0.00104     -0.80610     -0.80906
Training/policy_loss              -2.51743     0.00088     -2.51630     -2.51888
Training/qf1_loss                 13304.81445  1384.23832  14861.71680  11306.41406
Training/qf2_loss                 15430.10625  1435.75817  17008.42578  13353.50000
Training/pf_norm                  0.12410      0.03232     0.16041      0.07999
Training/qf1_norm                 3644.45518   108.85677   3755.96460   3500.56079
Training/qf2_norm                 246.55788    5.91186     252.17372    238.50665
log_std/mean                      -0.11928     0.00021     -0.11906     -0.11963
log_probs/mean                    -2.73551     0.00113     -2.73426     -2.73763
mean/mean                         0.00162      0.00006     0.00171      0.00153
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018212318420410156
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70689
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [37050]
collect time 0.0008852481842041016
inside mustsac before update, task 0, sumup 70689
inside mustsac after update, task 0, sumup 70669
inner_dict_sum {'sac_diff0': 0.00020003318786621094, 'sac_diff1': 0.006464242935180664, 'sac_diff2': 0.007924795150756836, 'sac_diff3': 0.010163545608520508, 'sac_diff4': 0.00712895393371582, 'sac_diff5': 0.04855608940124512, 'sac_diff6': 0.00039649009704589844, 'all': 0.08083415031433105}
diff5_list [0.010258197784423828, 0.009415149688720703, 0.009683609008789062, 0.009943485260009766, 0.009255647659301758]
time3 0.0008389949798583984
time4 0.0816504955291748
time5 0.08170652389526367
time7 0.009289979934692383
gen_weight_change tensor(-22.2664)
policy weight change tensor(38.1871, grad_fn=<SumBackward0>)
time8 0.0025937557220458984
train_time 0.11139464378356934
eval time 0.12118077278137207
epoch last part time 4.76837158203125e-06
2024-01-23 01:01:52,689 MainThread INFO: EPOCH:240
2024-01-23 01:01:52,690 MainThread INFO: Time Consumed:0.23569059371948242s
2024-01-23 01:01:52,690 MainThread INFO: Total Frames:36900s
  2%|▏         | 241/10000 [03:20<42:15,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12220.88407
Train_Epoch_Reward                19224.71871
Running_Training_Average_Rewards  12681.15555
Explore_Time                      0.00088
Train___Time                      0.11139
Eval____Time                      0.12118
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11919.07364
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.95426     2.25125     90.17979     83.77879
alpha_0                           0.88649      0.00013     0.88667      0.88632
Alpha_loss                        -0.80976     0.00087     -0.80891     -0.81136
Training/policy_loss              -2.51079     0.00508     -2.50405     -2.51672
Training/qf1_loss                 13481.77500  1203.21449  14581.43164  11267.01758
Training/qf2_loss                 15506.08848  1269.66598  16846.25195  13196.94043
Training/pf_norm                  0.13353      0.02809     0.18118      0.09583
Training/qf1_norm                 3442.25381   256.18908   3779.79419   3149.77515
Training/qf2_norm                 243.18827    5.75802     250.21645    236.36591
log_std/mean                      -0.13689     0.00633     -0.13077     -0.14910
log_probs/mean                    -2.72670     0.00625     -2.71573     -2.73431
mean/mean                         0.00187      0.00062     0.00286      0.00110
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018540382385253906
epoch last part time3 0.0025336742401123047
inside rlalgo, task 0, sumup 70669
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [37200]
collect time 0.0009303092956542969
inner_dict_sum {'sac_diff0': 0.00022745132446289062, 'sac_diff1': 0.006807088851928711, 'sac_diff2': 0.007785320281982422, 'sac_diff3': 0.009871482849121094, 'sac_diff4': 0.006713151931762695, 'sac_diff5': 0.03128170967102051, 'sac_diff6': 0.0003790855407714844, 'all': 0.0630652904510498}
diff5_list [0.0065174102783203125, 0.005990505218505859, 0.006285905838012695, 0.006375789642333984, 0.006112098693847656]
time3 0
time4 0.06383538246154785
time5 0.06388068199157715
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2664)
policy weight change tensor(38.2174, grad_fn=<SumBackward0>)
time8 0.0019407272338867188
train_time 0.07484197616577148
eval time 0.15103936195373535
epoch last part time 4.5299530029296875e-06
2024-01-23 01:01:52,943 MainThread INFO: EPOCH:241
2024-01-23 01:01:52,943 MainThread INFO: Time Consumed:0.22908782958984375s
2024-01-23 01:01:52,943 MainThread INFO: Total Frames:37050s
  2%|▏         | 242/10000 [03:20<41:48,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12182.52517
Train_Epoch_Reward                12264.49877
Running_Training_Average_Rewards  12399.43286
Explore_Time                      0.00093
Train___Time                      0.07484
Eval____Time                      0.15104
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12041.36881
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.61389     3.36453     94.76366     85.09893
alpha_0                           0.88605      0.00013     0.88623      0.88587
Alpha_loss                        -0.81393     0.00154     -0.81165     -0.81603
Training/policy_loss              -2.52510     0.00629     -2.51356     -2.53098
Training/qf1_loss                 14301.04492  1430.14176  16514.18164  12679.79199
Training/qf2_loss                 16400.32852  1490.32888  18756.37695  14708.87988
Training/pf_norm                  0.12879      0.01300     0.15008      0.11630
Training/qf1_norm                 3432.31523   141.95867   3690.82617   3296.22729
Training/qf2_norm                 259.56842    9.61319     277.12714    249.61571
log_std/mean                      -0.14656     0.00010     -0.14637     -0.14668
log_probs/mean                    -2.73332     0.00725     -2.72003     -2.73996
mean/mean                         0.00202      0.00001     0.00204      0.00200
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018393754959106445
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70669
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [37350]
collect time 0.0009162425994873047
inner_dict_sum {'sac_diff0': 0.00022745132446289062, 'sac_diff1': 0.0064411163330078125, 'sac_diff2': 0.007532358169555664, 'sac_diff3': 0.010060548782348633, 'sac_diff4': 0.006596088409423828, 'sac_diff5': 0.031439781188964844, 'sac_diff6': 0.0003859996795654297, 'all': 0.0626833438873291}
diff5_list [0.006461143493652344, 0.006275177001953125, 0.006161212921142578, 0.006449699401855469, 0.006092548370361328]
time3 0
time4 0.0634610652923584
time5 0.06350445747375488
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2664)
policy weight change tensor(38.1818, grad_fn=<SumBackward0>)
time8 0.0018880367279052734
train_time 0.07438778877258301
eval time 0.15594911575317383
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:53,198 MainThread INFO: EPOCH:242
2024-01-23 01:01:53,199 MainThread INFO: Time Consumed:0.2336583137512207s
2024-01-23 01:01:53,199 MainThread INFO: Total Frames:37200s
  2%|▏         | 243/10000 [03:20<41:43,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12137.34650
Train_Epoch_Reward                21079.14917
Running_Training_Average_Rewards  12835.00453
Explore_Time                      0.00091
Train___Time                      0.07439
Eval____Time                      0.15595
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11976.09224
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.18995     2.69571     94.59476     87.14546
alpha_0                           0.88561      0.00013     0.88579      0.88543
Alpha_loss                        -0.81713     0.00136     -0.81567     -0.81909
Training/policy_loss              -2.53103     0.00332     -2.52624     -2.53513
Training/qf1_loss                 14302.50625  1227.40876  15583.53125  12274.33398
Training/qf2_loss                 16304.48223  1266.12062  17684.62500  14214.01367
Training/pf_norm                  0.12547      0.02949     0.17103      0.08947
Training/qf1_norm                 3571.15991   110.57676   3761.12476   3445.33545
Training/qf2_norm                 265.11515    7.71242     277.82590    256.28699
log_std/mean                      -0.13748     0.00009     -0.13734     -0.13760
log_probs/mean                    -2.73194     0.00400     -2.72625     -2.73702
mean/mean                         -0.00080     0.00006     -0.00070     -0.00087
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018394947052001953
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70669
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [37500]
collect time 0.0008864402770996094
inner_dict_sum {'sac_diff0': 0.00023221969604492188, 'sac_diff1': 0.006550788879394531, 'sac_diff2': 0.007817506790161133, 'sac_diff3': 0.010489940643310547, 'sac_diff4': 0.007069826126098633, 'sac_diff5': 0.031398773193359375, 'sac_diff6': 0.00038123130798339844, 'all': 0.06394028663635254}
diff5_list [0.006279945373535156, 0.006455898284912109, 0.0061473846435546875, 0.006304264068603516, 0.006211280822753906]
time3 0
time4 0.06466007232666016
time5 0.06470298767089844
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2664)
policy weight change tensor(38.1257, grad_fn=<SumBackward0>)
time8 0.001960277557373047
train_time 0.07554817199707031
eval time 0.1618649959564209
epoch last part time 5.9604644775390625e-06
2024-01-23 01:01:53,461 MainThread INFO: EPOCH:243
2024-01-23 01:01:53,461 MainThread INFO: Time Consumed:0.24066638946533203s
2024-01-23 01:01:53,461 MainThread INFO: Total Frames:37350s
  2%|▏         | 244/10000 [03:21<42:05,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12084.81904
Train_Epoch_Reward                34854.22140
Running_Training_Average_Rewards  13858.29676
Explore_Time                      0.00088
Train___Time                      0.07555
Eval____Time                      0.16186
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11943.42945
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.53560     1.04059     91.37916     88.14959
alpha_0                           0.88517      0.00013     0.88534      0.88499
Alpha_loss                        -0.82098     0.00095     -0.82008     -0.82221
Training/policy_loss              -2.50872     0.00832     -2.49325     -2.51659
Training/qf1_loss                 14490.01836  1346.63955  16946.44922  12918.32715
Training/qf2_loss                 16589.57012  1364.25732  19054.60156  14966.32617
Training/pf_norm                  0.13847      0.01544     0.16100      0.11892
Training/qf1_norm                 3599.29775   58.32569    3698.00391   3516.48682
Training/qf2_norm                 242.73113    2.74585     247.51343    238.93982
log_std/mean                      -0.12602     0.00013     -0.12586     -0.12622
log_probs/mean                    -2.73592     0.00915     -2.71900     -2.74478
mean/mean                         0.00109      0.00008     0.00119      0.00098
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020064115524291992
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70669
epoch first part time 2.86102294921875e-06
replay_buffer._size: [37650]
collect time 0.0009870529174804688
inner_dict_sum {'sac_diff0': 0.00023603439331054688, 'sac_diff1': 0.007078886032104492, 'sac_diff2': 0.008288383483886719, 'sac_diff3': 0.011090517044067383, 'sac_diff4': 0.007012367248535156, 'sac_diff5': 0.03211402893066406, 'sac_diff6': 0.00038886070251464844, 'all': 0.06620907783508301}
diff5_list [0.006684541702270508, 0.006493091583251953, 0.0063359737396240234, 0.00614476203918457, 0.006455659866333008]
time3 0
time4 0.06695175170898438
time5 0.06699562072753906
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2664)
policy weight change tensor(38.1665, grad_fn=<SumBackward0>)
time8 0.0019829273223876953
train_time 0.07839179039001465
eval time 0.1554734706878662
epoch last part time 5.9604644775390625e-06
2024-01-23 01:01:53,722 MainThread INFO: EPOCH:244
2024-01-23 01:01:53,722 MainThread INFO: Time Consumed:0.237274169921875s
2024-01-23 01:01:53,723 MainThread INFO: Total Frames:37500s
  2%|▏         | 245/10000 [03:21<42:06,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12018.00996
Train_Epoch_Reward                30348.28869
Running_Training_Average_Rewards  13975.19344
Explore_Time                      0.00098
Train___Time                      0.07839
Eval____Time                      0.15547
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11885.45970
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.73012     2.49947     92.74296     86.29697
alpha_0                           0.88472      0.00013     0.88490      0.88455
Alpha_loss                        -0.82448     0.00138     -0.82229     -0.82604
Training/policy_loss              -2.51795     0.00598     -2.50718     -2.52505
Training/qf1_loss                 13969.94785  1198.48513  16339.02051  13137.02539
Training/qf2_loss                 16089.03848  1237.72966  18526.80078  15223.22363
Training/pf_norm                  0.13985      0.02845     0.18933      0.10416
Training/qf1_norm                 3662.23174   105.13563   3796.25391   3512.81787
Training/qf2_norm                 253.62076    6.83115     261.94281    244.17041
log_std/mean                      -0.13171     0.00011     -0.13160     -0.13188
log_probs/mean                    -2.73700     0.00680     -2.72460     -2.74502
mean/mean                         0.00100      0.00004     0.00105      0.00093
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01829385757446289
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70669
epoch first part time 8.106231689453125e-06
replay_buffer._size: [37800]
collect time 0.0009288787841796875
inside mustsac before update, task 0, sumup 70669
inside mustsac after update, task 0, sumup 71161
inner_dict_sum {'sac_diff0': 0.00022077560424804688, 'sac_diff1': 0.007648944854736328, 'sac_diff2': 0.008901119232177734, 'sac_diff3': 0.011672258377075195, 'sac_diff4': 0.008328914642333984, 'sac_diff5': 0.05497598648071289, 'sac_diff6': 0.0004432201385498047, 'all': 0.09219121932983398}
diff5_list [0.012128353118896484, 0.011209249496459961, 0.009928226470947266, 0.00959920883178711, 0.01211094856262207]
time3 0.0009057521820068359
time4 0.09308004379272461
time5 0.09314274787902832
time7 0.009406089782714844
gen_weight_change tensor(-22.3469)
policy weight change tensor(38.1705, grad_fn=<SumBackward0>)
time8 0.002042055130004883
train_time 0.12370681762695312
eval time 0.12433576583862305
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:53,995 MainThread INFO: EPOCH:245
2024-01-23 01:01:53,996 MainThread INFO: Time Consumed:0.25129175186157227s
2024-01-23 01:01:53,996 MainThread INFO: Total Frames:37650s
  2%|▏         | 246/10000 [03:21<42:49,  3.80it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11942.87533
Train_Epoch_Reward                4196.58171
Running_Training_Average_Rewards  13627.90505
Explore_Time                      0.00092
Train___Time                      0.12371
Eval____Time                      0.12434
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11859.94148
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.87426     0.83770    90.83293     88.79051
alpha_0                           0.88428      0.00013    0.88446      0.88410
Alpha_loss                        -0.82743     0.00034    -0.82695     -0.82796
Training/policy_loss              -2.51406     0.00969    -2.50233     -2.52507
Training/qf1_loss                 14511.98008  273.60302  14989.63086  14151.91211
Training/qf2_loss                 16660.80215  224.72395  16928.83984  16244.57520
Training/pf_norm                  0.14448      0.03198    0.17887      0.09346
Training/qf1_norm                 3609.45503   163.74016  3890.76294   3381.58447
Training/qf2_norm                 251.80480    3.38875    256.75284    246.70290
log_std/mean                      -0.13261     0.00714    -0.12262     -0.13933
log_probs/mean                    -2.73354     0.00768    -2.72127     -2.74389
mean/mean                         0.00118      0.00068    0.00212      0.00037
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018826723098754883
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71161
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [37950]
collect time 0.0008592605590820312
inner_dict_sum {'sac_diff0': 0.0002052783966064453, 'sac_diff1': 0.007565736770629883, 'sac_diff2': 0.008539199829101562, 'sac_diff3': 0.01113438606262207, 'sac_diff4': 0.007788658142089844, 'sac_diff5': 0.036043643951416016, 'sac_diff6': 0.0004229545593261719, 'all': 0.07169985771179199}
diff5_list [0.0075876712799072266, 0.0074460506439208984, 0.007118701934814453, 0.006990671157836914, 0.0069005489349365234]
time3 0
time4 0.07253217697143555
time5 0.07259058952331543
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3469)
policy weight change tensor(38.1982, grad_fn=<SumBackward0>)
time8 0.002076864242553711
train_time 0.08391761779785156
eval time 0.15090084075927734
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:54,256 MainThread INFO: EPOCH:246
2024-01-23 01:01:54,256 MainThread INFO: Time Consumed:0.2380979061126709s
2024-01-23 01:01:54,256 MainThread INFO: Total Frames:37800s
  2%|▏         | 247/10000 [03:21<42:39,  3.81it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11955.32739
Train_Epoch_Reward                15088.81850
Running_Training_Average_Rewards  14033.70369
Explore_Time                      0.00085
Train___Time                      0.08392
Eval____Time                      0.15090
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12075.04220
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.57113     1.85456    93.54316     88.64210
alpha_0                           0.88384      0.00013    0.88401      0.88366
Alpha_loss                        -0.83108     0.00124    -0.82908     -0.83232
Training/policy_loss              -2.50912     0.00574    -2.50147     -2.51768
Training/qf1_loss                 13858.42266  577.75395  14841.91211  13327.54492
Training/qf2_loss                 16028.52637  622.69529  17086.56836  15433.44727
Training/pf_norm                  0.10956      0.01483    0.12786      0.09019
Training/qf1_norm                 3642.00986   83.93498   3777.15015   3538.06958
Training/qf2_norm                 245.18909    4.91671    253.01649    239.92943
log_std/mean                      -0.13495     0.00007    -0.13482     -0.13501
log_probs/mean                    -2.73582     0.00644    -2.72776     -2.74545
mean/mean                         0.00076      0.00003    0.00079      0.00070
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01850104331970215
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71161
epoch first part time 2.86102294921875e-06
replay_buffer._size: [38100]
collect time 0.0009708404541015625
inner_dict_sum {'sac_diff0': 0.00020575523376464844, 'sac_diff1': 0.008064985275268555, 'sac_diff2': 0.009406805038452148, 'sac_diff3': 0.012621164321899414, 'sac_diff4': 0.008397579193115234, 'sac_diff5': 0.037880897521972656, 'sac_diff6': 0.0004391670227050781, 'all': 0.07701635360717773}
diff5_list [0.007524967193603516, 0.0075914859771728516, 0.007493495941162109, 0.007546186447143555, 0.007724761962890625]
time3 0
time4 0.07790946960449219
time5 0.07796311378479004
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3469)
policy weight change tensor(38.2586, grad_fn=<SumBackward0>)
time8 0.0018796920776367188
train_time 0.0895838737487793
eval time 0.16101288795471191
epoch last part time 6.4373016357421875e-06
2024-01-23 01:01:54,532 MainThread INFO: EPOCH:247
2024-01-23 01:01:54,532 MainThread INFO: Time Consumed:0.25396180152893066s
2024-01-23 01:01:54,532 MainThread INFO: Total Frames:37950s
  2%|▏         | 248/10000 [03:22<43:19,  3.75it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11962.84856
Train_Epoch_Reward                7364.07169
Running_Training_Average_Rewards  13456.42580
Explore_Time                      0.00097
Train___Time                      0.08958
Eval____Time                      0.16101
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12041.56604
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.64133     2.02298     91.77758     86.34418
alpha_0                           0.88340      0.00012     0.88357      0.88322
Alpha_loss                        -0.83368     0.00102     -0.83249     -0.83533
Training/policy_loss              -2.51186     0.00478     -2.50480     -2.51881
Training/qf1_loss                 13869.24121  1785.19492  16163.23633  11529.35059
Training/qf2_loss                 15971.68945  1816.90193  18288.43164  13581.62988
Training/pf_norm                  0.13113      0.00889     0.14225      0.11573
Training/qf1_norm                 3535.82754   85.08737    3674.89941   3439.04468
Training/qf2_norm                 248.36823    5.47265     256.87790    242.16068
log_std/mean                      -0.12471     0.00014     -0.12453     -0.12492
log_probs/mean                    -2.72958     0.00527     -2.72166     -2.73713
mean/mean                         0.00037      0.00010     0.00048      0.00021
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018430233001708984
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71161
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [38250]
collect time 0.0009489059448242188
inner_dict_sum {'sac_diff0': 0.00022292137145996094, 'sac_diff1': 0.007188558578491211, 'sac_diff2': 0.008318901062011719, 'sac_diff3': 0.010390043258666992, 'sac_diff4': 0.006977558135986328, 'sac_diff5': 0.03243851661682129, 'sac_diff6': 0.0003910064697265625, 'all': 0.06592750549316406}
diff5_list [0.006702423095703125, 0.006329536437988281, 0.006359577178955078, 0.0065631866455078125, 0.006483793258666992]
time3 0
time4 0.06670737266540527
time5 0.06676125526428223
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3469)
policy weight change tensor(38.3003, grad_fn=<SumBackward0>)
time8 0.0018496513366699219
train_time 0.07784557342529297
eval time 0.18340396881103516
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:54,818 MainThread INFO: EPOCH:248
2024-01-23 01:01:54,819 MainThread INFO: Time Consumed:0.2645254135131836s
2024-01-23 01:01:54,819 MainThread INFO: Total Frames:38100s
  2%|▏         | 249/10000 [03:22<44:22,  3.66it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11965.58318
Train_Epoch_Reward                12900.39463
Running_Training_Average_Rewards  13298.26734
Explore_Time                      0.00094
Train___Time                      0.07785
Eval____Time                      0.18340
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11982.68811
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.03778     0.65010    88.94138     87.13718
alpha_0                           0.88295      0.00012    0.88313      0.88278
Alpha_loss                        -0.83753     0.00119    -0.83632     -0.83956
Training/policy_loss              -2.51721     0.00526    -2.51166     -2.52657
Training/qf1_loss                 13619.84102  954.02466  14972.88574  12284.84180
Training/qf2_loss                 15832.59434  960.81115  17193.15234  14490.63770
Training/pf_norm                  0.10328      0.02157    0.14212      0.08290
Training/qf1_norm                 3573.55884   22.67374   3599.71289   3548.27002
Training/qf2_norm                 248.75472    1.70987    251.06120    246.34332
log_std/mean                      -0.12953     0.00005    -0.12945     -0.12957
log_probs/mean                    -2.73351     0.00594    -2.72775     -2.74438
mean/mean                         0.00032      0.00012    0.00049      0.00016
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.020055055618286133
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71161
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [38400]
collect time 0.0009582042694091797
inner_dict_sum {'sac_diff0': 0.00023865699768066406, 'sac_diff1': 0.006934642791748047, 'sac_diff2': 0.008582353591918945, 'sac_diff3': 0.010859966278076172, 'sac_diff4': 0.007063388824462891, 'sac_diff5': 0.03359198570251465, 'sac_diff6': 0.0003962516784667969, 'all': 0.06766724586486816}
diff5_list [0.007456779479980469, 0.006257057189941406, 0.0064508914947509766, 0.006494760513305664, 0.006932497024536133]
time3 0
time4 0.06844091415405273
time5 0.06849050521850586
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3469)
policy weight change tensor(38.3180, grad_fn=<SumBackward0>)
time8 0.00189971923828125
train_time 0.07967996597290039
eval time 0.1765587329864502
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:55,102 MainThread INFO: EPOCH:249
2024-01-23 01:01:55,102 MainThread INFO: Time Consumed:0.25966477394104004s
2024-01-23 01:01:55,103 MainThread INFO: Total Frames:38250s
  2%|▎         | 250/10000 [03:22<44:53,  3.62it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11971.13297
Train_Epoch_Reward                18096.16380
Running_Training_Average_Rewards  13837.29163
Explore_Time                      0.00095
Train___Time                      0.07968
Eval____Time                      0.17656
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11986.66801
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.51361     1.33079     93.56498     89.99407
alpha_0                           0.88251      0.00012     0.88269      0.88234
Alpha_loss                        -0.83999     0.00125     -0.83806     -0.84157
Training/policy_loss              -2.50466     0.00468     -2.49602     -2.50991
Training/qf1_loss                 15151.21367  1244.86424  16104.76074  12699.27832
Training/qf2_loss                 17410.67266  1251.64034  18315.34180  14934.90820
Training/pf_norm                  0.09266      0.02236     0.12958      0.06720
Training/qf1_norm                 3765.31885   69.22018    3863.71387   3684.85474
Training/qf2_norm                 259.05863    3.70483     264.81937    254.83264
log_std/mean                      -0.13152     0.00003     -0.13147     -0.13156
log_probs/mean                    -2.72624     0.00535     -2.71618     -2.73165
mean/mean                         0.00101      0.00003     0.00106      0.00098
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02006363868713379
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71161
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [38550]
collect time 0.0009300708770751953
inside mustsac before update, task 0, sumup 71161
inside mustsac after update, task 0, sumup 70008
inner_dict_sum {'sac_diff0': 0.00022411346435546875, 'sac_diff1': 0.008229732513427734, 'sac_diff2': 0.009771108627319336, 'sac_diff3': 0.013204813003540039, 'sac_diff4': 0.008874654769897461, 'sac_diff5': 0.06049776077270508, 'sac_diff6': 0.00045609474182128906, 'all': 0.1012582778930664}
diff5_list [0.012311458587646484, 0.01222991943359375, 0.01223611831665039, 0.012012481689453125, 0.011707782745361328]
time3 0.0009660720825195312
time4 0.10222434997558594
time5 0.1022946834564209
time7 0.009241819381713867
gen_weight_change tensor(-22.4972)
policy weight change tensor(38.3382, grad_fn=<SumBackward0>)
time8 0.0026810169219970703
train_time 0.13460755348205566
eval time 0.12942814826965332
epoch last part time 7.152557373046875e-06
2024-01-23 01:01:55,394 MainThread INFO: EPOCH:250
2024-01-23 01:01:55,394 MainThread INFO: Time Consumed:0.26740336418151855s
2024-01-23 01:01:55,394 MainThread INFO: Total Frames:38400s
  3%|▎         | 251/10000 [03:23<45:46,  3.55it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11976.90920
Train_Epoch_Reward                11730.54145
Running_Training_Average_Rewards  13645.47182
Explore_Time                      0.00092
Train___Time                      0.13461
Eval____Time                      0.12943
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11976.83599
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.24962     3.57702     94.04976     84.85430
alpha_0                           0.88207      0.00012     0.88225      0.88189
Alpha_loss                        -0.84425     0.00061     -0.84350     -0.84502
Training/policy_loss              -2.51418     0.00627     -2.50891     -2.52322
Training/qf1_loss                 13370.79023  1688.77745  15019.75977  10617.65918
Training/qf2_loss                 15625.45742  1744.11217  17622.96484  12916.21387
Training/pf_norm                  0.12530      0.03698     0.17649      0.06301
Training/qf1_norm                 3714.54238   249.32574   4186.11377   3453.09668
Training/qf2_norm                 254.21100    7.87214     263.52686    242.82277
log_std/mean                      -0.13246     0.00675     -0.12083     -0.13808
log_probs/mean                    -2.73338     0.00718     -2.72329     -2.74403
mean/mean                         0.00078      0.00089     0.00214      -0.00033
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020511150360107422
epoch last part time3 0.0028526782989501953
inside rlalgo, task 0, sumup 70008
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [38700]
collect time 0.0009777545928955078
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.007156848907470703, 'sac_diff2': 0.008702754974365234, 'sac_diff3': 0.011522531509399414, 'sac_diff4': 0.007639169692993164, 'sac_diff5': 0.03404712677001953, 'sac_diff6': 0.0003974437713623047, 'all': 0.06968045234680176}
diff5_list [0.007946491241455078, 0.006951570510864258, 0.006632566452026367, 0.0062787532806396484, 0.00623774528503418]
time3 0
time4 0.07047080993652344
time5 0.07051920890808105
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4972)
policy weight change tensor(38.3600, grad_fn=<SumBackward0>)
time8 0.001856088638305664
train_time 0.0818173885345459
eval time 0.15259790420532227
epoch last part time 5.7220458984375e-06
2024-01-23 01:01:55,660 MainThread INFO: EPOCH:251
2024-01-23 01:01:55,662 MainThread INFO: Time Consumed:0.24014568328857422s
2024-01-23 01:01:55,662 MainThread INFO: Total Frames:38550s
  3%|▎         | 252/10000 [03:23<44:55,  3.62it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11978.15409
Train_Epoch_Reward                23854.93665
Running_Training_Average_Rewards  13919.46271
Explore_Time                      0.00097
Train___Time                      0.08182
Eval____Time                      0.15260
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12053.81770
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.21031     2.38621    94.88918     88.52010
alpha_0                           0.88163      0.00012    0.88181      0.88145
Alpha_loss                        -0.84796     0.00087    -0.84706     -0.84943
Training/policy_loss              -2.52397     0.00253    -2.52022     -2.52790
Training/qf1_loss                 14424.65156  939.49543  15849.22168  13115.16602
Training/qf2_loss                 16862.75684  994.36827  18424.59180  15536.26562
Training/pf_norm                  0.09624      0.02235    0.13261      0.06265
Training/qf1_norm                 3914.13418   123.82738  4153.34180   3810.71094
Training/qf2_norm                 268.13340    6.81792    281.47452    263.11490
log_std/mean                      -0.13430     0.00004    -0.13422     -0.13434
log_probs/mean                    -2.73608     0.00269    -2.73209     -2.73998
mean/mean                         -0.00100     0.00005    -0.00096     -0.00109
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.023431062698364258
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70008
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [38850]
collect time 0.0012252330780029297
inner_dict_sum {'sac_diff0': 0.0002269744873046875, 'sac_diff1': 0.008643627166748047, 'sac_diff2': 0.010269880294799805, 'sac_diff3': 0.011852741241455078, 'sac_diff4': 0.008727073669433594, 'sac_diff5': 0.03576326370239258, 'sac_diff6': 0.0004303455352783203, 'all': 0.07591390609741211}
diff5_list [0.008401870727539062, 0.0065724849700927734, 0.007294893264770508, 0.006997346878051758, 0.0064966678619384766]
time3 0
time4 0.0767819881439209
time5 0.07683968544006348
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4972)
policy weight change tensor(38.2768, grad_fn=<SumBackward0>)
time8 0.0020627975463867188
train_time 0.08957982063293457
eval time 0.138230562210083
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:55,917 MainThread INFO: EPOCH:252
2024-01-23 01:01:55,918 MainThread INFO: Time Consumed:0.23138999938964844s
2024-01-23 01:01:55,918 MainThread INFO: Total Frames:38700s
  3%|▎         | 253/10000 [03:23<43:51,  3.70it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11985.33090
Train_Epoch_Reward                20095.75808
Running_Training_Average_Rewards  14274.47665
Explore_Time                      0.00122
Train___Time                      0.08958
Eval____Time                      0.13823
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12047.86030
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.25720     2.38335     92.79407     86.85732
alpha_0                           0.88119      0.00012     0.88137      0.88101
Alpha_loss                        -0.85082     0.00118     -0.84943     -0.85241
Training/policy_loss              -2.51269     0.00241     -2.50800     -2.51482
Training/qf1_loss                 13274.39160  1084.96270  14568.08398  11937.26074
Training/qf2_loss                 15669.23926  1139.27780  17064.24609  14247.48926
Training/pf_norm                  0.12120      0.03817     0.17666      0.07089
Training/qf1_norm                 3820.39780   122.11538   3997.05713   3686.52954
Training/qf2_norm                 264.60311    6.89624     274.85144    257.72272
log_std/mean                      -0.14382     0.00021     -0.14349     -0.14409
log_probs/mean                    -2.73203     0.00292     -2.72641     -2.73446
mean/mean                         -0.00040     0.00008     -0.00028     -0.00052
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01861739158630371
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70008
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [39000]
collect time 0.0009045600891113281
inner_dict_sum {'sac_diff0': 0.0002257823944091797, 'sac_diff1': 0.007325410842895508, 'sac_diff2': 0.008258581161499023, 'sac_diff3': 0.01079106330871582, 'sac_diff4': 0.0070497989654541016, 'sac_diff5': 0.033904075622558594, 'sac_diff6': 0.0004038810729980469, 'all': 0.06795859336853027}
diff5_list [0.006923675537109375, 0.00772404670715332, 0.0068759918212890625, 0.006234645843505859, 0.0061457157135009766]
time3 0
time4 0.06878328323364258
time5 0.06883645057678223
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4972)
policy weight change tensor(38.1494, grad_fn=<SumBackward0>)
time8 0.0019519329071044922
train_time 0.08005213737487793
eval time 0.1523149013519287
epoch last part time 7.152557373046875e-06
2024-01-23 01:01:56,175 MainThread INFO: EPOCH:253
2024-01-23 01:01:56,176 MainThread INFO: Time Consumed:0.23580360412597656s
2024-01-23 01:01:56,176 MainThread INFO: Total Frames:38850s
  3%|▎         | 254/10000 [03:23<43:19,  3.75it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11997.05052
Train_Epoch_Reward                27084.57402
Running_Training_Average_Rewards  14770.28222
Explore_Time                      0.00090
Train___Time                      0.08005
Eval____Time                      0.15231
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12060.62564
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.86407     1.99468     92.47636     86.61338
alpha_0                           0.88075      0.00012     0.88092      0.88057
Alpha_loss                        -0.85451     0.00107     -0.85296     -0.85592
Training/policy_loss              -2.50713     0.00551     -2.50100     -2.51482
Training/qf1_loss                 13499.91504  1336.26174  15877.15039  11909.84570
Training/qf2_loss                 15875.66445  1387.12647  18326.25391  14179.22852
Training/pf_norm                  0.12410      0.02294     0.16036      0.09457
Training/qf1_norm                 3825.69487   101.43783   3954.32642   3651.35498
Training/qf2_norm                 252.16610    5.47337     259.33084    243.16484
log_std/mean                      -0.13762     0.00020     -0.13730     -0.13788
log_probs/mean                    -2.73459     0.00617     -2.72756     -2.74308
mean/mean                         -0.00068     0.00014     -0.00049     -0.00091
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019625425338745117
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70008
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [39150]
collect time 0.0009837150573730469
inner_dict_sum {'sac_diff0': 0.00020551681518554688, 'sac_diff1': 0.0074079036712646484, 'sac_diff2': 0.008162736892700195, 'sac_diff3': 0.010619878768920898, 'sac_diff4': 0.007436990737915039, 'sac_diff5': 0.03305220603942871, 'sac_diff6': 0.0004000663757324219, 'all': 0.06728529930114746}
diff5_list [0.006815910339355469, 0.006353855133056641, 0.0066070556640625, 0.006846189498901367, 0.006429195404052734]
time3 0
time4 0.06806278228759766
time5 0.06811046600341797
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4972)
policy weight change tensor(37.9996, grad_fn=<SumBackward0>)
time8 0.0018842220306396484
train_time 0.0790565013885498
eval time 0.1545405387878418
epoch last part time 7.152557373046875e-06
2024-01-23 01:01:56,436 MainThread INFO: EPOCH:254
2024-01-23 01:01:56,436 MainThread INFO: Time Consumed:0.23699164390563965s
2024-01-23 01:01:56,436 MainThread INFO: Total Frames:39000s
  3%|▎         | 255/10000 [03:24<42:58,  3.78it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12006.54462
Train_Epoch_Reward                24929.56161
Running_Training_Average_Rewards  15246.30845
Explore_Time                      0.00098
Train___Time                      0.07906
Eval____Time                      0.15454
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11980.40073
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.83660     1.17240    91.52910     88.41772
alpha_0                           0.88031      0.00012    0.88048      0.88013
Alpha_loss                        -0.85691     0.00100    -0.85540     -0.85800
Training/policy_loss              -2.50246     0.00885    -2.48829     -2.51244
Training/qf1_loss                 13371.55566  533.82021  14280.90918  12685.14746
Training/qf2_loss                 16009.36348  534.01391  16874.40234  15289.35449
Training/pf_norm                  0.14941      0.01798    0.17082      0.12569
Training/qf1_norm                 4193.65498   69.59677   4307.94775   4117.21045
Training/qf2_norm                 255.34948    3.21889    260.03760    251.41292
log_std/mean                      -0.13380     0.00028    -0.13340     -0.13418
log_probs/mean                    -2.72702     0.00983    -2.71136     -2.73778
mean/mean                         -0.00112     0.00013    -0.00092     -0.00131
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018962383270263672
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70008
epoch first part time 3.337860107421875e-06
replay_buffer._size: [39300]
collect time 0.0009505748748779297
inside mustsac before update, task 0, sumup 70008
inside mustsac after update, task 0, sumup 70984
inner_dict_sum {'sac_diff0': 0.00020956993103027344, 'sac_diff1': 0.007009267807006836, 'sac_diff2': 0.007859468460083008, 'sac_diff3': 0.010602712631225586, 'sac_diff4': 0.007101774215698242, 'sac_diff5': 0.05100893974304199, 'sac_diff6': 0.0003981590270996094, 'all': 0.08418989181518555}
diff5_list [0.011723041534423828, 0.010019063949584961, 0.01002359390258789, 0.009607553482055664, 0.009635686874389648]
time3 0.0008416175842285156
time4 0.08500933647155762
time5 0.08505845069885254
time7 0.009168863296508789
gen_weight_change tensor(-22.5740)
policy weight change tensor(38.0012, grad_fn=<SumBackward0>)
time8 0.001874685287475586
train_time 0.11415982246398926
eval time 0.11603045463562012
epoch last part time 6.67572021484375e-06
2024-01-23 01:01:56,692 MainThread INFO: EPOCH:255
2024-01-23 01:01:56,692 MainThread INFO: Time Consumed:0.23363399505615234s
2024-01-23 01:01:56,693 MainThread INFO: Total Frames:39150s
  3%|▎         | 256/10000 [03:24<42:36,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12007.56372
Train_Epoch_Reward                14091.06727
Running_Training_Average_Rewards  15439.76644
Explore_Time                      0.00095
Train___Time                      0.11416
Eval____Time                      0.11603
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11870.13247
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.53392     1.65929     95.35839     90.54063
alpha_0                           0.87987      0.00012     0.88004      0.87969
Alpha_loss                        -0.86149     0.00113     -0.85993     -0.86316
Training/policy_loss              -2.51126     0.00644     -2.49944     -2.51765
Training/qf1_loss                 15573.79434  1206.97900  17518.58789  14235.16211
Training/qf2_loss                 18045.60352  1117.99574  19955.85938  16863.49609
Training/pf_norm                  0.12530      0.03377     0.16749      0.08056
Training/qf1_norm                 3950.63921   156.23803   4173.26904   3712.43433
Training/qf2_norm                 263.64148    10.43591    275.39413    246.22722
log_std/mean                      -0.12740     0.00648     -0.11742     -0.13556
log_probs/mean                    -2.73649     0.00489     -2.72976     -2.74429
mean/mean                         -0.00140     0.00056     -0.00060     -0.00207
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019870519638061523
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70984
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [39450]
collect time 0.0009071826934814453
inner_dict_sum {'sac_diff0': 0.00022935867309570312, 'sac_diff1': 0.006697416305541992, 'sac_diff2': 0.007702350616455078, 'sac_diff3': 0.00994420051574707, 'sac_diff4': 0.006647586822509766, 'sac_diff5': 0.031223535537719727, 'sac_diff6': 0.00037384033203125, 'all': 0.06281828880310059}
diff5_list [0.006675004959106445, 0.006297111511230469, 0.0061151981353759766, 0.0061054229736328125, 0.0060307979583740234]
time3 0
time4 0.06355905532836914
time5 0.06360220909118652
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5740)
policy weight change tensor(37.9458, grad_fn=<SumBackward0>)
time8 0.001918792724609375
train_time 0.07468390464782715
eval time 0.15615463256835938
epoch last part time 6.9141387939453125e-06
2024-01-23 01:01:56,950 MainThread INFO: EPOCH:256
2024-01-23 01:01:56,950 MainThread INFO: Time Consumed:0.23417043685913086s
2024-01-23 01:01:56,950 MainThread INFO: Total Frames:39300s
  3%|▎         | 257/10000 [03:24<42:19,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11953.64806
Train_Epoch_Reward                15795.23292
Running_Training_Average_Rewards  15150.22926
Explore_Time                      0.00090
Train___Time                      0.07468
Eval____Time                      0.15615
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11535.88563
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.81962     1.53263    93.30522     89.48150
alpha_0                           0.87943      0.00012    0.87960      0.87925
Alpha_loss                        -0.86444     0.00131    -0.86218     -0.86604
Training/policy_loss              -2.50658     0.00668    -2.49589     -2.51372
Training/qf1_loss                 14400.66484  649.70079  15632.66211  13723.34863
Training/qf2_loss                 17228.71367  686.02448  18506.03320  16462.97266
Training/pf_norm                  0.11529      0.00875    0.12758      0.10135
Training/qf1_norm                 4308.34697   83.71513   4393.75586   4168.53760
Training/qf2_norm                 261.25537    4.25728    265.50720    254.68446
log_std/mean                      -0.12163     0.00004    -0.12160     -0.12170
log_probs/mean                    -2.73327     0.00753    -2.72088     -2.74078
mean/mean                         -0.00236     0.00020    -0.00209     -0.00266
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018681764602661133
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70984
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [39600]
collect time 0.0009028911590576172
inner_dict_sum {'sac_diff0': 0.0002181529998779297, 'sac_diff1': 0.0072917938232421875, 'sac_diff2': 0.00820302963256836, 'sac_diff3': 0.011091232299804688, 'sac_diff4': 0.0072345733642578125, 'sac_diff5': 0.03389430046081543, 'sac_diff6': 0.0004112720489501953, 'all': 0.0683443546295166}
diff5_list [0.006543159484863281, 0.0063054561614990234, 0.007340908050537109, 0.007261037826538086, 0.00644373893737793]
time3 0
time4 0.06919646263122559
time5 0.06925201416015625
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5740)
policy weight change tensor(37.9323, grad_fn=<SumBackward0>)
time8 0.0019578933715820312
train_time 0.08054471015930176
eval time 0.15135908126831055
epoch last part time 7.152557373046875e-06
2024-01-23 01:01:57,207 MainThread INFO: EPOCH:257
2024-01-23 01:01:57,208 MainThread INFO: Time Consumed:0.2352457046508789s
2024-01-23 01:01:57,208 MainThread INFO: Total Frames:39450s
  3%|▎         | 258/10000 [03:24<42:11,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11900.57373
Train_Epoch_Reward                13401.15956
Running_Training_Average_Rewards  15307.98667
Explore_Time                      0.00090
Train___Time                      0.08054
Eval____Time                      0.15136
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11510.82275
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.40450     2.23606     92.48764     87.08474
alpha_0                           0.87899      0.00012     0.87916      0.87881
Alpha_loss                        -0.86783     0.00125     -0.86555     -0.86926
Training/policy_loss              -2.51883     0.00605     -2.51269     -2.52857
Training/qf1_loss                 12909.93301  1123.52574  14599.85645  11687.52344
Training/qf2_loss                 15412.31602  1193.96539  17192.60742  14129.12988
Training/pf_norm                  0.12385      0.02213     0.14826      0.09646
Training/qf1_norm                 3923.59692   120.92440   4079.40308   3812.83374
Training/qf2_norm                 272.91418    6.63075     282.01218    266.17233
log_std/mean                      -0.12904     0.00005     -0.12897     -0.12910
log_probs/mean                    -2.73340     0.00686     -2.72617     -2.74450
mean/mean                         -0.00206     0.00010     -0.00191     -0.00217
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019223690032958984
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70984
epoch first part time 3.337860107421875e-06
replay_buffer._size: [39750]
collect time 0.0009732246398925781
inner_dict_sum {'sac_diff0': 0.00023937225341796875, 'sac_diff1': 0.006882667541503906, 'sac_diff2': 0.008109569549560547, 'sac_diff3': 0.010489463806152344, 'sac_diff4': 0.006997823715209961, 'sac_diff5': 0.03293728828430176, 'sac_diff6': 0.0003857612609863281, 'all': 0.06604194641113281}
diff5_list [0.007718324661254883, 0.006327152252197266, 0.006556272506713867, 0.006211519241333008, 0.006124019622802734]
time3 0
time4 0.06680488586425781
time5 0.06685209274291992
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5740)
policy weight change tensor(37.9705, grad_fn=<SumBackward0>)
time8 0.0019915103912353516
train_time 0.07803845405578613
eval time 0.1592261791229248
epoch last part time 7.3909759521484375e-06
2024-01-23 01:01:57,471 MainThread INFO: EPOCH:258
2024-01-23 01:01:57,471 MainThread INFO: Time Consumed:0.24068379402160645s
2024-01-23 01:01:57,471 MainThread INFO: Total Frames:39600s
  3%|▎         | 259/10000 [03:25<42:20,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11859.27394
Train_Epoch_Reward                4062.29533
Running_Training_Average_Rewards  14845.71640
Explore_Time                      0.00097
Train___Time                      0.07804
Eval____Time                      0.15923
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11569.69013
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.45300     3.16755     96.07405     86.36702
alpha_0                           0.87855      0.00012     0.87872      0.87837
Alpha_loss                        -0.87117     0.00160     -0.86927     -0.87387
Training/policy_loss              -2.51178     0.00536     -2.50361     -2.52033
Training/qf1_loss                 15089.10918  1516.57661  16740.64648  12277.77930
Training/qf2_loss                 17443.57910  1608.47041  19231.41797  14474.01855
Training/pf_norm                  0.11170      0.02251     0.14928      0.08057
Training/qf1_norm                 3777.87842   162.84035   4007.57300   3515.59155
Training/qf2_norm                 272.52565    9.14882     285.82056    257.78738
log_std/mean                      -0.12577     0.00007     -0.12571     -0.12591
log_probs/mean                    -2.73323     0.00635     -2.72370     -2.74360
mean/mean                         -0.00314     0.00005     -0.00308     -0.00322
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01856374740600586
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70984
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [39900]
collect time 0.0009868144989013672
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.007216215133666992, 'sac_diff2': 0.008504152297973633, 'sac_diff3': 0.011126279830932617, 'sac_diff4': 0.007596254348754883, 'sac_diff5': 0.03390812873840332, 'sac_diff6': 0.00040650367736816406, 'all': 0.06896495819091797}
diff5_list [0.0068171024322509766, 0.006533384323120117, 0.0068662166595458984, 0.0072422027587890625, 0.006449222564697266]
time3 0
time4 0.06978750228881836
time5 0.0698387622833252
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5740)
policy weight change tensor(38.0278, grad_fn=<SumBackward0>)
time8 0.0019288063049316406
train_time 0.08114933967590332
eval time 0.14930391311645508
epoch last part time 7.152557373046875e-06
2024-01-23 01:01:57,727 MainThread INFO: EPOCH:259
2024-01-23 01:01:57,727 MainThread INFO: Time Consumed:0.23384785652160645s
2024-01-23 01:01:57,727 MainThread INFO: Total Frames:39750s
  3%|▎         | 260/10000 [03:25<42:08,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11824.61853
Train_Epoch_Reward                8205.81846
Running_Training_Average_Rewards  14501.42999
Explore_Time                      0.00098
Train___Time                      0.08115
Eval____Time                      0.14930
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11640.11396
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.13767     2.86945     92.55159     83.72910
alpha_0                           0.87811      0.00012     0.87828      0.87793
Alpha_loss                        -0.87476     0.00116     -0.87338     -0.87688
Training/policy_loss              -2.50399     0.00404     -2.49860     -2.50876
Training/qf1_loss                 12590.27109  1295.43714  14696.20605  10635.11914
Training/qf2_loss                 15116.94512  1370.55607  17341.11914  13034.17676
Training/pf_norm                  0.12122      0.02834     0.16313      0.07959
Training/qf1_norm                 4002.08086   146.05846   4221.38574   3773.61401
Training/qf2_norm                 256.82068    7.99809     269.01358    244.41515
log_std/mean                      -0.12920     0.00010     -0.12907     -0.12933
log_probs/mean                    -2.73487     0.00460     -2.72903     -2.74084
mean/mean                         -0.00249     0.00010     -0.00235     -0.00264
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019257068634033203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70984
epoch first part time 4.76837158203125e-06
replay_buffer._size: [40050]
collect time 0.0011830329895019531
inside mustsac before update, task 0, sumup 70984
inside mustsac after update, task 0, sumup 70279
inner_dict_sum {'sac_diff0': 0.0002071857452392578, 'sac_diff1': 0.007592678070068359, 'sac_diff2': 0.009146690368652344, 'sac_diff3': 0.011586427688598633, 'sac_diff4': 0.008513689041137695, 'sac_diff5': 0.05492663383483887, 'sac_diff6': 0.0004603862762451172, 'all': 0.09243369102478027}
diff5_list [0.011547088623046875, 0.01059269905090332, 0.012351751327514648, 0.010290384292602539, 0.010144710540771484]
time3 0.0009303092956542969
time4 0.09350275993347168
time5 0.09356522560119629
time7 0.00951075553894043
gen_weight_change tensor(-22.6479)
policy weight change tensor(38.0322, grad_fn=<SumBackward0>)
time8 0.002627849578857422
train_time 0.1248934268951416
eval time 0.10231375694274902
epoch last part time 5.4836273193359375e-06
2024-01-23 01:01:57,986 MainThread INFO: EPOCH:260
2024-01-23 01:01:57,986 MainThread INFO: Time Consumed:0.23080062866210938s
2024-01-23 01:01:57,986 MainThread INFO: Total Frames:39900s
  3%|▎         | 261/10000 [03:25<42:14,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11794.93513
Train_Epoch_Reward                7606.20716
Running_Training_Average_Rewards  14667.17438
Explore_Time                      0.00118
Train___Time                      0.12489
Eval____Time                      0.10231
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11680.00201
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.01884     2.04462    93.79221     88.21739
alpha_0                           0.87767      0.00012    0.87785      0.87749
Alpha_loss                        -0.87773     0.00027    -0.87724     -0.87797
Training/policy_loss              -2.50505     0.00864    -2.49515     -2.51694
Training/qf1_loss                 13759.33984  685.71289  14915.04492  13073.29492
Training/qf2_loss                 16398.41348  692.96542  17452.50391  15472.48145
Training/pf_norm                  0.12289      0.02600    0.16091      0.09172
Training/qf1_norm                 4104.53462   247.70971  4460.56641   3773.04053
Training/qf2_norm                 263.75250    9.84620    280.02655    249.38557
log_std/mean                      -0.12684     0.00489    -0.11802     -0.13222
log_probs/mean                    -2.73187     0.00689    -2.72336     -2.74246
mean/mean                         -0.00255     0.00057    -0.00174     -0.00350
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018985509872436523
epoch last part time3 0.002959728240966797
inside rlalgo, task 0, sumup 70279
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [40200]
collect time 0.0009665489196777344
inner_dict_sum {'sac_diff0': 0.00020647048950195312, 'sac_diff1': 0.006966352462768555, 'sac_diff2': 0.00832366943359375, 'sac_diff3': 0.01141977310180664, 'sac_diff4': 0.007274150848388672, 'sac_diff5': 0.032796621322631836, 'sac_diff6': 0.0003936290740966797, 'all': 0.06738066673278809}
diff5_list [0.007107257843017578, 0.006335735321044922, 0.006804943084716797, 0.006237506866455078, 0.006311178207397461]
time3 0
time4 0.06815385818481445
time5 0.06820154190063477
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6479)
policy weight change tensor(38.0693, grad_fn=<SumBackward0>)
time8 0.0019965171813964844
train_time 0.07974720001220703
eval time 0.1522355079650879
epoch last part time 5.9604644775390625e-06
2024-01-23 01:01:58,247 MainThread INFO: EPOCH:261
2024-01-23 01:01:58,247 MainThread INFO: Time Consumed:0.23536372184753418s
2024-01-23 01:01:58,247 MainThread INFO: Total Frames:40050s
  3%|▎         | 262/10000 [03:25<42:06,  3.85it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11760.58830
Train_Epoch_Reward                13469.22392
Running_Training_Average_Rewards  14692.84102
Explore_Time                      0.00096
Train___Time                      0.07975
Eval____Time                      0.15224
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11710.34941
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.41057     2.63382    92.89747     86.00560
alpha_0                           0.87723      0.00012    0.87741      0.87706
Alpha_loss                        -0.88168     0.00095    -0.87979     -0.88241
Training/policy_loss              -2.50009     0.00557    -2.49299     -2.50802
Training/qf1_loss                 13795.64805  841.19101  14806.07617  12314.00195
Training/qf2_loss                 16591.93594  924.15577  17682.11133  14954.40039
Training/pf_norm                  0.10764      0.01487    0.12828      0.08975
Training/qf1_norm                 4292.34561   147.37079  4419.11816   4035.88281
Training/qf2_norm                 249.85980    6.99040    256.42255    238.11409
log_std/mean                      -0.13021     0.00004    -0.13014     -0.13026
log_probs/mean                    -2.73628     0.00617    -2.72876     -2.74511
mean/mean                         -0.00401     0.00002    -0.00399     -0.00403
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018911123275756836
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70279
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [40350]
collect time 0.0008344650268554688
inner_dict_sum {'sac_diff0': 0.0002162456512451172, 'sac_diff1': 0.006678581237792969, 'sac_diff2': 0.007784128189086914, 'sac_diff3': 0.010615825653076172, 'sac_diff4': 0.0063762664794921875, 'sac_diff5': 0.03278684616088867, 'sac_diff6': 0.0004029273986816406, 'all': 0.06486082077026367}
diff5_list [0.0065746307373046875, 0.00623321533203125, 0.006386518478393555, 0.00644993782043457, 0.007142543792724609]
time3 0
time4 0.06565260887145996
time5 0.06569910049438477
time7 1.1920928955078125e-06
gen_weight_change tensor(-22.6479)
policy weight change tensor(38.0735, grad_fn=<SumBackward0>)
time8 0.0018961429595947266
train_time 0.07666325569152832
eval time 0.15896224975585938
epoch last part time 5.245208740234375e-06
2024-01-23 01:01:58,508 MainThread INFO: EPOCH:262
2024-01-23 01:01:58,508 MainThread INFO: Time Consumed:0.2387866973876953s
2024-01-23 01:01:58,509 MainThread INFO: Total Frames:40200s
  3%|▎         | 263/10000 [03:26<42:09,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11729.50598
Train_Epoch_Reward                9588.71865
Running_Training_Average_Rewards  14758.01230
Explore_Time                      0.00083
Train___Time                      0.07666
Eval____Time                      0.15896
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11737.03705
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.58289     1.25918     92.97714     90.02138
alpha_0                           0.87679      0.00012     0.87697      0.87662
Alpha_loss                        -0.88494     0.00031     -0.88463     -0.88545
Training/policy_loss              -2.51872     0.00673     -2.50724     -2.52607
Training/qf1_loss                 14202.34375  1116.97070  15594.27637  12552.99805
Training/qf2_loss                 17113.71797  1148.61599  18545.95703  15409.83398
Training/pf_norm                  0.12659      0.01894     0.14540      0.09614
Training/qf1_norm                 4470.14893   68.90102    4546.36133   4377.59961
Training/qf2_norm                 274.92240    3.70519     279.15408    270.32965
log_std/mean                      -0.13138     0.00005     -0.13132     -0.13145
log_probs/mean                    -2.73550     0.00731     -2.72298     -2.74335
mean/mean                         -0.00278     0.00006     -0.00268     -0.00286
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018302202224731445
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70279
epoch first part time 2.86102294921875e-06
replay_buffer._size: [40500]
collect time 0.0008828639984130859
inner_dict_sum {'sac_diff0': 0.0002181529998779297, 'sac_diff1': 0.0073735713958740234, 'sac_diff2': 0.008522510528564453, 'sac_diff3': 0.011469125747680664, 'sac_diff4': 0.007519245147705078, 'sac_diff5': 0.03441572189331055, 'sac_diff6': 0.00040650367736816406, 'all': 0.06992483139038086}
diff5_list [0.007601022720336914, 0.007135629653930664, 0.006968021392822266, 0.006303071975708008, 0.006407976150512695]
time3 0
time4 0.07069635391235352
time5 0.07074356079101562
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6479)
policy weight change tensor(38.1239, grad_fn=<SumBackward0>)
time8 0.001967906951904297
train_time 0.08204984664916992
eval time 0.14942169189453125
epoch last part time 6.198883056640625e-06
2024-01-23 01:01:58,765 MainThread INFO: EPOCH:263
2024-01-23 01:01:58,765 MainThread INFO: Time Consumed:0.2347724437713623s
2024-01-23 01:01:58,765 MainThread INFO: Total Frames:40350s
  3%|▎         | 264/10000 [03:26<1:06:44,  2.43it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11702.84477
Train_Epoch_Reward                20850.95731
Running_Training_Average_Rewards  15209.45750
Explore_Time                      0.00088
Train___Time                      0.08205
Eval____Time                      0.14942
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11794.01359
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.05149     1.64568     91.89809     87.02446
alpha_0                           0.87635      0.00012     0.87653      0.87618
Alpha_loss                        -0.88866     0.00082     -0.88716     -0.88945
Training/policy_loss              -2.51184     0.00318     -2.50609     -2.51519
Training/qf1_loss                 12179.31836  979.40494   13386.58105  10801.24414
Training/qf2_loss                 15216.91816  1038.21969  16541.94727  13741.83008
Training/pf_norm                  0.13742      0.02429     0.18030      0.10765
Training/qf1_norm                 4402.01436   106.53552   4584.89111   4258.73193
Training/qf2_norm                 257.34887    4.67671     265.43240    251.51227
log_std/mean                      -0.12044     0.00010     -0.12034     -0.12060
log_probs/mean                    -2.73809     0.00343     -2.73203     -2.74166
mean/mean                         -0.00287     0.00015     -0.00263     -0.00303
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.5261826515197754
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70279
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [40650]
collect time 0.0008995532989501953
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.007234811782836914, 'sac_diff2': 0.008593320846557617, 'sac_diff3': 0.010848760604858398, 'sac_diff4': 0.007136821746826172, 'sac_diff5': 0.03271031379699707, 'sac_diff6': 0.0004062652587890625, 'all': 0.06714081764221191}
diff5_list [0.006642341613769531, 0.0062999725341796875, 0.006858348846435547, 0.006712436676025391, 0.006197214126586914]
time3 0
time4 0.06798601150512695
time5 0.0680398941040039
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6479)
policy weight change tensor(38.2177, grad_fn=<SumBackward0>)
time8 0.0019600391387939453
train_time 0.07939791679382324
eval time 0.0006258487701416016
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:59,378 MainThread INFO: EPOCH:264
2024-01-23 01:01:59,378 MainThread INFO: Time Consumed:0.08316278457641602s
2024-01-23 01:01:59,378 MainThread INFO: Total Frames:40500s
  3%|▎         | 265/10000 [03:26<51:49,  3.13it/s]  --------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11689.69735
Train_Epoch_Reward                1413.42838
Running_Training_Average_Rewards  15033.64951
Explore_Time                      0.00089
Train___Time                      0.07940
Eval____Time                      0.00063
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11848.92651
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.27199     1.57449    90.33744     86.19634
alpha_0                           0.87592      0.00012    0.87609      0.87574
Alpha_loss                        -0.89146     0.00154    -0.89003     -0.89335
Training/policy_loss              -2.50309     0.00586    -2.49363     -2.51077
Training/qf1_loss                 12096.81660  738.01510  12921.04492  11115.94238
Training/qf2_loss                 14994.11738  799.12926  15916.34668  13938.13086
Training/pf_norm                  0.13073      0.02740    0.17531      0.09130
Training/qf1_norm                 4356.36543   100.75953  4502.27637   4239.66504
Training/qf2_norm                 257.18307    4.45425    262.93930    251.34521
log_std/mean                      -0.12764     0.00017    -0.12740     -0.12789
log_probs/mean                    -2.73381     0.00684    -2.72299     -2.74295
mean/mean                         -0.00127     0.00014    -0.00108     -0.00148
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018035411834716797
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70279
epoch first part time 2.86102294921875e-06
replay_buffer._size: [40775]
collect time 0.0008122920989990234
inside mustsac before update, task 0, sumup 70279
inside mustsac after update, task 0, sumup 70079
inner_dict_sum {'sac_diff0': 0.00020956993103027344, 'sac_diff1': 0.007852554321289062, 'sac_diff2': 0.008148431777954102, 'sac_diff3': 0.011031389236450195, 'sac_diff4': 0.007589101791381836, 'sac_diff5': 0.05108189582824707, 'sac_diff6': 0.00043201446533203125, 'all': 0.08634495735168457}
diff5_list [0.009911537170410156, 0.011149406433105469, 0.010752677917480469, 0.009690046310424805, 0.009578227996826172]
time3 0.0008878707885742188
time4 0.08727622032165527
time5 0.08733248710632324
time7 0.010137796401977539
gen_weight_change tensor(-22.7692)
policy weight change tensor(38.1787, grad_fn=<SumBackward0>)
time8 0.002246379852294922
train_time 0.11853528022766113
eval time 0.0004975795745849609
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:59,521 MainThread INFO: EPOCH:265
2024-01-23 01:01:59,522 MainThread INFO: Time Consumed:0.1220707893371582s
2024-01-23 01:01:59,522 MainThread INFO: Total Frames:40650s
  3%|▎         | 266/10000 [03:27<43:25,  3.74it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11687.57676
Train_Epoch_Reward                10974.15101
Running_Training_Average_Rewards  14657.73518
Explore_Time                      0.00081
Train___Time                      0.11854
Eval____Time                      0.00050
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11848.92651
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.01810     2.53516    92.37917     84.51627
alpha_0                           0.87548      0.00012    0.87565      0.87530
Alpha_loss                        -0.89486     0.00156    -0.89196     -0.89615
Training/policy_loss              -2.50788     0.00971    -2.48946     -2.51623
Training/qf1_loss                 12823.77676  583.27440  13484.40625  12114.71973
Training/qf2_loss                 15581.73652  664.21506  16465.71875  14765.34082
Training/pf_norm                  0.13004      0.02493    0.16216      0.09576
Training/qf1_norm                 4175.21396   186.24416  4407.44629   3939.86011
Training/qf2_norm                 259.88447    9.89402    274.99155    245.00722
log_std/mean                      -0.12648     0.00409    -0.12235     -0.13430
log_probs/mean                    -2.73401     0.00879    -2.71727     -2.74129
mean/mean                         -0.00151     0.00058    -0.00054     -0.00231
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.021320819854736328
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70079
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [40942]
collect time 0.000934600830078125
inner_dict_sum {'sac_diff0': 0.00019693374633789062, 'sac_diff1': 0.0068569183349609375, 'sac_diff2': 0.008213996887207031, 'sac_diff3': 0.010379791259765625, 'sac_diff4': 0.007317781448364258, 'sac_diff5': 0.03213691711425781, 'sac_diff6': 0.00037932395935058594, 'all': 0.06548166275024414}
diff5_list [0.00724482536315918, 0.006380558013916016, 0.00631260871887207, 0.006197452545166016, 0.006001472473144531]
time3 0
time4 0.06624174118041992
time5 0.06629204750061035
time7 7.152557373046875e-07
gen_weight_change tensor(-22.7692)
policy weight change tensor(38.2114, grad_fn=<SumBackward0>)
time8 0.0019583702087402344
train_time 0.07820916175842285
eval time 0.022072792053222656
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:59,650 MainThread INFO: EPOCH:266
2024-01-23 01:01:59,650 MainThread INFO: Time Consumed:0.10348320007324219s
2024-01-23 01:01:59,650 MainThread INFO: Total Frames:40800s
  3%|▎         | 267/10000 [03:27<36:37,  4.43it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11725.83374
Train_Epoch_Reward                27109.79014
Running_Training_Average_Rewards  15344.86241
Explore_Time                      0.00093
Train___Time                      0.07821
Eval____Time                      0.02207
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11918.45551
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.65272     1.43370     91.00603     87.59612
alpha_0                           0.87504      0.00012     0.87521      0.87486
Alpha_loss                        -0.89856     0.00089     -0.89736     -0.89949
Training/policy_loss              -2.51586     0.00289     -2.51267     -2.51942
Training/qf1_loss                 12913.84102  1242.68430  14303.06055  10694.49805
Training/qf2_loss                 15772.84199  1279.95695  17207.08789  13503.81543
Training/pf_norm                  0.15216      0.03260     0.18546      0.09577
Training/qf1_norm                 4294.16924   76.58096    4369.28613   4194.26709
Training/qf2_norm                 270.91007    4.17859     274.66589    264.86285
log_std/mean                      -0.14352     0.00006     -0.14342     -0.14361
log_probs/mean                    -2.73651     0.00318     -2.73258     -2.74061
mean/mean                         -0.00332     0.00023     -0.00300     -0.00364
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021129608154296875
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70079
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [41090]
collect time 0.0008931159973144531
inner_dict_sum {'sac_diff0': 0.00019931793212890625, 'sac_diff1': 0.007203340530395508, 'sac_diff2': 0.00863957405090332, 'sac_diff3': 0.011380195617675781, 'sac_diff4': 0.007663249969482422, 'sac_diff5': 0.03620195388793945, 'sac_diff6': 0.00042724609375, 'all': 0.07171487808227539}
diff5_list [0.00654911994934082, 0.006324052810668945, 0.007053375244140625, 0.00829005241394043, 0.007985353469848633]
time3 0
time4 0.07257604598999023
time5 0.07262897491455078
time7 9.5367431640625e-07
gen_weight_change tensor(-22.7692)
policy weight change tensor(38.1491, grad_fn=<SumBackward0>)
time8 0.002234935760498047
train_time 0.0849461555480957
eval time 0.17786026000976562
epoch last part time 5.0067901611328125e-06
2024-01-23 01:01:59,940 MainThread INFO: EPOCH:267
2024-01-23 01:01:59,941 MainThread INFO: Time Consumed:0.2660038471221924s
2024-01-23 01:01:59,941 MainThread INFO: Total Frames:40950s
  3%|▎         | 268/10000 [03:27<39:45,  4.08it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11780.63060
Train_Epoch_Reward                5691.12744
Running_Training_Average_Rewards  15156.66727
Explore_Time                      0.00089
Train___Time                      0.08495
Eval____Time                      0.17786
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12058.79133
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.74844     3.69986     95.32433     84.53432
alpha_0                           0.87460      0.00012     0.87478      0.87443
Alpha_loss                        -0.90147     0.00104     -0.90008     -0.90284
Training/policy_loss              -2.50828     0.00263     -2.50359     -2.51136
Training/qf1_loss                 13895.42402  1908.23436  16188.59668  10918.09375
Training/qf2_loss                 16647.86211  2023.60073  19069.33203  13452.57520
Training/pf_norm                  0.12895      0.05373     0.19567      0.06736
Training/qf1_norm                 4205.50103   204.30827   4436.44922   3846.57788
Training/qf2_norm                 271.83134    10.62150    284.77505    253.94247
log_std/mean                      -0.12449     0.00009     -0.12435     -0.12464
log_probs/mean                    -2.73309     0.00300     -2.72771     -2.73630
mean/mean                         -0.00076     0.00022     -0.00044     -0.00107
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020828962326049805
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70079
epoch first part time 2.384185791015625e-06
replay_buffer._size: [41250]
collect time 0.0009317398071289062
inner_dict_sum {'sac_diff0': 0.00019168853759765625, 'sac_diff1': 0.00642704963684082, 'sac_diff2': 0.007658481597900391, 'sac_diff3': 0.009825468063354492, 'sac_diff4': 0.0068209171295166016, 'sac_diff5': 0.031066179275512695, 'sac_diff6': 0.00037217140197753906, 'all': 0.062361955642700195}
diff5_list [0.006528139114379883, 0.006186485290527344, 0.0062334537506103516, 0.0060002803802490234, 0.006117820739746094]
time3 0
time4 0.0630943775177002
time5 0.063140869140625
time7 4.76837158203125e-07
gen_weight_change tensor(-22.7692)
policy weight change tensor(38.0883, grad_fn=<SumBackward0>)
time8 0.0018796920776367188
train_time 0.07466363906860352
eval time 0.1582629680633545
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:00,201 MainThread INFO: EPOCH:268
2024-01-23 01:02:00,201 MainThread INFO: Time Consumed:0.23637747764587402s
2024-01-23 01:02:00,201 MainThread INFO: Total Frames:41100s
  3%|▎         | 269/10000 [03:27<40:33,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11828.36429
Train_Epoch_Reward                20618.91499
Running_Training_Average_Rewards  15311.79370
Explore_Time                      0.00093
Train___Time                      0.07466
Eval____Time                      0.15826
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12047.02704
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.14192     1.83762     90.57031     85.54924
alpha_0                           0.87416      0.00012     0.87434      0.87399
Alpha_loss                        -0.90484     0.00071     -0.90357     -0.90566
Training/policy_loss              -2.49822     0.00645     -2.48995     -2.50677
Training/qf1_loss                 12942.86094  1891.29486  16425.99609  10657.06348
Training/qf2_loss                 15852.16660  1942.89195  19417.33984  13470.14844
Training/pf_norm                  0.12515      0.02771     0.15351      0.08835
Training/qf1_norm                 4206.82334   108.65497   4347.87354   4052.05322
Training/qf2_norm                 257.07276    5.11674     263.93945    249.82480
log_std/mean                      -0.13779     0.00017     -0.13761     -0.13806
log_probs/mean                    -2.73308     0.00710     -2.72360     -2.74231
mean/mean                         -0.00062     0.00005     -0.00057     -0.00071
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021745920181274414
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70079
epoch first part time 3.337860107421875e-06
replay_buffer._size: [41400]
collect time 0.0010166168212890625
inner_dict_sum {'sac_diff0': 0.0002090930938720703, 'sac_diff1': 0.007520437240600586, 'sac_diff2': 0.008875608444213867, 'sac_diff3': 0.011242866516113281, 'sac_diff4': 0.007534503936767578, 'sac_diff5': 0.033623695373535156, 'sac_diff6': 0.0004112720489501953, 'all': 0.06941747665405273}
diff5_list [0.007127046585083008, 0.006721973419189453, 0.006500720977783203, 0.006451845169067383, 0.006822109222412109]
time3 0
time4 0.07029056549072266
time5 0.07035541534423828
time7 7.152557373046875e-07
gen_weight_change tensor(-22.7692)
policy weight change tensor(38.1741, grad_fn=<SumBackward0>)
time8 0.002141237258911133
train_time 0.08284544944763184
eval time 0.15255951881408691
epoch last part time 7.152557373046875e-06
2024-01-23 01:02:00,465 MainThread INFO: EPOCH:269
2024-01-23 01:02:00,466 MainThread INFO: Time Consumed:0.23898983001708984s
2024-01-23 01:02:00,466 MainThread INFO: Total Frames:41250s
  3%|▎         | 270/10000 [03:28<41:13,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11867.55874
Train_Epoch_Reward                36714.73983
Running_Training_Average_Rewards  16423.50371
Explore_Time                      0.00101
Train___Time                      0.08285
Eval____Time                      0.15256
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12032.05839
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.38420     2.99837     95.17302     85.98771
alpha_0                           0.87373      0.00012     0.87390      0.87355
Alpha_loss                        -0.90802     0.00092     -0.90711     -0.90919
Training/policy_loss              -2.50394     0.00568     -2.49397     -2.51072
Training/qf1_loss                 12893.20664  1465.48368  14707.72656  11097.59375
Training/qf2_loss                 15783.55176  1569.18598  17666.76172  13822.83008
Training/pf_norm                  0.14845      0.02073     0.17468      0.11911
Training/qf1_norm                 4256.78516   176.29423   4525.77539   3996.10791
Training/qf2_norm                 269.21901    8.60045     283.00024    256.68826
log_std/mean                      -0.11633     0.00019     -0.11610     -0.11664
log_probs/mean                    -2.73170     0.00631     -2.72056     -2.73950
mean/mean                         -0.00018     0.00004     -0.00014     -0.00026
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021301746368408203
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70079
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [41550]
collect time 0.001008749008178711
inside mustsac before update, task 0, sumup 70079
inside mustsac after update, task 0, sumup 72010
inner_dict_sum {'sac_diff0': 0.00020313262939453125, 'sac_diff1': 0.006985902786254883, 'sac_diff2': 0.008549690246582031, 'sac_diff3': 0.010820865631103516, 'sac_diff4': 0.007696390151977539, 'sac_diff5': 0.05081820487976074, 'sac_diff6': 0.00040459632873535156, 'all': 0.0854787826538086}
diff5_list [0.010969877243041992, 0.010014772415161133, 0.009974956512451172, 0.009967327117919922, 0.009891271591186523]
time3 0.0008816719055175781
time4 0.08637714385986328
time5 0.08643817901611328
time7 0.009444952011108398
gen_weight_change tensor(-22.6649)
policy weight change tensor(38.1327, grad_fn=<SumBackward0>)
time8 0.0026285648345947266
train_time 0.11747908592224121
eval time 0.11895608901977539
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:00,730 MainThread INFO: EPOCH:270
2024-01-23 01:02:00,730 MainThread INFO: Time Consumed:0.23966765403747559s
2024-01-23 01:02:00,730 MainThread INFO: Total Frames:41400s
  3%|▎         | 271/10000 [03:28<41:52,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11896.55589
Train_Epoch_Reward                20014.49996
Running_Training_Average_Rewards  16449.82975
Explore_Time                      0.00100
Train___Time                      0.11748
Eval____Time                      0.11896
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11969.97353
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.61292     1.60561     93.36052     89.17494
alpha_0                           0.87329      0.00012     0.87346      0.87312
Alpha_loss                        -0.91158     0.00144     -0.90892     -0.91281
Training/policy_loss              -2.50107     0.00658     -2.49279     -2.51100
Training/qf1_loss                 14063.20039  1550.65765  16753.62891  12152.72461
Training/qf2_loss                 16890.22187  1652.04050  19720.14062  14777.65039
Training/pf_norm                  0.10659      0.01993     0.13597      0.08011
Training/qf1_norm                 4235.67119   197.77850   4561.31006   4027.66309
Training/qf2_norm                 261.62768    12.29022    279.76611    249.05678
log_std/mean                      -0.13509     0.00396     -0.13144     -0.14224
log_probs/mean                    -2.73309     0.00809     -2.71839     -2.74058
mean/mean                         0.00017      0.00074     0.00129      -0.00063
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02185964584350586
epoch last part time3 0.0026183128356933594
inside rlalgo, task 0, sumup 72010
epoch first part time 3.337860107421875e-06
replay_buffer._size: [41700]
collect time 0.0009729862213134766
inner_dict_sum {'sac_diff0': 0.0002181529998779297, 'sac_diff1': 0.007028341293334961, 'sac_diff2': 0.008296012878417969, 'sac_diff3': 0.01031041145324707, 'sac_diff4': 0.007079601287841797, 'sac_diff5': 0.032196998596191406, 'sac_diff6': 0.0004010200500488281, 'all': 0.06553053855895996}
diff5_list [0.006783246994018555, 0.006350994110107422, 0.006272792816162109, 0.006215095520019531, 0.006574869155883789]
time3 0
time4 0.06631302833557129
time5 0.06636333465576172
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6649)
policy weight change tensor(38.3589, grad_fn=<SumBackward0>)
time8 0.0019626617431640625
train_time 0.07810354232788086
eval time 0.1554276943206787
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:00,994 MainThread INFO: EPOCH:271
2024-01-23 01:02:00,995 MainThread INFO: Time Consumed:0.23673653602600098s
2024-01-23 01:02:00,995 MainThread INFO: Total Frames:41550s
  3%|▎         | 272/10000 [03:28<41:54,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11927.10155
Train_Epoch_Reward                11944.72259
Running_Training_Average_Rewards  16439.17054
Explore_Time                      0.00097
Train___Time                      0.07810
Eval____Time                      0.15543
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12015.80603
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.19144     2.12500     94.23598     88.14482
alpha_0                           0.87285      0.00012     0.87303      0.87268
Alpha_loss                        -0.91397     0.00098     -0.91241     -0.91508
Training/policy_loss              -2.49074     0.00796     -2.48075     -2.50508
Training/qf1_loss                 13580.95918  1208.94334  14913.73145  12113.53809
Training/qf2_loss                 16493.21895  1258.42305  17965.99414  14975.70020
Training/pf_norm                  0.14108      0.03721     0.18859      0.08505
Training/qf1_norm                 4378.10752   120.98829   4612.70166   4274.19727
Training/qf2_norm                 259.45134    5.89769     270.67627    253.81299
log_std/mean                      -0.11361     0.00039     -0.11316     -0.11424
log_probs/mean                    -2.72589     0.00887     -2.71444     -2.74173
mean/mean                         -0.00005     0.00007     0.00009      -0.00011
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019213438034057617
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 72010
epoch first part time 2.86102294921875e-06
replay_buffer._size: [41850]
collect time 0.0009272098541259766
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.0073163509368896484, 'sac_diff2': 0.008741617202758789, 'sac_diff3': 0.011266946792602539, 'sac_diff4': 0.00752711296081543, 'sac_diff5': 0.033711910247802734, 'sac_diff6': 0.0004191398620605469, 'all': 0.0691976547241211}
diff5_list [0.00736546516418457, 0.007268428802490234, 0.0066394805908203125, 0.006216764450073242, 0.006221771240234375]
time3 0
time4 0.07005691528320312
time5 0.07010889053344727
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6649)
policy weight change tensor(38.5640, grad_fn=<SumBackward0>)
time8 0.0019011497497558594
train_time 0.0821523666381836
eval time 0.15639758110046387
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:01,259 MainThread INFO: EPOCH:272
2024-01-23 01:02:01,259 MainThread INFO: Time Consumed:0.24195432662963867s
2024-01-23 01:02:01,260 MainThread INFO: Total Frames:41700s
  3%|▎         | 273/10000 [03:28<42:12,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11949.89984
Train_Epoch_Reward                1165.97577
Running_Training_Average_Rewards  15775.39810
Explore_Time                      0.00092
Train___Time                      0.08215
Eval____Time                      0.15640
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11965.01993
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.52703     0.92764    90.84645     88.28337
alpha_0                           0.87242      0.00012    0.87259      0.87224
Alpha_loss                        -0.91884     0.00131    -0.91737     -0.92091
Training/policy_loss              -2.51684     0.00693    -2.51188     -2.53001
Training/qf1_loss                 12846.53398  761.22731  14162.06250  11994.31543
Training/qf2_loss                 15543.56387  797.04356  16921.79297  14657.64648
Training/pf_norm                  0.11974      0.02245    0.15206      0.08248
Training/qf1_norm                 4156.42881   61.30249   4246.57861   4073.74707
Training/qf2_norm                 271.49996    2.80667    275.43292    267.59128
log_std/mean                      -0.12688     0.00030    -0.12644     -0.12729
log_probs/mean                    -2.73700     0.00789    -2.73110     -2.75214
mean/mean                         -0.00097     0.00004    -0.00090     -0.00103
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01897883415222168
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 72010
epoch first part time 2.86102294921875e-06
replay_buffer._size: [42000]
collect time 0.0009140968322753906
inner_dict_sum {'sac_diff0': 0.0001995563507080078, 'sac_diff1': 0.007086992263793945, 'sac_diff2': 0.008515357971191406, 'sac_diff3': 0.011469125747680664, 'sac_diff4': 0.0072095394134521484, 'sac_diff5': 0.03266739845275879, 'sac_diff6': 0.00041031837463378906, 'all': 0.06755828857421875}
diff5_list [0.006561756134033203, 0.006187915802001953, 0.0063936710357666016, 0.00615382194519043, 0.0073702335357666016]
time3 0
time4 0.06842923164367676
time5 0.06847596168518066
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6649)
policy weight change tensor(38.7438, grad_fn=<SumBackward0>)
time8 0.0020546913146972656
train_time 0.08015131950378418
eval time 0.1587991714477539
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:01,524 MainThread INFO: EPOCH:273
2024-01-23 01:02:01,524 MainThread INFO: Time Consumed:0.24228835105895996s
2024-01-23 01:02:01,524 MainThread INFO: Total Frames:41850s
  3%|▎         | 274/10000 [03:29<42:24,  3.82it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11963.31416
Train_Epoch_Reward                14698.49018
Running_Training_Average_Rewards  15103.54039
Explore_Time                      0.00091
Train___Time                      0.08015
Eval____Time                      0.15880
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11928.15677
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.55298     0.93484    89.93418     87.08299
alpha_0                           0.87198      0.00012    0.87216      0.87181
Alpha_loss                        -0.92128     0.00131    -0.91974     -0.92369
Training/policy_loss              -2.50394     0.00375    -2.49966     -2.51012
Training/qf1_loss                 12100.24707  447.89865  12553.38867  11515.91699
Training/qf2_loss                 15093.32480  448.00725  15565.33301  14492.10156
Training/pf_norm                  0.12400      0.03507    0.15436      0.07376
Training/qf1_norm                 4260.10186   60.64149   4345.66357   4161.28174
Training/qf2_norm                 274.45507    2.89268    278.59018    269.82404
log_std/mean                      -0.12318     0.00035    -0.12270     -0.12367
log_probs/mean                    -2.73020     0.00447    -2.72555     -2.73797
mean/mean                         0.00047      0.00003    0.00050      0.00044
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01882767677307129
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 72010
epoch first part time 2.86102294921875e-06
replay_buffer._size: [42150]
collect time 0.0008714199066162109
inner_dict_sum {'sac_diff0': 0.00021600723266601562, 'sac_diff1': 0.0073757171630859375, 'sac_diff2': 0.008670806884765625, 'sac_diff3': 0.010913848876953125, 'sac_diff4': 0.007272481918334961, 'sac_diff5': 0.03433108329772949, 'sac_diff6': 0.0004131793975830078, 'all': 0.06919312477111816}
diff5_list [0.008611202239990234, 0.006631135940551758, 0.006590366363525391, 0.006321907043457031, 0.006176471710205078]
time3 0
time4 0.07005143165588379
time5 0.07010746002197266
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6649)
policy weight change tensor(38.8386, grad_fn=<SumBackward0>)
time8 0.0019767284393310547
train_time 0.08190178871154785
eval time 0.15061593055725098
epoch last part time 9.5367431640625e-06
2024-01-23 01:02:01,782 MainThread INFO: EPOCH:274
2024-01-23 01:02:01,782 MainThread INFO: Time Consumed:0.23584485054016113s
2024-01-23 01:02:01,783 MainThread INFO: Total Frames:42000s
  3%|▎         | 275/10000 [03:29<42:15,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11974.08390
Train_Epoch_Reward                8330.51423
Running_Training_Average_Rewards  14369.61457
Explore_Time                      0.00087
Train___Time                      0.08190
Eval____Time                      0.15062
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11956.62393
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.17626     2.63407     94.50945     86.97780
alpha_0                           0.87154      0.00012     0.87172      0.87137
Alpha_loss                        -0.92510     0.00099     -0.92379     -0.92655
Training/policy_loss              -2.48435     0.00320     -2.47920     -2.48926
Training/qf1_loss                 12838.23301  1774.55203  15869.37695  11213.29883
Training/qf2_loss                 15930.77246  1871.97199  19136.12305  14212.00586
Training/pf_norm                  0.13088      0.02730     0.18279      0.10294
Training/qf1_norm                 4563.20156   164.24746   4842.27686   4382.98096
Training/qf2_norm                 252.73131    7.11692     264.41644    243.98401
log_std/mean                      -0.12861     0.00007     -0.12852     -0.12873
log_probs/mean                    -2.73348     0.00359     -2.72755     -2.73881
mean/mean                         0.00109      0.00005     0.00118      0.00104
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019303321838378906
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 72010
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [42300]
collect time 0.0008664131164550781
inside mustsac before update, task 0, sumup 72010
inside mustsac after update, task 0, sumup 70354
inner_dict_sum {'sac_diff0': 0.0002110004425048828, 'sac_diff1': 0.007043600082397461, 'sac_diff2': 0.008706092834472656, 'sac_diff3': 0.011124372482299805, 'sac_diff4': 0.0075342655181884766, 'sac_diff5': 0.0513463020324707, 'sac_diff6': 0.0004324913024902344, 'all': 0.08639812469482422}
diff5_list [0.010261058807373047, 0.01155996322631836, 0.010035514831542969, 0.009603738784790039, 0.009886026382446289]
time3 0.0008974075317382812
time4 0.08730506896972656
time5 0.08735966682434082
time7 0.010016918182373047
gen_weight_change tensor(-22.5462)
policy weight change tensor(38.7890, grad_fn=<SumBackward0>)
time8 0.0018687248229980469
train_time 0.11812376976013184
eval time 0.12007927894592285
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:02,047 MainThread INFO: EPOCH:275
2024-01-23 01:02:02,047 MainThread INFO: Time Consumed:0.24149703979492188s
2024-01-23 01:02:02,047 MainThread INFO: Total Frames:42150s
  3%|▎         | 276/10000 [03:29<42:25,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11985.41066
Train_Epoch_Reward                10002.36143
Running_Training_Average_Rewards  14563.14057
Explore_Time                      0.00085
Train___Time                      0.11812
Eval____Time                      0.12008
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11962.19412
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.90975     2.21526     91.54315     84.82104
alpha_0                           0.87111      0.00012     0.87128      0.87093
Alpha_loss                        -0.92916     0.00085     -0.92769     -0.93017
Training/policy_loss              -2.50548     0.00539     -2.50127     -2.51577
Training/qf1_loss                 12511.48262  1486.56195  14866.20117  10206.54395
Training/qf2_loss                 15530.79277  1452.58077  17770.54102  13220.66211
Training/pf_norm                  0.13842      0.00684     0.14833      0.12827
Training/qf1_norm                 4338.45752   99.76541    4485.77734   4214.89258
Training/qf2_norm                 263.32746    8.96629     275.20425    248.86877
log_std/mean                      -0.13654     0.00777     -0.12463     -0.14623
log_probs/mean                    -2.73853     0.00197     -2.73608     -2.74131
mean/mean                         -0.00005     0.00062     0.00086      -0.00057
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01898932456970215
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70354
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [42450]
collect time 0.0009639263153076172
inner_dict_sum {'sac_diff0': 0.00019502639770507812, 'sac_diff1': 0.0070879459381103516, 'sac_diff2': 0.008652687072753906, 'sac_diff3': 0.01145315170288086, 'sac_diff4': 0.007996559143066406, 'sac_diff5': 0.03533530235290527, 'sac_diff6': 0.00041031837463378906, 'all': 0.07113099098205566}
diff5_list [0.006578207015991211, 0.007002115249633789, 0.00719904899597168, 0.006954193115234375, 0.007601737976074219]
time3 0
time4 0.07201790809631348
time5 0.0720667839050293
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5462)
policy weight change tensor(38.9844, grad_fn=<SumBackward0>)
time8 0.0022268295288085938
train_time 0.08369302749633789
eval time 0.1637248992919922
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:02,320 MainThread INFO: EPOCH:276
2024-01-23 01:02:02,320 MainThread INFO: Time Consumed:0.2508242130279541s
2024-01-23 01:02:02,320 MainThread INFO: Total Frames:42300s
  3%|▎         | 277/10000 [03:29<42:58,  3.77it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11968.46266
Train_Epoch_Reward                6211.31342
Running_Training_Average_Rewards  14267.22373
Explore_Time                      0.00096
Train___Time                      0.08369
Eval____Time                      0.16372
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11748.97557
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.55975     1.57514     92.56985     87.95444
alpha_0                           0.87067      0.00012     0.87085      0.87050
Alpha_loss                        -0.93137     0.00156     -0.92889     -0.93308
Training/policy_loss              -2.48719     0.00414     -2.48060     -2.49345
Training/qf1_loss                 13944.10371  1485.73579  15986.97168  12247.32422
Training/qf2_loss                 17148.12422  1531.30282  19220.46484  15421.17676
Training/pf_norm                  0.15355      0.03822     0.19888      0.08854
Training/qf1_norm                 4549.18262   91.93020    4671.74756   4397.80469
Training/qf2_norm                 253.40870    4.28974     258.93216    246.38663
log_std/mean                      -0.11847     0.00040     -0.11799     -0.11908
log_probs/mean                    -2.73007     0.00508     -2.72191     -2.73756
mean/mean                         -0.00129     0.00016     -0.00106     -0.00151
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018964290618896484
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70354
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [42600]
collect time 0.0009491443634033203
inner_dict_sum {'sac_diff0': 0.00022339820861816406, 'sac_diff1': 0.007329702377319336, 'sac_diff2': 0.008817672729492188, 'sac_diff3': 0.01074361801147461, 'sac_diff4': 0.007131338119506836, 'sac_diff5': 0.03338789939880371, 'sac_diff6': 0.0003974437713623047, 'all': 0.06803107261657715}
diff5_list [0.007176399230957031, 0.007111310958862305, 0.006659030914306641, 0.006189823150634766, 0.006251335144042969]
time3 0
time4 0.0688474178314209
time5 0.06890583038330078
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5462)
policy weight change tensor(39.1213, grad_fn=<SumBackward0>)
time8 0.0019259452819824219
train_time 0.08004522323608398
eval time 0.1580960750579834
epoch last part time 7.62939453125e-06
2024-01-23 01:02:02,584 MainThread INFO: EPOCH:277
2024-01-23 01:02:02,584 MainThread INFO: Time Consumed:0.24153351783752441s
2024-01-23 01:02:02,584 MainThread INFO: Total Frames:42450s
  3%|▎         | 278/10000 [03:30<42:53,  3.78it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11943.73903
Train_Epoch_Reward                12160.88194
Running_Training_Average_Rewards  14427.11740
Explore_Time                      0.00094
Train___Time                      0.08005
Eval____Time                      0.15810
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11811.55504
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.02836     1.39253     91.51945     88.32127
alpha_0                           0.87024      0.00012     0.87041      0.87006
Alpha_loss                        -0.93531     0.00076     -0.93421     -0.93627
Training/policy_loss              -2.49543     0.00210     -2.49254     -2.49780
Training/qf1_loss                 14073.84648  1209.05782  16122.33008  12784.02344
Training/qf2_loss                 17122.48867  1250.06122  19216.40820  15747.72656
Training/pf_norm                  0.10947      0.01714     0.12944      0.08516
Training/qf1_norm                 4398.20195   91.31509    4480.33984   4276.45557
Training/qf2_norm                 268.56390    4.03358     272.96729    263.46976
log_std/mean                      -0.12824     0.00018     -0.12796     -0.12845
log_probs/mean                    -2.73421     0.00210     -2.73146     -2.73693
mean/mean                         0.00044      0.00002     0.00047      0.00041
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01859736442565918
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70354
epoch first part time 2.86102294921875e-06
replay_buffer._size: [42750]
collect time 0.0008480548858642578
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.006742000579833984, 'sac_diff2': 0.007852315902709961, 'sac_diff3': 0.010197877883911133, 'sac_diff4': 0.006798982620239258, 'sac_diff5': 0.031374454498291016, 'sac_diff6': 0.0003819465637207031, 'all': 0.0635685920715332}
diff5_list [0.006592988967895508, 0.0062677860260009766, 0.006257057189941406, 0.0064258575439453125, 0.0058307647705078125]
time3 0
time4 0.06432509422302246
time5 0.06436777114868164
time7 9.5367431640625e-07
gen_weight_change tensor(-22.5462)
policy weight change tensor(39.1699, grad_fn=<SumBackward0>)
time8 0.0018935203552246094
train_time 0.07529735565185547
eval time 0.15631103515625
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:02,841 MainThread INFO: EPOCH:278
2024-01-23 01:02:02,841 MainThread INFO: Time Consumed:0.23484277725219727s
2024-01-23 01:02:02,841 MainThread INFO: Total Frames:42600s
  3%|▎         | 279/10000 [03:30<42:30,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11925.49462
Train_Epoch_Reward                12830.72345
Running_Training_Average_Rewards  14424.79503
Explore_Time                      0.00084
Train___Time                      0.07530
Eval____Time                      0.15631
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11864.58291
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.27198     1.30046     91.23637     87.51777
alpha_0                           0.86980      0.00012     0.86998      0.86963
Alpha_loss                        -0.93882     0.00137     -0.93631     -0.94040
Training/policy_loss              -2.50410     0.00893     -2.48844     -2.51257
Training/qf1_loss                 13186.84297  986.54845   14585.39551  11962.51074
Training/qf2_loss                 16238.46426  1043.30020  17688.13281  14962.88477
Training/pf_norm                  0.08756      0.01488     0.10897      0.06633
Training/qf1_norm                 4493.25645   94.40929    4627.71631   4365.18652
Training/qf2_norm                 268.38914    3.79973     274.07394    263.06839
log_std/mean                      -0.12896     0.00008     -0.12884     -0.12906
log_probs/mean                    -2.73527     0.01013     -2.71726     -2.74446
mean/mean                         0.00005      0.00004     0.00010      -0.00002
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01856684684753418
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70354
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [42900]
collect time 0.0009331703186035156
inner_dict_sum {'sac_diff0': 0.00020885467529296875, 'sac_diff1': 0.006818532943725586, 'sac_diff2': 0.008531570434570312, 'sac_diff3': 0.010456562042236328, 'sac_diff4': 0.006880998611450195, 'sac_diff5': 0.031539201736450195, 'sac_diff6': 0.0004024505615234375, 'all': 0.06483817100524902}
diff5_list [0.006609678268432617, 0.006306171417236328, 0.006201744079589844, 0.00628352165222168, 0.0061380863189697266]
time3 0
time4 0.06559371948242188
time5 0.06563901901245117
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5462)
policy weight change tensor(39.1075, grad_fn=<SumBackward0>)
time8 0.0021200180053710938
train_time 0.07683730125427246
eval time 0.15612435340881348
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:03,099 MainThread INFO: EPOCH:279
2024-01-23 01:02:03,100 MainThread INFO: Time Consumed:0.23632407188415527s
2024-01-23 01:02:03,100 MainThread INFO: Total Frames:42750s
  3%|▎         | 280/10000 [03:30<42:18,  3.83it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11910.50320
Train_Epoch_Reward                12553.54277
Running_Training_Average_Rewards  14240.04100
Explore_Time                      0.00093
Train___Time                      0.07684
Eval____Time                      0.15612
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11882.14420
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.06946     1.46117    91.21816     86.67352
alpha_0                           0.86937      0.00012    0.86954      0.86919
Alpha_loss                        -0.94218     0.00106    -0.94071     -0.94366
Training/policy_loss              -2.49153     0.00039    -2.49098     -2.49209
Training/qf1_loss                 12948.26465  768.53215  14263.11523  11971.12891
Training/qf2_loss                 16037.07988  771.33153  17350.87500  15045.05176
Training/pf_norm                  0.11221      0.01953    0.13276      0.07591
Training/qf1_norm                 4310.80742   82.84509   4448.69873   4190.31152
Training/qf2_norm                 255.93393    4.01313    261.77368    249.26283
log_std/mean                      -0.13758     0.00018    -0.13728     -0.13777
log_probs/mean                    -2.73522     0.00083    -2.73425     -2.73614
mean/mean                         0.00145      0.00002    0.00147      0.00143
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018486499786376953
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70354
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [43050]
collect time 0.0008280277252197266
inside mustsac before update, task 0, sumup 70354
inside mustsac after update, task 0, sumup 71101
inner_dict_sum {'sac_diff0': 0.00021719932556152344, 'sac_diff1': 0.0077707767486572266, 'sac_diff2': 0.008978128433227539, 'sac_diff3': 0.011425018310546875, 'sac_diff4': 0.007518768310546875, 'sac_diff5': 0.052773237228393555, 'sac_diff6': 0.0004546642303466797, 'all': 0.08913779258728027}
diff5_list [0.010745525360107422, 0.011501073837280273, 0.009918689727783203, 0.009850025177001953, 0.010757923126220703]
time3 0.0008983612060546875
time4 0.09011030197143555
time5 0.09016728401184082
time7 0.009233713150024414
gen_weight_change tensor(-22.3582)
policy weight change tensor(39.0147, grad_fn=<SumBackward0>)
time8 0.002746105194091797
train_time 0.12100577354431152
eval time 0.11200189590454102
epoch last part time 7.3909759521484375e-06
2024-01-23 01:02:03,358 MainThread INFO: EPOCH:280
2024-01-23 01:02:03,358 MainThread INFO: Time Consumed:0.2362961769104004s
2024-01-23 01:02:03,358 MainThread INFO: Total Frames:42900s
  3%|▎         | 281/10000 [03:30<42:22,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11904.14279
Train_Epoch_Reward                28581.87421
Running_Training_Average_Rewards  14801.75209
Explore_Time                      0.00082
Train___Time                      0.12101
Eval____Time                      0.11200
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11906.36940
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.72111     3.80387     95.12897     84.68575
alpha_0                           0.86893      0.00012     0.86911      0.86876
Alpha_loss                        -0.94541     0.00191     -0.94352     -0.94826
Training/policy_loss              -2.50241     0.00830     -2.48914     -2.51341
Training/qf1_loss                 12919.73945  1254.96272  14167.48438  10593.89648
Training/qf2_loss                 16254.92090  1452.32520  17458.69141  13501.92676
Training/pf_norm                  0.11282      0.02471     0.14709      0.07291
Training/qf1_norm                 4712.84141   299.44521   5106.92236   4213.74414
Training/qf2_norm                 275.02714    10.17844    285.22113    255.82523
log_std/mean                      -0.13386     0.00577     -0.12575     -0.14112
log_probs/mean                    -2.73422     0.01000     -2.72076     -2.74965
mean/mean                         -0.00034     0.00113     0.00150      -0.00180
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019511938095092773
epoch last part time3 0.0029790401458740234
inside rlalgo, task 0, sumup 71101
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [43200]
collect time 0.0009722709655761719
inner_dict_sum {'sac_diff0': 0.00023436546325683594, 'sac_diff1': 0.006735801696777344, 'sac_diff2': 0.008686304092407227, 'sac_diff3': 0.010675668716430664, 'sac_diff4': 0.007243633270263672, 'sac_diff5': 0.03254103660583496, 'sac_diff6': 0.0003867149353027344, 'all': 0.06650352478027344}
diff5_list [0.007205486297607422, 0.006827592849731445, 0.00630640983581543, 0.006234169006347656, 0.005967378616333008]
time3 0
time4 0.06725192070007324
time5 0.06729769706726074
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3582)
policy weight change tensor(38.8158, grad_fn=<SumBackward0>)
time8 0.0019497871398925781
train_time 0.07845377922058105
eval time 0.15449166297912598
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:03,620 MainThread INFO: EPOCH:281
2024-01-23 01:02:03,620 MainThread INFO: Time Consumed:0.23636674880981445s
2024-01-23 01:02:03,620 MainThread INFO: Total Frames:43050s
  3%|▎         | 282/10000 [03:31<42:13,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11809.24037
Train_Epoch_Reward                4368.01146
Running_Training_Average_Rewards  14152.18792
Explore_Time                      0.00097
Train___Time                      0.07845
Eval____Time                      0.15449
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11066.78183
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.09560     1.12281    92.76381     89.43496
alpha_0                           0.86850      0.00012    0.86867      0.86832
Alpha_loss                        -0.94856     0.00125    -0.94701     -0.95035
Training/policy_loss              -2.51780     0.00496    -2.50943     -2.52474
Training/qf1_loss                 13229.88184  896.80149  14490.36133  11932.47168
Training/qf2_loss                 16571.29473  934.62825  17889.59180  15182.51270
Training/pf_norm                  0.14506      0.03472    0.18397      0.08240
Training/qf1_norm                 4686.36602   76.23732   4787.06396   4559.63184
Training/qf2_norm                 294.51551    3.46939    299.67435    289.32953
log_std/mean                      -0.14571     0.00042    -0.14503     -0.14617
log_probs/mean                    -2.73267     0.00577    -2.72324     -2.74078
mean/mean                         0.00170      0.00004    0.00177      0.00165
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0190887451171875
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71101
epoch first part time 2.86102294921875e-06
replay_buffer._size: [43350]
collect time 0.0010135173797607422
inner_dict_sum {'sac_diff0': 0.0002472400665283203, 'sac_diff1': 0.00758671760559082, 'sac_diff2': 0.00919032096862793, 'sac_diff3': 0.011426925659179688, 'sac_diff4': 0.007800579071044922, 'sac_diff5': 0.0339961051940918, 'sac_diff6': 0.0004220008850097656, 'all': 0.07066988945007324}
diff5_list [0.006976604461669922, 0.0063359737396240234, 0.00645756721496582, 0.007059812545776367, 0.007166147232055664]
time3 0
time4 0.0715491771697998
time5 0.07160091400146484
time7 9.5367431640625e-07
gen_weight_change tensor(-22.3582)
policy weight change tensor(38.5883, grad_fn=<SumBackward0>)
time8 0.002067089080810547
train_time 0.08310127258300781
eval time 0.14909076690673828
epoch last part time 7.3909759521484375e-06
2024-01-23 01:02:03,878 MainThread INFO: EPOCH:282
2024-01-23 01:02:03,879 MainThread INFO: Time Consumed:0.23565149307250977s
2024-01-23 01:02:03,879 MainThread INFO: Total Frames:43200s
  3%|▎         | 283/10000 [03:31<42:06,  3.85it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11717.04918
Train_Epoch_Reward                7120.18958
Running_Training_Average_Rewards  13719.66897
Explore_Time                      0.00101
Train___Time                      0.08310
Eval____Time                      0.14909
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11043.10799
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.41421     0.97919    89.85966     87.30784
alpha_0                           0.86806      0.00012    0.86824      0.86789
Alpha_loss                        -0.95104     0.00099    -0.94998     -0.95240
Training/policy_loss              -2.48500     0.00266    -2.48229     -2.48899
Training/qf1_loss                 11770.70234  705.93213  12416.17676  10506.00293
Training/qf2_loss                 14981.75645  721.49776  15705.59863  13682.32324
Training/pf_norm                  0.09223      0.02589    0.13172      0.05393
Training/qf1_norm                 4432.66279   67.75282   4537.35400   4347.90137
Training/qf2_norm                 259.86666    2.79558    263.95660    256.60281
log_std/mean                      -0.13928     0.00038    -0.13878     -0.13986
log_probs/mean                    -2.72636     0.00301    -2.72323     -2.73122
mean/mean                         -0.00059     0.00005    -0.00051     -0.00064
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018897056579589844
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71101
epoch first part time 2.86102294921875e-06
replay_buffer._size: [43500]
collect time 0.0008764266967773438
inner_dict_sum {'sac_diff0': 0.00023818016052246094, 'sac_diff1': 0.006926774978637695, 'sac_diff2': 0.008347511291503906, 'sac_diff3': 0.010814189910888672, 'sac_diff4': 0.007115364074707031, 'sac_diff5': 0.03223729133605957, 'sac_diff6': 0.0003955364227294922, 'all': 0.06607484817504883}
diff5_list [0.006971836090087891, 0.006400585174560547, 0.006528377532958984, 0.00618433952331543, 0.006152153015136719]
time3 0
time4 0.06682825088500977
time5 0.06687593460083008
time7 9.5367431640625e-07
gen_weight_change tensor(-22.3582)
policy weight change tensor(38.4476, grad_fn=<SumBackward0>)
time8 0.0019385814666748047
train_time 0.07793736457824707
eval time 0.15370821952819824
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:04,136 MainThread INFO: EPOCH:283
2024-01-23 01:02:04,136 MainThread INFO: Time Consumed:0.23492717742919922s
2024-01-23 01:02:04,136 MainThread INFO: Total Frames:43350s
  3%|▎         | 284/10000 [03:31<42:02,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11628.88615
Train_Epoch_Reward                15440.03360
Running_Training_Average_Rewards  13331.51762
Explore_Time                      0.00087
Train___Time                      0.07794
Eval____Time                      0.15371
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11046.52655
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.82590     3.54304     97.74812     88.39031
alpha_0                           0.86763      0.00012     0.86780      0.86746
Alpha_loss                        -0.95599     0.00090     -0.95518     -0.95715
Training/policy_loss              -2.48728     0.00313     -2.48314     -2.49159
Training/qf1_loss                 12637.03730  1811.08407  15823.03418  11158.16992
Training/qf2_loss                 16156.42871  1952.80486  19626.94336  14573.86523
Training/pf_norm                  0.11411      0.02474     0.16290      0.09616
Training/qf1_norm                 4882.35791   238.45061   5344.67578   4703.23584
Training/qf2_norm                 259.20515    9.85165     278.42230    252.24496
log_std/mean                      -0.13074     0.00021     -0.13048     -0.13107
log_probs/mean                    -2.73753     0.00347     -2.73276     -2.74200
mean/mean                         -0.00094     0.00015     -0.00070     -0.00109
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019804000854492188
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71101
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [43650]
collect time 0.0009632110595703125
inner_dict_sum {'sac_diff0': 0.00023436546325683594, 'sac_diff1': 0.006979465484619141, 'sac_diff2': 0.007997274398803711, 'sac_diff3': 0.010210275650024414, 'sac_diff4': 0.006754636764526367, 'sac_diff5': 0.03298377990722656, 'sac_diff6': 0.0003898143768310547, 'all': 0.06554961204528809}
diff5_list [0.0066945552825927734, 0.0071353912353515625, 0.006802797317504883, 0.0061419010162353516, 0.006209135055541992]
time3 0
time4 0.06629037857055664
time5 0.06634092330932617
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3582)
policy weight change tensor(38.3638, grad_fn=<SumBackward0>)
time8 0.001985311508178711
train_time 0.07744407653808594
eval time 0.14718294143676758
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:04,388 MainThread INFO: EPOCH:284
2024-01-23 01:02:04,388 MainThread INFO: Time Consumed:0.2281649112701416s
2024-01-23 01:02:04,389 MainThread INFO: Total Frames:43500s
  3%|▎         | 285/10000 [03:32<41:38,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11538.13269
Train_Epoch_Reward                6239.39762
Running_Training_Average_Rewards  12708.51215
Explore_Time                      0.00096
Train___Time                      0.07744
Eval____Time                      0.14718
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11049.08935
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.09118     0.79350    90.22384     88.35207
alpha_0                           0.86720      0.00012    0.86737      0.86702
Alpha_loss                        -0.95970     0.00043    -0.95894     -0.96010
Training/policy_loss              -2.49348     0.00604    -2.48649     -2.50464
Training/qf1_loss                 12367.56602  638.69837  12984.45020  11199.80273
Training/qf2_loss                 15822.01445  668.93906  16479.97656  14620.11523
Training/pf_norm                  0.11250      0.05858    0.22126      0.05405
Training/qf1_norm                 4730.49473   64.62141   4828.10205   4668.04785
Training/qf2_norm                 258.38854    2.23992    261.54868    256.14969
log_std/mean                      -0.13276     0.00013    -0.13255     -0.13290
log_probs/mean                    -2.73992     0.00660    -2.73240     -2.75221
mean/mean                         -0.00037     0.00006    -0.00031     -0.00047
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01934981346130371
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71101
epoch first part time 2.86102294921875e-06
replay_buffer._size: [43800]
collect time 0.0009992122650146484
inside mustsac before update, task 0, sumup 71101
inside mustsac after update, task 0, sumup 71089
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.0067865848541259766, 'sac_diff2': 0.008326292037963867, 'sac_diff3': 0.010624885559082031, 'sac_diff4': 0.007574796676635742, 'sac_diff5': 0.05199480056762695, 'sac_diff6': 0.00041937828063964844, 'all': 0.08594107627868652}
diff5_list [0.010428190231323242, 0.009685993194580078, 0.009634971618652344, 0.010239124298095703, 0.012006521224975586]
time3 0.0008726119995117188
time4 0.08684515953063965
time5 0.08689522743225098
time7 0.009161710739135742
gen_weight_change tensor(-22.3224)
policy weight change tensor(38.3308, grad_fn=<SumBackward0>)
time8 0.002109050750732422
train_time 0.11652231216430664
eval time 0.11796379089355469
epoch last part time 1.71661376953125e-05
2024-01-23 01:02:04,649 MainThread INFO: EPOCH:285
2024-01-23 01:02:04,649 MainThread INFO: Time Consumed:0.2379302978515625s
2024-01-23 01:02:04,650 MainThread INFO: Total Frames:43650s
  3%|▎         | 286/10000 [03:32<41:47,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11444.90865
Train_Epoch_Reward                3644.69354
Running_Training_Average_Rewards  12360.29970
Explore_Time                      0.00099
Train___Time                      0.11652
Eval____Time                      0.11796
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11029.95364
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.12277     1.31858     89.83688     86.85458
alpha_0                           0.86676      0.00012     0.86694      0.86659
Alpha_loss                        -0.96213     0.00104     -0.96113     -0.96340
Training/policy_loss              -2.49621     0.00538     -2.48816     -2.50189
Training/qf1_loss                 12026.90430  1412.89707  14656.41699  10420.56641
Training/qf2_loss                 15289.96289  1411.39813  18030.11133  13959.48145
Training/pf_norm                  0.12060      0.04713     0.17468      0.05108
Training/qf1_norm                 4509.23223   144.15681   4708.71484   4320.07422
Training/qf2_norm                 265.21046    9.20791     276.43243    250.81796
log_std/mean                      -0.12844     0.00186     -0.12583     -0.13102
log_probs/mean                    -2.73335     0.00447     -2.72636     -2.73892
mean/mean                         -0.00021     0.00058     0.00093      -0.00060
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01871490478515625
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 2.384185791015625e-06
replay_buffer._size: [43950]
collect time 0.0008995532989501953
inner_dict_sum {'sac_diff0': 0.00021028518676757812, 'sac_diff1': 0.007372617721557617, 'sac_diff2': 0.008768081665039062, 'sac_diff3': 0.011320352554321289, 'sac_diff4': 0.00733494758605957, 'sac_diff5': 0.035051584243774414, 'sac_diff6': 0.0004119873046875, 'all': 0.07046985626220703}
diff5_list [0.007136106491088867, 0.006543874740600586, 0.007852554321289062, 0.006765127182006836, 0.0067539215087890625]
time3 0
time4 0.07129311561584473
time5 0.07134556770324707
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3224)
policy weight change tensor(38.3318, grad_fn=<SumBackward0>)
time8 0.001995563507080078
train_time 0.08250236511230469
eval time 0.14594125747680664
epoch last part time 7.3909759521484375e-06
2024-01-23 01:02:04,903 MainThread INFO: EPOCH:286
2024-01-23 01:02:04,903 MainThread INFO: Time Consumed:0.23178315162658691s
2024-01-23 01:02:04,904 MainThread INFO: Total Frames:43800s
  3%|▎         | 287/10000 [03:32<41:38,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11464.39426
Train_Epoch_Reward                13216.09788
Running_Training_Average_Rewards  12274.32853
Explore_Time                      0.00089
Train___Time                      0.08250
Eval____Time                      0.14594
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11943.83174
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.08040     1.64011     94.77911     90.15852
alpha_0                           0.86633      0.00012     0.86650      0.86616
Alpha_loss                        -0.96492     0.00128     -0.96267     -0.96640
Training/policy_loss              -2.50173     0.00391     -2.49672     -2.50845
Training/qf1_loss                 14212.04688  1730.85193  17129.74023  12405.06250
Training/qf2_loss                 17689.90820  1777.41014  20637.55859  15741.98242
Training/pf_norm                  0.11929      0.01859     0.15230      0.09778
Training/qf1_norm                 4989.48516   119.04592   5115.50879   4778.71777
Training/qf2_norm                 297.00349    5.20757     302.14984    287.68683
log_std/mean                      -0.12767     0.00009     -0.12758     -0.12782
log_probs/mean                    -2.72930     0.00460     -2.72296     -2.73679
mean/mean                         0.00056      0.00001     0.00058      0.00055
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01964092254638672
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 2.86102294921875e-06
replay_buffer._size: [44100]
collect time 0.0009288787841796875
inner_dict_sum {'sac_diff0': 0.0002200603485107422, 'sac_diff1': 0.0068378448486328125, 'sac_diff2': 0.008138179779052734, 'sac_diff3': 0.010005712509155273, 'sac_diff4': 0.0070345401763916016, 'sac_diff5': 0.03171396255493164, 'sac_diff6': 0.0003821849822998047, 'all': 0.06433248519897461}
diff5_list [0.0067479610443115234, 0.0061833858489990234, 0.006159543991088867, 0.006214141845703125, 0.0064089298248291016]
time3 0
time4 0.06508469581604004
time5 0.06513071060180664
time7 9.5367431640625e-07
gen_weight_change tensor(-22.3224)
policy weight change tensor(38.3512, grad_fn=<SumBackward0>)
time8 0.0019016265869140625
train_time 0.07610273361206055
eval time 0.15296530723571777
epoch last part time 8.821487426757812e-06
2024-01-23 01:02:05,159 MainThread INFO: EPOCH:287
2024-01-23 01:02:05,160 MainThread INFO: Time Consumed:0.23252534866333008s
2024-01-23 01:02:05,160 MainThread INFO: Total Frames:43950s
  3%|▎         | 288/10000 [03:32<41:41,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11475.10788
Train_Epoch_Reward                19216.27254
Running_Training_Average_Rewards  12468.16563
Explore_Time                      0.00092
Train___Time                      0.07610
Eval____Time                      0.15297
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11918.69120
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.46062     2.16831     92.36703     86.58121
alpha_0                           0.86590      0.00012     0.86607      0.86572
Alpha_loss                        -0.96967     0.00155     -0.96756     -0.97196
Training/policy_loss              -2.51280     0.00620     -2.50294     -2.51970
Training/qf1_loss                 12460.40293  1389.79227  14159.37988  10304.76367
Training/qf2_loss                 16461.64805  1494.73505  18275.93945  14097.42676
Training/pf_norm                  0.12734      0.02099     0.15576      0.10306
Training/qf1_norm                 5099.80244   171.43775   5251.63867   4797.97900
Training/qf2_norm                 288.35453    6.69405     294.09155    276.42694
log_std/mean                      -0.13506     0.00007     -0.13497     -0.13513
log_probs/mean                    -2.73889     0.00724     -2.72777     -2.74688
mean/mean                         0.00050      0.00002     0.00053      0.00047
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0219419002532959
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 3.337860107421875e-06
replay_buffer._size: [44250]
collect time 0.0009639263153076172
inner_dict_sum {'sac_diff0': 0.00022602081298828125, 'sac_diff1': 0.007240772247314453, 'sac_diff2': 0.008415460586547852, 'sac_diff3': 0.011347293853759766, 'sac_diff4': 0.0075795650482177734, 'sac_diff5': 0.03298521041870117, 'sac_diff6': 0.00038170814514160156, 'all': 0.0681760311126709}
diff5_list [0.0076982975006103516, 0.006462574005126953, 0.006346940994262695, 0.0062177181243896484, 0.0062596797943115234]
time3 0
time4 0.06893563270568848
time5 0.06897974014282227
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3224)
policy weight change tensor(38.2986, grad_fn=<SumBackward0>)
time8 0.0018744468688964844
train_time 0.08050346374511719
eval time 0.155076265335083
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:05,424 MainThread INFO: EPOCH:288
2024-01-23 01:02:05,425 MainThread INFO: Time Consumed:0.23889994621276855s
2024-01-23 01:02:05,425 MainThread INFO: Total Frames:44100s
  3%|▎         | 289/10000 [03:33<41:52,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11474.74871
Train_Epoch_Reward                13627.41768
Running_Training_Average_Rewards  12787.00304
Explore_Time                      0.00096
Train___Time                      0.08050
Eval____Time                      0.15508
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11860.99126
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.31144     3.40101     94.61154     84.99971
alpha_0                           0.86546      0.00012     0.86564      0.86529
Alpha_loss                        -0.97193     0.00099     -0.97043     -0.97319
Training/policy_loss              -2.49155     0.00099     -2.49063     -2.49347
Training/qf1_loss                 11963.33320  2362.35920  16415.92578  9728.22949
Training/qf2_loss                 15353.62539  2518.06060  20101.01953  12968.29395
Training/pf_norm                  0.11769      0.02649     0.15405      0.08647
Training/qf1_norm                 4550.75029   231.76210   4985.50195   4327.96436
Training/qf2_norm                 267.60791    9.91022     286.03647    258.01346
log_std/mean                      -0.13508     0.00011     -0.13494     -0.13525
log_probs/mean                    -2.73125     0.00116     -2.73020     -2.73349
mean/mean                         0.00166      0.00006     0.00174      0.00156
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018428564071655273
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [44400]
collect time 0.0009026527404785156
inner_dict_sum {'sac_diff0': 0.0002505779266357422, 'sac_diff1': 0.007894277572631836, 'sac_diff2': 0.009796857833862305, 'sac_diff3': 0.012340545654296875, 'sac_diff4': 0.008234977722167969, 'sac_diff5': 0.03973245620727539, 'sac_diff6': 0.0005002021789550781, 'all': 0.0787498950958252}
diff5_list [0.007863759994506836, 0.0076291561126708984, 0.007782936096191406, 0.008567571640014648, 0.007889032363891602]
time3 0
time4 0.07950520515441895
time5 0.07955336570739746
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3224)
policy weight change tensor(38.2412, grad_fn=<SumBackward0>)
time8 0.0020613670349121094
train_time 0.09114670753479004
eval time 0.13634967803955078
epoch last part time 5.7220458984375e-06
2024-01-23 01:02:05,677 MainThread INFO: EPOCH:289
2024-01-23 01:02:05,677 MainThread INFO: Time Consumed:0.23070716857910156s
2024-01-23 01:02:05,677 MainThread INFO: Total Frames:44250s
  3%|▎         | 290/10000 [03:33<41:35,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11468.38798
Train_Epoch_Reward                6923.29902
Running_Training_Average_Rewards  12744.25239
Explore_Time                      0.00090
Train___Time                      0.09115
Eval____Time                      0.13635
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11818.53687
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.04471     2.77953     93.62574     86.27619
alpha_0                           0.86503      0.00012     0.86520      0.86486
Alpha_loss                        -0.97683     0.00165     -0.97506     -0.97958
Training/policy_loss              -2.50896     0.00441     -2.50358     -2.51686
Training/qf1_loss                 12519.52871  2123.07789  15518.12305  10355.57715
Training/qf2_loss                 16103.44746  2228.58276  19171.42969  13841.59668
Training/pf_norm                  0.12640      0.02033     0.16440      0.10902
Training/qf1_norm                 4812.15771   190.31027   5130.83594   4637.91943
Training/qf2_norm                 274.66313    8.29018     288.29034    266.48956
log_std/mean                      -0.13967     0.00015     -0.13945     -0.13985
log_probs/mean                    -2.74181     0.00541     -2.73526     -2.75149
mean/mean                         -0.00051     0.00002     -0.00049     -0.00054
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018840789794921875
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [44550]
collect time 0.0008296966552734375
inside mustsac before update, task 0, sumup 71089
inside mustsac after update, task 0, sumup 70777
inner_dict_sum {'sac_diff0': 0.0002570152282714844, 'sac_diff1': 0.008495092391967773, 'sac_diff2': 0.01041555404663086, 'sac_diff3': 0.013106584548950195, 'sac_diff4': 0.009192705154418945, 'sac_diff5': 0.06342768669128418, 'sac_diff6': 0.000518798828125, 'all': 0.10541343688964844}
diff5_list [0.012798070907592773, 0.013659954071044922, 0.011837959289550781, 0.012358665466308594, 0.01277303695678711]
time3 0.0011665821075439453
time4 0.10628771781921387
time5 0.10634136199951172
time7 0.010454893112182617
gen_weight_change tensor(-22.3093)
policy weight change tensor(38.2751, grad_fn=<SumBackward0>)
time8 0.0029625892639160156
train_time 0.14058923721313477
eval time 0.08812880516052246
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:05,931 MainThread INFO: EPOCH:290
2024-01-23 01:02:05,932 MainThread INFO: Time Consumed:0.232163667678833s
2024-01-23 01:02:05,932 MainThread INFO: Total Frames:44400s
  3%|▎         | 291/10000 [03:33<41:35,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11452.13064
Train_Epoch_Reward                14038.88696
Running_Training_Average_Rewards  12958.67505
Explore_Time                      0.00083
Train___Time                      0.14059
Eval____Time                      0.08813
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11743.79594
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.98778     0.95293    92.69106     89.85883
alpha_0                           0.86460      0.00012    0.86477      0.86442
Alpha_loss                        -0.97896     0.00130    -0.97679     -0.98023
Training/policy_loss              -2.49519     0.00865    -2.48449     -2.50573
Training/qf1_loss                 12887.30586  910.73392  13991.05176  11299.28516
Training/qf2_loss                 16633.66641  881.12833  17501.85547  15054.03320
Training/pf_norm                  0.13265      0.03600    0.19429      0.09071
Training/qf1_norm                 5061.46797   139.39704  5191.38672   4832.07910
Training/qf2_norm                 277.93500    9.06646    292.51743    267.44022
log_std/mean                      -0.13248     0.00520    -0.12782     -0.13918
log_probs/mean                    -2.73327     0.00391    -2.72758     -2.73794
mean/mean                         -0.00012     0.00106    0.00193      -0.00108
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018930912017822266
epoch last part time3 0.002696990966796875
inside rlalgo, task 0, sumup 70777
epoch first part time 2.86102294921875e-06
replay_buffer._size: [44700]
collect time 0.0009267330169677734
inner_dict_sum {'sac_diff0': 0.0002009868621826172, 'sac_diff1': 0.00635075569152832, 'sac_diff2': 0.007525205612182617, 'sac_diff3': 0.00936436653137207, 'sac_diff4': 0.006279945373535156, 'sac_diff5': 0.02964162826538086, 'sac_diff6': 0.0003666877746582031, 'all': 0.059729576110839844}
diff5_list [0.006102800369262695, 0.005800962448120117, 0.005879878997802734, 0.006091117858886719, 0.005766868591308594]
time3 0
time4 0.06044507026672363
time5 0.0604863166809082
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3093)
policy weight change tensor(38.1749, grad_fn=<SumBackward0>)
time8 0.0017852783203125
train_time 0.07109546661376953
eval time 0.15421724319458008
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:06,184 MainThread INFO: EPOCH:291
2024-01-23 01:02:06,185 MainThread INFO: Time Consumed:0.2285614013671875s
2024-01-23 01:02:06,185 MainThread INFO: Total Frames:44550s
  3%|▎         | 292/10000 [03:33<41:14,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11540.28400
Train_Epoch_Reward                32818.44990
Running_Training_Average_Rewards  13603.64925
Explore_Time                      0.00092
Train___Time                      0.07110
Eval____Time                      0.15422
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11948.31548
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.15103     2.33877     92.80341     86.52083
alpha_0                           0.86416      0.00012     0.86434      0.86399
Alpha_loss                        -0.98294     0.00185     -0.98097     -0.98576
Training/policy_loss              -2.49676     0.00533     -2.49115     -2.50494
Training/qf1_loss                 12755.70488  951.96728   13936.58594  11258.07227
Training/qf2_loss                 16438.14863  1002.12661  17730.79688  14868.34961
Training/pf_norm                  0.13870      0.02097     0.16047      0.10461
Training/qf1_norm                 4868.14033   162.04860   5048.15723   4624.45312
Training/qf2_norm                 268.64229    6.73977     276.33328    258.24109
log_std/mean                      -0.14143     0.00018     -0.14113     -0.14163
log_probs/mean                    -2.73746     0.00654     -2.73070     -2.74748
mean/mean                         -0.00004     0.00007     0.00002      -0.00016
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01836085319519043
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70777
epoch first part time 2.86102294921875e-06
replay_buffer._size: [44850]
collect time 0.0008752346038818359
inner_dict_sum {'sac_diff0': 0.00022864341735839844, 'sac_diff1': 0.0065424442291259766, 'sac_diff2': 0.007929086685180664, 'sac_diff3': 0.01004648208618164, 'sac_diff4': 0.006917238235473633, 'sac_diff5': 0.031286001205444336, 'sac_diff6': 0.00037217140197753906, 'all': 0.06332206726074219}
diff5_list [0.006386518478393555, 0.006243228912353516, 0.006120443344116211, 0.006554841995239258, 0.005980968475341797]
time3 0
time4 0.06405925750732422
time5 0.0641024112701416
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3093)
policy weight change tensor(38.1198, grad_fn=<SumBackward0>)
time8 0.0018112659454345703
train_time 0.07483887672424316
eval time 0.14451909065246582
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:06,429 MainThread INFO: EPOCH:292
2024-01-23 01:02:06,429 MainThread INFO: Time Consumed:0.22246193885803223s
2024-01-23 01:02:06,429 MainThread INFO: Total Frames:44700s
  3%|▎         | 293/10000 [03:34<40:43,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11629.85775
Train_Epoch_Reward                15643.29325
Running_Training_Average_Rewards  13805.46840
Explore_Time                      0.00087
Train___Time                      0.07484
Eval____Time                      0.14452
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11938.84548
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.11763     1.43520    90.75761     86.82835
alpha_0                           0.86373      0.00012    0.86391      0.86356
Alpha_loss                        -0.98582     0.00150    -0.98426     -0.98832
Training/policy_loss              -2.49478     0.00371    -2.48990     -2.50113
Training/qf1_loss                 12275.83555  560.05484  13030.50098  11507.67773
Training/qf2_loss                 15799.31094  576.25849  16523.15039  14911.38574
Training/pf_norm                  0.12061      0.02557    0.16932      0.09756
Training/qf1_norm                 4800.21094   104.15418  4927.85645   4629.38721
Training/qf2_norm                 271.07568    4.25171    275.89957    264.24136
log_std/mean                      -0.12996     0.00004    -0.12992     -0.13004
log_probs/mean                    -2.73407     0.00459    -2.72805     -2.74198
mean/mean                         0.00089      0.00001    0.00090      0.00088
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018109798431396484
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70777
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [45000]
collect time 0.00089263916015625
inner_dict_sum {'sac_diff0': 0.0002522468566894531, 'sac_diff1': 0.00665283203125, 'sac_diff2': 0.007783651351928711, 'sac_diff3': 0.00957632064819336, 'sac_diff4': 0.006543397903442383, 'sac_diff5': 0.030964374542236328, 'sac_diff6': 0.0003790855407714844, 'all': 0.06215190887451172}
diff5_list [0.006336212158203125, 0.006296396255493164, 0.006054401397705078, 0.006230831146240234, 0.0060465335845947266]
time3 0
time4 0.0628669261932373
time5 0.06291007995605469
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3093)
policy weight change tensor(38.0052, grad_fn=<SumBackward0>)
time8 0.0018649101257324219
train_time 0.0736703872680664
eval time 0.15170645713806152
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:06,679 MainThread INFO: EPOCH:293
2024-01-23 01:02:06,679 MainThread INFO: Time Consumed:0.22858333587646484s
2024-01-23 01:02:06,680 MainThread INFO: Total Frames:44850s
  3%|▎         | 294/10000 [03:34<40:40,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11712.83473
Train_Epoch_Reward                12754.89005
Running_Training_Average_Rewards  13535.59949
Explore_Time                      0.00089
Train___Time                      0.07367
Eval____Time                      0.15171
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11876.29631
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.31353     1.87650     93.25194     88.32170
alpha_0                           0.86330      0.00012     0.86347      0.86313
Alpha_loss                        -0.98928     0.00082     -0.98774     -0.98997
Training/policy_loss              -2.49891     0.00339     -2.49359     -2.50302
Training/qf1_loss                 12066.74922  1302.29709  13888.49219  10327.74512
Training/qf2_loss                 16053.46973  1397.10945  18014.26172  14213.56543
Training/pf_norm                  0.14868      0.01014     0.16670      0.13656
Training/qf1_norm                 4921.35381   140.43051   5133.17578   4776.39600
Training/qf2_norm                 284.57095    5.76918     293.59259    278.53506
log_std/mean                      -0.14138     0.00028     -0.14095     -0.14173
log_probs/mean                    -2.73469     0.00370     -2.72904     -2.73939
mean/mean                         -0.00063     0.00011     -0.00045     -0.00074
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018590688705444336
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70777
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [45150]
collect time 0.0009338855743408203
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.006712436676025391, 'sac_diff2': 0.00795292854309082, 'sac_diff3': 0.010209798812866211, 'sac_diff4': 0.00658416748046875, 'sac_diff5': 0.03149104118347168, 'sac_diff6': 0.00038051605224609375, 'all': 0.06355595588684082}
diff5_list [0.006407976150512695, 0.006196260452270508, 0.006687641143798828, 0.00638890266418457, 0.005810260772705078]
time3 0
time4 0.06429767608642578
time5 0.0643472671508789
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3093)
policy weight change tensor(37.8936, grad_fn=<SumBackward0>)
time8 0.001794576644897461
train_time 0.07495784759521484
eval time 0.14381814002990723
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:06,923 MainThread INFO: EPOCH:294
2024-01-23 01:02:06,923 MainThread INFO: Time Consumed:0.22194433212280273s
2024-01-23 01:02:06,924 MainThread INFO: Total Frames:45000s
  3%|▎         | 295/10000 [03:34<40:17,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11792.66625
Train_Epoch_Reward                8429.35505
Running_Training_Average_Rewards  13769.46372
Explore_Time                      0.00093
Train___Time                      0.07496
Eval____Time                      0.14382
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11847.40458
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.61066     2.54794     93.02800     86.75729
alpha_0                           0.86287      0.00012     0.86304      0.86270
Alpha_loss                        -0.99302     0.00117     -0.99148     -0.99457
Training/policy_loss              -2.49450     0.00260     -2.49196     -2.49920
Training/qf1_loss                 12040.43984  1499.08318  14462.70801  10099.59082
Training/qf2_loss                 16089.69355  1606.88665  18584.80859  13944.25586
Training/pf_norm                  0.13157      0.02485     0.17297      0.10504
Training/qf1_norm                 5096.11172   193.85111   5279.10107   4802.91748
Training/qf2_norm                 274.18293    7.43179     281.44427    262.88870
log_std/mean                      -0.13553     0.00016     -0.13529     -0.13575
log_probs/mean                    -2.73720     0.00307     -2.73407     -2.74254
mean/mean                         -0.00050     0.00010     -0.00038     -0.00062
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018167734146118164
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70777
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [45300]
collect time 0.0008995532989501953
inside mustsac before update, task 0, sumup 70777
inside mustsac after update, task 0, sumup 70912
inner_dict_sum {'sac_diff0': 0.00019288063049316406, 'sac_diff1': 0.006463289260864258, 'sac_diff2': 0.00781559944152832, 'sac_diff3': 0.009891986846923828, 'sac_diff4': 0.006900787353515625, 'sac_diff5': 0.04665207862854004, 'sac_diff6': 0.00039005279541015625, 'all': 0.07830667495727539}
diff5_list [0.009817361831665039, 0.009172916412353516, 0.009465694427490234, 0.009127140045166016, 0.009068965911865234]
time3 0.0008614063262939453
time4 0.07907629013061523
time5 0.07912349700927734
time7 0.00897216796875
gen_weight_change tensor(-22.2414)
policy weight change tensor(37.9369, grad_fn=<SumBackward0>)
time8 0.0017786026000976562
train_time 0.1077573299407959
eval time 0.11319875717163086
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:07,169 MainThread INFO: EPOCH:295
2024-01-23 01:02:07,169 MainThread INFO: Time Consumed:0.22408127784729004s
2024-01-23 01:02:07,170 MainThread INFO: Total Frames:45150s
  3%|▎         | 296/10000 [03:34<40:09,  4.03it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11868.92313
Train_Epoch_Reward                19200.27020
Running_Training_Average_Rewards  14043.66769
Explore_Time                      0.00090
Train___Time                      0.10776
Eval____Time                      0.11320
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11792.52249
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.40561     3.12555     95.12148     86.33122
alpha_0                           0.86244      0.00012     0.86261      0.86226
Alpha_loss                        -0.99518     0.00086     -0.99383     -0.99642
Training/policy_loss              -2.49262     0.01065     -2.47185     -2.50112
Training/qf1_loss                 12204.37617  1156.35545  13990.89648  10625.78125
Training/qf2_loss                 16011.23652  1142.95401  17921.05078  14420.87695
Training/pf_norm                  0.10886      0.01713     0.13954      0.09333
Training/qf1_norm                 4999.24102   267.91293   5383.17041   4665.48340
Training/qf2_norm                 281.17206    18.59902    306.50696    262.12573
log_std/mean                      -0.13034     0.00258     -0.12586     -0.13325
log_probs/mean                    -2.72902     0.00876     -2.71939     -2.74146
mean/mean                         0.00013      0.00083     0.00104      -0.00132
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018716096878051758
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70912
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [45450]
collect time 0.0008268356323242188
inner_dict_sum {'sac_diff0': 0.0002391338348388672, 'sac_diff1': 0.006646633148193359, 'sac_diff2': 0.007877349853515625, 'sac_diff3': 0.009876489639282227, 'sac_diff4': 0.0066645145416259766, 'sac_diff5': 0.03132367134094238, 'sac_diff6': 0.0003762245178222656, 'all': 0.0630040168762207}
diff5_list [0.00646662712097168, 0.006176948547363281, 0.006087064743041992, 0.0062024593353271484, 0.006390571594238281]
time3 0
time4 0.06374645233154297
time5 0.06379055976867676
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2414)
policy weight change tensor(37.8568, grad_fn=<SumBackward0>)
time8 0.0017995834350585938
train_time 0.07472014427185059
eval time 0.14551019668579102
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:07,415 MainThread INFO: EPOCH:296
2024-01-23 01:02:07,415 MainThread INFO: Time Consumed:0.22324895858764648s
2024-01-23 01:02:07,415 MainThread INFO: Total Frames:45300s
  3%|▎         | 297/10000 [03:35<39:59,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11803.32500
Train_Epoch_Reward                13608.55311
Running_Training_Average_Rewards  13593.62645
Explore_Time                      0.00082
Train___Time                      0.07472
Eval____Time                      0.14551
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11287.85040
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.59747     1.92553     91.40073     85.97278
alpha_0                           0.86201      0.00012     0.86218      0.86183
Alpha_loss                        -0.99945     0.00185     -0.99662     -1.00145
Training/policy_loss              -2.49485     0.00516     -2.48676     -2.50110
Training/qf1_loss                 12790.33242  1198.06670  14400.73633  11094.68652
Training/qf2_loss                 16211.05664  1253.01226  17882.39453  14362.37012
Training/pf_norm                  0.13633      0.01462     0.15734      0.11913
Training/qf1_norm                 4709.38027   128.44151   4814.33545   4468.47998
Training/qf2_norm                 270.32653    5.55665     275.42722    259.85284
log_std/mean                      -0.13425     0.00009     -0.13411     -0.13436
log_probs/mean                    -2.73509     0.00635     -2.72515     -2.74265
mean/mean                         -0.00025     0.00004     -0.00021     -0.00031
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018132925033569336
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70912
epoch first part time 2.384185791015625e-06
replay_buffer._size: [45600]
collect time 0.0008947849273681641
inner_dict_sum {'sac_diff0': 0.0002372264862060547, 'sac_diff1': 0.006602048873901367, 'sac_diff2': 0.0076808929443359375, 'sac_diff3': 0.009710550308227539, 'sac_diff4': 0.006506443023681641, 'sac_diff5': 0.03092813491821289, 'sac_diff6': 0.00037789344787597656, 'all': 0.062043190002441406}
diff5_list [0.00635528564453125, 0.006073713302612305, 0.006041049957275391, 0.006386756896972656, 0.006071329116821289]
time3 0
time4 0.06278300285339355
time5 0.06282567977905273
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2414)
policy weight change tensor(37.7532, grad_fn=<SumBackward0>)
time8 0.0018630027770996094
train_time 0.07350730895996094
eval time 0.15327119827270508
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:07,666 MainThread INFO: EPOCH:297
2024-01-23 01:02:07,667 MainThread INFO: Time Consumed:0.22996139526367188s
2024-01-23 01:02:07,667 MainThread INFO: Total Frames:45450s
  3%|▎         | 298/10000 [03:35<40:13,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11733.36977
Train_Epoch_Reward                8468.86764
Running_Training_Average_Rewards  13686.21779
Explore_Time                      0.00089
Train___Time                      0.07351
Eval____Time                      0.15327
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11219.13889
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.16841     3.49278    94.81688     84.66000
alpha_0                           0.86157      0.00012    0.86175      0.86140
Alpha_loss                        -1.00238     0.00124    -1.00006     -1.00373
Training/policy_loss              -2.48940     0.00684    -2.48011     -2.50095
Training/qf1_loss                 12330.26543  595.63971  13232.41895  11419.06445
Training/qf2_loss                 16211.16641  726.35559  17326.56641  15055.77832
Training/pf_norm                  0.12207      0.03350    0.17716      0.08569
Training/qf1_norm                 4871.27568   239.08583  5195.15527   4492.70947
Training/qf2_norm                 270.13284    10.12205   283.64639    254.08498
log_std/mean                      -0.12713     0.00012    -0.12694     -0.12729
log_probs/mean                    -2.73218     0.00782    -2.72109     -2.74507
mean/mean                         0.00172      0.00014    0.00191      0.00152
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01858806610107422
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70912
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [45750]
collect time 0.0007836818695068359
inner_dict_sum {'sac_diff0': 0.00023651123046875, 'sac_diff1': 0.006756305694580078, 'sac_diff2': 0.007778167724609375, 'sac_diff3': 0.01006627082824707, 'sac_diff4': 0.006716251373291016, 'sac_diff5': 0.031137466430664062, 'sac_diff6': 0.00037860870361328125, 'all': 0.06306958198547363}
diff5_list [0.006258249282836914, 0.006158590316772461, 0.006057024002075195, 0.006573677062988281, 0.006089925765991211]
time3 0
time4 0.06381607055664062
time5 0.0638580322265625
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2414)
policy weight change tensor(37.6626, grad_fn=<SumBackward0>)
time8 0.0018696784973144531
train_time 0.07450652122497559
eval time 0.14481234550476074
epoch last part time 4.0531158447265625e-06
2024-01-23 01:02:07,911 MainThread INFO: EPOCH:298
2024-01-23 01:02:07,911 MainThread INFO: Time Consumed:0.22225284576416016s
2024-01-23 01:02:07,911 MainThread INFO: Total Frames:45600s
  3%|▎         | 299/10000 [03:35<39:59,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11665.69537
Train_Epoch_Reward                8851.07940
Running_Training_Average_Rewards  13293.95661
Explore_Time                      0.00078
Train___Time                      0.07451
Eval____Time                      0.14481
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11184.24724
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.86987     2.31061     91.67432     85.95607
alpha_0                           0.86114      0.00012     0.86132      0.86097
Alpha_loss                        -1.00572     0.00070     -1.00474     -1.00646
Training/policy_loss              -2.49018     0.00477     -2.48411     -2.49757
Training/qf1_loss                 11094.52617  1257.86883  12916.48242  9056.33105
Training/qf2_loss                 15142.51582  1370.53502  17140.86914  12946.64062
Training/pf_norm                  0.11325      0.01597     0.13640      0.09341
Training/qf1_norm                 5049.35547   189.20935   5288.78564   4815.84619
Training/qf2_norm                 275.33185    7.04718     283.90567    266.69461
log_std/mean                      -0.13166     0.00017     -0.13142     -0.13189
log_probs/mean                    -2.73197     0.00531     -2.72538     -2.74031
mean/mean                         -0.00179     0.00017     -0.00157     -0.00205
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018266677856445312
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70912
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [45900]
collect time 0.0009069442749023438
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.006634712219238281, 'sac_diff2': 0.007756948471069336, 'sac_diff3': 0.010456562042236328, 'sac_diff4': 0.006682872772216797, 'sac_diff5': 0.031646013259887695, 'sac_diff6': 0.00038695335388183594, 'all': 0.06378912925720215}
diff5_list [0.00676417350769043, 0.006188392639160156, 0.006245851516723633, 0.00632023811340332, 0.006127357482910156]
time3 0
time4 0.06451177597045898
time5 0.06455421447753906
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2414)
policy weight change tensor(37.6417, grad_fn=<SumBackward0>)
time8 0.001867532730102539
train_time 0.07522773742675781
eval time 0.15035295486450195
epoch last part time 4.0531158447265625e-06
2024-01-23 01:02:08,162 MainThread INFO: EPOCH:299
2024-01-23 01:02:08,162 MainThread INFO: Time Consumed:0.2287616729736328s
2024-01-23 01:02:08,162 MainThread INFO: Total Frames:45750s
  3%|▎         | 300/10000 [03:35<40:09,  4.03it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11607.34998
Train_Epoch_Reward                9367.95049
Running_Training_Average_Rewards  12382.39696
Explore_Time                      0.00090
Train___Time                      0.07523
Eval____Time                      0.15035
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11235.08296
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.12517     1.64426    92.87871     88.19155
alpha_0                           0.86071      0.00012    0.86089      0.86054
Alpha_loss                        -1.00905     0.00189    -1.00691     -1.01130
Training/policy_loss              -2.48799     0.00793    -2.47982     -2.50083
Training/qf1_loss                 12936.90957  793.42491  14151.35352  11993.87598
Training/qf2_loss                 16114.97617  821.10964  17341.17969  15170.05273
Training/pf_norm                  0.11784      0.03209    0.16334      0.07694
Training/qf1_norm                 4422.45830   97.67143   4590.18848   4317.73389
Training/qf2_norm                 270.10563    4.77737    278.27393    264.58420
log_std/mean                      -0.12908     0.00003    -0.12902     -0.12913
log_probs/mean                    -2.73176     0.00932    -2.72194     -2.74665
mean/mean                         -0.00140     0.00010    -0.00127     -0.00155
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018417835235595703
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70912
epoch first part time 2.86102294921875e-06
replay_buffer._size: [46050]
collect time 0.0008900165557861328
inside mustsac before update, task 0, sumup 70912
inside mustsac after update, task 0, sumup 70672
inner_dict_sum {'sac_diff0': 0.0002040863037109375, 'sac_diff1': 0.006932973861694336, 'sac_diff2': 0.008060693740844727, 'sac_diff3': 0.010086536407470703, 'sac_diff4': 0.007023334503173828, 'sac_diff5': 0.0504150390625, 'sac_diff6': 0.0004143714904785156, 'all': 0.08313703536987305}
diff5_list [0.010051727294921875, 0.009414434432983398, 0.009931087493896484, 0.011096954345703125, 0.009920835494995117]
time3 0.0008647441864013672
time4 0.08396077156066895
time5 0.08400869369506836
time7 0.009033203125
gen_weight_change tensor(-22.2475)
policy weight change tensor(37.7138, grad_fn=<SumBackward0>)
time8 0.002505064010620117
train_time 0.11350464820861816
eval time 0.11386847496032715
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:08,414 MainThread INFO: EPOCH:300
2024-01-23 01:02:08,414 MainThread INFO: Time Consumed:0.23042869567871094s
2024-01-23 01:02:08,414 MainThread INFO: Total Frames:45900s
  3%|▎         | 301/10000 [03:36<40:28,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11562.78488
Train_Epoch_Reward                20935.13929
Running_Training_Average_Rewards  12413.08494
Explore_Time                      0.00089
Train___Time                      0.11350
Eval____Time                      0.11387
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11298.14501
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.33125     2.66025     93.36949     85.88253
alpha_0                           0.86028      0.00012     0.86045      0.86011
Alpha_loss                        -1.01235     0.00144     -1.00984     -1.01419
Training/policy_loss              -2.49237     0.01274     -2.47060     -2.50917
Training/qf1_loss                 11593.85527  1795.00250  14911.95020  9820.90332
Training/qf2_loss                 15463.58320  2011.78736  18954.01172  13414.19043
Training/pf_norm                  0.11410      0.02558     0.13266      0.06356
Training/qf1_norm                 4948.99443   341.41240   5290.35107   4372.71436
Training/qf2_norm                 278.17377    16.69681    302.57666    257.09729
log_std/mean                      -0.12780     0.00413     -0.12353     -0.13481
log_probs/mean                    -2.73129     0.00580     -2.72358     -2.73670
mean/mean                         -0.00116     0.00089     0.00006      -0.00208
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01825547218322754
epoch last part time3 0.002646207809448242
inside rlalgo, task 0, sumup 70672
epoch first part time 2.86102294921875e-06
replay_buffer._size: [46200]
collect time 0.0009236335754394531
inner_dict_sum {'sac_diff0': 0.00022554397583007812, 'sac_diff1': 0.00659489631652832, 'sac_diff2': 0.007711172103881836, 'sac_diff3': 0.009789228439331055, 'sac_diff4': 0.006617546081542969, 'sac_diff5': 0.03164100646972656, 'sac_diff6': 0.0003826618194580078, 'all': 0.06296205520629883}
diff5_list [0.0063550472259521484, 0.006188154220581055, 0.006538867950439453, 0.006459236145019531, 0.006099700927734375]
time3 0
time4 0.0636894702911377
time5 0.06373286247253418
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2475)
policy weight change tensor(37.7376, grad_fn=<SumBackward0>)
time8 0.0018579959869384766
train_time 0.07429146766662598
eval time 0.1503894329071045
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:08,666 MainThread INFO: EPOCH:301
2024-01-23 01:02:08,666 MainThread INFO: Time Consumed:0.22795319557189941s
2024-01-23 01:02:08,667 MainThread INFO: Total Frames:46050s
  3%|▎         | 302/10000 [03:36<40:27,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11487.09557
Train_Epoch_Reward                884.30834
Running_Training_Average_Rewards  12044.40447
Explore_Time                      0.00092
Train___Time                      0.07429
Eval____Time                      0.15039
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11191.42232
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.05882     2.80447    93.12067     85.65767
alpha_0                           0.85985      0.00012    0.86002      0.85968
Alpha_loss                        -1.01570     0.00168    -1.01239     -1.01696
Training/policy_loss              -2.50433     0.00617    -2.49369     -2.51267
Training/qf1_loss                 12098.06191  622.79171  12739.66699  11301.77637
Training/qf2_loss                 16107.82734  735.49440  16903.40625  15193.72070
Training/pf_norm                  0.12961      0.02479    0.17391      0.10843
Training/qf1_norm                 5153.74287   207.31870  5387.14600   4826.28320
Training/qf2_norm                 296.77328    8.93861    306.66302    282.85904
log_std/mean                      -0.12660     0.00007    -0.12653     -0.12672
log_probs/mean                    -2.73122     0.00736    -2.71818     -2.74072
mean/mean                         -0.00128     0.00002    -0.00125     -0.00131
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018999814987182617
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70672
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [46350]
collect time 0.0008528232574462891
inner_dict_sum {'sac_diff0': 0.0002372264862060547, 'sac_diff1': 0.0066258907318115234, 'sac_diff2': 0.007681608200073242, 'sac_diff3': 0.010168790817260742, 'sac_diff4': 0.00670170783996582, 'sac_diff5': 0.03208637237548828, 'sac_diff6': 0.00038051605224609375, 'all': 0.06388211250305176}
diff5_list [0.006524801254272461, 0.006251335144042969, 0.006097555160522461, 0.006804227828979492, 0.0064084529876708984]
time3 0
time4 0.06461429595947266
time5 0.06465816497802734
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2475)
policy weight change tensor(37.8518, grad_fn=<SumBackward0>)
time8 0.0018930435180664062
train_time 0.07549786567687988
eval time 0.15475940704345703
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:08,922 MainThread INFO: EPOCH:302
2024-01-23 01:02:08,923 MainThread INFO: Time Consumed:0.233381986618042s
2024-01-23 01:02:08,923 MainThread INFO: Total Frames:46200s
  3%|▎         | 303/10000 [03:36<40:44,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11413.99849
Train_Epoch_Reward                17967.82822
Running_Training_Average_Rewards  12604.46621
Explore_Time                      0.00085
Train___Time                      0.07550
Eval____Time                      0.15476
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11207.87476
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.79275     1.80527     91.37199     85.94005
alpha_0                           0.85942      0.00012     0.85959      0.85925
Alpha_loss                        -1.01868     0.00117     -1.01732     -1.02025
Training/policy_loss              -2.49321     0.00168     -2.48991     -2.49466
Training/qf1_loss                 11877.66309  973.94960   12855.15430  10160.33789
Training/qf2_loss                 15856.85332  1045.65717  16944.43359  13993.20312
Training/pf_norm                  0.13743      0.02028     0.16161      0.11165
Training/qf1_norm                 4940.34268   134.45657   5122.99072   4726.01758
Training/qf2_norm                 284.09478    5.51233     291.92178    275.32199
log_std/mean                      -0.12506     0.00027     -0.12474     -0.12550
log_probs/mean                    -2.72867     0.00221     -2.72443     -2.73031
mean/mean                         -0.00094     0.00008     -0.00083     -0.00106
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01904606819152832
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70672
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [46500]
collect time 0.0009007453918457031
inner_dict_sum {'sac_diff0': 0.0002434253692626953, 'sac_diff1': 0.007895469665527344, 'sac_diff2': 0.00938868522644043, 'sac_diff3': 0.011850833892822266, 'sac_diff4': 0.008131265640258789, 'sac_diff5': 0.03903698921203613, 'sac_diff6': 0.0004947185516357422, 'all': 0.0770413875579834}
diff5_list [0.008133172988891602, 0.007763385772705078, 0.007970333099365234, 0.007658958435058594, 0.007511138916015625]
time3 0
time4 0.07780051231384277
time5 0.07784605026245117
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2475)
policy weight change tensor(37.9711, grad_fn=<SumBackward0>)
time8 0.0020236968994140625
train_time 0.08977675437927246
eval time 0.1359086036682129
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:09,174 MainThread INFO: EPOCH:303
2024-01-23 01:02:09,174 MainThread INFO: Time Consumed:0.22888922691345215s
2024-01-23 01:02:09,174 MainThread INFO: Total Frames:46350s
  3%|▎         | 304/10000 [03:36<40:44,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11351.47423
Train_Epoch_Reward                14707.50564
Running_Training_Average_Rewards  12604.76673
Explore_Time                      0.00090
Train___Time                      0.08978
Eval____Time                      0.13591
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11251.05371
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.08415     2.48708     94.90059     88.14319
alpha_0                           0.85899      0.00012     0.85917      0.85882
Alpha_loss                        -1.02202     0.00222     -1.01932     -1.02598
Training/policy_loss              -2.49031     0.00740     -2.48311     -2.50458
Training/qf1_loss                 13043.60938  934.48159   14379.06543  11569.36914
Training/qf2_loss                 16670.96953  1038.12645  18244.86523  15104.72656
Training/pf_norm                  0.09992      0.02294     0.13542      0.06736
Training/qf1_norm                 4754.79199   175.38969   5095.84229   4624.58447
Training/qf2_norm                 282.21809    7.52773     296.80930    276.40405
log_std/mean                      -0.13601     0.00020     -0.13571     -0.13627
log_probs/mean                    -2.72850     0.00894     -2.71959     -2.74566
mean/mean                         -0.00198     0.00004     -0.00192     -0.00204
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01946544647216797
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70672
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [46650]
collect time 0.0009005069732666016
inner_dict_sum {'sac_diff0': 0.0002503395080566406, 'sac_diff1': 0.0077702999114990234, 'sac_diff2': 0.009444475173950195, 'sac_diff3': 0.012662410736083984, 'sac_diff4': 0.008544683456420898, 'sac_diff5': 0.039611101150512695, 'sac_diff6': 0.0004885196685791016, 'all': 0.07877182960510254}
diff5_list [0.008002758026123047, 0.0078084468841552734, 0.007903337478637695, 0.008195161819458008, 0.007701396942138672]
time3 0
time4 0.07956314086914062
time5 0.07960820198059082
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2475)
policy weight change tensor(37.9800, grad_fn=<SumBackward0>)
time8 0.001982450485229492
train_time 0.09122681617736816
eval time 0.13203883171081543
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:09,423 MainThread INFO: EPOCH:304
2024-01-23 01:02:09,424 MainThread INFO: Time Consumed:0.22644472122192383s
2024-01-23 01:02:09,424 MainThread INFO: Total Frames:46500s
  3%|▎         | 305/10000 [03:37<40:34,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11287.97392
Train_Epoch_Reward                10508.87066
Running_Training_Average_Rewards  12677.37861
Explore_Time                      0.00090
Train___Time                      0.09123
Eval____Time                      0.13204
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11212.40146
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.95726     2.02956     91.36595     85.26732
alpha_0                           0.85856      0.00012     0.85874      0.85839
Alpha_loss                        -1.02636     0.00127     -1.02488     -1.02853
Training/policy_loss              -2.49318     0.00441     -2.48814     -2.50136
Training/qf1_loss                 11391.50508  1103.44894  13057.16699  10346.69824
Training/qf2_loss                 14862.29922  1164.62831  16658.67773  13709.56445
Training/pf_norm                  0.12241      0.01560     0.15352      0.11230
Training/qf1_norm                 4598.03691   128.96690   4814.71240   4423.92188
Training/qf2_norm                 278.81901    6.14292     289.15280    270.64935
log_std/mean                      -0.13666     0.00008     -0.13651     -0.13672
log_probs/mean                    -2.73491     0.00516     -2.72955     -2.74473
mean/mean                         -0.00249     0.00005     -0.00241     -0.00254
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018822908401489258
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70672
epoch first part time 2.86102294921875e-06
replay_buffer._size: [46800]
collect time 0.0007905960083007812
inside mustsac before update, task 0, sumup 70672
inside mustsac after update, task 0, sumup 70887
inner_dict_sum {'sac_diff0': 0.00026106834411621094, 'sac_diff1': 0.0080108642578125, 'sac_diff2': 0.009992599487304688, 'sac_diff3': 0.01257014274597168, 'sac_diff4': 0.00889897346496582, 'sac_diff5': 0.06042981147766113, 'sac_diff6': 0.0005011558532714844, 'all': 0.10066461563110352}
diff5_list [0.01244807243347168, 0.012102603912353516, 0.012024402618408203, 0.011610746383666992, 0.012243986129760742]
time3 0.0012111663818359375
time4 0.1015310287475586
time5 0.10158300399780273
time7 0.009228944778442383
gen_weight_change tensor(-22.3184)
policy weight change tensor(38.0717, grad_fn=<SumBackward0>)
time8 0.0019571781158447266
train_time 0.13291001319885254
eval time 0.10143184661865234
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:09,683 MainThread INFO: EPOCH:305
2024-01-23 01:02:09,683 MainThread INFO: Time Consumed:0.23742055892944336s
2024-01-23 01:02:09,684 MainThread INFO: Total Frames:46650s
  3%|▎         | 306/10000 [03:37<40:59,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11228.00123
Train_Epoch_Reward                39297.51943
Running_Training_Average_Rewards  13653.88388
Explore_Time                      0.00079
Train___Time                      0.13291
Eval____Time                      0.10143
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11192.79555
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.51466     1.03331    90.44178     87.61544
alpha_0                           0.85813      0.00012    0.85831      0.85796
Alpha_loss                        -1.02923     0.00048    -1.02861     -1.03001
Training/policy_loss              -2.49627     0.00582    -2.48748     -2.50479
Training/qf1_loss                 11474.42656  287.48542  11799.18750  11000.57617
Training/qf2_loss                 15614.18633  373.81423  16051.57227  15186.21582
Training/pf_norm                  0.11059      0.01494    0.12725      0.08548
Training/qf1_norm                 5144.51455   147.74363  5311.90625   4868.70312
Training/qf2_norm                 286.57658    5.84738    292.69275    277.98480
log_std/mean                      -0.13392     0.00608    -0.12646     -0.14315
log_probs/mean                    -2.73168     0.00693    -2.71882     -2.73975
mean/mean                         -0.00119     0.00096    0.00035      -0.00230
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01886582374572754
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70887
epoch first part time 2.86102294921875e-06
replay_buffer._size: [46950]
collect time 0.0008795261383056641
inner_dict_sum {'sac_diff0': 0.00024890899658203125, 'sac_diff1': 0.007964372634887695, 'sac_diff2': 0.009549617767333984, 'sac_diff3': 0.012479066848754883, 'sac_diff4': 0.008844614028930664, 'sac_diff5': 0.040166378021240234, 'sac_diff6': 0.0004813671112060547, 'all': 0.07973432540893555}
diff5_list [0.008296966552734375, 0.007871627807617188, 0.008733034133911133, 0.0076406002044677734, 0.007624149322509766]
time3 0
time4 0.08051633834838867
time5 0.0805656909942627
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3184)
policy weight change tensor(38.0226, grad_fn=<SumBackward0>)
time8 0.001987457275390625
train_time 0.09204697608947754
eval time 0.13583922386169434
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:09,937 MainThread INFO: EPOCH:306
2024-01-23 01:02:09,937 MainThread INFO: Time Consumed:0.23109197616577148s
2024-01-23 01:02:09,937 MainThread INFO: Total Frames:46800s
  3%|▎         | 307/10000 [03:37<41:00,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11214.97396
Train_Epoch_Reward                25559.95292
Running_Training_Average_Rewards  14298.83853
Explore_Time                      0.00087
Train___Time                      0.09205
Eval____Time                      0.13584
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11157.57767
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.59340     3.55987     93.17320     83.90885
alpha_0                           0.85771      0.00012     0.85788      0.85753
Alpha_loss                        -1.03292     0.00175     -1.03029     -1.03556
Training/policy_loss              -2.51624     0.00432     -2.50947     -2.52302
Training/qf1_loss                 11185.99492  1482.02901  13820.62207  9421.38770
Training/qf2_loss                 15544.45078  1677.28368  18458.98828  13485.30664
Training/pf_norm                  0.10200      0.01908     0.12104      0.06628
Training/qf1_norm                 5303.51455   306.97114   5706.83350   4893.04053
Training/qf2_norm                 313.09907    12.23558    328.78076    296.85522
log_std/mean                      -0.13967     0.00011     -0.13946     -0.13977
log_probs/mean                    -2.73379     0.00537     -2.72536     -2.74217
mean/mean                         0.00020      0.00013     0.00038      0.00003
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019553184509277344
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70887
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [47100]
collect time 0.0008492469787597656
inner_dict_sum {'sac_diff0': 0.0002453327178955078, 'sac_diff1': 0.00778651237487793, 'sac_diff2': 0.009274005889892578, 'sac_diff3': 0.01205754280090332, 'sac_diff4': 0.008382320404052734, 'sac_diff5': 0.03874087333679199, 'sac_diff6': 0.0004839897155761719, 'all': 0.07697057723999023}
diff5_list [0.007807493209838867, 0.007735490798950195, 0.008133172988891602, 0.007563591003417969, 0.007501125335693359]
time3 0
time4 0.07773208618164062
time5 0.07777953147888184
time7 9.5367431640625e-07
gen_weight_change tensor(-22.3184)
policy weight change tensor(37.9130, grad_fn=<SumBackward0>)
time8 0.002004861831665039
train_time 0.08925366401672363
eval time 0.16753721237182617
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:10,220 MainThread INFO: EPOCH:307
2024-01-23 01:02:10,220 MainThread INFO: Time Consumed:0.25998592376708984s
2024-01-23 01:02:10,220 MainThread INFO: Total Frames:46950s
  3%|▎         | 308/10000 [03:37<42:23,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11216.00882
Train_Epoch_Reward                4943.08897
Running_Training_Average_Rewards  14058.24543
Explore_Time                      0.00085
Train___Time                      0.08925
Eval____Time                      0.16754
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11229.48748
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.15981     2.88709     94.24045     87.15287
alpha_0                           0.85728      0.00012     0.85745      0.85711
Alpha_loss                        -1.03607     0.00066     -1.03510     -1.03688
Training/policy_loss              -2.48001     0.00636     -2.47283     -2.49093
Training/qf1_loss                 11638.51758  1746.10870  14264.10449  9933.60742
Training/qf2_loss                 15872.25762  1891.02823  18635.39062  14004.04980
Training/pf_norm                  0.12247      0.01432     0.14606      0.10710
Training/qf1_norm                 5315.07324   233.50032   5653.40039   5070.92236
Training/qf2_norm                 274.37813    8.50278     286.33118    265.65329
log_std/mean                      -0.13043     0.00020     -0.13016     -0.13074
log_probs/mean                    -2.73238     0.00710     -2.72459     -2.74459
mean/mean                         0.00000      0.00011     0.00013      -0.00015
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018810510635375977
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70887
epoch first part time 2.86102294921875e-06
replay_buffer._size: [47250]
collect time 0.0008637905120849609
inner_dict_sum {'sac_diff0': 0.00025200843811035156, 'sac_diff1': 0.008374929428100586, 'sac_diff2': 0.010063886642456055, 'sac_diff3': 0.012711048126220703, 'sac_diff4': 0.008888959884643555, 'sac_diff5': 0.04055309295654297, 'sac_diff6': 0.0004944801330566406, 'all': 0.08133840560913086}
diff5_list [0.00844120979309082, 0.0076563358306884766, 0.0077135562896728516, 0.00811457633972168, 0.00862741470336914]
time3 0
time4 0.08214330673217773
time5 0.08219528198242188
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3184)
policy weight change tensor(37.8224, grad_fn=<SumBackward0>)
time8 0.006399393081665039
train_time 0.09842681884765625
eval time 0.15601325035095215
epoch last part time 8.821487426757812e-06
2024-01-23 01:02:10,500 MainThread INFO: EPOCH:308
2024-01-23 01:02:10,501 MainThread INFO: Time Consumed:0.2579476833343506s
2024-01-23 01:02:10,501 MainThread INFO: Total Frames:47100s
  3%|▎         | 309/10000 [03:38<43:18,  3.73it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11225.03406
Train_Epoch_Reward                37703.10954
Running_Training_Average_Rewards  14887.32496
Explore_Time                      0.00086
Train___Time                      0.09843
Eval____Time                      0.15601
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11274.49964
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.46369     1.94213     93.79026     88.28060
alpha_0                           0.85685      0.00012     0.85702      0.85668
Alpha_loss                        -1.04049     0.00125     -1.03824     -1.04200
Training/policy_loss              -2.50333     0.00355     -2.49886     -2.50966
Training/qf1_loss                 11413.77578  959.14795   13276.31543  10658.82031
Training/qf2_loss                 15690.92656  1047.63205  17712.84961  14862.46094
Training/pf_norm                  0.10991      0.01889     0.12650      0.07389
Training/qf1_norm                 5226.81260   151.16983   5481.61133   5043.03027
Training/qf2_norm                 293.03451    5.99829     303.24991    286.03607
log_std/mean                      -0.13200     0.00015     -0.13177     -0.13216
log_probs/mean                    -2.73917     0.00423     -2.73334     -2.74635
mean/mean                         0.00132      0.00004     0.00135      0.00125
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02022576332092285
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70887
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [47400]
collect time 0.0009512901306152344
inner_dict_sum {'sac_diff0': 0.0002503395080566406, 'sac_diff1': 0.009394645690917969, 'sac_diff2': 0.011368036270141602, 'sac_diff3': 0.013322114944458008, 'sac_diff4': 0.00963139533996582, 'sac_diff5': 0.03986382484436035, 'sac_diff6': 0.0004818439483642578, 'all': 0.08431220054626465}
diff5_list [0.009377479553222656, 0.00788259506225586, 0.0073244571685791016, 0.007627248764038086, 0.0076520442962646484]
time3 0
time4 0.08510255813598633
time5 0.08515429496765137
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3184)
policy weight change tensor(37.7973, grad_fn=<SumBackward0>)
time8 0.002042531967163086
train_time 0.09770560264587402
eval time 0.13554620742797852
epoch last part time 7.152557373046875e-06
2024-01-23 01:02:10,761 MainThread INFO: EPOCH:309
2024-01-23 01:02:10,761 MainThread INFO: Time Consumed:0.2365860939025879s
2024-01-23 01:02:10,761 MainThread INFO: Total Frames:47250s
  3%|▎         | 310/10000 [03:38<42:53,  3.76it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11227.69501
Train_Epoch_Reward                44615.71490
Running_Training_Average_Rewards  15956.06404
Explore_Time                      0.00094
Train___Time                      0.09771
Eval____Time                      0.13555
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11261.69251
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.65598     2.18754    95.35281     88.80226
alpha_0                           0.85642      0.00012    0.85659      0.85625
Alpha_loss                        -1.04285     0.00119    -1.04192     -1.04515
Training/policy_loss              -2.49099     0.00415    -2.48589     -2.49587
Training/qf1_loss                 12034.13457  598.38267  12819.48242  11185.55762
Training/qf2_loss                 16533.59863  655.56848  17277.26172  15680.66113
Training/pf_norm                  0.14349      0.02782    0.17954      0.10550
Training/qf1_norm                 5442.23135   177.51619  5733.02979   5195.47363
Training/qf2_norm                 283.32986    6.48544    294.21384    274.79910
log_std/mean                      -0.12575     0.00006    -0.12568     -0.12585
log_probs/mean                    -2.73264     0.00479    -2.72697     -2.73879
mean/mean                         -0.00048     0.00003    -0.00043     -0.00053
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019008159637451172
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70887
epoch first part time 3.337860107421875e-06
replay_buffer._size: [47550]
collect time 0.00092315673828125
inside mustsac before update, task 0, sumup 70887
inside mustsac after update, task 0, sumup 71263
inner_dict_sum {'sac_diff0': 0.00028514862060546875, 'sac_diff1': 0.008832931518554688, 'sac_diff2': 0.010712862014770508, 'sac_diff3': 0.013338565826416016, 'sac_diff4': 0.00928950309753418, 'sac_diff5': 0.06387162208557129, 'sac_diff6': 0.0005564689636230469, 'all': 0.1068871021270752}
diff5_list [0.013733148574829102, 0.013131141662597656, 0.012310028076171875, 0.011962413787841797, 0.01273488998413086]
time3 0.0012514591217041016
time4 0.10792112350463867
time5 0.1079859733581543
time7 0.009507417678833008
gen_weight_change tensor(-22.2630)
policy weight change tensor(37.8513, grad_fn=<SumBackward0>)
time8 0.002860546112060547
train_time 0.1419076919555664
eval time 0.08219647407531738
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:11,011 MainThread INFO: EPOCH:310
2024-01-23 01:02:11,011 MainThread INFO: Time Consumed:0.22728514671325684s
2024-01-23 01:02:11,012 MainThread INFO: Total Frames:47400s
  3%|▎         | 311/10000 [03:38<42:17,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11229.96775
Train_Epoch_Reward                14381.29371
Running_Training_Average_Rewards  15482.71135
Explore_Time                      0.00092
Train___Time                      0.14191
Eval____Time                      0.08220
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11320.87238
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.86370     2.19412     93.27047     87.15874
alpha_0                           0.85599      0.00012     0.85616      0.85582
Alpha_loss                        -1.04571     0.00086     -1.04423     -1.04692
Training/policy_loss              -2.49221     0.01362     -2.47452     -2.51055
Training/qf1_loss                 10805.97871  1113.30846  12633.25488  9510.52246
Training/qf2_loss                 15217.12168  1195.59354  17019.21484  13713.53613
Training/pf_norm                  0.10928      0.01326     0.12574      0.09314
Training/qf1_norm                 5240.91592   170.43797   5462.03076   5020.55078
Training/qf2_norm                 287.74236    17.06090    314.28159    266.02454
log_std/mean                      -0.13026     0.00560     -0.12126     -0.13852
log_probs/mean                    -2.72939     0.00311     -2.72566     -2.73511
mean/mean                         -0.00006     0.00048     0.00065      -0.00062
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01880192756652832
epoch last part time3 0.0032949447631835938
inside rlalgo, task 0, sumup 71263
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [47700]
collect time 0.0008966922760009766
inner_dict_sum {'sac_diff0': 0.0002048015594482422, 'sac_diff1': 0.0070497989654541016, 'sac_diff2': 0.008509397506713867, 'sac_diff3': 0.010169506072998047, 'sac_diff4': 0.007097721099853516, 'sac_diff5': 0.031238555908203125, 'sac_diff6': 0.000377655029296875, 'all': 0.06464743614196777}
diff5_list [0.006964206695556641, 0.006239414215087891, 0.006042957305908203, 0.006001710891723633, 0.005990266799926758]
time3 0
time4 0.0654137134552002
time5 0.06546473503112793
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2630)
policy weight change tensor(37.8999, grad_fn=<SumBackward0>)
time8 0.002010345458984375
train_time 0.07726740837097168
eval time 0.15911293029785156
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:11,276 MainThread INFO: EPOCH:311
2024-01-23 01:02:11,276 MainThread INFO: Time Consumed:0.23956727981567383s
2024-01-23 01:02:11,276 MainThread INFO: Total Frames:47550s
  3%|▎         | 312/10000 [03:38<42:23,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11263.28241
Train_Epoch_Reward                4441.26465
Running_Training_Average_Rewards  15485.15313
Explore_Time                      0.00089
Train___Time                      0.07727
Eval____Time                      0.15911
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11524.56898
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.53588     4.03316     98.37257     87.69033
alpha_0                           0.85556      0.00012     0.85573      0.85539
Alpha_loss                        -1.04873     0.00146     -1.04605     -1.04999
Training/policy_loss              -2.49052     0.00652     -2.47966     -2.49663
Training/qf1_loss                 13108.77285  2470.20660  16697.68359  10279.35059
Training/qf2_loss                 17582.02324  2679.59283  21494.62109  14503.92676
Training/pf_norm                  0.08398      0.01175     0.10246      0.07078
Training/qf1_norm                 5496.23945   329.99893   5974.62158   5111.63135
Training/qf2_norm                 295.48306    12.47668    313.38922    280.67587
log_std/mean                      -0.13264     0.00010     -0.13251     -0.13279
log_probs/mean                    -2.72718     0.00755     -2.71425     -2.73426
mean/mean                         -0.00042     0.00001     -0.00040     -0.00043
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021277427673339844
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71263
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [47850]
collect time 0.0008957386016845703
inner_dict_sum {'sac_diff0': 0.00019669532775878906, 'sac_diff1': 0.006667613983154297, 'sac_diff2': 0.008215904235839844, 'sac_diff3': 0.010081291198730469, 'sac_diff4': 0.006982326507568359, 'sac_diff5': 0.030845165252685547, 'sac_diff6': 0.00039649009704589844, 'all': 0.0633854866027832}
diff5_list [0.006337642669677734, 0.006051778793334961, 0.0063021183013916016, 0.006041526794433594, 0.006112098693847656]
time3 0
time4 0.06415247917175293
time5 0.06419992446899414
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2630)
policy weight change tensor(38.0056, grad_fn=<SumBackward0>)
time8 0.0019180774688720703
train_time 0.07572102546691895
eval time 0.15935778617858887
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:11,539 MainThread INFO: EPOCH:312
2024-01-23 01:02:11,539 MainThread INFO: Time Consumed:0.23830866813659668s
2024-01-23 01:02:11,539 MainThread INFO: Total Frames:47700s
  3%|▎         | 313/10000 [03:39<42:26,  3.80it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11300.63149
Train_Epoch_Reward                13068.78919
Running_Training_Average_Rewards  15683.43978
Explore_Time                      0.00089
Train___Time                      0.07572
Eval____Time                      0.15936
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11581.36553
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.52765     2.69987    93.70280     86.75962
alpha_0                           0.85514      0.00012    0.85531      0.85496
Alpha_loss                        -1.05354     0.00151    -1.05104     -1.05540
Training/policy_loss              -2.48505     0.00331    -2.47924     -2.48788
Training/qf1_loss                 11320.70410  632.69842  12133.82129  10502.96582
Training/qf2_loss                 16045.38242  789.13722  17048.69922  15000.44531
Training/pf_norm                  0.11201      0.01998    0.14869      0.08910
Training/qf1_norm                 5441.49590   232.63682  5716.80469   5120.86182
Training/qf2_norm                 278.74330    8.07023    288.21802    267.54404
log_std/mean                      -0.13090     0.00025    -0.13059     -0.13128
log_probs/mean                    -2.73642     0.00417    -2.72904     -2.74006
mean/mean                         -0.00149     0.00004    -0.00145     -0.00156
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02200794219970703
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71263
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [48000]
collect time 0.0008990764617919922
inner_dict_sum {'sac_diff0': 0.00020623207092285156, 'sac_diff1': 0.0069425106048583984, 'sac_diff2': 0.008208036422729492, 'sac_diff3': 0.010200023651123047, 'sac_diff4': 0.006940126419067383, 'sac_diff5': 0.03145551681518555, 'sac_diff6': 0.0003826618194580078, 'all': 0.06433510780334473}
diff5_list [0.006767749786376953, 0.006129741668701172, 0.006166219711303711, 0.006307840347290039, 0.006083965301513672]
time3 0
time4 0.06510257720947266
time5 0.06514930725097656
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2630)
policy weight change tensor(38.1004, grad_fn=<SumBackward0>)
time8 0.002022266387939453
train_time 0.07699990272521973
eval time 0.1541585922241211
epoch last part time 5.7220458984375e-06
2024-01-23 01:02:11,799 MainThread INFO: EPOCH:313
2024-01-23 01:02:11,799 MainThread INFO: Time Consumed:0.2344050407409668s
2024-01-23 01:02:11,799 MainThread INFO: Total Frames:47850s
  3%|▎         | 314/10000 [03:39<42:17,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11339.99888
Train_Epoch_Reward                11176.02196
Running_Training_Average_Rewards  15541.30606
Explore_Time                      0.00089
Train___Time                      0.07700
Eval____Time                      0.15416
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11644.72760
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.46624     2.67083     91.45828     84.29814
alpha_0                           0.85471      0.00012     0.85488      0.85454
Alpha_loss                        -1.05538     0.00056     -1.05449     -1.05619
Training/policy_loss              -2.48584     0.00286     -2.48236     -2.48867
Training/qf1_loss                 11786.55566  1196.67761  12961.25488  9730.83984
Training/qf2_loss                 16121.27754  1328.92134  17341.52734  13784.99414
Training/pf_norm                  0.12089      0.01886     0.14564      0.09081
Training/qf1_norm                 5180.46260   216.10125   5345.42871   4764.65674
Training/qf2_norm                 290.49932    8.20428     296.68314    274.65164
log_std/mean                      -0.11958     0.00012     -0.11943     -0.11977
log_probs/mean                    -2.72668     0.00301     -2.72288     -2.72963
mean/mean                         -0.00152     0.00008     -0.00140     -0.00163
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021992206573486328
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71263
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [48150]
collect time 0.0009026527404785156
inner_dict_sum {'sac_diff0': 0.0002014636993408203, 'sac_diff1': 0.0070912837982177734, 'sac_diff2': 0.008512496948242188, 'sac_diff3': 0.0103607177734375, 'sac_diff4': 0.006914377212524414, 'sac_diff5': 0.03216981887817383, 'sac_diff6': 0.0003879070281982422, 'all': 0.06563806533813477}
diff5_list [0.006826162338256836, 0.0063457489013671875, 0.006356239318847656, 0.00629734992980957, 0.006344318389892578]
time3 0
time4 0.06641364097595215
time5 0.06646299362182617
time7 9.5367431640625e-07
gen_weight_change tensor(-22.2630)
policy weight change tensor(38.1285, grad_fn=<SumBackward0>)
time8 0.0018994808197021484
train_time 0.0781402587890625
eval time 0.14952421188354492
epoch last part time 5.7220458984375e-06
2024-01-23 01:02:12,055 MainThread INFO: EPOCH:314
2024-01-23 01:02:12,055 MainThread INFO: Time Consumed:0.23083257675170898s
2024-01-23 01:02:12,055 MainThread INFO: Total Frames:48000s
  3%|▎         | 315/10000 [03:39<42:00,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11386.29633
Train_Epoch_Reward                19436.28132
Running_Training_Average_Rewards  15981.20218
Explore_Time                      0.00090
Train___Time                      0.07814
Eval____Time                      0.14952
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11675.37598
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.88098     1.53013    92.60122     88.01873
alpha_0                           0.85428      0.00012    0.85445      0.85411
Alpha_loss                        -1.05929     0.00172    -1.05644     -1.06155
Training/policy_loss              -2.48265     0.00411    -2.47499     -2.48710
Training/qf1_loss                 11116.94824  810.36041  12040.98047  9660.69824
Training/qf2_loss                 15927.02168  905.88704  16932.66992  14277.35645
Training/pf_norm                  0.11024      0.01815    0.14346      0.08783
Training/qf1_norm                 5461.54209   136.23406  5601.51172   5202.83301
Training/qf2_norm                 283.57672    4.56964    288.68863    275.01852
log_std/mean                      -0.14095     0.00005    -0.14088     -0.14101
log_probs/mean                    -2.73016     0.00518    -2.72060     -2.73597
mean/mean                         -0.00079     0.00003    -0.00075     -0.00083
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02193427085876465
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71263
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [48300]
collect time 0.0009284019470214844
inside mustsac before update, task 0, sumup 71263
inside mustsac after update, task 0, sumup 70935
inner_dict_sum {'sac_diff0': 0.00020813941955566406, 'sac_diff1': 0.007122516632080078, 'sac_diff2': 0.009099483489990234, 'sac_diff3': 0.01132655143737793, 'sac_diff4': 0.00794363021850586, 'sac_diff5': 0.05189371109008789, 'sac_diff6': 0.000415802001953125, 'all': 0.08800983428955078}
diff5_list [0.01092982292175293, 0.010704994201660156, 0.010135412216186523, 0.010255813598632812, 0.009867668151855469]
time3 0.0008907318115234375
time4 0.08896541595458984
time5 0.08902525901794434
time7 0.009664058685302734
gen_weight_change tensor(-22.2421)
policy weight change tensor(38.1809, grad_fn=<SumBackward0>)
time8 0.0018911361694335938
train_time 0.11975264549255371
eval time 0.10789823532104492
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:12,311 MainThread INFO: EPOCH:315
2024-01-23 01:02:12,311 MainThread INFO: Time Consumed:0.2308049201965332s
2024-01-23 01:02:12,312 MainThread INFO: Total Frames:48150s
  3%|▎         | 316/10000 [03:39<41:49,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11437.43580
Train_Epoch_Reward                3518.13390
Running_Training_Average_Rewards  15976.98353
Explore_Time                      0.00092
Train___Time                      0.11975
Eval____Time                      0.10790
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11704.19020
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.38858     2.94323     95.39477     86.39729
alpha_0                           0.85385      0.00012     0.85403      0.85368
Alpha_loss                        -1.06342     0.00162     -1.06159     -1.06616
Training/policy_loss              -2.49257     0.01013     -2.48365     -2.51218
Training/qf1_loss                 11658.07344  804.68694   12574.67090  10181.24219
Training/qf2_loss                 16464.48242  1069.81030  17740.74219  14490.15039
Training/pf_norm                  0.11170      0.02659     0.14627      0.07694
Training/qf1_norm                 5507.32070   316.73045   5966.26904   4988.15381
Training/qf2_norm                 294.97280    12.00460    312.87173    278.68143
log_std/mean                      -0.13155     0.00816     -0.12281     -0.14351
log_probs/mean                    -2.73498     0.00547     -2.72721     -2.74378
mean/mean                         -0.00050     0.00040     0.00017      -0.00104
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.022402286529541016
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70935
epoch first part time 3.337860107421875e-06
replay_buffer._size: [48450]
collect time 0.00095367431640625
inner_dict_sum {'sac_diff0': 0.00019812583923339844, 'sac_diff1': 0.006886482238769531, 'sac_diff2': 0.008597612380981445, 'sac_diff3': 0.010440587997436523, 'sac_diff4': 0.007029056549072266, 'sac_diff5': 0.031098127365112305, 'sac_diff6': 0.0003790855407714844, 'all': 0.06462907791137695}
diff5_list [0.006733894348144531, 0.006082773208618164, 0.0061571598052978516, 0.006087541580200195, 0.0060367584228515625]
time3 0
time4 0.06541752815246582
time5 0.06546711921691895
time7 9.5367431640625e-07
gen_weight_change tensor(-22.2421)
policy weight change tensor(38.0999, grad_fn=<SumBackward0>)
time8 0.0022177696228027344
train_time 0.0774543285369873
eval time 0.14400649070739746
epoch last part time 5.9604644775390625e-06
2024-01-23 01:02:12,561 MainThread INFO: EPOCH:316
2024-01-23 01:02:12,562 MainThread INFO: Time Consumed:0.2247636318206787s
2024-01-23 01:02:12,562 MainThread INFO: Total Frames:48300s
  3%|▎         | 317/10000 [03:40<41:22,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11538.17452
Train_Epoch_Reward                9557.82042
Running_Training_Average_Rewards  15855.04094
Explore_Time                      0.00095
Train___Time                      0.07745
Eval____Time                      0.14401
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12164.96485
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.52622     2.33543     93.63055     87.80923
alpha_0                           0.85343      0.00012     0.85360      0.85326
Alpha_loss                        -1.06685     0.00155     -1.06536     -1.06943
Training/policy_loss              -2.48894     0.00631     -2.48094     -2.49897
Training/qf1_loss                 12588.90801  1795.27232  15836.27148  10920.79102
Training/qf2_loss                 16906.44414  1870.01821  20206.61914  15072.51758
Training/pf_norm                  0.09771      0.01337     0.11070      0.07484
Training/qf1_norm                 5357.95293   190.19957   5534.00928   5049.54443
Training/qf2_norm                 284.04007    7.00721     290.45694    272.89050
log_std/mean                      -0.14235     0.00015     -0.14214     -0.14255
log_probs/mean                    -2.73541     0.00741     -2.72600     -2.74742
mean/mean                         -0.00085     0.00004     -0.00078     -0.00089
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021767616271972656
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70935
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [48600]
collect time 0.0009374618530273438
inner_dict_sum {'sac_diff0': 0.00019502639770507812, 'sac_diff1': 0.00650787353515625, 'sac_diff2': 0.008095264434814453, 'sac_diff3': 0.009840726852416992, 'sac_diff4': 0.006916046142578125, 'sac_diff5': 0.030864715576171875, 'sac_diff6': 0.0003752708435058594, 'all': 0.06279492378234863}
diff5_list [0.006419181823730469, 0.006334066390991211, 0.006121397018432617, 0.0059854984283447266, 0.0060045719146728516]
time3 0
time4 0.06355524063110352
time5 0.06360483169555664
time7 9.5367431640625e-07
gen_weight_change tensor(-22.2421)
policy weight change tensor(38.0336, grad_fn=<SumBackward0>)
time8 0.0020546913146972656
train_time 0.07538819313049316
eval time 0.149367094039917
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:12,815 MainThread INFO: EPOCH:317
2024-01-23 01:02:12,815 MainThread INFO: Time Consumed:0.22803807258605957s
2024-01-23 01:02:12,815 MainThread INFO: Total Frames:48450s
  3%|▎         | 318/10000 [03:40<41:19,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11632.48305
Train_Epoch_Reward                1830.77208
Running_Training_Average_Rewards  15275.52426
Explore_Time                      0.00093
Train___Time                      0.07539
Eval____Time                      0.14937
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12172.57278
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.71988     2.09178    93.87358     87.72518
alpha_0                           0.85300      0.00012    0.85317      0.85283
Alpha_loss                        -1.07055     0.00085    -1.06907     -1.07138
Training/policy_loss              -2.48022     0.00392    -2.47572     -2.48590
Training/qf1_loss                 12175.90762  715.04170  13124.27148  11003.31348
Training/qf2_loss                 16750.20508  800.35759  17718.40234  15337.15039
Training/pf_norm                  0.12236      0.00887    0.13468      0.10713
Training/qf1_norm                 5637.85186   181.51131  5826.09473   5291.01221
Training/qf2_norm                 274.59556    6.16043    281.04089    262.86914
log_std/mean                      -0.13216     0.00008    -0.13208     -0.13230
log_probs/mean                    -2.73747     0.00434    -2.73236     -2.74340
mean/mean                         -0.00009     0.00008    0.00004      -0.00019
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.023907184600830078
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70935
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [48750]
collect time 0.0009381771087646484
inner_dict_sum {'sac_diff0': 0.0002040863037109375, 'sac_diff1': 0.006661653518676758, 'sac_diff2': 0.008183956146240234, 'sac_diff3': 0.010103464126586914, 'sac_diff4': 0.006891727447509766, 'sac_diff5': 0.03091144561767578, 'sac_diff6': 0.00039458274841308594, 'all': 0.06335091590881348}
diff5_list [0.00669550895690918, 0.0060787200927734375, 0.0059888362884521484, 0.0061452388763427734, 0.006003141403198242]
time3 0
time4 0.06410694122314453
time5 0.06415510177612305
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2421)
policy weight change tensor(37.9845, grad_fn=<SumBackward0>)
time8 0.0018939971923828125
train_time 0.07573080062866211
eval time 0.14575862884521484
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:13,067 MainThread INFO: EPOCH:318
2024-01-23 01:02:13,067 MainThread INFO: Time Consumed:0.22470378875732422s
2024-01-23 01:02:13,067 MainThread INFO: Total Frames:48600s
  3%|▎         | 319/10000 [03:40<41:04,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11729.84990
Train_Epoch_Reward                21803.92074
Running_Training_Average_Rewards  15548.07436
Explore_Time                      0.00093
Train___Time                      0.07573
Eval____Time                      0.14576
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12248.16817
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.89316     0.93135    92.32335     89.56514
alpha_0                           0.85257      0.00012    0.85274      0.85240
Alpha_loss                        -1.07382     0.00084    -1.07243     -1.07478
Training/policy_loss              -2.49820     0.00273    -2.49474     -2.50202
Training/qf1_loss                 11790.22910  487.91623  12741.73535  11363.58398
Training/qf2_loss                 16299.35195  543.31269  17361.51758  15854.94043
Training/pf_norm                  0.08489      0.03244    0.12999      0.03863
Training/qf1_norm                 5283.27090   79.01634   5415.33398   5177.45166
Training/qf2_norm                 297.78941    2.85856    302.30157    293.71344
log_std/mean                      -0.13895     0.00009    -0.13882     -0.13907
log_probs/mean                    -2.73683     0.00298    -2.73306     -2.74133
mean/mean                         -0.00185     0.00002    -0.00182     -0.00188
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.022977828979492188
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70935
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [48900]
collect time 0.0009169578552246094
inner_dict_sum {'sac_diff0': 0.00019669532775878906, 'sac_diff1': 0.006839275360107422, 'sac_diff2': 0.008542776107788086, 'sac_diff3': 0.010524988174438477, 'sac_diff4': 0.007008075714111328, 'sac_diff5': 0.0317533016204834, 'sac_diff6': 0.0003840923309326172, 'all': 0.06524920463562012}
diff5_list [0.006757259368896484, 0.006201028823852539, 0.006392240524291992, 0.0063550472259521484, 0.006047725677490234]
time3 0
time4 0.06601786613464355
time5 0.06607913970947266
time7 9.5367431640625e-07
gen_weight_change tensor(-22.2421)
policy weight change tensor(37.8998, grad_fn=<SumBackward0>)
time8 0.0018768310546875
train_time 0.0776815414428711
eval time 0.1434013843536377
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:13,318 MainThread INFO: EPOCH:319
2024-01-23 01:02:13,318 MainThread INFO: Time Consumed:0.22434067726135254s
2024-01-23 01:02:13,318 MainThread INFO: Total Frames:48750s
  3%|▎         | 320/10000 [03:40<40:52,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11834.54366
Train_Epoch_Reward                9827.20930
Running_Training_Average_Rewards  15644.87137
Explore_Time                      0.00091
Train___Time                      0.07768
Eval____Time                      0.14340
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12308.63011
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.86907     2.01262     91.50896     86.60306
alpha_0                           0.85215      0.00012     0.85232      0.85198
Alpha_loss                        -1.07752     0.00165     -1.07531     -1.07960
Training/policy_loss              -2.49679     0.00611     -2.48795     -2.50565
Training/qf1_loss                 11159.03203  1041.84019  12167.27246  9309.55371
Training/qf2_loss                 16019.10645  1168.90680  17135.00391  13939.31543
Training/pf_norm                  0.09115      0.01065     0.10245      0.07183
Training/qf1_norm                 5392.19502   181.91676   5539.12939   5095.45068
Training/qf2_norm                 290.91385    6.31612     296.00433    280.53119
log_std/mean                      -0.13610     0.00018     -0.13584     -0.13635
log_probs/mean                    -2.73888     0.00722     -2.72852     -2.74897
mean/mean                         -0.00052     0.00006     -0.00041     -0.00058
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02259039878845215
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70935
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [49050]
collect time 0.0009288787841796875
inside mustsac before update, task 0, sumup 70935
inside mustsac after update, task 0, sumup 70396
inner_dict_sum {'sac_diff0': 0.00020647048950195312, 'sac_diff1': 0.006876945495605469, 'sac_diff2': 0.008479595184326172, 'sac_diff3': 0.010051488876342773, 'sac_diff4': 0.007293701171875, 'sac_diff5': 0.04924321174621582, 'sac_diff6': 0.0003993511199951172, 'all': 0.0825507640838623}
diff5_list [0.010667562484741211, 0.009841442108154297, 0.009795904159545898, 0.009702205657958984, 0.00923609733581543]
time3 0.0008742809295654297
time4 0.08342194557189941
time5 0.08347916603088379
time7 0.009266138076782227
gen_weight_change tensor(-22.3416)
policy weight change tensor(37.9845, grad_fn=<SumBackward0>)
time8 0.0025644302368164062
train_time 0.11391067504882812
eval time 0.10734176635742188
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:13,568 MainThread INFO: EPOCH:320
2024-01-23 01:02:13,568 MainThread INFO: Time Consumed:0.22440147399902344s
2024-01-23 01:02:13,568 MainThread INFO: Total Frames:48900s
  3%|▎         | 321/10000 [03:41<40:50,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11937.26449
Train_Epoch_Reward                2753.40589
Running_Training_Average_Rewards  15268.68867
Explore_Time                      0.00092
Train___Time                      0.11391
Eval____Time                      0.10734
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12348.08064
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.26903     0.98052     92.35030     89.48064
alpha_0                           0.85172      0.00012     0.85189      0.85155
Alpha_loss                        -1.07935     0.00095     -1.07833     -1.08061
Training/policy_loss              -2.48282     0.01402     -2.47117     -2.50992
Training/qf1_loss                 11809.77090  1017.50591  13616.54492  10703.42383
Training/qf2_loss                 16670.14844  1073.08085  18611.41602  15570.40918
Training/pf_norm                  0.11699      0.01797     0.14159      0.09698
Training/qf1_norm                 5554.50371   130.08279   5738.03516   5333.08545
Training/qf2_norm                 292.22820    10.20409    308.86633    280.26923
log_std/mean                      -0.12778     0.00605     -0.12069     -0.13613
log_probs/mean                    -2.72927     0.00153     -2.72714     -2.73130
mean/mean                         -0.00051     0.00060     0.00014      -0.00162
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02220916748046875
epoch last part time3 0.0025637149810791016
inside rlalgo, task 0, sumup 70396
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [49200]
collect time 0.0009324550628662109
inner_dict_sum {'sac_diff0': 0.0001933574676513672, 'sac_diff1': 0.006654262542724609, 'sac_diff2': 0.008215188980102539, 'sac_diff3': 0.00982356071472168, 'sac_diff4': 0.007197380065917969, 'sac_diff5': 0.031131505966186523, 'sac_diff6': 0.00038170814514160156, 'all': 0.06359696388244629}
diff5_list [0.006727695465087891, 0.0061228275299072266, 0.006266593933105469, 0.006031513214111328, 0.005982875823974609]
time3 0
time4 0.06435298919677734
time5 0.06440281867980957
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3416)
policy weight change tensor(37.9944, grad_fn=<SumBackward0>)
time8 0.0018966197967529297
train_time 0.07594895362854004
eval time 0.15680217742919922
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:13,832 MainThread INFO: EPOCH:321
2024-01-23 01:02:13,832 MainThread INFO: Time Consumed:0.23600268363952637s
2024-01-23 01:02:13,833 MainThread INFO: Total Frames:49050s
  3%|▎         | 322/10000 [03:41<41:11,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11986.61362
Train_Epoch_Reward                17925.99624
Running_Training_Average_Rewards  14772.27355
Explore_Time                      0.00093
Train___Time                      0.07595
Eval____Time                      0.15680
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12018.06037
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.91975     1.27313    90.68510     87.08170
alpha_0                           0.85130      0.00012    0.85147      0.85112
Alpha_loss                        -1.08330     0.00204    -1.07957     -1.08499
Training/policy_loss              -2.47424     0.00857    -2.46211     -2.48348
Training/qf1_loss                 10389.17227  813.14063  11823.62305  9625.40625
Training/qf2_loss                 15331.33242  874.39580  16872.68359  14435.52832
Training/pf_norm                  0.12137      0.03106    0.15677      0.07266
Training/qf1_norm                 5359.80762   116.47898  5523.19824   5189.78711
Training/qf2_norm                 277.08466    3.82434    282.41278    271.44910
log_std/mean                      -0.11520     0.00008    -0.11512     -0.11534
log_probs/mean                    -2.73292     0.01011    -2.71807     -2.74350
mean/mean                         0.00006      0.00009    0.00015      -0.00009
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.021611452102661133
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70396
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [49350]
collect time 0.0009274482727050781
inner_dict_sum {'sac_diff0': 0.00019693374633789062, 'sac_diff1': 0.007352590560913086, 'sac_diff2': 0.008866071701049805, 'sac_diff3': 0.010892391204833984, 'sac_diff4': 0.007819414138793945, 'sac_diff5': 0.035236358642578125, 'sac_diff6': 0.0004096031188964844, 'all': 0.07077336311340332}
diff5_list [0.008438348770141602, 0.0070722103118896484, 0.006337165832519531, 0.0063304901123046875, 0.007058143615722656]
time3 0
time4 0.07157468795776367
time5 0.07162618637084961
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3416)
policy weight change tensor(38.0608, grad_fn=<SumBackward0>)
time8 0.00197601318359375
train_time 0.08356213569641113
eval time 0.1540992259979248
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:14,098 MainThread INFO: EPOCH:322
2024-01-23 01:02:14,098 MainThread INFO: Time Consumed:0.2409343719482422s
2024-01-23 01:02:14,099 MainThread INFO: Total Frames:49200s
  3%|▎         | 323/10000 [03:41<41:47,  3.86it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12026.59533
Train_Epoch_Reward                4640.61356
Running_Training_Average_Rewards  14405.51756
Explore_Time                      0.00092
Train___Time                      0.08356
Eval____Time                      0.15410
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11981.18256
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.38360     1.88818    92.37231     87.16096
alpha_0                           0.85087      0.00012    0.85104      0.85070
Alpha_loss                        -1.08732     0.00140    -1.08544     -1.08908
Training/policy_loss              -2.49296     0.00609    -2.48304     -2.50205
Training/qf1_loss                 10902.05215  479.06323  11465.80762  10072.32324
Training/qf2_loss                 15868.31543  571.96301  16423.76758  14837.54199
Training/pf_norm                  0.10210      0.02374    0.12365      0.06002
Training/qf1_norm                 5434.05557   168.89726  5612.77344   5143.09521
Training/qf2_norm                 293.94073    5.95901    300.33408    283.75342
log_std/mean                      -0.13603     0.00012    -0.13584     -0.13619
log_probs/mean                    -2.73694     0.00713    -2.72528     -2.74743
mean/mean                         -0.00196     0.00005    -0.00187     -0.00202
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.022656679153442383
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70396
epoch first part time 3.814697265625e-06
replay_buffer._size: [49500]
collect time 0.0009510517120361328
inner_dict_sum {'sac_diff0': 0.0002090930938720703, 'sac_diff1': 0.006938934326171875, 'sac_diff2': 0.008267641067504883, 'sac_diff3': 0.010329723358154297, 'sac_diff4': 0.0071561336517333984, 'sac_diff5': 0.03157448768615723, 'sac_diff6': 0.00037980079650878906, 'all': 0.06485581398010254}
diff5_list [0.006713390350341797, 0.006320953369140625, 0.006152629852294922, 0.006253242492675781, 0.0061342716217041016]
time3 0
time4 0.06562089920043945
time5 0.06567025184631348
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3416)
policy weight change tensor(38.1020, grad_fn=<SumBackward0>)
time8 0.001973390579223633
train_time 0.07758402824401855
eval time 0.15059661865234375
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:14,356 MainThread INFO: EPOCH:323
2024-01-23 01:02:14,357 MainThread INFO: Time Consumed:0.23143434524536133s
2024-01-23 01:02:14,357 MainThread INFO: Total Frames:49350s
  3%|▎         | 324/10000 [03:41<41:41,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12061.12486
Train_Epoch_Reward                10481.00414
Running_Training_Average_Rewards  14329.72136
Explore_Time                      0.00095
Train___Time                      0.07758
Eval____Time                      0.15060
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11990.02290
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.92644     2.62429     95.34839     87.85653
alpha_0                           0.85044      0.00012     0.85061      0.85027
Alpha_loss                        -1.09084     0.00125     -1.08890     -1.09254
Training/policy_loss              -2.50628     0.00486     -2.49838     -2.51139
Training/qf1_loss                 12211.19199  1538.53986  14417.37695  9770.99219
Training/qf2_loss                 17349.71016  1662.19909  19715.10352  14614.78320
Training/pf_norm                  0.09586      0.02013     0.12008      0.07335
Training/qf1_norm                 5657.85527   243.89919   5962.52686   5275.99854
Training/qf2_norm                 315.86987    9.09025     327.43265    301.58185
log_std/mean                      -0.13007     0.00004     -0.12999     -0.13011
log_probs/mean                    -2.73786     0.00569     -2.72907     -2.74421
mean/mean                         0.00016      0.00005     0.00022      0.00009
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02214837074279785
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70396
epoch first part time 2.86102294921875e-06
replay_buffer._size: [49650]
collect time 0.0008928775787353516
inner_dict_sum {'sac_diff0': 0.00019693374633789062, 'sac_diff1': 0.006674289703369141, 'sac_diff2': 0.008287906646728516, 'sac_diff3': 0.009943246841430664, 'sac_diff4': 0.007041215896606445, 'sac_diff5': 0.03135251998901367, 'sac_diff6': 0.0003752708435058594, 'all': 0.06387138366699219}
diff5_list [0.006658792495727539, 0.006079912185668945, 0.006018638610839844, 0.0063016414642333984, 0.006293535232543945]
time3 0
time4 0.06462597846984863
time5 0.06467342376708984
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3416)
policy weight change tensor(38.0709, grad_fn=<SumBackward0>)
time8 0.0019965171813964844
train_time 0.07665896415710449
eval time 0.1516268253326416
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:14,613 MainThread INFO: EPOCH:324
2024-01-23 01:02:14,614 MainThread INFO: Time Consumed:0.23154711723327637s
2024-01-23 01:02:14,614 MainThread INFO: Total Frames:49500s
  3%|▎         | 325/10000 [03:42<41:36,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12097.11479
Train_Epoch_Reward                11002.53569
Running_Training_Average_Rewards  14415.49405
Explore_Time                      0.00089
Train___Time                      0.07666
Eval____Time                      0.15163
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12035.27532
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.17957     1.66688     95.04801     90.29159
alpha_0                           0.85002      0.00012     0.85019      0.84985
Alpha_loss                        -1.09319     0.00119     -1.09158     -1.09483
Training/policy_loss              -2.47342     0.00257     -2.47082     -2.47830
Training/qf1_loss                 12285.77246  961.05007   13912.04980  10976.31055
Training/qf2_loss                 17446.29941  1060.12163  19256.73633  16019.78613
Training/pf_norm                  0.11646      0.01583     0.13143      0.09565
Training/qf1_norm                 5593.60850   150.61680   5856.10986   5423.55859
Training/qf2_norm                 291.48608    4.98668     300.12259    285.83630
log_std/mean                      -0.13576     0.00011     -0.13556     -0.13589
log_probs/mean                    -2.73160     0.00308     -2.72916     -2.73756
mean/mean                         0.00074      0.00004     0.00078      0.00066
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.022210359573364258
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70396
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [49800]
collect time 0.000946044921875
inside mustsac before update, task 0, sumup 70396
inside mustsac after update, task 0, sumup 70847
inner_dict_sum {'sac_diff0': 0.0002028942108154297, 'sac_diff1': 0.007821321487426758, 'sac_diff2': 0.009264707565307617, 'sac_diff3': 0.011456727981567383, 'sac_diff4': 0.00803995132446289, 'sac_diff5': 0.05245375633239746, 'sac_diff6': 0.00041031837463378906, 'all': 0.08964967727661133}
diff5_list [0.010833501815795898, 0.00960850715637207, 0.010803699493408203, 0.011478900909423828, 0.009729146957397461]
time3 0.000873565673828125
time4 0.0905752182006836
time5 0.09063363075256348
time7 0.009209871292114258
gen_weight_change tensor(-22.4367)
policy weight change tensor(38.1236, grad_fn=<SumBackward0>)
time8 0.0018246173858642578
train_time 0.1208193302154541
eval time 0.10554957389831543
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:14,868 MainThread INFO: EPOCH:325
2024-01-23 01:02:14,869 MainThread INFO: Time Consumed:0.22951459884643555s
2024-01-23 01:02:14,869 MainThread INFO: Total Frames:49650s
  3%|▎         | 326/10000 [03:42<41:23,  3.90it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12130.84004
Train_Epoch_Reward                6926.37872
Running_Training_Average_Rewards  14006.36433
Explore_Time                      0.00094
Train___Time                      0.12082
Eval____Time                      0.10555
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12041.44267
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.08719     1.51840    90.91859     86.33885
alpha_0                           0.84959      0.00012    0.84976      0.84942
Alpha_loss                        -1.09717     0.00072    -1.09578     -1.09776
Training/policy_loss              -2.48779     0.01192    -2.46884     -2.50404
Training/qf1_loss                 10402.76641  349.93057  10772.95020  9883.38086
Training/qf2_loss                 15497.02129  477.91357  16065.19043  14748.91406
Training/pf_norm                  0.13543      0.03013    0.18655      0.09618
Training/qf1_norm                 5423.66094   190.00430  5661.10205   5099.32959
Training/qf2_norm                 288.54937    14.55587   308.51114    272.14948
log_std/mean                      -0.12982     0.00359    -0.12309     -0.13337
log_probs/mean                    -2.73534     0.00544    -2.72858     -2.74390
mean/mean                         0.00083      0.00130    0.00307      -0.00083
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.020522356033325195
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70847
epoch first part time 2.86102294921875e-06
replay_buffer._size: [49950]
collect time 0.0009274482727050781
inner_dict_sum {'sac_diff0': 0.00020194053649902344, 'sac_diff1': 0.006590604782104492, 'sac_diff2': 0.008016586303710938, 'sac_diff3': 0.009804487228393555, 'sac_diff4': 0.006610393524169922, 'sac_diff5': 0.030040979385375977, 'sac_diff6': 0.0003657341003417969, 'all': 0.0616307258605957}
diff5_list [0.006413459777832031, 0.00577855110168457, 0.005934476852416992, 0.0059511661529541016, 0.005963325500488281]
time3 0
time4 0.062361717224121094
time5 0.0624086856842041
time7 9.5367431640625e-07
gen_weight_change tensor(-22.4367)
policy weight change tensor(38.0491, grad_fn=<SumBackward0>)
time8 0.004116058349609375
train_time 0.0761566162109375
eval time 0.14371991157531738
epoch last part time 1.1682510375976562e-05
2024-01-23 01:02:15,115 MainThread INFO: EPOCH:326
2024-01-23 01:02:15,116 MainThread INFO: Time Consumed:0.22313642501831055s
2024-01-23 01:02:15,116 MainThread INFO: Total Frames:49800s
  3%|▎         | 327/10000 [03:42<40:49,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12134.72769
Train_Epoch_Reward                6220.29118
Running_Training_Average_Rewards  13760.08894
Explore_Time                      0.00092
Train___Time                      0.07616
Eval____Time                      0.14372
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12203.84134
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.03963     1.30015    89.46032     86.05432
alpha_0                           0.84917      0.00012    0.84934      0.84900
Alpha_loss                        -1.10106     0.00132    -1.09985     -1.10348
Training/policy_loss              -2.49350     0.00341    -2.48864     -2.49867
Training/qf1_loss                 10736.65586  813.83108  11634.74414  9591.87695
Training/qf2_loss                 15786.39473  873.07968  16682.69141  14584.61426
Training/pf_norm                  0.12633      0.02290    0.16242      0.09604
Training/qf1_norm                 5430.76182   121.37905  5565.58154   5248.41992
Training/qf2_norm                 294.76550    4.28203    299.60889    288.23227
log_std/mean                      -0.13377     0.00016    -0.13352     -0.13397
log_probs/mean                    -2.73851     0.00412    -2.73294     -2.74510
mean/mean                         -0.00004     0.00013    0.00012      -0.00023
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018797636032104492
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70847
epoch first part time 6.67572021484375e-06
replay_buffer._size: [50100]
collect time 0.0008401870727539062
inner_dict_sum {'sac_diff0': 0.0001990795135498047, 'sac_diff1': 0.007018566131591797, 'sac_diff2': 0.008464574813842773, 'sac_diff3': 0.010284662246704102, 'sac_diff4': 0.0072994232177734375, 'sac_diff5': 0.03235816955566406, 'sac_diff6': 0.0003840923309326172, 'all': 0.0660085678100586}
diff5_list [0.0067403316497802734, 0.0063169002532958984, 0.006234407424926758, 0.0064465999603271484, 0.006619930267333984]
time3 0
time4 0.06676173210144043
time5 0.06680560111999512
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4367)
policy weight change tensor(38.0692, grad_fn=<SumBackward0>)
time8 0.0018298625946044922
train_time 0.0785987377166748
eval time 0.14248442649841309
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:15,362 MainThread INFO: EPOCH:327
2024-01-23 01:02:15,362 MainThread INFO: Time Consumed:0.22417521476745605s
2024-01-23 01:02:15,362 MainThread INFO: Total Frames:49950s
  3%|▎         | 328/10000 [03:42<40:27,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12143.20811
Train_Epoch_Reward                3508.25260
Running_Training_Average_Rewards  13594.73510
Explore_Time                      0.00083
Train___Time                      0.07860
Eval____Time                      0.14248
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12257.37705
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.07559     0.94579     91.64312     89.15436
alpha_0                           0.84874      0.00012     0.84891      0.84857
Alpha_loss                        -1.10273     0.00149     -1.10032     -1.10451
Training/policy_loss              -2.48753     0.00474     -2.48244     -2.49524
Training/qf1_loss                 11825.06426  978.65102   13596.59961  10940.08398
Training/qf2_loss                 16669.74297  1033.35327  18542.11328  15750.80273
Training/pf_norm                  0.12231      0.01865     0.15301      0.09706
Training/qf1_norm                 5389.46426   84.39628    5533.14258   5314.02100
Training/qf2_norm                 298.56910    3.16516     303.80389    295.41534
log_std/mean                      -0.12633     0.00012     -0.12622     -0.12656
log_probs/mean                    -2.72813     0.00572     -2.72165     -2.73705
mean/mean                         0.00140      0.00012     0.00153      0.00122
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018138408660888672
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70847
epoch first part time 2.384185791015625e-06
replay_buffer._size: [50250]
collect time 0.0008842945098876953
inner_dict_sum {'sac_diff0': 0.00019502639770507812, 'sac_diff1': 0.0063152313232421875, 'sac_diff2': 0.007512807846069336, 'sac_diff3': 0.00936126708984375, 'sac_diff4': 0.006167888641357422, 'sac_diff5': 0.02991771697998047, 'sac_diff6': 0.00037288665771484375, 'all': 0.059842824935913086}
diff5_list [0.006133556365966797, 0.005995035171508789, 0.005836963653564453, 0.0061223506927490234, 0.005829811096191406]
time3 0
time4 0.06054520606994629
time5 0.060585737228393555
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4367)
policy weight change tensor(38.1054, grad_fn=<SumBackward0>)
time8 0.0018460750579833984
train_time 0.0717923641204834
eval time 0.1493823528289795
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:15,608 MainThread INFO: EPOCH:328
2024-01-23 01:02:15,608 MainThread INFO: Time Consumed:0.22432732582092285s
2024-01-23 01:02:15,608 MainThread INFO: Total Frames:50100s
  3%|▎         | 329/10000 [03:43<40:13,  4.01it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12143.01164
Train_Epoch_Reward                10638.64863
Running_Training_Average_Rewards  13654.32074
Explore_Time                      0.00088
Train___Time                      0.07179
Eval____Time                      0.14938
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12246.20342
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.14508     1.42154    92.88454     88.87028
alpha_0                           0.84832      0.00012    0.84849      0.84815
Alpha_loss                        -1.10740     0.00144    -1.10522     -1.10921
Training/policy_loss              -2.48327     0.00260    -2.47937     -2.48655
Training/qf1_loss                 11637.29453  675.04276  12650.77539  10926.77344
Training/qf2_loss                 16373.57812  744.89682  17483.21680  15644.01270
Training/pf_norm                  0.13994      0.03372    0.17964      0.09508
Training/qf1_norm                 5652.15615   131.18810  5806.65576   5437.18799
Training/qf2_norm                 294.02720    4.50669    299.49130    286.76547
log_std/mean                      -0.13340     0.00009    -0.13326     -0.13351
log_probs/mean                    -2.73610     0.00337    -2.73103     -2.73996
mean/mean                         -0.00082     0.00004    -0.00075     -0.00085
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01831793785095215
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70847
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [50400]
collect time 0.0008370876312255859
inner_dict_sum {'sac_diff0': 0.0002040863037109375, 'sac_diff1': 0.00679779052734375, 'sac_diff2': 0.008054494857788086, 'sac_diff3': 0.010426044464111328, 'sac_diff4': 0.0067408084869384766, 'sac_diff5': 0.03230547904968262, 'sac_diff6': 0.00039315223693847656, 'all': 0.06492185592651367}
diff5_list [0.007079124450683594, 0.006178379058837891, 0.006440639495849609, 0.006338834762573242, 0.006268501281738281]
time3 0
time4 0.06569194793701172
time5 0.06573915481567383
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4367)
policy weight change tensor(38.1102, grad_fn=<SumBackward0>)
time8 0.001996278762817383
train_time 0.07744598388671875
eval time 0.16395020484924316
epoch last part time 5.7220458984375e-06
2024-01-23 01:02:15,874 MainThread INFO: EPOCH:329
2024-01-23 01:02:15,875 MainThread INFO: Time Consumed:0.2445693016052246s
2024-01-23 01:02:15,875 MainThread INFO: Total Frames:50250s
  3%|▎         | 330/10000 [03:43<41:02,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12132.68143
Train_Epoch_Reward                15011.69118
Running_Training_Average_Rewards  13842.44543
Explore_Time                      0.00083
Train___Time                      0.07745
Eval____Time                      0.16395
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12205.32797
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.29604     1.65696     91.56465     86.82632
alpha_0                           0.84790      0.00012     0.84806      0.84773
Alpha_loss                        -1.11057     0.00118     -1.10920     -1.11242
Training/policy_loss              -2.48658     0.00365     -2.48328     -2.49242
Training/qf1_loss                 9830.15928   1482.84929  12360.43262  7987.25635
Training/qf2_loss                 15583.47480  1581.83905  18191.22852  13539.10938
Training/pf_norm                  0.14397      0.01861     0.16652      0.11223
Training/qf1_norm                 5716.83604   183.84450   5964.26807   5441.61377
Training/qf2_norm                 293.45231    5.24763     300.80649    285.70709
log_std/mean                      -0.13363     0.00003     -0.13359     -0.13367
log_probs/mean                    -2.73487     0.00429     -2.73066     -2.74200
mean/mean                         -0.00050     0.00021     -0.00019     -0.00078
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018258094787597656
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70847
epoch first part time 3.337860107421875e-06
replay_buffer._size: [50550]
collect time 0.000865936279296875
inside mustsac before update, task 0, sumup 70847
inside mustsac after update, task 0, sumup 70771
inner_dict_sum {'sac_diff0': 0.00020933151245117188, 'sac_diff1': 0.00697636604309082, 'sac_diff2': 0.008651971817016602, 'sac_diff3': 0.01072382926940918, 'sac_diff4': 0.0075647830963134766, 'sac_diff5': 0.050676584243774414, 'sac_diff6': 0.00042176246643066406, 'all': 0.08522462844848633}
diff5_list [0.010697364807128906, 0.010343074798583984, 0.010052680969238281, 0.009605646133422852, 0.00997781753540039]
time3 0.0008699893951416016
time4 0.08615565299987793
time5 0.086212158203125
time7 0.00881648063659668
gen_weight_change tensor(-22.5529)
policy weight change tensor(38.1546, grad_fn=<SumBackward0>)
time8 0.0025942325592041016
train_time 0.11641073226928711
eval time 0.12585997581481934
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:16,142 MainThread INFO: EPOCH:330
2024-01-23 01:02:16,142 MainThread INFO: Time Consumed:0.24535250663757324s
2024-01-23 01:02:16,142 MainThread INFO: Total Frames:50400s
  3%|▎         | 331/10000 [03:43<41:47,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12120.51577
Train_Epoch_Reward                11271.25989
Running_Training_Average_Rewards  13520.31612
Explore_Time                      0.00086
Train___Time                      0.11641
Eval____Time                      0.12586
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12226.42410
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.78311     1.27033     92.06491     88.25321
alpha_0                           0.84747      0.00012     0.84764      0.84730
Alpha_loss                        -1.11360     0.00113     -1.11176     -1.11534
Training/policy_loss              -2.48864     0.01020     -2.47226     -2.50071
Training/qf1_loss                 11449.23613  1523.93958  14042.30664  9493.82324
Training/qf2_loss                 16565.79629  1502.38672  19270.51562  14882.61719
Training/pf_norm                  0.11803      0.01652     0.13960      0.09539
Training/qf1_norm                 5501.89600   122.82009   5735.80371   5395.29492
Training/qf2_norm                 296.87667    17.93875    326.12268    275.17078
log_std/mean                      -0.12561     0.00476     -0.11969     -0.13106
log_probs/mean                    -2.73285     0.00558     -2.72386     -2.73926
mean/mean                         0.00008      0.00062     0.00102      -0.00082
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018319368362426758
epoch last part time3 0.0027861595153808594
inside rlalgo, task 0, sumup 70771
epoch first part time 2.384185791015625e-06
replay_buffer._size: [50700]
collect time 0.0009655952453613281
inner_dict_sum {'sac_diff0': 0.00019669532775878906, 'sac_diff1': 0.00673985481262207, 'sac_diff2': 0.00804448127746582, 'sac_diff3': 0.010358572006225586, 'sac_diff4': 0.007274150848388672, 'sac_diff5': 0.03241300582885742, 'sac_diff6': 0.0003917217254638672, 'all': 0.06541848182678223}
diff5_list [0.006476640701293945, 0.00714421272277832, 0.0063059329986572266, 0.006417989730834961, 0.006068229675292969]
time3 0
time4 0.06618785858154297
time5 0.06623530387878418
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5529)
policy weight change tensor(38.2407, grad_fn=<SumBackward0>)
time8 0.003194093704223633
train_time 0.08213400840759277
eval time 0.15239214897155762
epoch last part time 9.5367431640625e-06
2024-01-23 01:02:16,404 MainThread INFO: EPOCH:331
2024-01-23 01:02:16,405 MainThread INFO: Time Consumed:0.2380969524383545s
2024-01-23 01:02:16,405 MainThread INFO: Total Frames:50550s
  3%|▎         | 332/10000 [03:44<41:50,  3.85it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12164.32874
Train_Epoch_Reward                57771.17757
Running_Training_Average_Rewards  15416.54509
Explore_Time                      0.00096
Train___Time                      0.08213
Eval____Time                      0.15239
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12456.19002
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.82461     1.91804    95.32034     89.51408
alpha_0                           0.84705      0.00012    0.84722      0.84688
Alpha_loss                        -1.11654     0.00072    -1.11514     -1.11721
Training/policy_loss              -2.46946     0.00300    -2.46554     -2.47443
Training/qf1_loss                 11724.43203  533.49052  12658.63184  11192.91113
Training/qf2_loss                 17097.25742  608.24466  18013.36328  16388.31445
Training/pf_norm                  0.12866      0.03824    0.17974      0.08494
Training/qf1_norm                 5582.97129   177.36886  5905.12402   5368.77588
Training/qf2_norm                 294.17177    5.96498    305.12628    287.35583
log_std/mean                      -0.12206     0.00018    -0.12187     -0.12237
log_probs/mean                    -2.73028     0.00328    -2.72618     -2.73586
mean/mean                         0.00081      0.00006    0.00088      0.00072
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019533157348632812
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70771
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [50850]
collect time 0.0009315013885498047
inner_dict_sum {'sac_diff0': 0.00020623207092285156, 'sac_diff1': 0.007442951202392578, 'sac_diff2': 0.009380578994750977, 'sac_diff3': 0.010749578475952148, 'sac_diff4': 0.0077664852142333984, 'sac_diff5': 0.031884193420410156, 'sac_diff6': 0.00039076805114746094, 'all': 0.06782078742980957}
diff5_list [0.007414102554321289, 0.006353616714477539, 0.005906105041503906, 0.006093502044677734, 0.0061168670654296875]
time3 0
time4 0.0685586929321289
time5 0.06860589981079102
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5529)
policy weight change tensor(38.3131, grad_fn=<SumBackward0>)
time8 0.0018029212951660156
train_time 0.08008170127868652
eval time 0.14243078231811523
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:16,653 MainThread INFO: EPOCH:332
2024-01-23 01:02:16,653 MainThread INFO: Time Consumed:0.22568750381469727s
2024-01-23 01:02:16,654 MainThread INFO: Total Frames:50700s
  3%|▎         | 333/10000 [03:44<41:15,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12211.14665
Train_Epoch_Reward                17241.31989
Running_Training_Average_Rewards  15392.32815
Explore_Time                      0.00093
Train___Time                      0.08008
Eval____Time                      0.14243
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12449.36169
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.96018     2.32154     93.42699     86.87058
alpha_0                           0.84662      0.00012     0.84679      0.84645
Alpha_loss                        -1.12036     0.00141     -1.11860     -1.12208
Training/policy_loss              -2.48335     0.00306     -2.47879     -2.48803
Training/qf1_loss                 10525.32402  1718.58674  12773.77148  8383.46387
Training/qf2_loss                 16030.98164  1877.63411  18374.34375  13664.11523
Training/pf_norm                  0.11012      0.01584     0.12763      0.08037
Training/qf1_norm                 5668.27617   241.53969   6022.64209   5346.72852
Training/qf2_norm                 291.79957    7.38282     302.68201    282.18900
log_std/mean                      -0.13716     0.00010     -0.13701     -0.13730
log_probs/mean                    -2.73298     0.00382     -2.72744     -2.73856
mean/mean                         0.00066      0.00018     0.00090      0.00040
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018086671829223633
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70771
epoch first part time 2.384185791015625e-06
replay_buffer._size: [51000]
collect time 0.0009264945983886719
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.006873607635498047, 'sac_diff2': 0.007984161376953125, 'sac_diff3': 0.009999275207519531, 'sac_diff4': 0.0068204402923583984, 'sac_diff5': 0.032259225845336914, 'sac_diff6': 0.00039267539978027344, 'all': 0.06455135345458984}
diff5_list [0.0063571929931640625, 0.006066560745239258, 0.006182670593261719, 0.006286144256591797, 0.007366657257080078]
time3 0
time4 0.06530547142028809
time5 0.06534790992736816
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5529)
policy weight change tensor(38.3004, grad_fn=<SumBackward0>)
time8 0.0019731521606445312
train_time 0.07635498046875
eval time 0.14890527725219727
epoch last part time 1.2874603271484375e-05
2024-01-23 01:02:16,903 MainThread INFO: EPOCH:333
2024-01-23 01:02:16,903 MainThread INFO: Time Consumed:0.22835397720336914s
2024-01-23 01:02:16,904 MainThread INFO: Total Frames:50850s
  3%|▎         | 334/10000 [03:44<40:57,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12258.62404
Train_Epoch_Reward                3835.55212
Running_Training_Average_Rewards  15029.92970
Explore_Time                      0.00092
Train___Time                      0.07635
Eval____Time                      0.14891
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12464.79683
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.92672     2.07027     94.08315     88.01405
alpha_0                           0.84620      0.00012     0.84637      0.84603
Alpha_loss                        -1.12426     0.00111     -1.12237     -1.12559
Training/policy_loss              -2.48351     0.00139     -2.48116     -2.48521
Training/qf1_loss                 11422.01211  1843.81522  14738.63281  9680.87500
Training/qf2_loss                 16545.98945  1946.29272  19936.85547  14604.48438
Training/pf_norm                  0.12608      0.01967     0.14990      0.09477
Training/qf1_norm                 5717.17490   202.94531   6025.44580   5434.20020
Training/qf2_norm                 293.65371    6.57670     303.77386    284.63721
log_std/mean                      -0.13222     0.00006     -0.13210     -0.13227
log_probs/mean                    -2.73616     0.00170     -2.73294     -2.73777
mean/mean                         0.00025      0.00020     0.00056      0.00000
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018076419830322266
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70771
epoch first part time 2.384185791015625e-06
replay_buffer._size: [51150]
collect time 0.0009009838104248047
inner_dict_sum {'sac_diff0': 0.00022983551025390625, 'sac_diff1': 0.006324052810668945, 'sac_diff2': 0.0074920654296875, 'sac_diff3': 0.010025262832641602, 'sac_diff4': 0.006596088409423828, 'sac_diff5': 0.030373573303222656, 'sac_diff6': 0.0003802776336669922, 'all': 0.06142115592956543}
diff5_list [0.006296396255493164, 0.006101846694946289, 0.0060443878173828125, 0.006035804748535156, 0.005895137786865234]
time3 0
time4 0.06214547157287598
time5 0.062186479568481445
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5529)
policy weight change tensor(38.3103, grad_fn=<SumBackward0>)
time8 0.0018749237060546875
train_time 0.07273197174072266
eval time 0.1527848243713379
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:17,153 MainThread INFO: EPOCH:334
2024-01-23 01:02:17,154 MainThread INFO: Time Consumed:0.22870755195617676s
2024-01-23 01:02:17,154 MainThread INFO: Total Frames:51000s
  3%|▎         | 335/10000 [03:44<40:47,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12306.39861
Train_Epoch_Reward                9507.67423
Running_Training_Average_Rewards  14996.55648
Explore_Time                      0.00090
Train___Time                      0.07273
Eval____Time                      0.15278
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12513.02096
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.58140     2.09211     92.31954     86.02892
alpha_0                           0.84578      0.00012     0.84595      0.84561
Alpha_loss                        -1.12708     0.00157     -1.12459     -1.12897
Training/policy_loss              -2.47537     0.00681     -2.46644     -2.48501
Training/qf1_loss                 10291.83613  950.54615   11654.02344  8916.31250
Training/qf2_loss                 15555.91172  1062.90695  17108.73438  13955.35645
Training/pf_norm                  0.11316      0.02873     0.14998      0.06887
Training/qf1_norm                 5461.92764   198.13754   5721.91895   5124.41016
Training/qf2_norm                 284.81251    6.42288     293.23660    273.92062
log_std/mean                      -0.12629     0.00006     -0.12624     -0.12641
log_probs/mean                    -2.73291     0.00799     -2.72204     -2.74420
mean/mean                         0.00115      0.00013     0.00131      0.00097
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01865220069885254
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70771
epoch first part time 3.814697265625e-06
replay_buffer._size: [51300]
collect time 0.0014047622680664062
inside mustsac before update, task 0, sumup 70771
inside mustsac after update, task 0, sumup 71252
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.007380008697509766, 'sac_diff2': 0.008968830108642578, 'sac_diff3': 0.011153459548950195, 'sac_diff4': 0.00829315185546875, 'sac_diff5': 0.0545501708984375, 'sac_diff6': 0.00048470497131347656, 'all': 0.09105134010314941}
diff5_list [0.013292789459228516, 0.009574174880981445, 0.009402275085449219, 0.0093841552734375, 0.01289677619934082]
time3 0.0009138584136962891
time4 0.09212017059326172
time5 0.09218263626098633
time7 0.009746074676513672
gen_weight_change tensor(-22.6000)
policy weight change tensor(38.3288, grad_fn=<SumBackward0>)
time8 0.0018925666809082031
train_time 0.1229102611541748
eval time 0.10789251327514648
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:17,410 MainThread INFO: EPOCH:335
2024-01-23 01:02:17,411 MainThread INFO: Time Consumed:0.2346203327178955s
2024-01-23 01:02:17,411 MainThread INFO: Total Frames:51150s
  3%|▎         | 336/10000 [03:45<40:56,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12352.75798
Train_Epoch_Reward                13126.33378
Running_Training_Average_Rewards  14124.18363
Explore_Time                      0.00140
Train___Time                      0.12291
Eval____Time                      0.10789
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12505.03643
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.03837     2.50213     94.94359     87.31284
alpha_0                           0.84535      0.00012     0.84552      0.84519
Alpha_loss                        -1.13151     0.00156     -1.12929     -1.13310
Training/policy_loss              -2.49228     0.01455     -2.47335     -2.51088
Training/qf1_loss                 11338.10352  1243.33105  12980.94434  9754.49414
Training/qf2_loss                 16780.36855  1519.12525  18893.94141  14845.62793
Training/pf_norm                  0.10840      0.02846     0.14575      0.06635
Training/qf1_norm                 5666.54287   263.16483   6093.87256   5295.29443
Training/qf2_norm                 304.97588    14.22701    323.55246    288.55038
log_std/mean                      -0.13488     0.00596     -0.12854     -0.14420
log_probs/mean                    -2.73922     0.00555     -2.73402     -2.74709
mean/mean                         0.00172      0.00119     0.00294      0.00028
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018177032470703125
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71252
epoch first part time 2.86102294921875e-06
replay_buffer._size: [51450]
collect time 0.0008387565612792969
inner_dict_sum {'sac_diff0': 0.00021028518676757812, 'sac_diff1': 0.00696110725402832, 'sac_diff2': 0.008173704147338867, 'sac_diff3': 0.010442256927490234, 'sac_diff4': 0.007207155227661133, 'sac_diff5': 0.03267097473144531, 'sac_diff6': 0.00039386749267578125, 'all': 0.06605935096740723}
diff5_list [0.006551504135131836, 0.006141185760498047, 0.0067119598388671875, 0.0061452388763427734, 0.007121086120605469]
time3 0
time4 0.06688308715820312
time5 0.06693243980407715
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6000)
policy weight change tensor(38.3893, grad_fn=<SumBackward0>)
time8 0.0018308162689208984
train_time 0.07786917686462402
eval time 0.15087509155273438
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:17,664 MainThread INFO: EPOCH:336
2024-01-23 01:02:17,664 MainThread INFO: Time Consumed:0.23197317123413086s
2024-01-23 01:02:17,664 MainThread INFO: Total Frames:51300s
  3%|▎         | 337/10000 [03:45<40:55,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12434.68045
Train_Epoch_Reward                18685.30577
Running_Training_Average_Rewards  13895.02872
Explore_Time                      0.00083
Train___Time                      0.07787
Eval____Time                      0.15088
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13023.06601
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.22974     1.65449     92.42055     87.76119
alpha_0                           0.84493      0.00012     0.84510      0.84476
Alpha_loss                        -1.13286     0.00171     -1.12953     -1.13449
Training/policy_loss              -2.46871     0.00529     -2.45918     -2.47471
Training/qf1_loss                 10667.53203  1044.57710  11847.20215  9151.61230
Training/qf2_loss                 16125.82656  1156.05942  17422.09375  14450.53418
Training/pf_norm                  0.07791      0.02048     0.11375      0.05767
Training/qf1_norm                 5670.96367   166.20756   5888.20898   5420.33936
Training/qf2_norm                 296.12230    5.13192     302.95621    288.63815
log_std/mean                      -0.13275     0.00010     -0.13261     -0.13291
log_probs/mean                    -2.72722     0.00639     -2.71545     -2.73404
mean/mean                         0.00138      0.00005     0.00142      0.00129
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018427371978759766
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71252
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [51600]
collect time 0.0009558200836181641
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.0070209503173828125, 'sac_diff2': 0.008419036865234375, 'sac_diff3': 0.010584592819213867, 'sac_diff4': 0.007227897644042969, 'sac_diff5': 0.03307223320007324, 'sac_diff6': 0.00041413307189941406, 'all': 0.06695175170898438}
diff5_list [0.006628513336181641, 0.006189107894897461, 0.006002664566040039, 0.007071733474731445, 0.007180213928222656]
time3 0
time4 0.06783032417297363
time5 0.06788086891174316
time7 9.5367431640625e-07
gen_weight_change tensor(-22.6000)
policy weight change tensor(38.3926, grad_fn=<SumBackward0>)
time8 0.0018568038940429688
train_time 0.07905721664428711
eval time 0.1549229621887207
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:17,923 MainThread INFO: EPOCH:337
2024-01-23 01:02:17,924 MainThread INFO: Time Consumed:0.23733162879943848s
2024-01-23 01:02:17,924 MainThread INFO: Total Frames:51450s
  3%|▎         | 338/10000 [03:45<41:10,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12513.58449
Train_Epoch_Reward                9248.69012
Running_Training_Average_Rewards  14038.54876
Explore_Time                      0.00095
Train___Time                      0.07906
Eval____Time                      0.15492
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13046.41745
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.35447     2.72201     93.23000     85.02467
alpha_0                           0.84451      0.00012     0.84468      0.84434
Alpha_loss                        -1.13711     0.00125     -1.13504     -1.13840
Training/policy_loss              -2.49046     0.00412     -2.48382     -2.49415
Training/qf1_loss                 10312.14414  1346.45303  11770.20801  8623.14160
Training/qf2_loss                 15957.87285  1543.82841  17704.61914  13903.77637
Training/pf_norm                  0.11735      0.01761     0.14663      0.09600
Training/qf1_norm                 5514.43418   284.65040   5914.97852   5066.24072
Training/qf2_norm                 304.80602    9.13430     317.64929    290.21814
log_std/mean                      -0.14145     0.00009     -0.14133     -0.14156
log_probs/mean                    -2.73246     0.00491     -2.72419     -2.73682
mean/mean                         0.00092      0.00012     0.00109      0.00075
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018279075622558594
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71252
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [51750]
collect time 0.0009627342224121094
inner_dict_sum {'sac_diff0': 0.0002048015594482422, 'sac_diff1': 0.007030487060546875, 'sac_diff2': 0.008591890335083008, 'sac_diff3': 0.011034965515136719, 'sac_diff4': 0.007310628890991211, 'sac_diff5': 0.03311324119567871, 'sac_diff6': 0.0004036426544189453, 'all': 0.06768965721130371}
diff5_list [0.006518125534057617, 0.006123065948486328, 0.007737874984741211, 0.006588459014892578, 0.0061457157135009766]
time3 0
time4 0.06853175163269043
time5 0.06858372688293457
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6000)
policy weight change tensor(38.4091, grad_fn=<SumBackward0>)
time8 0.0019130706787109375
train_time 0.07962727546691895
eval time 0.15779614448547363
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:18,186 MainThread INFO: EPOCH:338
2024-01-23 01:02:18,186 MainThread INFO: Time Consumed:0.2408449649810791s
2024-01-23 01:02:18,187 MainThread INFO: Total Frames:51600s
  3%|▎         | 339/10000 [03:45<41:32,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12593.23762
Train_Epoch_Reward                12990.78585
Running_Training_Average_Rewards  13214.80464
Explore_Time                      0.00096
Train___Time                      0.07963
Eval____Time                      0.15780
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13042.73467
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.01877     2.82376     94.49846     85.67842
alpha_0                           0.84409      0.00012     0.84426      0.84392
Alpha_loss                        -1.14155     0.00190     -1.13947     -1.14449
Training/policy_loss              -2.48263     0.00465     -2.47724     -2.49014
Training/qf1_loss                 9782.48242   921.51647   11157.51562  8391.68555
Training/qf2_loss                 15294.48047  1103.73112  16993.75391  13600.17676
Training/pf_norm                  0.11565      0.03345     0.14944      0.07164
Training/qf1_norm                 5612.89004   287.46462   6068.68506   5170.33496
Training/qf2_norm                 298.98555    9.02552     313.32156    285.24176
log_std/mean                      -0.11600     0.00011     -0.11588     -0.11618
log_probs/mean                    -2.73881     0.00583     -2.73213     -2.74817
mean/mean                         0.00291      0.00018     0.00312      0.00262
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019080638885498047
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71252
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [51900]
collect time 0.0008673667907714844
inner_dict_sum {'sac_diff0': 0.00022745132446289062, 'sac_diff1': 0.006842851638793945, 'sac_diff2': 0.007963895797729492, 'sac_diff3': 0.009938716888427734, 'sac_diff4': 0.00679469108581543, 'sac_diff5': 0.03232717514038086, 'sac_diff6': 0.0004057884216308594, 'all': 0.06450057029724121}
diff5_list [0.00672459602355957, 0.006200075149536133, 0.006081342697143555, 0.006925106048583984, 0.006396055221557617]
time3 0
time4 0.06531572341918945
time5 0.06536316871643066
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6000)
policy weight change tensor(38.3870, grad_fn=<SumBackward0>)
time8 0.002050638198852539
train_time 0.07676458358764648
eval time 0.1501619815826416
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:18,439 MainThread INFO: EPOCH:339
2024-01-23 01:02:18,439 MainThread INFO: Time Consumed:0.2302241325378418s
2024-01-23 01:02:18,440 MainThread INFO: Total Frames:51750s
  3%|▎         | 340/10000 [03:46<41:17,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12675.13665
Train_Epoch_Reward                10793.82915
Running_Training_Average_Rewards  12087.40845
Explore_Time                      0.00086
Train___Time                      0.07676
Eval____Time                      0.15016
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13024.31832
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.14202     1.92478     96.10767     90.67530
alpha_0                           0.84367      0.00012     0.84383      0.84350
Alpha_loss                        -1.14380     0.00092     -1.14233     -1.14485
Training/policy_loss              -2.47668     0.00218     -2.47398     -2.47984
Training/qf1_loss                 11860.03594  1420.26445  13376.98438  9930.65137
Training/qf2_loss                 18284.92578  1580.82140  19918.16016  16212.48438
Training/pf_norm                  0.09669      0.02012     0.13109      0.07005
Training/qf1_norm                 5985.17617   223.62606   6325.60254   5691.25879
Training/qf2_norm                 312.65531    6.35604     322.36264    304.34338
log_std/mean                      -0.14321     0.00012     -0.14301     -0.14335
log_probs/mean                    -2.73223     0.00246     -2.72952     -2.73572
mean/mean                         -0.00138     0.00017     -0.00114     -0.00163
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018675804138183594
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71252
epoch first part time 3.337860107421875e-06
replay_buffer._size: [52050]
collect time 0.0008432865142822266
inside mustsac before update, task 0, sumup 71252
inside mustsac after update, task 0, sumup 70545
inner_dict_sum {'sac_diff0': 0.0002067089080810547, 'sac_diff1': 0.00712132453918457, 'sac_diff2': 0.008595705032348633, 'sac_diff3': 0.011055946350097656, 'sac_diff4': 0.007254600524902344, 'sac_diff5': 0.05120229721069336, 'sac_diff6': 0.00041794776916503906, 'all': 0.08585453033447266}
diff5_list [0.010883569717407227, 0.010985136032104492, 0.010077238082885742, 0.009472370147705078, 0.00978398323059082]
time3 0.0008661746978759766
time4 0.0867769718170166
time5 0.08683037757873535
time7 0.008893251419067383
gen_weight_change tensor(-22.5590)
policy weight change tensor(38.3433, grad_fn=<SumBackward0>)
time8 0.0026733875274658203
train_time 0.11705517768859863
eval time 0.11063551902770996
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:18,693 MainThread INFO: EPOCH:340
2024-01-23 01:02:18,693 MainThread INFO: Time Consumed:0.23099875450134277s
2024-01-23 01:02:18,693 MainThread INFO: Total Frames:51900s
  3%|▎         | 341/10000 [03:46<41:17,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12755.73411
Train_Epoch_Reward                30910.56030
Running_Training_Average_Rewards  12638.38400
Explore_Time                      0.00084
Train___Time                      0.11706
Eval____Time                      0.11064
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13032.39871
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.06557     2.21591     94.18050     87.80096
alpha_0                           0.84324      0.00012     0.84341      0.84307
Alpha_loss                        -1.14795     0.00025     -1.14757     -1.14824
Training/policy_loss              -2.49748     0.01519     -2.48228     -2.51914
Training/qf1_loss                 11648.02578  1601.84637  13946.94824  9565.47070
Training/qf2_loss                 17529.86484  1622.59379  19806.81250  15408.17969
Training/pf_norm                  0.11188      0.02453     0.14140      0.06782
Training/qf1_norm                 5782.97207   230.02915   5962.43066   5341.16357
Training/qf2_norm                 316.11387    14.40443    341.44312    302.78049
log_std/mean                      -0.13465     0.00440     -0.12595     -0.13781
log_probs/mean                    -2.73679     0.00539     -2.72916     -2.74250
mean/mean                         0.00054      0.00055     0.00140      -0.00011
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019196033477783203
epoch last part time3 0.002894878387451172
inside rlalgo, task 0, sumup 70545
epoch first part time 1.8358230590820312e-05
replay_buffer._size: [52200]
collect time 0.0008711814880371094
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.008324384689331055, 'sac_diff2': 0.009855985641479492, 'sac_diff3': 0.012441158294677734, 'sac_diff4': 0.008633613586425781, 'sac_diff5': 0.04090476036071777, 'sac_diff6': 0.00046563148498535156, 'all': 0.08083534240722656}
diff5_list [0.006928920745849609, 0.011831045150756836, 0.007181644439697266, 0.0069158077239990234, 0.008047342300415039]
time3 0
time4 0.08183550834655762
time5 0.08189821243286133
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5590)
policy weight change tensor(38.3111, grad_fn=<SumBackward0>)
time8 0.0024602413177490234
train_time 0.09438538551330566
eval time 0.14577388763427734
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:18,962 MainThread INFO: EPOCH:341
2024-01-23 01:02:18,962 MainThread INFO: Time Consumed:0.24374938011169434s
2024-01-23 01:02:18,962 MainThread INFO: Total Frames:52050s
  3%|▎         | 342/10000 [03:46<41:45,  3.85it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12711.40358
Train_Epoch_Reward                8379.91245
Running_Training_Average_Rewards  12769.67226
Explore_Time                      0.00087
Train___Time                      0.09439
Eval____Time                      0.14577
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12012.88472
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.75968     2.22046    90.73143     83.95474
alpha_0                           0.84282      0.00012    0.84299      0.84265
Alpha_loss                        -1.15010     0.00198    -1.14749     -1.15256
Training/policy_loss              -2.50559     0.00538    -2.49626     -2.51053
Training/qf1_loss                 8798.44990   628.02942  9567.29297   7723.56104
Training/qf2_loss                 14917.79707  824.69991  15932.33105  13485.53906
Training/pf_norm                  0.10748      0.03150    0.16878      0.08278
Training/qf1_norm                 5275.24687   257.11235  5627.37891   4838.37354
Training/qf2_norm                 320.24442    7.76318    330.88617    307.10809
log_std/mean                      -0.13184     0.00004    -0.13176     -0.13188
log_probs/mean                    -2.72964     0.00670    -2.71831     -2.73616
mean/mean                         0.00012      0.00016    0.00037      -0.00008
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019562721252441406
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70545
epoch first part time 4.291534423828125e-06
replay_buffer._size: [52350]
collect time 0.00090789794921875
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.0073070526123046875, 'sac_diff2': 0.008697748184204102, 'sac_diff3': 0.01100468635559082, 'sac_diff4': 0.0076520442962646484, 'sac_diff5': 0.033763885498046875, 'sac_diff6': 0.00041031837463378906, 'all': 0.06905674934387207}
diff5_list [0.0067784786224365234, 0.007013082504272461, 0.006134510040283203, 0.00658106803894043, 0.007256746292114258]
time3 0
time4 0.06991410255432129
time5 0.06996679306030273
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5590)
policy weight change tensor(38.2745, grad_fn=<SumBackward0>)
time8 0.0019795894622802734
train_time 0.08176469802856445
eval time 0.14756464958190918
epoch last part time 5.7220458984375e-06
2024-01-23 01:02:19,217 MainThread INFO: EPOCH:342
2024-01-23 01:02:19,218 MainThread INFO: Time Consumed:0.23270916938781738s
2024-01-23 01:02:19,218 MainThread INFO: Total Frames:52200s
  3%|▎         | 343/10000 [03:48<1:50:46,  1.45it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12666.92886
Train_Epoch_Reward                14439.04710
Running_Training_Average_Rewards  12815.34753
Explore_Time                      0.00090
Train___Time                      0.08176
Eval____Time                      0.14756
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12004.61445
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.11534     1.64400    92.42727     87.69839
alpha_0                           0.84240      0.00012    0.84257      0.84223
Alpha_loss                        -1.15308     0.00085    -1.15151     -1.15393
Training/policy_loss              -2.49147     0.00344    -2.48763     -2.49562
Training/qf1_loss                 10237.95938  382.48051  10808.46094  9753.49805
Training/qf2_loss                 16127.65469  509.73445  16800.19727  15416.63574
Training/pf_norm                  0.10811      0.01643    0.12803      0.08356
Training/qf1_norm                 5569.10107   176.44313  5815.83105   5312.95557
Training/qf2_norm                 314.40607    5.60446    322.26389    306.06351
log_std/mean                      -0.14020     0.00004    -0.14016     -0.14029
log_probs/mean                    -2.72744     0.00383    -2.72351     -2.73239
mean/mean                         -0.00026     0.00008    -0.00012     -0.00034
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 1.4524104595184326
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70545
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [52500]
collect time 0.0009889602661132812
inner_dict_sum {'sac_diff0': 0.00022792816162109375, 'sac_diff1': 0.007004976272583008, 'sac_diff2': 0.008524179458618164, 'sac_diff3': 0.010419130325317383, 'sac_diff4': 0.006894111633300781, 'sac_diff5': 0.03233599662780762, 'sac_diff6': 0.000415802001953125, 'all': 0.06582212448120117}
diff5_list [0.0075185298919677734, 0.006630659103393555, 0.005994558334350586, 0.005939483642578125, 0.006252765655517578]
time3 0
time4 0.06665682792663574
time5 0.06670880317687988
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5590)
policy weight change tensor(38.2969, grad_fn=<SumBackward0>)
time8 0.0019001960754394531
train_time 0.07776451110839844
eval time 0.0006170272827148438
epoch last part time 3.814697265625e-06
2024-01-23 01:02:20,755 MainThread INFO: EPOCH:343
2024-01-23 01:02:20,755 MainThread INFO: Time Consumed:0.08150982856750488s
2024-01-23 01:02:20,755 MainThread INFO: Total Frames:52350s
  3%|▎         | 344/10000 [03:48<1:22:33,  1.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12625.18296
Train_Epoch_Reward                13590.95095
Running_Training_Average_Rewards  12895.84516
Explore_Time                      0.00098
Train___Time                      0.07776
Eval____Time                      0.00062
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12047.33783
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.84928     2.26515     92.69563     87.01383
alpha_0                           0.84198      0.00012     0.84215      0.84181
Alpha_loss                        -1.15755     0.00122     -1.15572     -1.15936
Training/policy_loss              -2.47692     0.00193     -2.47460     -2.47945
Training/qf1_loss                 10952.92813  868.82785   11826.79785  9452.65039
Training/qf2_loss                 16568.50859  1028.86319  17541.84766  14791.51562
Training/pf_norm                  0.13435      0.01600     0.15835      0.11263
Training/qf1_norm                 5686.85791   229.25657   5879.80469   5298.76953
Training/qf2_norm                 319.24056    7.67148     325.59473    306.21231
log_std/mean                      -0.12401     0.00010     -0.12390     -0.12416
log_probs/mean                    -2.73384     0.00246     -2.73105     -2.73682
mean/mean                         -0.00067     0.00004     -0.00060     -0.00070
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0187685489654541
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70545
epoch first part time 2.86102294921875e-06
replay_buffer._size: [52625]
collect time 0.0008122920989990234
inner_dict_sum {'sac_diff0': 0.00022745132446289062, 'sac_diff1': 0.007001638412475586, 'sac_diff2': 0.008711576461791992, 'sac_diff3': 0.01092982292175293, 'sac_diff4': 0.007079362869262695, 'sac_diff5': 0.03341221809387207, 'sac_diff6': 0.0003941059112548828, 'all': 0.06775617599487305}
diff5_list [0.006325483322143555, 0.006459236145019531, 0.008001565933227539, 0.006330966949462891, 0.006294965744018555]
time3 0
time4 0.06855416297912598
time5 0.06861257553100586
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5590)
policy weight change tensor(38.2196, grad_fn=<SumBackward0>)
time8 0.0018355846405029297
train_time 0.07979679107666016
eval time 0.0005428791046142578
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:20,861 MainThread INFO: EPOCH:344
2024-01-23 01:02:20,861 MainThread INFO: Time Consumed:0.08333969116210938s
2024-01-23 01:02:20,861 MainThread INFO: Total Frames:52500s
  3%|▎         | 345/10000 [03:48<1:02:51,  2.56it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12578.61464
Train_Epoch_Reward                3406.12175
Running_Training_Average_Rewards  12361.50651
Explore_Time                      0.00081
Train___Time                      0.07980
Eval____Time                      0.00054
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12047.33783
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.69737     2.40597    93.36971     86.72093
alpha_0                           0.84156      0.00012    0.84173      0.84139
Alpha_loss                        -1.16069     0.00141    -1.15937     -1.16330
Training/policy_loss              -2.48734     0.00336    -2.48443     -2.49310
Training/qf1_loss                 10379.97402  748.60287  11411.36035  9278.16309
Training/qf2_loss                 16018.09043  933.29194  17359.97070  14768.43262
Training/pf_norm                  0.12719      0.02627    0.16918      0.09627
Training/qf1_norm                 5506.58643   247.03698  5883.38623   5199.46826
Training/qf2_norm                 309.56779    8.16367    321.97372    299.35562
log_std/mean                      -0.14911     0.00032    -0.14860     -0.14944
log_probs/mean                    -2.73252     0.00413    -2.72879     -2.73988
mean/mean                         0.00056      0.00008    0.00068      0.00044
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018246173858642578
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70545
epoch first part time 3.337860107421875e-06
replay_buffer._size: [52745]
collect time 0.0008375644683837891
inside mustsac before update, task 0, sumup 70545
inside mustsac after update, task 0, sumup 70755
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.0076138973236083984, 'sac_diff2': 0.008522987365722656, 'sac_diff3': 0.011461496353149414, 'sac_diff4': 0.007575273513793945, 'sac_diff5': 0.052155256271362305, 'sac_diff6': 0.0004134178161621094, 'all': 0.0879521369934082}
diff5_list [0.011211872100830078, 0.011658430099487305, 0.009922981262207031, 0.009708881378173828, 0.009653091430664062]
time3 0.0008573532104492188
time4 0.08885502815246582
time5 0.08890962600708008
time7 0.008980035781860352
gen_weight_change tensor(-22.4911)
policy weight change tensor(38.1883, grad_fn=<SumBackward0>)
time8 0.001882314682006836
train_time 0.11814475059509277
eval time 0.014868974685668945
epoch last part time 5.9604644775390625e-06
2024-01-23 01:02:21,019 MainThread INFO: EPOCH:345
2024-01-23 01:02:21,019 MainThread INFO: Time Consumed:0.1362295150756836s
2024-01-23 01:02:21,019 MainThread INFO: Total Frames:52650s
  3%|▎         | 346/10000 [03:48<51:41,  3.11it/s]  --------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12528.86038
Train_Epoch_Reward                16572.32072
Running_Training_Average_Rewards  12796.64607
Explore_Time                      0.00083
Train___Time                      0.11814
Eval____Time                      0.01487
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12007.49383
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.57969     1.42542     92.63603     88.22615
alpha_0                           0.84114      0.00012     0.84131      0.84097
Alpha_loss                        -1.16390     0.00089     -1.16269     -1.16513
Training/policy_loss              -2.48450     0.01249     -2.47492     -2.50849
Training/qf1_loss                 10574.42227  968.73590   11901.44727  9286.73145
Training/qf2_loss                 16525.35762  1055.31218  17772.66211  15039.61426
Training/pf_norm                  0.09747      0.02596     0.13807      0.06638
Training/qf1_norm                 5596.92695   181.56936   5867.81104   5302.49658
Training/qf2_norm                 306.96896    10.23141    324.44736    297.50739
log_std/mean                      -0.13084     0.00825     -0.11799     -0.13977
log_probs/mean                    -2.73166     0.00094     -2.73088     -2.73312
mean/mean                         0.00056      0.00089     0.00170      -0.00091
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019599437713623047
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70755
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [52936]
collect time 0.0008871555328369141
inner_dict_sum {'sac_diff0': 0.00022649765014648438, 'sac_diff1': 0.006850481033325195, 'sac_diff2': 0.007929563522338867, 'sac_diff3': 0.010244369506835938, 'sac_diff4': 0.007146596908569336, 'sac_diff5': 0.03268742561340332, 'sac_diff6': 0.00039315223693847656, 'all': 0.06547808647155762}
diff5_list [0.006731510162353516, 0.006479024887084961, 0.006504058837890625, 0.006323337554931641, 0.006649494171142578]
time3 0
time4 0.06623601913452148
time5 0.06627774238586426
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4911)
policy weight change tensor(38.0687, grad_fn=<SumBackward0>)
time8 0.0019190311431884766
train_time 0.07735180854797363
eval time 0.15319609642028809
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:21,276 MainThread INFO: EPOCH:346
2024-01-23 01:02:21,276 MainThread INFO: Time Consumed:0.2338731288909912s
2024-01-23 01:02:21,276 MainThread INFO: Total Frames:52800s
  3%|▎         | 347/10000 [03:48<48:35,  3.31it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12471.66699
Train_Epoch_Reward                6233.73411
Running_Training_Average_Rewards  12685.84319
Explore_Time                      0.00088
Train___Time                      0.07735
Eval____Time                      0.15320
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12451.13204
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.72087     1.71024    91.72339     86.82122
alpha_0                           0.84072      0.00012    0.84089      0.84055
Alpha_loss                        -1.16781     0.00165    -1.16483     -1.16935
Training/policy_loss              -2.49160     0.00555    -2.48409     -2.49880
Training/qf1_loss                 9339.99688   575.07452  10270.39648  8495.22266
Training/qf2_loss                 15487.38301  582.89110  16557.24023  14837.00293
Training/pf_norm                  0.15502      0.02931    0.19644      0.11839
Training/qf1_norm                 5485.92539   191.20830  5696.46045   5159.95020
Training/qf2_norm                 319.86734    5.85560    326.80774    309.91336
log_std/mean                      -0.12269     0.00025    -0.12234     -0.12304
log_probs/mean                    -2.73475     0.00666    -2.72536     -2.74339
mean/mean                         0.00059      0.00011    0.00071      0.00040
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01986861228942871
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70755
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [53100]
collect time 0.0009453296661376953
inner_dict_sum {'sac_diff0': 0.00020837783813476562, 'sac_diff1': 0.007362842559814453, 'sac_diff2': 0.00880742073059082, 'sac_diff3': 0.011303186416625977, 'sac_diff4': 0.00861811637878418, 'sac_diff5': 0.03402972221374512, 'sac_diff6': 0.00041413307189941406, 'all': 0.07074379920959473}
diff5_list [0.007428884506225586, 0.006762266159057617, 0.007179737091064453, 0.006403446197509766, 0.006255388259887695]
time3 0
time4 0.07152581214904785
time5 0.07157778739929199
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4911)
policy weight change tensor(37.9419, grad_fn=<SumBackward0>)
time8 0.0020318031311035156
train_time 0.08321523666381836
eval time 0.14467334747314453
epoch last part time 1.0013580322265625e-05
2024-01-23 01:02:21,531 MainThread INFO: EPOCH:347
2024-01-23 01:02:21,531 MainThread INFO: Time Consumed:0.23137664794921875s
2024-01-23 01:02:21,531 MainThread INFO: Total Frames:52950s
  3%|▎         | 348/10000 [03:49<46:15,  3.48it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12407.11779
Train_Epoch_Reward                10328.23525
Running_Training_Average_Rewards  12969.09196
Explore_Time                      0.00094
Train___Time                      0.08322
Eval____Time                      0.14467
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12400.92548
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.82750     2.15605     93.48608     87.35576
alpha_0                           0.84030      0.00012     0.84046      0.84013
Alpha_loss                        -1.17193     0.00131     -1.16994     -1.17395
Training/policy_loss              -2.48126     0.00367     -2.47679     -2.48751
Training/qf1_loss                 9711.71328   1038.78298  10919.82422  8256.28906
Training/qf2_loss                 16133.54277  1194.20941  17339.18359  14348.78320
Training/pf_norm                  0.12630      0.03029     0.16456      0.09074
Training/qf1_norm                 5665.74814   250.20074   5975.49219   5260.32715
Training/qf2_norm                 294.44659    6.76017     302.52197    283.40231
log_std/mean                      -0.13715     0.00020     -0.13685     -0.13739
log_probs/mean                    -2.73912     0.00440     -2.73439     -2.74685
mean/mean                         0.00071      0.00017     0.00094      0.00047
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01860189437866211
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70755
epoch first part time 2.86102294921875e-06
replay_buffer._size: [53250]
collect time 0.000926971435546875
inner_dict_sum {'sac_diff0': 0.0002357959747314453, 'sac_diff1': 0.006956577301025391, 'sac_diff2': 0.008636951446533203, 'sac_diff3': 0.010509490966796875, 'sac_diff4': 0.006949901580810547, 'sac_diff5': 0.032003164291381836, 'sac_diff6': 0.00040984153747558594, 'all': 0.06570172309875488}
diff5_list [0.0071299076080322266, 0.0063343048095703125, 0.006202220916748047, 0.006174802780151367, 0.006161928176879883]
time3 0
time4 0.06644630432128906
time5 0.06648993492126465
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4911)
policy weight change tensor(37.8335, grad_fn=<SumBackward0>)
time8 0.0018680095672607422
train_time 0.07744288444519043
eval time 0.15176844596862793
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:21,785 MainThread INFO: EPOCH:348
2024-01-23 01:02:21,786 MainThread INFO: Time Consumed:0.2324819564819336s
2024-01-23 01:02:21,786 MainThread INFO: Total Frames:53100s
  3%|▎         | 349/10000 [03:49<44:40,  3.60it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12336.83852
Train_Epoch_Reward                19544.01876
Running_Training_Average_Rewards  12893.76190
Explore_Time                      0.00091
Train___Time                      0.07744
Eval____Time                      0.15177
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12339.94199
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.86418     1.24496    93.62278     90.59296
alpha_0                           0.83988      0.00012    0.84004      0.83971
Alpha_loss                        -1.17333     0.00101    -1.17229     -1.17503
Training/policy_loss              -2.48826     0.00150    -2.48619     -2.49014
Training/qf1_loss                 10496.91895  350.74305  11030.68750  10097.86914
Training/qf2_loss                 17098.86445  369.26403  17737.56250  16679.85742
Training/pf_norm                  0.13247      0.03090    0.18504      0.09299
Training/qf1_norm                 5793.04453   148.72130  5994.91113   5636.51318
Training/qf2_norm                 335.50621    4.52710    341.84006    331.03412
log_std/mean                      -0.12495     0.00010    -0.12484     -0.12511
log_probs/mean                    -2.72780     0.00182    -2.72568     -2.72984
mean/mean                         -0.00152     0.00006    -0.00141     -0.00158
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018788814544677734
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70755
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [53400]
collect time 0.0008594989776611328
inner_dict_sum {'sac_diff0': 0.00020813941955566406, 'sac_diff1': 0.0075948238372802734, 'sac_diff2': 0.009381532669067383, 'sac_diff3': 0.011923551559448242, 'sac_diff4': 0.007989883422851562, 'sac_diff5': 0.03477811813354492, 'sac_diff6': 0.00044417381286621094, 'all': 0.07232022285461426}
diff5_list [0.007237672805786133, 0.007088899612426758, 0.006701946258544922, 0.006845235824584961, 0.0069043636322021484]
time3 0
time4 0.07318305969238281
time5 0.07323551177978516
time7 9.5367431640625e-07
gen_weight_change tensor(-22.4911)
policy weight change tensor(37.7727, grad_fn=<SumBackward0>)
time8 0.001999378204345703
train_time 0.08478236198425293
eval time 0.14926767349243164
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:22,045 MainThread INFO: EPOCH:349
2024-01-23 01:02:22,045 MainThread INFO: Time Consumed:0.23725056648254395s
2024-01-23 01:02:22,045 MainThread INFO: Total Frames:53250s
  4%|▎         | 350/10000 [03:49<43:48,  3.67it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12264.33801
Train_Epoch_Reward                13634.70934
Running_Training_Average_Rewards  13020.67856
Explore_Time                      0.00085
Train___Time                      0.08478
Eval____Time                      0.14927
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12299.31321
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.21528     2.70964     95.58296     88.22178
alpha_0                           0.83946      0.00012     0.83962      0.83929
Alpha_loss                        -1.17869     0.00108     -1.17724     -1.18032
Training/policy_loss              -2.49946     0.00071     -2.49827     -2.50048
Training/qf1_loss                 10407.26055  1928.48756  12798.43555  8509.81836
Training/qf2_loss                 16392.69082  2136.90540  19039.85156  14239.57324
Training/pf_norm                  0.10265      0.02619     0.13463      0.06189
Training/qf1_norm                 5829.54893   301.92028   6317.31885   5501.57275
Training/qf2_norm                 325.57889    9.41443     340.89886    315.35727
log_std/mean                      -0.12762     0.00009     -0.12749     -0.12773
log_probs/mean                    -2.73921     0.00095     -2.73797     -2.74083
mean/mean                         -0.00048     0.00001     -0.00047     -0.00050
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018758296966552734
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70755
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [53550]
collect time 0.0009014606475830078
inside mustsac before update, task 0, sumup 70755
inside mustsac after update, task 0, sumup 70347
inner_dict_sum {'sac_diff0': 0.0002181529998779297, 'sac_diff1': 0.007710456848144531, 'sac_diff2': 0.00908660888671875, 'sac_diff3': 0.01122903823852539, 'sac_diff4': 0.008115291595458984, 'sac_diff5': 0.05379819869995117, 'sac_diff6': 0.00041103363037109375, 'all': 0.09056878089904785}
diff5_list [0.011974811553955078, 0.010740518569946289, 0.01097726821899414, 0.009914636611938477, 0.010190963745117188]
time3 0.0008609294891357422
time4 0.09140992164611816
time5 0.09146308898925781
time7 0.010193347930908203
gen_weight_change tensor(-22.4214)
policy weight change tensor(37.7432, grad_fn=<SumBackward0>)
time8 0.003000020980834961
train_time 0.12359070777893066
eval time 0.10661840438842773
epoch last part time 5.7220458984375e-06
2024-01-23 01:02:22,301 MainThread INFO: EPOCH:350
2024-01-23 01:02:22,301 MainThread INFO: Time Consumed:0.23334574699401855s
2024-01-23 01:02:22,301 MainThread INFO: Total Frames:53400s
  4%|▎         | 351/10000 [03:49<43:17,  3.72it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12188.95316
Train_Epoch_Reward                21094.52149
Running_Training_Average_Rewards  13632.04908
Explore_Time                      0.00090
Train___Time                      0.12359
Eval____Time                      0.10662
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12278.55022
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.25402     1.89606     93.67169     87.89140
alpha_0                           0.83904      0.00012     0.83920      0.83887
Alpha_loss                        -1.18125     0.00130     -1.17927     -1.18270
Training/policy_loss              -2.48714     0.00780     -2.47319     -2.49467
Training/qf1_loss                 9101.86621   1030.32565  11064.97461  8183.13965
Training/qf2_loss                 15257.63008  1106.53291  17380.58203  14282.75195
Training/pf_norm                  0.12727      0.04597     0.17120      0.04786
Training/qf1_norm                 5569.91484   234.80639   5957.61816   5234.00146
Training/qf2_norm                 309.12858    7.44218     319.84415    298.61798
log_std/mean                      -0.12634     0.00696     -0.11828     -0.13535
log_probs/mean                    -2.73459     0.00821     -2.71943     -2.74283
mean/mean                         -0.00039     0.00063     0.00021      -0.00152
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02168750762939453
epoch last part time3 0.002589702606201172
inside rlalgo, task 0, sumup 70347
epoch first part time 3.337860107421875e-06
replay_buffer._size: [53700]
collect time 0.0009031295776367188
inner_dict_sum {'sac_diff0': 0.00020766258239746094, 'sac_diff1': 0.007319927215576172, 'sac_diff2': 0.008558511734008789, 'sac_diff3': 0.010468721389770508, 'sac_diff4': 0.007331371307373047, 'sac_diff5': 0.03233623504638672, 'sac_diff6': 0.00038814544677734375, 'all': 0.06661057472229004}
diff5_list [0.0073451995849609375, 0.006189107894897461, 0.00614619255065918, 0.006255388259887695, 0.006400346755981445]
time3 0
time4 0.06737709045410156
time5 0.0674281120300293
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4214)
policy weight change tensor(37.7310, grad_fn=<SumBackward0>)
time8 0.0020520687103271484
train_time 0.07947707176208496
eval time 0.14986848831176758
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:22,561 MainThread INFO: EPOCH:351
2024-01-23 01:02:22,562 MainThread INFO: Time Consumed:0.23269367218017578s
2024-01-23 01:02:22,562 MainThread INFO: Total Frames:53550s
  4%|▎         | 352/10000 [03:50<42:42,  3.77it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12209.15438
Train_Epoch_Reward                4219.68126
Running_Training_Average_Rewards  13175.17192
Explore_Time                      0.00090
Train___Time                      0.07948
Eval____Time                      0.14987
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12214.89694
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.74837     1.50949    92.45078     88.25616
alpha_0                           0.83862      0.00012    0.83878      0.83845
Alpha_loss                        -1.18414     0.00152    -1.18214     -1.18661
Training/policy_loss              -2.49514     0.00372    -2.49023     -2.50000
Training/qf1_loss                 9689.28750   743.92733  10929.49805  8972.25586
Training/qf2_loss                 15635.88359  821.58021  16917.38672  14816.22168
Training/pf_norm                  0.10038      0.01854    0.12264      0.06628
Training/qf1_norm                 5588.58086   164.20535  5880.48340   5415.20996
Training/qf2_norm                 324.90712    5.47624    334.68845    319.87308
log_std/mean                      -0.13333     0.00004    -0.13329     -0.13339
log_probs/mean                    -2.73188     0.00465    -2.72614     -2.73823
mean/mean                         0.00029      0.00008    0.00039      0.00018
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.021410703659057617
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70347
epoch first part time 2.86102294921875e-06
replay_buffer._size: [53850]
collect time 0.0008862018585205078
inner_dict_sum {'sac_diff0': 0.00019621849060058594, 'sac_diff1': 0.006714820861816406, 'sac_diff2': 0.008354425430297852, 'sac_diff3': 0.010155200958251953, 'sac_diff4': 0.007032155990600586, 'sac_diff5': 0.03127169609069824, 'sac_diff6': 0.0003788471221923828, 'all': 0.06410336494445801}
diff5_list [0.006695747375488281, 0.006179094314575195, 0.006145477294921875, 0.00624394416809082, 0.00600743293762207]
time3 0
time4 0.06485795974731445
time5 0.06490683555603027
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4214)
policy weight change tensor(37.8113, grad_fn=<SumBackward0>)
time8 0.0019609928131103516
train_time 0.07652449607849121
eval time 0.15537810325622559
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:22,821 MainThread INFO: EPOCH:352
2024-01-23 01:02:22,822 MainThread INFO: Time Consumed:0.23510003089904785s
2024-01-23 01:02:22,822 MainThread INFO: Total Frames:53700s
  4%|▎         | 353/10000 [03:50<42:23,  3.79it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12228.45194
Train_Epoch_Reward                12424.74921
Running_Training_Average_Rewards  13434.64311
Explore_Time                      0.00088
Train___Time                      0.07652
Eval____Time                      0.15538
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12197.59004
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.05666     0.85570    92.45491     90.07034
alpha_0                           0.83820      0.00012    0.83837      0.83803
Alpha_loss                        -1.18716     0.00112    -1.18593     -1.18861
Training/policy_loss              -2.47941     0.00335    -2.47375     -2.48286
Training/qf1_loss                 10569.72773  671.50859  11375.45996  9628.18164
Training/qf2_loss                 16378.76387  636.50862  17210.35938  15530.07031
Training/pf_norm                  0.12229      0.02369    0.14879      0.07789
Training/qf1_norm                 5640.92021   88.32087   5785.48389   5537.12598
Training/qf2_norm                 307.99499    2.73969    312.45221    304.80457
log_std/mean                      -0.12771     0.00026    -0.12737     -0.12809
log_probs/mean                    -2.72992     0.00401    -2.72304     -2.73429
mean/mean                         0.00012      0.00010    0.00028      -0.00001
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.020782947540283203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70347
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [54000]
collect time 0.0009281635284423828
inner_dict_sum {'sac_diff0': 0.00020313262939453125, 'sac_diff1': 0.007007598876953125, 'sac_diff2': 0.008410930633544922, 'sac_diff3': 0.010545492172241211, 'sac_diff4': 0.007456779479980469, 'sac_diff5': 0.03271651268005371, 'sac_diff6': 0.0003924369812011719, 'all': 0.06673288345336914}
diff5_list [0.007981300354003906, 0.006190299987792969, 0.006067752838134766, 0.0063076019287109375, 0.006169557571411133]
time3 0
time4 0.06749629974365234
time5 0.06754660606384277
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4214)
policy weight change tensor(37.8274, grad_fn=<SumBackward0>)
time8 0.0019631385803222656
train_time 0.07967853546142578
eval time 0.15414023399353027
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:23,083 MainThread INFO: EPOCH:353
2024-01-23 01:02:23,083 MainThread INFO: Time Consumed:0.23702096939086914s
2024-01-23 01:02:23,084 MainThread INFO: Total Frames:53850s
  4%|▎         | 354/10000 [03:50<42:19,  3.80it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12242.73736
Train_Epoch_Reward                22384.50572
Running_Training_Average_Rewards  13831.42649
Explore_Time                      0.00092
Train___Time                      0.07968
Eval____Time                      0.15414
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12190.19201
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.15691     2.91459     94.11113     85.06522
alpha_0                           0.83778      0.00012     0.83795      0.83761
Alpha_loss                        -1.19210     0.00089     -1.19094     -1.19356
Training/policy_loss              -2.51614     0.00162     -2.51388     -2.51826
Training/qf1_loss                 9453.21260   1126.78987  10890.85352  7859.37744
Training/qf2_loss                 15679.93906  1359.58284  17448.13672  13642.13770
Training/pf_norm                  0.14790      0.01574     0.16708      0.12541
Training/qf1_norm                 5693.66426   336.25241   6159.42529   5113.67920
Training/qf2_norm                 342.21154    10.68626    356.68347    323.54361
log_std/mean                      -0.13525     0.00003     -0.13521     -0.13529
log_probs/mean                    -2.73880     0.00178     -2.73610     -2.74113
mean/mean                         -0.00220     0.00021     -0.00192     -0.00249
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021376609802246094
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70347
epoch first part time 3.337860107421875e-06
replay_buffer._size: [54150]
collect time 0.0008778572082519531
inner_dict_sum {'sac_diff0': 0.00019788742065429688, 'sac_diff1': 0.006682395935058594, 'sac_diff2': 0.007948160171508789, 'sac_diff3': 0.010110616683959961, 'sac_diff4': 0.006913900375366211, 'sac_diff5': 0.031449317932128906, 'sac_diff6': 0.0003921985626220703, 'all': 0.06369447708129883}
diff5_list [0.006433725357055664, 0.006152629852294922, 0.006411314010620117, 0.006142854690551758, 0.006308794021606445]
time3 0
time4 0.0644540786743164
time5 0.06450271606445312
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4214)
policy weight change tensor(37.7693, grad_fn=<SumBackward0>)
time8 0.001962900161743164
train_time 0.07628202438354492
eval time 0.1530320644378662
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:23,340 MainThread INFO: EPOCH:354
2024-01-23 01:02:23,341 MainThread INFO: Time Consumed:0.23253726959228516s
2024-01-23 01:02:23,341 MainThread INFO: Total Frames:54000s
  4%|▎         | 355/10000 [03:50<42:02,  3.82it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12260.55454
Train_Epoch_Reward                10650.25664
Running_Training_Average_Rewards  13819.68386
Explore_Time                      0.00087
Train___Time                      0.07628
Eval____Time                      0.15303
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12225.50965
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.43781     1.53016    92.07486     87.92632
alpha_0                           0.83736      0.00012    0.83753      0.83719
Alpha_loss                        -1.19419     0.00186    -1.19189     -1.19704
Training/policy_loss              -2.47248     0.00670    -2.46503     -2.48258
Training/qf1_loss                 10223.63398  665.87622  11211.21387  9292.51660
Training/qf2_loss                 16085.67520  751.58348  17169.34961  15056.23730
Training/pf_norm                  0.15588      0.03745    0.22882      0.12761
Training/qf1_norm                 5700.57412   167.36579  5876.76123   5427.46631
Training/qf2_norm                 301.05760    5.07688    306.47086    292.68118
log_std/mean                      -0.13285     0.00009    -0.13271     -0.13294
log_probs/mean                    -2.73154     0.00800    -2.72321     -2.74382
mean/mean                         -0.00060     0.00010    -0.00043     -0.00071
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.021512746810913086
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70347
epoch first part time 3.337860107421875e-06
replay_buffer._size: [54300]
collect time 0.0008571147918701172
inside mustsac before update, task 0, sumup 70347
inside mustsac after update, task 0, sumup 71235
inner_dict_sum {'sac_diff0': 0.00020122528076171875, 'sac_diff1': 0.00699615478515625, 'sac_diff2': 0.008713483810424805, 'sac_diff3': 0.010772943496704102, 'sac_diff4': 0.007601261138916016, 'sac_diff5': 0.056566476821899414, 'sac_diff6': 0.00046634674072265625, 'all': 0.09131789207458496}
diff5_list [0.010965585708618164, 0.009958744049072266, 0.009925365447998047, 0.009912490844726562, 0.015804290771484375]
time3 0.0009038448333740234
time4 0.0924067497253418
time5 0.09247756004333496
time7 0.010744571685791016
gen_weight_change tensor(-22.6518)
policy weight change tensor(37.8415, grad_fn=<SumBackward0>)
time8 0.0024335384368896484
train_time 0.12479496002197266
eval time 0.1089484691619873
epoch last part time 7.62939453125e-06
2024-01-23 01:02:23,602 MainThread INFO: EPOCH:355
2024-01-23 01:02:23,603 MainThread INFO: Time Consumed:0.23682737350463867s
2024-01-23 01:02:23,603 MainThread INFO: Total Frames:54150s
  4%|▎         | 356/10000 [03:51<42:04,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12285.47569
Train_Epoch_Reward                15796.28962
Running_Training_Average_Rewards  14115.34755
Explore_Time                      0.00085
Train___Time                      0.12479
Eval____Time                      0.10895
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12256.70536
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.64885     2.63018     94.57353     86.80092
alpha_0                           0.83694      0.00012     0.83711      0.83677
Alpha_loss                        -1.19791     0.00171     -1.19567     -1.20014
Training/policy_loss              -2.49000     0.01073     -2.47863     -2.50947
Training/qf1_loss                 9248.74375   1293.87700  11633.47363  8018.75488
Training/qf2_loss                 15427.34941  1424.17363  18088.68750  13841.21680
Training/pf_norm                  0.11153      0.03817     0.17858      0.06710
Training/qf1_norm                 5479.11836   301.55472   6066.24512   5227.22266
Training/qf2_norm                 317.18505    11.75633    329.26895    296.18332
log_std/mean                      -0.13043     0.00428     -0.12330     -0.13562
log_probs/mean                    -2.73353     0.00500     -2.72477     -2.73851
mean/mean                         -0.00160     0.00039     -0.00117     -0.00223
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021953582763671875
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71235
epoch first part time 5.0067901611328125e-06
replay_buffer._size: [54450]
collect time 0.0009698867797851562
inner_dict_sum {'sac_diff0': 0.0001990795135498047, 'sac_diff1': 0.008012056350708008, 'sac_diff2': 0.010032176971435547, 'sac_diff3': 0.011463403701782227, 'sac_diff4': 0.008411407470703125, 'sac_diff5': 0.03362226486206055, 'sac_diff6': 0.0003883838653564453, 'all': 0.0721287727355957}
diff5_list [0.00775456428527832, 0.006444454193115234, 0.006189584732055664, 0.006247997283935547, 0.006985664367675781]
time3 0
time4 0.0729074478149414
time5 0.07295870780944824
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6518)
policy weight change tensor(37.7471, grad_fn=<SumBackward0>)
time8 0.002001523971557617
train_time 0.08527517318725586
eval time 0.1486668586730957
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:23,865 MainThread INFO: EPOCH:356
2024-01-23 01:02:23,865 MainThread INFO: Time Consumed:0.23735690116882324s
2024-01-23 01:02:23,866 MainThread INFO: Total Frames:54300s
  4%|▎         | 357/10000 [03:51<42:14,  3.81it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12268.77365
Train_Epoch_Reward                5672.08473
Running_Training_Average_Rewards  14097.07401
Explore_Time                      0.00097
Train___Time                      0.08528
Eval____Time                      0.14867
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12284.11156
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.51767     1.63713    91.92770     87.02224
alpha_0                           0.83652      0.00012    0.83669      0.83636
Alpha_loss                        -1.20213     0.00094    -1.20033     -1.20303
Training/policy_loss              -2.50364     0.00293    -2.50096     -2.50822
Training/qf1_loss                 9261.06484   627.94958  10373.23242  8581.58496
Training/qf2_loss                 15497.55801  729.20650  16816.77148  14732.40820
Training/pf_norm                  0.11753      0.03528    0.18061      0.08112
Training/qf1_norm                 5280.74131   186.66101  5554.35010   4999.69775
Training/qf2_norm                 327.66579    5.79733    336.23648    318.83139
log_std/mean                      -0.14040     0.00018    -0.14010     -0.14058
log_probs/mean                    -2.73836     0.00333    -2.73550     -2.74356
mean/mean                         -0.00154     0.00002    -0.00151     -0.00157
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02438187599182129
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71235
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [54600]
collect time 0.001050710678100586
inner_dict_sum {'sac_diff0': 0.00020194053649902344, 'sac_diff1': 0.007088184356689453, 'sac_diff2': 0.008453369140625, 'sac_diff3': 0.010404348373413086, 'sac_diff4': 0.0069081783294677734, 'sac_diff5': 0.03162527084350586, 'sac_diff6': 0.0004131793975830078, 'all': 0.0650944709777832}
diff5_list [0.006733417510986328, 0.006201028823852539, 0.006197214126586914, 0.006074666976928711, 0.006418943405151367]
time3 0
time4 0.06585431098937988
time5 0.06589984893798828
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6518)
policy weight change tensor(37.6809, grad_fn=<SumBackward0>)
time8 0.0019240379333496094
train_time 0.07830810546875
eval time 0.1489419937133789
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:24,124 MainThread INFO: EPOCH:357
2024-01-23 01:02:24,124 MainThread INFO: Time Consumed:0.23073315620422363s
2024-01-23 01:02:24,125 MainThread INFO: Total Frames:54450s
  4%|▎         | 358/10000 [03:51<49:44,  3.23it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12259.54156
Train_Epoch_Reward                4816.32155
Running_Training_Average_Rewards  14140.67630
Explore_Time                      0.00104
Train___Time                      0.07831
Eval____Time                      0.14894
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12308.60458
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.37307     1.92485     93.36985     88.19823
alpha_0                           0.83610      0.00012     0.83627      0.83594
Alpha_loss                        -1.20475     0.00053     -1.20392     -1.20524
Training/policy_loss              -2.51792     0.00405     -2.51273     -2.52263
Training/qf1_loss                 10283.79531  1241.70091  11216.76270  7837.86719
Training/qf2_loss                 16898.66953  1405.79567  18034.99609  14128.38867
Training/pf_norm                  0.10479      0.01165     0.12763      0.09557
Training/qf1_norm                 5506.59814   228.52534   5738.49951   5120.10938
Training/qf2_norm                 349.42188    7.20436     356.72833    337.50409
log_std/mean                      -0.12851     0.00009     -0.12843     -0.12868
log_probs/mean                    -2.73416     0.00451     -2.72821     -2.73951
mean/mean                         -0.00363     0.00006     -0.00354     -0.00369
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.18371057510375977
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71235
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [54750]
collect time 0.0009245872497558594
inner_dict_sum {'sac_diff0': 0.0002033710479736328, 'sac_diff1': 0.006975889205932617, 'sac_diff2': 0.00812983512878418, 'sac_diff3': 0.010574579238891602, 'sac_diff4': 0.0068700313568115234, 'sac_diff5': 0.03161501884460449, 'sac_diff6': 0.0004296302795410156, 'all': 0.06479835510253906}
diff5_list [0.006567239761352539, 0.006337881088256836, 0.006166696548461914, 0.0062084197998046875, 0.006334781646728516]
time3 0
time4 0.06554865837097168
time5 0.06559491157531738
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6518)
policy weight change tensor(37.6286, grad_fn=<SumBackward0>)
time8 0.0019483566284179688
train_time 0.07742667198181152
eval time 0.0005145072937011719
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:24,393 MainThread INFO: EPOCH:358
2024-01-23 01:02:24,393 MainThread INFO: Time Consumed:0.08112406730651855s
2024-01-23 01:02:24,393 MainThread INFO: Total Frames:54600s
  4%|▎         | 359/10000 [03:52<39:47,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12257.96118
Train_Epoch_Reward                14910.24846
Running_Training_Average_Rewards  14283.06296
Explore_Time                      0.00092
Train___Time                      0.07743
Eval____Time                      0.00051
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12324.13827
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.47175     2.18688     92.36035     86.09353
alpha_0                           0.83569      0.00012     0.83585      0.83552
Alpha_loss                        -1.20767     0.00058     -1.20676     -1.20844
Training/policy_loss              -2.51809     0.00226     -2.51476     -2.52076
Training/qf1_loss                 8654.01650   1285.43228  10798.46191  7615.12988
Training/qf2_loss                 15614.75430  1489.65191  18063.26562  14214.04688
Training/pf_norm                  0.11183      0.01295     0.12833      0.09270
Training/qf1_norm                 5086.54316   277.57111   5466.50928   4659.86523
Training/qf2_norm                 347.53953    8.49214     358.78088    334.46066
log_std/mean                      -0.13711     0.00014     -0.13692     -0.13732
log_probs/mean                    -2.73162     0.00220     -2.72841     -2.73410
mean/mean                         -0.00169     0.00003     -0.00162     -0.00171
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018574237823486328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71235
epoch first part time 2.86102294921875e-06
replay_buffer._size: [54875]
collect time 0.0009210109710693359
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.007245540618896484, 'sac_diff2': 0.008109807968139648, 'sac_diff3': 0.010806083679199219, 'sac_diff4': 0.007477283477783203, 'sac_diff5': 0.032451629638671875, 'sac_diff6': 0.00039267539978027344, 'all': 0.06669378280639648}
diff5_list [0.0071218013763427734, 0.006292581558227539, 0.0066072940826416016, 0.0062389373779296875, 0.0061910152435302734]
time3 0
time4 0.06748318672180176
time5 0.06753301620483398
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6518)
policy weight change tensor(37.6632, grad_fn=<SumBackward0>)
time8 0.0018880367279052734
train_time 0.07895112037658691
eval time 0.13552355766296387
epoch last part time 7.3909759521484375e-06
2024-01-23 01:02:24,633 MainThread INFO: EPOCH:359
2024-01-23 01:02:24,633 MainThread INFO: Time Consumed:0.21785783767700195s
2024-01-23 01:02:24,633 MainThread INFO: Total Frames:54750s
  4%|▎         | 360/10000 [03:52<39:25,  4.08it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12260.44369
Train_Epoch_Reward                13413.42975
Running_Training_Average_Rewards  14229.78758
Explore_Time                      0.00092
Train___Time                      0.07895
Eval____Time                      0.13552
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12324.13827
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.82101     1.50279     90.66424     86.16782
alpha_0                           0.83527      0.00012     0.83544      0.83510
Alpha_loss                        -1.21156     0.00226     -1.20797     -1.21486
Training/policy_loss              -2.48469     0.00776     -2.47130     -2.49340
Training/qf1_loss                 8460.70557   1077.71939  9824.71094   6946.66309
Training/qf2_loss                 15065.68184  1201.66864  16593.40039  13496.11914
Training/pf_norm                  0.12327      0.02685     0.15543      0.07501
Training/qf1_norm                 5344.57148   192.65778   5594.90039   5011.99268
Training/qf2_norm                 313.86029    5.04966     319.82074    304.84833
log_std/mean                      -0.12745     0.00009     -0.12738     -0.12760
log_probs/mean                    -2.73452     0.00940     -2.71833     -2.74537
mean/mean                         -0.00104     0.00019     -0.00078     -0.00129
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01864027976989746
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71235
epoch first part time 3.337860107421875e-06
replay_buffer._size: [55050]
collect time 0.0008599758148193359
inside mustsac before update, task 0, sumup 71235
inside mustsac after update, task 0, sumup 70481
inner_dict_sum {'sac_diff0': 0.0002186298370361328, 'sac_diff1': 0.0077822208404541016, 'sac_diff2': 0.009131669998168945, 'sac_diff3': 0.01137852668762207, 'sac_diff4': 0.00801706314086914, 'sac_diff5': 0.05177140235900879, 'sac_diff6': 0.00045180320739746094, 'all': 0.08875131607055664}
diff5_list [0.011677980422973633, 0.010748624801635742, 0.009927511215209961, 0.009675025939941406, 0.009742259979248047]
time3 0.0009281635284423828
time4 0.0896599292755127
time5 0.08971452713012695
time7 0.00943613052368164
gen_weight_change tensor(-22.8480)
policy weight change tensor(37.7179, grad_fn=<SumBackward0>)
time8 0.0028438568115234375
train_time 0.121307373046875
eval time 0.11060833930969238
epoch last part time 5.9604644775390625e-06
2024-01-23 01:02:24,890 MainThread INFO: EPOCH:360
2024-01-23 01:02:24,890 MainThread INFO: Time Consumed:0.23510980606079102s
2024-01-23 01:02:24,890 MainThread INFO: Total Frames:54900s
  4%|▎         | 361/10000 [03:52<40:14,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12257.61383
Train_Epoch_Reward                20378.78290
Running_Training_Average_Rewards  14533.37168
Explore_Time                      0.00085
Train___Time                      0.12131
Eval____Time                      0.11061
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12250.25165
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.22118     2.16460     94.72252     88.78858
alpha_0                           0.83485      0.00012     0.83502      0.83468
Alpha_loss                        -1.21546     0.00124     -1.21350     -1.21695
Training/policy_loss              -2.50441     0.01110     -2.48916     -2.51567
Training/qf1_loss                 10111.31699  1169.01191  12116.97949  8473.59961
Training/qf2_loss                 16809.31172  1316.28548  19040.09180  14899.00586
Training/pf_norm                  0.12051      0.00883     0.13206      0.10808
Training/qf1_norm                 5730.11143   284.38788   5995.79688   5239.45020
Training/qf2_norm                 337.87677    12.68534    356.33514    316.68503
log_std/mean                      -0.12994     0.00885     -0.11682     -0.14243
log_probs/mean                    -2.73749     0.00584     -2.73157     -2.74667
mean/mean                         -0.00107     0.00053     -0.00033     -0.00182
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020624399185180664
epoch last part time3 0.0030815601348876953
inside rlalgo, task 0, sumup 70481
epoch first part time 2.86102294921875e-06
replay_buffer._size: [55200]
collect time 0.0010259151458740234
inner_dict_sum {'sac_diff0': 0.0002048015594482422, 'sac_diff1': 0.006917715072631836, 'sac_diff2': 0.00845026969909668, 'sac_diff3': 0.011461973190307617, 'sac_diff4': 0.0075228214263916016, 'sac_diff5': 0.03242135047912598, 'sac_diff6': 0.0003867149353027344, 'all': 0.06736564636230469}
diff5_list [0.006689786911010742, 0.006707191467285156, 0.0064008235931396484, 0.006418704986572266, 0.006204843521118164]
time3 0
time4 0.06812119483947754
time5 0.06816864013671875
time7 4.76837158203125e-07
gen_weight_change tensor(-22.8480)
policy weight change tensor(37.7915, grad_fn=<SumBackward0>)
time8 0.0020279884338378906
train_time 0.07998442649841309
eval time 0.15162038803100586
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:25,152 MainThread INFO: EPOCH:361
2024-01-23 01:02:25,152 MainThread INFO: Time Consumed:0.23499774932861328s
2024-01-23 01:02:25,153 MainThread INFO: Total Frames:55050s
  4%|▎         | 362/10000 [03:52<40:32,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12307.46476
Train_Epoch_Reward                22759.13940
Running_Training_Average_Rewards  13366.30375
Explore_Time                      0.00102
Train___Time                      0.07998
Eval____Time                      0.15162
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12713.40623
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.76974     1.73924     94.59314     89.81874
alpha_0                           0.83443      0.00012     0.83460      0.83427
Alpha_loss                        -1.21757     0.00170     -1.21512     -1.21993
Training/policy_loss              -2.49343     0.00359     -2.48880     -2.49758
Training/qf1_loss                 10147.77813  1132.62967  11339.03809  8079.82715
Training/qf2_loss                 16788.57129  1273.27679  18215.63672  14529.93262
Training/pf_norm                  0.11765      0.01918     0.15407      0.09756
Training/qf1_norm                 5867.27031   217.98328   6220.38574   5618.29346
Training/qf2_norm                 334.26138    6.10151     343.92154    327.33640
log_std/mean                      -0.13057     0.00016     -0.13036     -0.13079
log_probs/mean                    -2.73052     0.00465     -2.72440     -2.73611
mean/mean                         -0.00204     0.00006     -0.00197     -0.00214
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01836538314819336
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70481
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [55350]
collect time 0.0008575916290283203
inner_dict_sum {'sac_diff0': 0.00020551681518554688, 'sac_diff1': 0.007270097732543945, 'sac_diff2': 0.008100509643554688, 'sac_diff3': 0.010493040084838867, 'sac_diff4': 0.007197856903076172, 'sac_diff5': 0.031945228576660156, 'sac_diff6': 0.00039005279541015625, 'all': 0.06560230255126953}
diff5_list [0.0065953731536865234, 0.006253480911254883, 0.006328105926513672, 0.006604671478271484, 0.006163597106933594]
time3 0
time4 0.0664224624633789
time5 0.06647372245788574
time7 7.152557373046875e-07
gen_weight_change tensor(-22.8480)
policy weight change tensor(37.8521, grad_fn=<SumBackward0>)
time8 0.0019457340240478516
train_time 0.07799839973449707
eval time 0.15770888328552246
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:25,413 MainThread INFO: EPOCH:362
2024-01-23 01:02:25,413 MainThread INFO: Time Consumed:0.23899555206298828s
2024-01-23 01:02:25,414 MainThread INFO: Total Frames:55200s
  4%|▎         | 363/10000 [03:53<40:56,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12355.59988
Train_Epoch_Reward                31543.07526
Running_Training_Average_Rewards  13843.02892
Explore_Time                      0.00085
Train___Time                      0.07800
Eval____Time                      0.15771
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12678.94124
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.17639     2.24670     96.37158     89.96660
alpha_0                           0.83402      0.00012     0.83418      0.83385
Alpha_loss                        -1.22175     0.00153     -1.21979     -1.22389
Training/policy_loss              -2.50537     0.00237     -2.50277     -2.50860
Training/qf1_loss                 10184.55098  1530.17136  12853.11816  8662.41211
Training/qf2_loss                 17026.55176  1719.88508  20133.08398  15373.38867
Training/pf_norm                  0.09408      0.03674     0.15088      0.04763
Training/qf1_norm                 5793.62754   283.94603   6330.93750   5537.67822
Training/qf2_norm                 334.73668    7.94465     349.50748    326.80057
log_std/mean                      -0.13195     0.00011     -0.13180     -0.13208
log_probs/mean                    -2.73499     0.00326     -2.73156     -2.73940
mean/mean                         -0.00100     0.00009     -0.00087     -0.00111
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0181429386138916
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70481
epoch first part time 2.86102294921875e-06
replay_buffer._size: [55500]
collect time 0.0009567737579345703
inner_dict_sum {'sac_diff0': 0.00022864341735839844, 'sac_diff1': 0.007074117660522461, 'sac_diff2': 0.008265972137451172, 'sac_diff3': 0.010911941528320312, 'sac_diff4': 0.007340908050537109, 'sac_diff5': 0.032956838607788086, 'sac_diff6': 0.0004112720489501953, 'all': 0.06718969345092773}
diff5_list [0.007038116455078125, 0.006916522979736328, 0.0063550472259521484, 0.006407976150512695, 0.006239175796508789]
time3 0
time4 0.06800603866577148
time5 0.06805825233459473
time7 7.152557373046875e-07
gen_weight_change tensor(-22.8480)
policy weight change tensor(37.8539, grad_fn=<SumBackward0>)
time8 0.0019867420196533203
train_time 0.07954621315002441
eval time 0.1513051986694336
epoch last part time 7.152557373046875e-06
2024-01-23 01:02:25,670 MainThread INFO: EPOCH:363
2024-01-23 01:02:25,670 MainThread INFO: Time Consumed:0.23434782028198242s
2024-01-23 01:02:25,670 MainThread INFO: Total Frames:55350s
  4%|▎         | 364/10000 [03:53<41:03,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12397.58521
Train_Epoch_Reward                39891.85450
Running_Training_Average_Rewards  15044.90567
Explore_Time                      0.00095
Train___Time                      0.07955
Eval____Time                      0.15131
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12610.04525
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.24682     1.66971     92.94602     88.55200
alpha_0                           0.83360      0.00012     0.83377      0.83343
Alpha_loss                        -1.22570     0.00168     -1.22366     -1.22834
Training/policy_loss              -2.49616     0.00496     -2.49007     -2.50147
Training/qf1_loss                 8487.15537   1430.99188  11313.27637  7416.77637
Training/qf2_loss                 15855.53359  1602.35761  18989.28320  14584.14062
Training/pf_norm                  0.12329      0.00782     0.13332      0.11017
Training/qf1_norm                 4885.32480   237.23238   5287.30615   4631.35596
Training/qf2_norm                 324.49698    5.59342     333.39111    318.88177
log_std/mean                      -0.13270     0.00006     -0.13262     -0.13277
log_probs/mean                    -2.73821     0.00604     -2.73072     -2.74529
mean/mean                         -0.00144     0.00008     -0.00134     -0.00155
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019268512725830078
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70481
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [55650]
collect time 0.0010523796081542969
inner_dict_sum {'sac_diff0': 0.0002231597900390625, 'sac_diff1': 0.006598234176635742, 'sac_diff2': 0.008067607879638672, 'sac_diff3': 0.010361671447753906, 'sac_diff4': 0.007053375244140625, 'sac_diff5': 0.03180551528930664, 'sac_diff6': 0.0003895759582519531, 'all': 0.0644991397857666}
diff5_list [0.006646394729614258, 0.006154537200927734, 0.00605320930480957, 0.006846427917480469, 0.006104946136474609]
time3 0
time4 0.06526398658752441
time5 0.06530904769897461
time7 7.152557373046875e-07
gen_weight_change tensor(-22.8480)
policy weight change tensor(37.8261, grad_fn=<SumBackward0>)
time8 0.0019004344940185547
train_time 0.07671999931335449
eval time 0.1537013053894043
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:25,927 MainThread INFO: EPOCH:364
2024-01-23 01:02:25,927 MainThread INFO: Time Consumed:0.23391389846801758s
2024-01-23 01:02:25,927 MainThread INFO: Total Frames:55500s
  4%|▎         | 365/10000 [03:53<41:08,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12429.99374
Train_Epoch_Reward                50652.64173
Running_Training_Average_Rewards  16416.40459
Explore_Time                      0.00105
Train___Time                      0.07672
Eval____Time                      0.15370
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12549.59496
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.29632     1.88892     93.99104     88.34740
alpha_0                           0.83318      0.00012     0.83335      0.83302
Alpha_loss                        -1.22878     0.00111     -1.22675     -1.22989
Training/policy_loss              -2.49260     0.00459     -2.48808     -2.49875
Training/qf1_loss                 10372.10410  1036.02953  11862.38867  9143.75391
Training/qf2_loss                 16889.79004  1204.68775  18628.55859  15362.31543
Training/pf_norm                  0.10698      0.02416     0.13441      0.07481
Training/qf1_norm                 5729.86865   223.11175   6052.59033   5381.12646
Training/qf2_norm                 326.26515    6.53227     335.71799    316.04391
log_std/mean                      -0.13068     0.00003     -0.13063     -0.13073
log_probs/mean                    -2.73661     0.00537     -2.73133     -2.74358
mean/mean                         -0.00029     0.00003     -0.00026     -0.00033
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01976633071899414
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70481
epoch first part time 3.337860107421875e-06
replay_buffer._size: [55800]
collect time 0.0009043216705322266
inside mustsac before update, task 0, sumup 70481
inside mustsac after update, task 0, sumup 70803
inner_dict_sum {'sac_diff0': 0.000217437744140625, 'sac_diff1': 0.007176876068115234, 'sac_diff2': 0.008240461349487305, 'sac_diff3': 0.010608673095703125, 'sac_diff4': 0.00770115852355957, 'sac_diff5': 0.05245518684387207, 'sac_diff6': 0.0004248619079589844, 'all': 0.08682465553283691}
diff5_list [0.01198434829711914, 0.009827375411987305, 0.009716987609863281, 0.011078596115112305, 0.009847879409790039]
time3 0.0008814334869384766
time4 0.08770751953125
time5 0.08777260780334473
time7 0.009010553359985352
gen_weight_change tensor(-23.0202)
policy weight change tensor(37.8620, grad_fn=<SumBackward0>)
time8 0.001922607421875
train_time 0.11753082275390625
eval time 0.12048149108886719
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:26,191 MainThread INFO: EPOCH:365
2024-01-23 01:02:26,192 MainThread INFO: Time Consumed:0.24135780334472656s
2024-01-23 01:02:26,192 MainThread INFO: Total Frames:55650s
  4%|▎         | 366/10000 [03:53<41:31,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12449.88346
Train_Epoch_Reward                2949.78590
Running_Training_Average_Rewards  16077.18632
Explore_Time                      0.00090
Train___Time                      0.11753
Eval____Time                      0.12048
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12455.60261
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.29176     2.20101     91.41171     85.36195
alpha_0                           0.83277      0.00012     0.83293      0.83260
Alpha_loss                        -1.23131     0.00123     -1.22981     -1.23355
Training/policy_loss              -2.49251     0.01096     -2.47807     -2.51113
Training/qf1_loss                 8822.60537   862.23185   9931.88086   7512.43506
Training/qf2_loss                 15578.92715  1252.30060  16777.10547  13318.61426
Training/pf_norm                  0.11024      0.01018     0.12506      0.09477
Training/qf1_norm                 5236.07324   112.06490   5382.64746   5054.52783
Training/qf2_norm                 326.23600    12.46514    339.65961    306.41605
log_std/mean                      -0.13154     0.00521     -0.12321     -0.13932
log_probs/mean                    -2.73201     0.00621     -2.72380     -2.73976
mean/mean                         -0.00013     0.00098     0.00119      -0.00124
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01889801025390625
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70803
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [55950]
collect time 0.0009396076202392578
inner_dict_sum {'sac_diff0': 0.00021696090698242188, 'sac_diff1': 0.007292747497558594, 'sac_diff2': 0.00860285758972168, 'sac_diff3': 0.0109405517578125, 'sac_diff4': 0.0075740814208984375, 'sac_diff5': 0.03341794013977051, 'sac_diff6': 0.0004112720489501953, 'all': 0.06845641136169434}
diff5_list [0.0070302486419677734, 0.00726318359375, 0.006371736526489258, 0.006515979766845703, 0.0062367916107177734]
time3 0
time4 0.06930017471313477
time5 0.0693509578704834
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0202)
policy weight change tensor(37.7963, grad_fn=<SumBackward0>)
time8 0.002045869827270508
train_time 0.08072090148925781
eval time 0.15469861030578613
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:26,453 MainThread INFO: EPOCH:366
2024-01-23 01:02:26,453 MainThread INFO: Time Consumed:0.23884987831115723s
2024-01-23 01:02:26,453 MainThread INFO: Total Frames:55800s
  4%|▎         | 367/10000 [03:54<41:46,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12439.95154
Train_Epoch_Reward                6997.60248
Running_Training_Average_Rewards  15687.59621
Explore_Time                      0.00093
Train___Time                      0.08072
Eval____Time                      0.15470
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12184.79229
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.05555     2.32183    95.87941     89.28596
alpha_0                           0.83235      0.00012    0.83252      0.83218
Alpha_loss                        -1.23436     0.00072    -1.23334     -1.23526
Training/policy_loss              -2.46182     0.00395    -2.45579     -2.46733
Training/qf1_loss                 10205.76738  735.10684  11478.19336  9303.35254
Training/qf2_loss                 17088.19805  900.24480  18613.07812  15809.60156
Training/pf_norm                  0.08220      0.01934    0.11764      0.05887
Training/qf1_norm                 5763.04287   279.96282  6118.92432   5314.92920
Training/qf2_norm                 312.69658    7.50316    321.93774    300.51791
log_std/mean                      -0.13368     0.00017    -0.13347     -0.13392
log_probs/mean                    -2.73031     0.00453    -2.72319     -2.73670
mean/mean                         -0.00079     0.00004    -0.00074     -0.00085
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02132391929626465
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70803
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [56100]
collect time 0.0009655952453613281
inner_dict_sum {'sac_diff0': 0.0002319812774658203, 'sac_diff1': 0.00753331184387207, 'sac_diff2': 0.008846521377563477, 'sac_diff3': 0.011513948440551758, 'sac_diff4': 0.007961750030517578, 'sac_diff5': 0.03541135787963867, 'sac_diff6': 0.00040340423583984375, 'all': 0.07190227508544922}
diff5_list [0.008177995681762695, 0.00803518295288086, 0.006739377975463867, 0.006442070007324219, 0.006016731262207031]
time3 0
time4 0.07266998291015625
time5 0.07271885871887207
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0202)
policy weight change tensor(37.7401, grad_fn=<SumBackward0>)
time8 0.0019135475158691406
train_time 0.0842738151550293
eval time 0.14906048774719238
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:26,715 MainThread INFO: EPOCH:367
2024-01-23 01:02:26,715 MainThread INFO: Time Consumed:0.23658204078674316s
2024-01-23 01:02:26,715 MainThread INFO: Total Frames:55950s
  4%|▎         | 368/10000 [03:54<41:42,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12426.21369
Train_Epoch_Reward                32812.23170
Running_Training_Average_Rewards  16473.04760
Explore_Time                      0.00096
Train___Time                      0.08427
Eval____Time                      0.14906
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12171.22617
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.27635     2.72861     93.85702     86.57956
alpha_0                           0.83193      0.00012     0.83210      0.83177
Alpha_loss                        -1.23839     0.00144     -1.23604     -1.23988
Training/policy_loss              -2.49515     0.00226     -2.49123     -2.49755
Training/qf1_loss                 10012.80674  1625.55192  12240.64844  8082.21338
Training/qf2_loss                 16429.30918  1845.67838  19111.56836  14259.33789
Training/pf_norm                  0.12556      0.02966     0.16191      0.07389
Training/qf1_norm                 5468.87568   339.49695   6034.19922   5120.45508
Training/qf2_norm                 324.36664    9.57513     340.53900    315.09955
log_std/mean                      -0.13147     0.00011     -0.13132     -0.13163
log_probs/mean                    -2.73388     0.00309     -2.72845     -2.73718
mean/mean                         0.00055      0.00011     0.00065      0.00036
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018378257751464844
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70803
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [56250]
collect time 0.0009086132049560547
inner_dict_sum {'sac_diff0': 0.00024271011352539062, 'sac_diff1': 0.0065975189208984375, 'sac_diff2': 0.007850408554077148, 'sac_diff3': 0.009992837905883789, 'sac_diff4': 0.006685495376586914, 'sac_diff5': 0.03122854232788086, 'sac_diff6': 0.0003745555877685547, 'all': 0.0629720687866211}
diff5_list [0.0064640045166015625, 0.0062329769134521484, 0.00632166862487793, 0.00623321533203125, 0.005976676940917969]
time3 0
time4 0.06370425224304199
time5 0.06374621391296387
time7 9.5367431640625e-07
gen_weight_change tensor(-23.0202)
policy weight change tensor(37.6476, grad_fn=<SumBackward0>)
time8 0.001834869384765625
train_time 0.07467770576477051
eval time 0.1561439037322998
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:26,971 MainThread INFO: EPOCH:368
2024-01-23 01:02:26,971 MainThread INFO: Time Consumed:0.23396015167236328s
2024-01-23 01:02:26,971 MainThread INFO: Total Frames:56100s
  4%|▎         | 369/10000 [03:54<41:30,  3.87it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12407.67266
Train_Epoch_Reward                2249.64665
Running_Training_Average_Rewards  16115.00963
Explore_Time                      0.00090
Train___Time                      0.07468
Eval____Time                      0.15614
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12138.72795
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.93843     1.38464    91.76805     88.51498
alpha_0                           0.83152      0.00012    0.83168      0.83135
Alpha_loss                        -1.24282     0.00093    -1.24147     -1.24430
Training/policy_loss              -2.50409     0.00673    -2.49135     -2.51122
Training/qf1_loss                 9049.16064   873.62052  9805.49902   7891.45801
Training/qf2_loss                 16012.66406  997.69469  16912.27930  14682.65918
Training/pf_norm                  0.11678      0.02064    0.14959      0.09785
Training/qf1_norm                 5218.08242   174.80447  5454.14453   5046.96094
Training/qf2_norm                 333.31094    5.01537    340.10712    328.01987
log_std/mean                      -0.13558     0.00020    -0.13528     -0.13582
log_probs/mean                    -2.73967     0.00772    -2.72504     -2.74756
mean/mean                         0.00034      0.00007    0.00044      0.00024
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018265247344970703
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70803
epoch first part time 2.384185791015625e-06
replay_buffer._size: [56400]
collect time 0.0009264945983886719
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.006607770919799805, 'sac_diff2': 0.007698774337768555, 'sac_diff3': 0.009935379028320312, 'sac_diff4': 0.00669407844543457, 'sac_diff5': 0.031232595443725586, 'sac_diff6': 0.0003981590270996094, 'all': 0.06278109550476074}
diff5_list [0.006409883499145508, 0.005926847457885742, 0.00604557991027832, 0.006569862365722656, 0.006280422210693359]
time3 0
time4 0.0635080337524414
time5 0.0635519027709961
time7 4.76837158203125e-07
gen_weight_change tensor(-23.0202)
policy weight change tensor(37.5807, grad_fn=<SumBackward0>)
time8 0.0019257068634033203
train_time 0.07427811622619629
eval time 0.15047979354858398
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:27,220 MainThread INFO: EPOCH:369
2024-01-23 01:02:27,221 MainThread INFO: Time Consumed:0.2279798984527588s
2024-01-23 01:02:27,221 MainThread INFO: Total Frames:56250s
  4%|▎         | 370/10000 [03:54<41:03,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12386.12324
Train_Epoch_Reward                15645.17401
Running_Training_Average_Rewards  16276.72112
Explore_Time                      0.00092
Train___Time                      0.07428
Eval____Time                      0.15048
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12108.64402
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.97085     2.47789     96.79084     90.05550
alpha_0                           0.83110      0.00012     0.83127      0.83093
Alpha_loss                        -1.24502     0.00194     -1.24259     -1.24743
Training/policy_loss              -2.48537     0.00463     -2.48075     -2.49204
Training/qf1_loss                 10154.87773  1059.41555  11664.72656  8386.69629
Training/qf2_loss                 17466.25078  1299.92137  19506.54492  15456.59961
Training/pf_norm                  0.09587      0.01724     0.11799      0.07197
Training/qf1_norm                 5567.50762   342.09573   6231.99023   5305.95117
Training/qf2_norm                 331.90200    8.60831     348.61310    325.12878
log_std/mean                      -0.12008     0.00005     -0.12000     -0.12016
log_probs/mean                    -2.73334     0.00584     -2.72747     -2.74149
mean/mean                         -0.00042     0.00002     -0.00040     -0.00046
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.017893075942993164
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70803
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [56550]
collect time 0.0009362697601318359
inside mustsac before update, task 0, sumup 70803
inside mustsac after update, task 0, sumup 70487
inner_dict_sum {'sac_diff0': 0.0001990795135498047, 'sac_diff1': 0.00648188591003418, 'sac_diff2': 0.008242130279541016, 'sac_diff3': 0.010494470596313477, 'sac_diff4': 0.007244586944580078, 'sac_diff5': 0.049422502517700195, 'sac_diff6': 0.0004029273986816406, 'all': 0.08248758316040039}
diff5_list [0.010103225708007812, 0.009564638137817383, 0.010362625122070312, 0.009485721588134766, 0.009906291961669922]
time3 0.0008997917175292969
time4 0.08329916000366211
time5 0.08334732055664062
time7 0.009128570556640625
gen_weight_change tensor(-22.8872)
policy weight change tensor(37.5997, grad_fn=<SumBackward0>)
time8 0.002506256103515625
train_time 0.11300325393676758
eval time 0.11347794532775879
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:27,471 MainThread INFO: EPOCH:370
2024-01-23 01:02:27,472 MainThread INFO: Time Consumed:0.22958159446716309s
2024-01-23 01:02:27,472 MainThread INFO: Total Frames:56400s
  4%|▎         | 371/10000 [03:55<40:56,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12376.86964
Train_Epoch_Reward                9234.74426
Running_Training_Average_Rewards  15554.19392
Explore_Time                      0.00093
Train___Time                      0.11300
Eval____Time                      0.11348
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12157.71569
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.07595     2.40152     94.36920     88.06264
alpha_0                           0.83069      0.00012     0.83085      0.83052
Alpha_loss                        -1.24848     0.00115     -1.24684     -1.25027
Training/policy_loss              -2.50115     0.00921     -2.48787     -2.51586
Training/qf1_loss                 9057.70107   1133.94847  10461.96680  7411.10938
Training/qf2_loss                 15908.36699  1317.63183  18015.83789  14341.11719
Training/pf_norm                  0.13746      0.01700     0.15378      0.10656
Training/qf1_norm                 5492.28398   381.99028   5896.96973   4852.43604
Training/qf2_norm                 340.98314    4.16353     346.21774    335.73154
log_std/mean                      -0.12991     0.00456     -0.12405     -0.13639
log_probs/mean                    -2.73382     0.00630     -2.72496     -2.74157
mean/mean                         -0.00057     0.00051     0.00035      -0.00107
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.017745256423950195
epoch last part time3 0.0026121139526367188
inside rlalgo, task 0, sumup 70487
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [56700]
collect time 0.0009024143218994141
inner_dict_sum {'sac_diff0': 0.00023984909057617188, 'sac_diff1': 0.006549358367919922, 'sac_diff2': 0.007845163345336914, 'sac_diff3': 0.010192632675170898, 'sac_diff4': 0.006991386413574219, 'sac_diff5': 0.03134560585021973, 'sac_diff6': 0.0003802776336669922, 'all': 0.06354427337646484}
diff5_list [0.006348371505737305, 0.006157398223876953, 0.00626683235168457, 0.0064334869384765625, 0.006139516830444336]
time3 0
time4 0.06427788734436035
time5 0.06432247161865234
time7 4.76837158203125e-07
gen_weight_change tensor(-22.8872)
policy weight change tensor(37.5153, grad_fn=<SumBackward0>)
time8 0.0018570423126220703
train_time 0.07485270500183105
eval time 0.14917516708374023
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:27,722 MainThread INFO: EPOCH:371
2024-01-23 01:02:27,722 MainThread INFO: Time Consumed:0.22715044021606445s
2024-01-23 01:02:27,723 MainThread INFO: Total Frames:56550s
  4%|▎         | 372/10000 [03:55<40:37,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12305.38568
Train_Epoch_Reward                4443.31309
Running_Training_Average_Rewards  15422.97394
Explore_Time                      0.00090
Train___Time                      0.07485
Eval____Time                      0.14918
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11998.56664
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.34166     2.45308     93.28633     87.10701
alpha_0                           0.83027      0.00012     0.83044      0.83010
Alpha_loss                        -1.25181     0.00068     -1.25096     -1.25286
Training/policy_loss              -2.48507     0.00478     -2.47635     -2.49035
Training/qf1_loss                 8525.18672   1410.19818  9996.14453   6768.10840
Training/qf2_loss                 15882.06035  1619.19933  17649.72266  13929.23438
Training/pf_norm                  0.09466      0.02399     0.12703      0.06278
Training/qf1_norm                 4983.97520   343.61675   5387.05420   4522.25049
Training/qf2_norm                 318.25610    8.42359     328.06210    306.79971
log_std/mean                      -0.13484     0.00018     -0.13460     -0.13510
log_probs/mean                    -2.73363     0.00541     -2.72378     -2.73971
mean/mean                         -0.00143     0.00010     -0.00129     -0.00156
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01841115951538086
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70487
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [56850]
collect time 0.0007958412170410156
inner_dict_sum {'sac_diff0': 0.0002117156982421875, 'sac_diff1': 0.007277727127075195, 'sac_diff2': 0.012768745422363281, 'sac_diff3': 0.012186765670776367, 'sac_diff4': 0.008461713790893555, 'sac_diff5': 0.03406500816345215, 'sac_diff6': 0.0003895759582519531, 'all': 0.07536125183105469}
diff5_list [0.006581306457519531, 0.007966041564941406, 0.006904125213623047, 0.006273508071899414, 0.00634002685546875]
time3 0
time4 0.0761258602142334
time5 0.07618260383605957
time7 7.152557373046875e-07
gen_weight_change tensor(-22.8872)
policy weight change tensor(37.5036, grad_fn=<SumBackward0>)
time8 0.0023272037506103516
train_time 0.0884559154510498
eval time 0.14082694053649902
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:27,977 MainThread INFO: EPOCH:372
2024-01-23 01:02:27,977 MainThread INFO: Time Consumed:0.23260951042175293s
2024-01-23 01:02:27,977 MainThread INFO: Total Frames:56700s
  4%|▎         | 373/10000 [03:55<40:42,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12238.59828
Train_Epoch_Reward                2116.57772
Running_Training_Average_Rewards  15012.22496
Explore_Time                      0.00079
Train___Time                      0.08846
Eval____Time                      0.14083
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12011.06720
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.55135     2.30503    92.67949     86.26614
alpha_0                           0.82985      0.00012    0.83002      0.82969
Alpha_loss                        -1.25519     0.00174    -1.25262     -1.25703
Training/policy_loss              -2.47556     0.00633    -2.46880     -2.48693
Training/qf1_loss                 9115.79873   723.84920  9932.56055   7780.82666
Training/qf2_loss                 16599.75352  951.88074  17387.84375  14732.43555
Training/pf_norm                  0.10986      0.01928    0.14585      0.09403
Training/qf1_norm                 4814.36572   325.72513  5112.79980   4217.40625
Training/qf2_norm                 312.73962    7.71175    319.92596    298.36993
log_std/mean                      -0.12816     0.00004    -0.12810     -0.12819
log_probs/mean                    -2.73371     0.00768    -2.72543     -2.74719
mean/mean                         -0.00187     0.00011    -0.00171     -0.00203
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018967390060424805
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70487
epoch first part time 4.76837158203125e-06
replay_buffer._size: [57000]
collect time 0.0008814334869384766
inner_dict_sum {'sac_diff0': 0.0002276897430419922, 'sac_diff1': 0.006688356399536133, 'sac_diff2': 0.008176565170288086, 'sac_diff3': 0.010356426239013672, 'sac_diff4': 0.006819248199462891, 'sac_diff5': 0.031201839447021484, 'sac_diff6': 0.00038743019104003906, 'all': 0.0638575553894043}
diff5_list [0.00652766227722168, 0.0061244964599609375, 0.006079435348510742, 0.006224155426025391, 0.006246089935302734]
time3 0
time4 0.0645895004272461
time5 0.06463098526000977
time7 9.5367431640625e-07
gen_weight_change tensor(-22.8872)
policy weight change tensor(37.6142, grad_fn=<SumBackward0>)
time8 0.0018901824951171875
train_time 0.07571911811828613
eval time 0.15180230140686035
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:28,230 MainThread INFO: EPOCH:373
2024-01-23 01:02:28,230 MainThread INFO: Time Consumed:0.23065400123596191s
2024-01-23 01:02:28,230 MainThread INFO: Total Frames:56850s
  4%|▎         | 374/10000 [03:55<40:39,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12176.59617
Train_Epoch_Reward                38037.28049
Running_Training_Average_Rewards  15827.10261
Explore_Time                      0.00088
Train___Time                      0.07572
Eval____Time                      0.15180
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11990.02413
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.13382     3.86770     96.26820     86.14674
alpha_0                           0.82944      0.00012     0.82961      0.82927
Alpha_loss                        -1.25767     0.00106     -1.25622     -1.25922
Training/policy_loss              -2.48265     0.00626     -2.47329     -2.48962
Training/qf1_loss                 10141.93730  1804.02350  12756.27148  8055.43848
Training/qf2_loss                 17539.00801  2178.77400  20449.87109  14791.26758
Training/pf_norm                  0.14413      0.03404     0.19833      0.09390
Training/qf1_norm                 5330.97061   518.23496   5870.43701   4536.23242
Training/qf2_norm                 334.23127    13.62699    349.12527    313.17004
log_std/mean                      -0.11525     0.00022     -0.11499     -0.11561
log_probs/mean                    -2.72893     0.00728     -2.71821     -2.73725
mean/mean                         0.00021      0.00003     0.00025      0.00017
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01854252815246582
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70487
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [57150]
collect time 0.0008227825164794922
inner_dict_sum {'sac_diff0': 0.00023412704467773438, 'sac_diff1': 0.006837129592895508, 'sac_diff2': 0.007994413375854492, 'sac_diff3': 0.010162830352783203, 'sac_diff4': 0.006592273712158203, 'sac_diff5': 0.031597137451171875, 'sac_diff6': 0.0003790855407714844, 'all': 0.0637969970703125}
diff5_list [0.006409406661987305, 0.0061228275299072266, 0.006136655807495117, 0.006372928619384766, 0.006555318832397461]
time3 0
time4 0.06453919410705566
time5 0.06458258628845215
time7 7.152557373046875e-07
gen_weight_change tensor(-22.8872)
policy weight change tensor(37.8317, grad_fn=<SumBackward0>)
time8 0.0018393993377685547
train_time 0.07501578330993652
eval time 0.15317893028259277
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:28,483 MainThread INFO: EPOCH:374
2024-01-23 01:02:28,483 MainThread INFO: Time Consumed:0.2312297821044922s
2024-01-23 01:02:28,483 MainThread INFO: Total Frames:57000s
  4%|▍         | 375/10000 [03:56<40:37,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12119.59405
Train_Epoch_Reward                9352.98190
Running_Training_Average_Rewards  16025.33129
Explore_Time                      0.00082
Train___Time                      0.07502
Eval____Time                      0.15318
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11979.57381
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.62766     1.24224    95.08157     91.73322
alpha_0                           0.82903      0.00012    0.82919      0.82886
Alpha_loss                        -1.26193     0.00150    -1.25919     -1.26377
Training/policy_loss              -2.49597     0.00517    -2.49030     -2.50409
Training/qf1_loss                 9724.86465   638.10451  10454.81055  8530.50195
Training/qf2_loss                 17062.45625  738.80922  18043.08203  15749.11719
Training/pf_norm                  0.09434      0.01457    0.11375      0.07682
Training/qf1_norm                 5599.37187   160.17799  5915.69043   5493.65820
Training/qf2_norm                 345.11815    4.61176    354.19867    341.54617
log_std/mean                      -0.12109     0.00039    -0.12054     -0.12166
log_probs/mean                    -2.73371     0.00625    -2.72626     -2.74354
mean/mean                         -0.00019     0.00006    -0.00010     -0.00028
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018217086791992188
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70487
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [57300]
collect time 0.0008034706115722656
inside mustsac before update, task 0, sumup 70487
inside mustsac after update, task 0, sumup 70683
inner_dict_sum {'sac_diff0': 0.0001976490020751953, 'sac_diff1': 0.006378173828125, 'sac_diff2': 0.0073375701904296875, 'sac_diff3': 0.00959014892578125, 'sac_diff4': 0.006697654724121094, 'sac_diff5': 0.04862260818481445, 'sac_diff6': 0.00038504600524902344, 'all': 0.0792088508605957}
diff5_list [0.010696172714233398, 0.009244441986083984, 0.010030984878540039, 0.009459733963012695, 0.009191274642944336]
time3 0.000820159912109375
time4 0.07999205589294434
time5 0.08003807067871094
time7 0.008842229843139648
gen_weight_change tensor(-22.9863)
policy weight change tensor(37.8708, grad_fn=<SumBackward0>)
time8 0.001734018325805664
train_time 0.10800051689147949
eval time 0.11649155616760254
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:28,732 MainThread INFO: EPOCH:375
2024-01-23 01:02:28,732 MainThread INFO: Time Consumed:0.22758913040161133s
2024-01-23 01:02:28,733 MainThread INFO: Total Frames:57150s
  4%|▍         | 376/10000 [03:56<40:27,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12071.93596
Train_Epoch_Reward                6788.18761
Running_Training_Average_Rewards  15699.19352
Explore_Time                      0.00080
Train___Time                      0.10800
Eval____Time                      0.11649
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11979.02171
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.84563     2.18581     93.44435     87.22676
alpha_0                           0.82861      0.00012     0.82878      0.82845
Alpha_loss                        -1.26582     0.00175     -1.26415     -1.26904
Training/policy_loss              -2.49890     0.00916     -2.48486     -2.50867
Training/qf1_loss                 8914.43008   865.76571   9768.81934   7352.04688
Training/qf2_loss                 16218.06289  1128.75768  17364.32422  14417.95312
Training/pf_norm                  0.11938      0.02479     0.15164      0.07956
Training/qf1_norm                 5118.82881   316.77139   5411.26123   4576.16357
Training/qf2_norm                 337.44163    13.82164    359.97290    319.75012
log_std/mean                      -0.13134     0.00657     -0.12574     -0.14110
log_probs/mean                    -2.73647     0.00530     -2.73119     -2.74644
mean/mean                         -0.00032     0.00040     0.00037      -0.00078
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01851963996887207
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70683
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [57450]
collect time 0.0009100437164306641
inner_dict_sum {'sac_diff0': 0.00023317337036132812, 'sac_diff1': 0.00680232048034668, 'sac_diff2': 0.007855892181396484, 'sac_diff3': 0.010295391082763672, 'sac_diff4': 0.006858110427856445, 'sac_diff5': 0.03239297866821289, 'sac_diff6': 0.0003771781921386719, 'all': 0.06481504440307617}
diff5_list [0.0064640045166015625, 0.006800413131713867, 0.00629425048828125, 0.0067081451416015625, 0.0061261653900146484]
time3 0
time4 0.06554698944091797
time5 0.06558942794799805
time7 7.152557373046875e-07
gen_weight_change tensor(-22.9863)
policy weight change tensor(37.9586, grad_fn=<SumBackward0>)
time8 0.00191497802734375
train_time 0.0766456127166748
eval time 0.15951800346374512
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:28,994 MainThread INFO: EPOCH:376
2024-01-23 01:02:28,994 MainThread INFO: Time Consumed:0.2393932342529297s
2024-01-23 01:02:28,994 MainThread INFO: Total Frames:57300s
  4%|▍         | 377/10000 [03:56<40:53,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12068.21311
Train_Epoch_Reward                4171.85442
Running_Training_Average_Rewards  15630.46419
Explore_Time                      0.00091
Train___Time                      0.07665
Eval____Time                      0.15952
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12147.56383
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.33837     2.55631     95.32288     88.31235
alpha_0                           0.82820      0.00012     0.82836      0.82803
Alpha_loss                        -1.26773     0.00120     -1.26616     -1.26962
Training/policy_loss              -2.50916     0.00567     -2.49888     -2.51447
Training/qf1_loss                 8585.80391   1169.74059  10869.67773  7686.46729
Training/qf2_loss                 16080.92324  1451.02130  18857.80859  14839.64062
Training/pf_norm                  0.12010      0.03171     0.17675      0.08448
Training/qf1_norm                 5089.98486   354.57686   5630.31396   4654.08887
Training/qf2_norm                 355.94897    9.68233     371.05472    344.55405
log_std/mean                      -0.14376     0.00012     -0.14353     -0.14386
log_probs/mean                    -2.72874     0.00678     -2.71667     -2.73520
mean/mean                         0.00015      0.00006     0.00022      0.00006
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018372774124145508
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70683
epoch first part time 2.384185791015625e-06
replay_buffer._size: [57600]
collect time 0.0007965564727783203
inner_dict_sum {'sac_diff0': 0.0002243518829345703, 'sac_diff1': 0.006412506103515625, 'sac_diff2': 0.0078105926513671875, 'sac_diff3': 0.009487390518188477, 'sac_diff4': 0.006640434265136719, 'sac_diff5': 0.030576467514038086, 'sac_diff6': 0.0003771781921386719, 'all': 0.061528921127319336}
diff5_list [0.0063724517822265625, 0.006182193756103516, 0.006132841110229492, 0.006003141403198242, 0.0058858394622802734]
time3 0
time4 0.062270402908325195
time5 0.06231212615966797
time7 7.152557373046875e-07
gen_weight_change tensor(-22.9863)
policy weight change tensor(38.0475, grad_fn=<SumBackward0>)
time8 0.0017819404602050781
train_time 0.07275724411010742
eval time 0.15247488021850586
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:29,244 MainThread INFO: EPOCH:377
2024-01-23 01:02:29,244 MainThread INFO: Time Consumed:0.2282423973083496s
2024-01-23 01:02:29,244 MainThread INFO: Total Frames:57450s
  4%|▍         | 378/10000 [03:56<40:38,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12070.62378
Train_Epoch_Reward                13927.74817
Running_Training_Average_Rewards  15750.44796
Explore_Time                      0.00079
Train___Time                      0.07276
Eval____Time                      0.15247
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12195.33283
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.08904     2.52204     93.38251     86.36564
alpha_0                           0.82778      0.00012     0.82795      0.82762
Alpha_loss                        -1.27219     0.00176     -1.26960     -1.27487
Training/policy_loss              -2.48892     0.00630     -2.48361     -2.50074
Training/qf1_loss                 9570.08379   1356.91456  11541.10449  7576.55176
Training/qf2_loss                 17345.93496  1592.86261  19590.04297  14763.33301
Training/pf_norm                  0.12419      0.02202     0.15988      0.09220
Training/qf1_norm                 4712.77148   366.06609   5078.18555   4034.41357
Training/qf2_norm                 340.17613    9.06454     348.27139    323.14682
log_std/mean                      -0.12875     0.00019     -0.12855     -0.12905
log_probs/mean                    -2.73456     0.00757     -2.72795     -2.74875
mean/mean                         0.00120      0.00002     0.00122      0.00116
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01814556121826172
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70683
epoch first part time 2.86102294921875e-06
replay_buffer._size: [57750]
collect time 0.0008449554443359375
inner_dict_sum {'sac_diff0': 0.00023603439331054688, 'sac_diff1': 0.006748199462890625, 'sac_diff2': 0.007707118988037109, 'sac_diff3': 0.009822845458984375, 'sac_diff4': 0.0066220760345458984, 'sac_diff5': 0.031193256378173828, 'sac_diff6': 0.0003771781921386719, 'all': 0.06270670890808105}
diff5_list [0.006220340728759766, 0.00620579719543457, 0.006130218505859375, 0.006526470184326172, 0.006110429763793945]
time3 0
time4 0.06343197822570801
time5 0.06347417831420898
time7 4.76837158203125e-07
gen_weight_change tensor(-22.9863)
policy weight change tensor(38.0906, grad_fn=<SumBackward0>)
time8 0.0018355846405029297
train_time 0.07401895523071289
eval time 0.15761137008666992
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:29,500 MainThread INFO: EPOCH:378
2024-01-23 01:02:29,501 MainThread INFO: Time Consumed:0.2347126007080078s
2024-01-23 01:02:29,501 MainThread INFO: Total Frames:57600s
  4%|▍         | 379/10000 [03:57<40:46,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12079.17424
Train_Epoch_Reward                7766.29423
Running_Training_Average_Rewards  15357.85714
Explore_Time                      0.00084
Train___Time                      0.07402
Eval____Time                      0.15761
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12224.23257
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.98462     3.06050     97.02094     88.62847
alpha_0                           0.82737      0.00012     0.82753      0.82720
Alpha_loss                        -1.27470     0.00124     -1.27256     -1.27578
Training/policy_loss              -2.48741     0.00401     -2.48426     -2.49517
Training/qf1_loss                 10478.67578  1085.43367  12124.31543  9156.10742
Training/qf2_loss                 17339.52891  1338.40278  19472.81641  15766.91211
Training/pf_norm                  0.13495      0.03443     0.19411      0.09046
Training/qf1_norm                 5767.73887   380.99417   6391.60352   5362.20020
Training/qf2_norm                 344.05471    10.90730    362.16815    332.07925
log_std/mean                      -0.13608     0.00010     -0.13590     -0.13616
log_probs/mean                    -2.73005     0.00484     -2.72585     -2.73929
mean/mean                         0.00061      0.00004     0.00066      0.00056
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01809859275817871
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70683
epoch first part time 2.86102294921875e-06
replay_buffer._size: [57900]
collect time 0.0009107589721679688
inner_dict_sum {'sac_diff0': 0.00020551681518554688, 'sac_diff1': 0.006333112716674805, 'sac_diff2': 0.007456779479980469, 'sac_diff3': 0.009543418884277344, 'sac_diff4': 0.006447792053222656, 'sac_diff5': 0.030830860137939453, 'sac_diff6': 0.00036907196044921875, 'all': 0.06118655204772949}
diff5_list [0.0063016414642333984, 0.0060443878173828125, 0.006123542785644531, 0.006217241287231445, 0.006144046783447266]
time3 0
time4 0.06189727783203125
time5 0.06193733215332031
time7 9.5367431640625e-07
gen_weight_change tensor(-22.9863)
policy weight change tensor(38.0635, grad_fn=<SumBackward0>)
time8 0.0018630027770996094
train_time 0.07269859313964844
eval time 0.15752243995666504
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:29,755 MainThread INFO: EPOCH:379
2024-01-23 01:02:29,756 MainThread INFO: Time Consumed:0.23340988159179688s
2024-01-23 01:02:29,756 MainThread INFO: Total Frames:57750s
  4%|▍         | 380/10000 [03:57<40:50,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12087.53600
Train_Epoch_Reward                16849.39662
Running_Training_Average_Rewards  15465.01338
Explore_Time                      0.00091
Train___Time                      0.07270
Eval____Time                      0.15752
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12192.26157
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.22129     4.01733     99.50266     87.12230
alpha_0                           0.82696      0.00012     0.82712      0.82679
Alpha_loss                        -1.27738     0.00150     -1.27471     -1.27891
Training/policy_loss              -2.50009     0.00315     -2.49473     -2.50363
Training/qf1_loss                 9661.44141   1703.11420  12314.55273  6987.36426
Training/qf2_loss                 17325.70273  2154.74272  20688.67969  13965.33398
Training/pf_norm                  0.10910      0.02957     0.14389      0.06393
Training/qf1_norm                 5407.30107   570.60469   6300.72021   4535.67041
Training/qf2_norm                 359.22942    14.76377    382.26514    336.64563
log_std/mean                      -0.13345     0.00006     -0.13335     -0.13352
log_probs/mean                    -2.72640     0.00397     -2.71945     -2.73044
mean/mean                         0.00099      0.00007     0.00111      0.00090
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018866777420043945
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70683
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [58050]
collect time 0.0008127689361572266
inside mustsac before update, task 0, sumup 70683
inside mustsac after update, task 0, sumup 70822
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.00683283805847168, 'sac_diff2': 0.009253740310668945, 'sac_diff3': 0.010383367538452148, 'sac_diff4': 0.007151126861572266, 'sac_diff5': 0.049408912658691406, 'sac_diff6': 0.00040912628173828125, 'all': 0.08364415168762207}
diff5_list [0.010297775268554688, 0.009660959243774414, 0.010134696960449219, 0.009593963623046875, 0.009721517562866211]
time3 0.0008339881896972656
time4 0.08444714546203613
time5 0.08449912071228027
time7 0.00940704345703125
gen_weight_change tensor(-23.1068)
policy weight change tensor(38.0872, grad_fn=<SumBackward0>)
time8 0.0027132034301757812
train_time 0.11499261856079102
eval time 0.11844873428344727
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:30,014 MainThread INFO: EPOCH:380
2024-01-23 01:02:30,014 MainThread INFO: Time Consumed:0.23641371726989746s
2024-01-23 01:02:30,015 MainThread INFO: Total Frames:57900s
  4%|▍         | 381/10000 [03:57<41:08,  3.90it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12090.22636
Train_Epoch_Reward                19421.61405
Running_Training_Average_Rewards  15409.24980
Explore_Time                      0.00081
Train___Time                      0.11499
Eval____Time                      0.11845
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12184.61932
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.26884     2.21181    96.42172     89.83596
alpha_0                           0.82654      0.00012    0.82671      0.82638
Alpha_loss                        -1.28225     0.00052    -1.28156     -1.28283
Training/policy_loss              -2.49570     0.01818    -2.47567     -2.52267
Training/qf1_loss                 9248.29980   600.55084  10223.23828  8485.94922
Training/qf2_loss                 16777.12949  666.21085  17735.71289  15931.71094
Training/pf_norm                  0.12515      0.02796    0.17000      0.09060
Training/qf1_norm                 5227.31250   324.14104  5607.12158   4660.17090
Training/qf2_norm                 345.74381    16.98135   372.59927    322.21924
log_std/mean                      -0.13241     0.00843    -0.12096     -0.14701
log_probs/mean                    -2.73433     0.00321    -2.72850     -2.73777
mean/mean                         0.00081      0.00108    0.00239      -0.00081
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018311023712158203
epoch last part time3 0.0026078224182128906
inside rlalgo, task 0, sumup 70822
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [58200]
collect time 0.0009517669677734375
inner_dict_sum {'sac_diff0': 0.00021219253540039062, 'sac_diff1': 0.006473064422607422, 'sac_diff2': 0.007788181304931641, 'sac_diff3': 0.00977468490600586, 'sac_diff4': 0.006507158279418945, 'sac_diff5': 0.030266761779785156, 'sac_diff6': 0.0003685951232910156, 'all': 0.06139063835144043}
diff5_list [0.006338357925415039, 0.005991697311401367, 0.00579833984375, 0.006196498870849609, 0.005941867828369141]
time3 0
time4 0.06210589408874512
time5 0.062146663665771484
time7 7.152557373046875e-07
gen_weight_change tensor(-23.1068)
policy weight change tensor(38.0951, grad_fn=<SumBackward0>)
time8 0.0018324851989746094
train_time 0.07271218299865723
eval time 0.15181326866149902
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:30,266 MainThread INFO: EPOCH:381
2024-01-23 01:02:30,266 MainThread INFO: Time Consumed:0.22776484489440918s
2024-01-23 01:02:30,266 MainThread INFO: Total Frames:58050s
  4%|▍         | 382/10000 [03:57<40:49,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12082.12912
Train_Epoch_Reward                11115.97881
Running_Training_Average_Rewards  15639.12639
Explore_Time                      0.00094
Train___Time                      0.07271
Eval____Time                      0.15181
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11917.59426
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.48629     2.50451     96.30414     89.58529
alpha_0                           0.82613      0.00012     0.82629      0.82596
Alpha_loss                        -1.28544     0.00197     -1.28168     -1.28697
Training/policy_loss              -2.50942     0.00518     -2.49950     -2.51361
Training/qf1_loss                 9950.14932   1816.83242  12596.63965  8091.74707
Training/qf2_loss                 17781.56895  2114.79234  20889.58008  15609.78613
Training/pf_norm                  0.11480      0.02409     0.14740      0.07754
Training/qf1_norm                 5050.97207   377.48825   5621.64160   4599.20361
Training/qf2_norm                 362.31284    9.52495     376.89487    351.36774
log_std/mean                      -0.13399     0.00007     -0.13387     -0.13406
log_probs/mean                    -2.73341     0.00655     -2.72076     -2.73852
mean/mean                         0.00092      0.00001     0.00094      0.00090
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019162654876708984
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70822
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [58350]
collect time 0.0009577274322509766
inner_dict_sum {'sac_diff0': 0.00022840499877929688, 'sac_diff1': 0.0067179203033447266, 'sac_diff2': 0.007770538330078125, 'sac_diff3': 0.009910821914672852, 'sac_diff4': 0.006790637969970703, 'sac_diff5': 0.03170347213745117, 'sac_diff6': 0.0003757476806640625, 'all': 0.06349754333496094}
diff5_list [0.00662684440612793, 0.006409883499145508, 0.006108283996582031, 0.006574392318725586, 0.005984067916870117]
time3 0
time4 0.06422948837280273
time5 0.0642709732055664
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1068)
policy weight change tensor(38.1115, grad_fn=<SumBackward0>)
time8 0.0018908977508544922
train_time 0.0751495361328125
eval time 0.15027117729187012
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:30,518 MainThread INFO: EPOCH:382
2024-01-23 01:02:30,518 MainThread INFO: Time Consumed:0.2286546230316162s
2024-01-23 01:02:30,518 MainThread INFO: Total Frames:58200s
  4%|▍         | 383/10000 [03:58<40:38,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12080.29916
Train_Epoch_Reward                27208.39617
Running_Training_Average_Rewards  16131.91462
Explore_Time                      0.00095
Train___Time                      0.07515
Eval____Time                      0.15027
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11992.76756
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.47621     1.03716     94.35975     91.25243
alpha_0                           0.82572      0.00012     0.82588      0.82555
Alpha_loss                        -1.28883     0.00147     -1.28675     -1.29134
Training/policy_loss              -2.48574     0.00312     -2.48161     -2.49036
Training/qf1_loss                 9755.92256   1403.37181  12197.16992  8065.48193
Training/qf2_loss                 17447.12676  1503.36726  20100.98828  15686.08887
Training/pf_norm                  0.12764      0.02639     0.16766      0.09782
Training/qf1_norm                 5281.88057   164.54652   5572.03711   5076.54004
Training/qf2_norm                 340.46495    3.67419     347.24225    336.23856
log_std/mean                      -0.13051     0.00008     -0.13041     -0.13061
log_probs/mean                    -2.73352     0.00389     -2.72898     -2.73959
mean/mean                         -0.00006     0.00010     0.00010      -0.00017
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018555402755737305
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70822
epoch first part time 2.86102294921875e-06
replay_buffer._size: [58500]
collect time 0.0009343624114990234
inner_dict_sum {'sac_diff0': 0.00024008750915527344, 'sac_diff1': 0.006644010543823242, 'sac_diff2': 0.007658720016479492, 'sac_diff3': 0.009688615798950195, 'sac_diff4': 0.006616830825805664, 'sac_diff5': 0.031085968017578125, 'sac_diff6': 0.0003826618194580078, 'all': 0.06231689453125}
diff5_list [0.006070852279663086, 0.006314516067504883, 0.00608515739440918, 0.006636142730712891, 0.005979299545288086]
time3 0
time4 0.06305861473083496
time5 0.06310176849365234
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1068)
policy weight change tensor(38.1517, grad_fn=<SumBackward0>)
time8 0.0018510818481445312
train_time 0.07384181022644043
eval time 0.15706753730773926
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:30,774 MainThread INFO: EPOCH:383
2024-01-23 01:02:30,774 MainThread INFO: Time Consumed:0.23415184020996094s
2024-01-23 01:02:30,774 MainThread INFO: Total Frames:58350s
  4%|▍         | 384/10000 [03:58<40:45,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12086.87743
Train_Epoch_Reward                13519.49098
Running_Training_Average_Rewards  15836.41413
Explore_Time                      0.00093
Train___Time                      0.07384
Eval____Time                      0.15707
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12055.80679
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.02695     1.63503    94.33450     89.71503
alpha_0                           0.82530      0.00012    0.82547      0.82514
Alpha_loss                        -1.29175     0.00112    -1.29021     -1.29314
Training/policy_loss              -2.50725     0.00331    -2.50319     -2.51317
Training/qf1_loss                 9340.05996   746.80590  10072.98633  8284.05176
Training/qf2_loss                 16954.06641  886.28293  17856.96680  15825.66211
Training/pf_norm                  0.09602      0.01710    0.11511      0.06827
Training/qf1_norm                 5239.77666   220.41773  5548.44482   4925.73828
Training/qf2_norm                 357.34425    6.17449    365.99460    348.79056
log_std/mean                      -0.13443     0.00009    -0.13433     -0.13459
log_probs/mean                    -2.73119     0.00394    -2.72683     -2.73844
mean/mean                         -0.00080     0.00001    -0.00078     -0.00080
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018401145935058594
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70822
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [58650]
collect time 0.0009377002716064453
inner_dict_sum {'sac_diff0': 0.0002276897430419922, 'sac_diff1': 0.0066852569580078125, 'sac_diff2': 0.007649660110473633, 'sac_diff3': 0.009926795959472656, 'sac_diff4': 0.006746768951416016, 'sac_diff5': 0.03140115737915039, 'sac_diff6': 0.00038814544677734375, 'all': 0.06302547454833984}
diff5_list [0.006575107574462891, 0.006223917007446289, 0.0062596797943115234, 0.006296634674072266, 0.006045818328857422]
time3 0
time4 0.06376099586486816
time5 0.06380319595336914
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1068)
policy weight change tensor(38.1740, grad_fn=<SumBackward0>)
time8 0.0018811225891113281
train_time 0.07447099685668945
eval time 0.15848588943481445
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:31,032 MainThread INFO: EPOCH:384
2024-01-23 01:02:31,033 MainThread INFO: Time Consumed:0.23616623878479004s
2024-01-23 01:02:31,033 MainThread INFO: Total Frames:58500s
  4%|▍         | 385/10000 [03:58<40:55,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12099.26863
Train_Epoch_Reward                4273.98718
Running_Training_Average_Rewards  15623.87181
Explore_Time                      0.00093
Train___Time                      0.07447
Eval____Time                      0.15849
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12103.48589
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.75837     1.78691     95.80021     91.12965
alpha_0                           0.82489      0.00012     0.82506      0.82473
Alpha_loss                        -1.29689     0.00081     -1.29540     -1.29770
Training/policy_loss              -2.51388     0.00428     -2.51022     -2.52116
Training/qf1_loss                 8847.95391   1316.47739  11057.80078  7472.24316
Training/qf2_loss                 16852.30312  1536.36059  19422.80859  15280.43750
Training/pf_norm                  0.08750      0.01847     0.10522      0.06042
Training/qf1_norm                 4921.77939   273.10099   5395.59912   4664.06494
Training/qf2_norm                 369.78718    6.94619     381.42477    363.24768
log_std/mean                      -0.14189     0.00003     -0.14184     -0.14192
log_probs/mean                    -2.74045     0.00491     -2.73619     -2.74868
mean/mean                         0.00209      0.00005     0.00216      0.00202
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01804828643798828
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70822
epoch first part time 2.86102294921875e-06
replay_buffer._size: [58800]
collect time 0.0008816719055175781
inside mustsac before update, task 0, sumup 70822
inside mustsac after update, task 0, sumup 70853
inner_dict_sum {'sac_diff0': 0.00020122528076171875, 'sac_diff1': 0.006935834884643555, 'sac_diff2': 0.008403778076171875, 'sac_diff3': 0.010515928268432617, 'sac_diff4': 0.00740361213684082, 'sac_diff5': 0.05025196075439453, 'sac_diff6': 0.0004029273986816406, 'all': 0.08411526679992676}
diff5_list [0.010251760482788086, 0.010718345642089844, 0.010318517684936523, 0.00956273078918457, 0.009400606155395508]
time3 0.0008943080902099609
time4 0.08493375778198242
time5 0.08498311042785645
time7 0.009297847747802734
gen_weight_change tensor(-23.1479)
policy weight change tensor(38.1612, grad_fn=<SumBackward0>)
time8 0.0018603801727294922
train_time 0.11444211006164551
eval time 0.12129902839660645
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:31,293 MainThread INFO: EPOCH:385
2024-01-23 01:02:31,293 MainThread INFO: Time Consumed:0.23891162872314453s
2024-01-23 01:02:31,293 MainThread INFO: Total Frames:58650s
  4%|▍         | 386/10000 [03:58<41:10,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12115.14184
Train_Epoch_Reward                16561.87823
Running_Training_Average_Rewards  15649.39143
Explore_Time                      0.00088
Train___Time                      0.11444
Eval____Time                      0.12130
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12137.75379
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.90489     1.64577    93.97623     88.92647
alpha_0                           0.82448      0.00012    0.82464      0.82431
Alpha_loss                        -1.29977     0.00131    -1.29767     -1.30167
Training/policy_loss              -2.48974     0.00687    -2.48154     -2.49749
Training/qf1_loss                 9620.74551   466.03204  10348.69238  9039.89355
Training/qf2_loss                 17372.11484  593.83622  18370.33789  16820.14062
Training/pf_norm                  0.12835      0.01521    0.15395      0.10770
Training/qf1_norm                 4962.06260   402.01446  5512.79443   4291.03174
Training/qf2_norm                 338.96110    16.04707   364.58792    320.84482
log_std/mean                      -0.13341     0.00635    -0.12633     -0.14360
log_probs/mean                    -2.73788     0.00441    -2.73391     -2.74420
mean/mean                         -0.00040     0.00110    0.00146      -0.00137
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01830458641052246
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70853
epoch first part time 2.86102294921875e-06
replay_buffer._size: [58950]
collect time 0.0009365081787109375
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.0069239139556884766, 'sac_diff2': 0.008173227310180664, 'sac_diff3': 0.010221004486083984, 'sac_diff4': 0.007002830505371094, 'sac_diff5': 0.033095598220825195, 'sac_diff6': 0.00038242340087890625, 'all': 0.06602096557617188}
diff5_list [0.006638050079345703, 0.007275581359863281, 0.006871461868286133, 0.0062103271484375, 0.006100177764892578]
time3 0
time4 0.06677675247192383
time5 0.06682252883911133
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1479)
policy weight change tensor(38.1357, grad_fn=<SumBackward0>)
time8 0.0019109249114990234
train_time 0.07781052589416504
eval time 0.1772754192352295
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:31,573 MainThread INFO: EPOCH:386
2024-01-23 01:02:31,573 MainThread INFO: Time Consumed:0.2583143711090088s
2024-01-23 01:02:31,573 MainThread INFO: Total Frames:58800s
  4%|▍         | 387/10000 [03:59<42:20,  3.78it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12125.32149
Train_Epoch_Reward                11240.14668
Running_Training_Average_Rewards  15834.99350
Explore_Time                      0.00093
Train___Time                      0.07781
Eval____Time                      0.17728
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12249.36029
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.14524     1.72488     94.27684     90.23965
alpha_0                           0.82407      0.00012     0.82423      0.82390
Alpha_loss                        -1.30314     0.00209     -1.30127     -1.30700
Training/policy_loss              -2.49847     0.00573     -2.49117     -2.50871
Training/qf1_loss                 8739.44434   1152.99002  10895.48633  7761.27979
Training/qf2_loss                 16255.86250  1311.89155  18617.22266  15061.48145
Training/pf_norm                  0.11677      0.02730     0.15967      0.07570
Training/qf1_norm                 5256.09219   239.75600   5552.48291   5002.07178
Training/qf2_norm                 348.05311    6.22000     355.75793    340.94864
log_std/mean                      -0.13047     0.00006     -0.13038     -0.13053
log_probs/mean                    -2.73788     0.00715     -2.72916     -2.75087
mean/mean                         -0.00083     0.00007     -0.00071     -0.00089
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019171953201293945
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70853
epoch first part time 2.86102294921875e-06
replay_buffer._size: [59100]
collect time 0.0009839534759521484
inner_dict_sum {'sac_diff0': 0.0002181529998779297, 'sac_diff1': 0.0070192813873291016, 'sac_diff2': 0.008222341537475586, 'sac_diff3': 0.010472774505615234, 'sac_diff4': 0.006952047348022461, 'sac_diff5': 0.03186964988708496, 'sac_diff6': 0.0003871917724609375, 'all': 0.06514143943786621}
diff5_list [0.006705045700073242, 0.006334066390991211, 0.006297111511230469, 0.006281614303588867, 0.006251811981201172]
time3 0
time4 0.06591653823852539
time5 0.06596255302429199
time7 9.5367431640625e-07
gen_weight_change tensor(-23.1479)
policy weight change tensor(38.1253, grad_fn=<SumBackward0>)
time8 0.0019664764404296875
train_time 0.07748651504516602
eval time 0.17304587364196777
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:31,850 MainThread INFO: EPOCH:387
2024-01-23 01:02:31,850 MainThread INFO: Time Consumed:0.25379252433776855s
2024-01-23 01:02:31,850 MainThread INFO: Total Frames:58950s
  4%|▍         | 388/10000 [03:59<42:52,  3.74it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12131.85057
Train_Epoch_Reward                26070.89158
Running_Training_Average_Rewards  16543.47916
Explore_Time                      0.00098
Train___Time                      0.07749
Eval____Time                      0.17305
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12260.62367
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.23272     2.25939     95.29509     89.76431
alpha_0                           0.82365      0.00012     0.82382      0.82349
Alpha_loss                        -1.30480     0.00242     -1.30110     -1.30809
Training/policy_loss              -2.50439     0.00594     -2.49489     -2.51203
Training/qf1_loss                 9498.24619   1308.77702  10919.36426  7335.83350
Training/qf2_loss                 17382.93984  1547.67461  19062.16406  14969.94336
Training/pf_norm                  0.12380      0.02743     0.15747      0.07451
Training/qf1_norm                 4966.02871   344.33997   5426.87988   4607.35840
Training/qf2_norm                 372.07952    8.96315     384.48410    362.32373
log_std/mean                      -0.12795     0.00010     -0.12784     -0.12810
log_probs/mean                    -2.72907     0.00761     -2.71692     -2.73907
mean/mean                         -0.00056     0.00001     -0.00054     -0.00058
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018220901489257812
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70853
epoch first part time 2.86102294921875e-06
replay_buffer._size: [59250]
collect time 0.0009717941284179688
inner_dict_sum {'sac_diff0': 0.0002808570861816406, 'sac_diff1': 0.008398294448852539, 'sac_diff2': 0.009671926498413086, 'sac_diff3': 0.012088537216186523, 'sac_diff4': 0.008659124374389648, 'sac_diff5': 0.03898811340332031, 'sac_diff6': 0.0004966259002685547, 'all': 0.0785834789276123}
diff5_list [0.008289098739624023, 0.007905006408691406, 0.007730245590209961, 0.007538795471191406, 0.007524967193603516]
time3 0
time4 0.07940483093261719
time5 0.07945704460144043
time7 7.152557373046875e-07
gen_weight_change tensor(-23.1479)
policy weight change tensor(38.1218, grad_fn=<SumBackward0>)
time8 0.0020515918731689453
train_time 0.09128594398498535
eval time 0.1421499252319336
epoch last part time 7.867813110351562e-06
2024-01-23 01:02:32,108 MainThread INFO: EPOCH:388
2024-01-23 01:02:32,109 MainThread INFO: Time Consumed:0.236893892288208s
2024-01-23 01:02:32,109 MainThread INFO: Total Frames:59100s
  4%|▍         | 389/10000 [03:59<42:27,  3.77it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12138.01938
Train_Epoch_Reward                5818.31775
Running_Training_Average_Rewards  16240.41481
Explore_Time                      0.00097
Train___Time                      0.09129
Eval____Time                      0.14215
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12285.92071
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.70504     3.68225     99.05183     89.45261
alpha_0                           0.82324      0.00012     0.82341      0.82308
Alpha_loss                        -1.30963     0.00178     -1.30733     -1.31225
Training/policy_loss              -2.49486     0.00459     -2.48834     -2.50264
Training/qf1_loss                 9595.02051   1653.76468  12658.39453  8035.30859
Training/qf2_loss                 17480.88340  2090.53722  21313.56445  15503.44531
Training/pf_norm                  0.09519      0.01937     0.12030      0.06535
Training/qf1_norm                 5041.25703   539.37443   5955.59277   4582.14844
Training/qf2_norm                 352.82236    13.46808    375.93146    340.64612
log_std/mean                      -0.13149     0.00006     -0.13141     -0.13157
log_probs/mean                    -2.73661     0.00587     -2.72823     -2.74660
mean/mean                         -0.00069     0.00012     -0.00052     -0.00084
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018638134002685547
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70853
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [59400]
collect time 0.0010228157043457031
inner_dict_sum {'sac_diff0': 0.00026702880859375, 'sac_diff1': 0.008387088775634766, 'sac_diff2': 0.01027226448059082, 'sac_diff3': 0.012791633605957031, 'sac_diff4': 0.009039163589477539, 'sac_diff5': 0.04056406021118164, 'sac_diff6': 0.0005161762237548828, 'all': 0.08183741569519043}
diff5_list [0.008287429809570312, 0.007773876190185547, 0.008463621139526367, 0.00834512710571289, 0.0076940059661865234]
time3 0
time4 0.08272433280944824
time5 0.0827796459197998
time7 9.5367431640625e-07
gen_weight_change tensor(-23.1479)
policy weight change tensor(38.1301, grad_fn=<SumBackward0>)
time8 0.002073049545288086
train_time 0.09479999542236328
eval time 0.1320781707763672
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:32,361 MainThread INFO: EPOCH:389
2024-01-23 01:02:32,361 MainThread INFO: Time Consumed:0.23022699356079102s
2024-01-23 01:02:32,361 MainThread INFO: Total Frames:59250s
  4%|▍         | 390/10000 [03:59<41:52,  3.82it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12155.20378
Train_Epoch_Reward                8961.94162
Running_Training_Average_Rewards  16092.03187
Explore_Time                      0.00102
Train___Time                      0.09480
Eval____Time                      0.13208
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12364.10553
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       94.28042     1.85271    96.20331     91.78035
alpha_0                           0.82283      0.00012    0.82299      0.82267
Alpha_loss                        -1.31315     0.00113    -1.31139     -1.31440
Training/policy_loss              -2.49852     0.00139    -2.49716     -2.50115
Training/qf1_loss                 9731.40215   797.72769  10918.01172  8946.18750
Training/qf2_loss                 18107.19023  999.14645  19545.88281  16994.16992
Training/pf_norm                  0.11609      0.00110    0.11724      0.11415
Training/qf1_norm                 4826.52354   268.43236  5085.25928   4456.52930
Training/qf2_norm                 365.75271    6.92644    373.05078    356.31238
log_std/mean                      -0.12070     0.00005    -0.12066     -0.12079
log_probs/mean                    -2.73734     0.00182    -2.73525     -2.74064
mean/mean                         -0.00003     0.00007    0.00008      -0.00012
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019037723541259766
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70853
epoch first part time 3.337860107421875e-06
replay_buffer._size: [59550]
collect time 0.0008747577667236328
inside mustsac before update, task 0, sumup 70853
inside mustsac after update, task 0, sumup 71027
inner_dict_sum {'sac_diff0': 0.0002548694610595703, 'sac_diff1': 0.00782632827758789, 'sac_diff2': 0.009289264678955078, 'sac_diff3': 0.011670112609863281, 'sac_diff4': 0.008382797241210938, 'sac_diff5': 0.05704998970031738, 'sac_diff6': 0.0005152225494384766, 'all': 0.09498858451843262}
diff5_list [0.01215505599975586, 0.011915445327758789, 0.011122703552246094, 0.010977745056152344, 0.010879039764404297]
time3 0.001125335693359375
time4 0.095794677734375
time5 0.09584474563598633
time7 0.0094146728515625
gen_weight_change tensor(-23.0769)
policy weight change tensor(38.1057, grad_fn=<SumBackward0>)
time8 0.002582550048828125
train_time 0.12770438194274902
eval time 0.10354781150817871
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:32,618 MainThread INFO: EPOCH:390
2024-01-23 01:02:32,618 MainThread INFO: Time Consumed:0.2344064712524414s
2024-01-23 01:02:32,618 MainThread INFO: Total Frames:59400s
  4%|▍         | 391/10000 [04:00<41:48,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12184.18634
Train_Epoch_Reward                22514.10496
Running_Training_Average_Rewards  16163.20927
Explore_Time                      0.00087
Train___Time                      0.12770
Eval____Time                      0.10355
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12474.44495
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.07148     1.52353     91.48954     87.28529
alpha_0                           0.82242      0.00012     0.82258      0.82225
Alpha_loss                        -1.31531     0.00108     -1.31343     -1.31666
Training/policy_loss              -2.47655     0.02471     -2.43682     -2.50359
Training/qf1_loss                 8124.16602   1172.31231  9425.99023   6282.56445
Training/qf2_loss                 15779.82891  1298.15971  17351.55859  13666.35352
Training/pf_norm                  0.08449      0.02334     0.11373      0.04664
Training/qf1_norm                 4594.21592   401.28209   5335.70264   4199.99365
Training/qf2_norm                 328.68173    20.97057    346.99884    299.72464
log_std/mean                      -0.13797     0.00463     -0.13058     -0.14393
log_probs/mean                    -2.73115     0.00407     -2.72627     -2.73808
mean/mean                         -0.00039     0.00092     0.00073      -0.00194
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019344806671142578
epoch last part time3 0.0026102066040039062
inside rlalgo, task 0, sumup 71027
epoch first part time 2.86102294921875e-06
replay_buffer._size: [59700]
collect time 0.0009520053863525391
inner_dict_sum {'sac_diff0': 0.00027561187744140625, 'sac_diff1': 0.008080482482910156, 'sac_diff2': 0.010123968124389648, 'sac_diff3': 0.011873006820678711, 'sac_diff4': 0.013462305068969727, 'sac_diff5': 0.04022407531738281, 'sac_diff6': 0.00047779083251953125, 'all': 0.08451724052429199}
diff5_list [0.0075757503509521484, 0.010113000869750977, 0.007489442825317383, 0.007528066635131836, 0.007517814636230469]
time3 0
time4 0.08528971672058105
time5 0.08534669876098633
time7 4.76837158203125e-07
gen_weight_change tensor(-23.0769)
policy weight change tensor(38.1022, grad_fn=<SumBackward0>)
time8 0.002389669418334961
train_time 0.09774041175842285
eval time 0.1285102367401123
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:32,873 MainThread INFO: EPOCH:391
2024-01-23 01:02:32,874 MainThread INFO: Time Consumed:0.22975659370422363s
2024-01-23 01:02:32,874 MainThread INFO: Total Frames:59550s
  4%|▍         | 392/10000 [04:00<41:23,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12185.52527
Train_Epoch_Reward                13974.96735
Running_Training_Average_Rewards  15870.40354
Explore_Time                      0.00095
Train___Time                      0.09774
Eval____Time                      0.12851
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11930.98352
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.99390     2.85995     94.46153     86.38568
alpha_0                           0.82201      0.00012     0.82217      0.82184
Alpha_loss                        -1.31948     0.00244     -1.31561     -1.32197
Training/policy_loss              -2.50347     0.00842     -2.49322     -2.51686
Training/qf1_loss                 7859.01318   963.40021   9061.67676   6754.05615
Training/qf2_loss                 15833.40137  1325.22478  17349.62109  14103.65430
Training/pf_norm                  0.13216      0.04350     0.21223      0.07966
Training/qf1_norm                 4388.23691   430.40221   4884.03223   3702.30615
Training/qf2_norm                 350.09597    10.67367    363.08499    332.97195
log_std/mean                      -0.13269     0.00003     -0.13264     -0.13273
log_probs/mean                    -2.73528     0.01035     -2.72239     -2.75141
mean/mean                         0.00032      0.00003     0.00035      0.00028
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01968693733215332
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71027
epoch first part time 4.76837158203125e-06
replay_buffer._size: [59850]
collect time 0.0009839534759521484
inner_dict_sum {'sac_diff0': 0.00026488304138183594, 'sac_diff1': 0.007960319519042969, 'sac_diff2': 0.009147167205810547, 'sac_diff3': 0.011818885803222656, 'sac_diff4': 0.007932901382446289, 'sac_diff5': 0.03771853446960449, 'sac_diff6': 0.0004849433898925781, 'all': 0.07532763481140137}
diff5_list [0.007565021514892578, 0.007295846939086914, 0.007878303527832031, 0.007399082183837891, 0.007580280303955078]
time3 0
time4 0.07608771324157715
time5 0.07613253593444824
time7 9.5367431640625e-07
gen_weight_change tensor(-23.0769)
policy weight change tensor(38.0388, grad_fn=<SumBackward0>)
time8 0.0019028186798095703
train_time 0.08780384063720703
eval time 0.14301490783691406
epoch last part time 5.9604644775390625e-06
2024-01-23 01:02:33,131 MainThread INFO: EPOCH:392
2024-01-23 01:02:33,131 MainThread INFO: Time Consumed:0.23409438133239746s
2024-01-23 01:02:33,131 MainThread INFO: Total Frames:59700s
  4%|▍         | 393/10000 [04:00<41:20,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12191.29375
Train_Epoch_Reward                11423.45707
Running_Training_Average_Rewards  15199.74960
Explore_Time                      0.00098
Train___Time                      0.08780
Eval____Time                      0.14301
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12050.45232
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.66237     2.17496     93.12406     86.73287
alpha_0                           0.82160      0.00012     0.82176      0.82143
Alpha_loss                        -1.32376     0.00107     -1.32203     -1.32543
Training/policy_loss              -2.48084     0.00349     -2.47486     -2.48471
Training/qf1_loss                 8396.25801   1728.06952  10883.35742  5912.57812
Training/qf2_loss                 16538.48516  1950.65864  19132.71875  13503.63086
Training/pf_norm                  0.09320      0.03860     0.14692      0.04349
Training/qf1_norm                 3943.06997   374.19097   4378.17969   3278.99268
Training/qf2_norm                 329.00753    7.61349     337.59430    315.14719
log_std/mean                      -0.13067     0.00016     -0.13041     -0.13084
log_probs/mean                    -2.73992     0.00413     -2.73323     -2.74497
mean/mean                         0.00141      0.00016     0.00164      0.00120
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019425153732299805
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71027
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [60000]
collect time 0.0010058879852294922
inner_dict_sum {'sac_diff0': 0.00024771690368652344, 'sac_diff1': 0.007952690124511719, 'sac_diff2': 0.009380340576171875, 'sac_diff3': 0.012431859970092773, 'sac_diff4': 0.008134603500366211, 'sac_diff5': 0.03845095634460449, 'sac_diff6': 0.0004780292510986328, 'all': 0.07707619667053223}
diff5_list [0.00779271125793457, 0.007589101791381836, 0.0077097415924072266, 0.007693052291870117, 0.007666349411010742]
time3 0
time4 0.07783150672912598
time5 0.07787632942199707
time7 4.76837158203125e-07
gen_weight_change tensor(-23.0769)
policy weight change tensor(38.0353, grad_fn=<SumBackward0>)
time8 0.0019092559814453125
train_time 0.08936071395874023
eval time 0.13676786422729492
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:33,383 MainThread INFO: EPOCH:393
2024-01-23 01:02:33,383 MainThread INFO: Time Consumed:0.22939705848693848s
2024-01-23 01:02:33,383 MainThread INFO: Total Frames:59850s
  4%|▍         | 394/10000 [04:00<41:02,  3.90it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12192.98930
Train_Epoch_Reward                7908.84302
Running_Training_Average_Rewards  14133.64922
Explore_Time                      0.00100
Train___Time                      0.08936
Eval____Time                      0.13677
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12072.76238
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.91255     1.42911    92.53335     88.66167
alpha_0                           0.82118      0.00012    0.82135      0.82102
Alpha_loss                        -1.32633     0.00097    -1.32513     -1.32765
Training/policy_loss              -2.47804     0.00550    -2.46999     -2.48662
Training/qf1_loss                 9251.23672   638.72045  9950.98047   8305.79395
Training/qf2_loss                 16908.43164  808.97539  17799.68359  15808.88770
Training/pf_norm                  0.09843      0.02155    0.13089      0.07770
Training/qf1_norm                 4790.61504   202.94431  5042.48682   4469.35498
Training/qf2_norm                 326.95318    4.99936    332.56058    319.23837
log_std/mean                      -0.12585     0.00009    -0.12576     -0.12600
log_probs/mean                    -2.73580     0.00643    -2.72632     -2.74595
mean/mean                         -0.00132     0.00003    -0.00126     -0.00135
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01903820037841797
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71027
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [60150]
collect time 0.0010533332824707031
inner_dict_sum {'sac_diff0': 0.00027441978454589844, 'sac_diff1': 0.007652997970581055, 'sac_diff2': 0.009014368057250977, 'sac_diff3': 0.011794805526733398, 'sac_diff4': 0.008118391036987305, 'sac_diff5': 0.038059234619140625, 'sac_diff6': 0.0004851818084716797, 'all': 0.07539939880371094}
diff5_list [0.007954597473144531, 0.007613658905029297, 0.007522106170654297, 0.007447242736816406, 0.007521629333496094]
time3 0
time4 0.076141357421875
time5 0.0761880874633789
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0769)
policy weight change tensor(37.9970, grad_fn=<SumBackward0>)
time8 0.0019526481628417969
train_time 0.08761358261108398
eval time 0.1394197940826416
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:33,636 MainThread INFO: EPOCH:394
2024-01-23 01:02:33,637 MainThread INFO: Time Consumed:0.23034977912902832s
2024-01-23 01:02:33,637 MainThread INFO: Total Frames:60000s
  4%|▍         | 395/10000 [04:01<40:51,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12189.64669
Train_Epoch_Reward                18424.27374
Running_Training_Average_Rewards  13059.37028
Explore_Time                      0.00105
Train___Time                      0.08761
Eval____Time                      0.13942
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12070.05979
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.09726     2.28535    96.80386     90.14135
alpha_0                           0.82077      0.00012    0.82094      0.82061
Alpha_loss                        -1.32933     0.00245    -1.32566     -1.33237
Training/policy_loss              -2.50327     0.00605    -2.49409     -2.51020
Training/qf1_loss                 8776.08809   802.96428  9690.74707   7759.88574
Training/qf2_loss                 17157.66367  988.05949  18065.55078  15742.48633
Training/pf_norm                  0.12566      0.02861    0.15741      0.08301
Training/qf1_norm                 4409.59131   349.80405  4983.36572   4010.09814
Training/qf2_norm                 362.36465    8.57999    376.23828    351.16330
log_std/mean                      -0.13693     0.00015    -0.13670     -0.13710
log_probs/mean                    -2.73394     0.00763    -2.72217     -2.74253
mean/mean                         -0.00106     0.00011    -0.00093     -0.00124
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018484830856323242
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71027
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [60300]
collect time 0.0009424686431884766
inside mustsac before update, task 0, sumup 71027
inside mustsac after update, task 0, sumup 70908
inner_dict_sum {'sac_diff0': 0.0002551078796386719, 'sac_diff1': 0.007521152496337891, 'sac_diff2': 0.009082555770874023, 'sac_diff3': 0.011911869049072266, 'sac_diff4': 0.008273601531982422, 'sac_diff5': 0.058629751205444336, 'sac_diff6': 0.00048804283142089844, 'all': 0.09616208076477051}
diff5_list [0.013060569763183594, 0.011667013168334961, 0.011190414428710938, 0.011392831802368164, 0.01131892204284668]
time3 0.0011162757873535156
time4 0.0969843864440918
time5 0.09703540802001953
time7 0.009666681289672852
gen_weight_change tensor(-23.0356)
policy weight change tensor(38.0036, grad_fn=<SumBackward0>)
time8 0.001957416534423828
train_time 0.12847661972045898
eval time 0.099945068359375
epoch last part time 5.7220458984375e-06
2024-01-23 01:02:33,890 MainThread INFO: EPOCH:395
2024-01-23 01:02:33,890 MainThread INFO: Time Consumed:0.23163056373596191s
2024-01-23 01:02:33,890 MainThread INFO: Total Frames:60150s
  4%|▍         | 396/10000 [04:01<40:49,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12184.72588
Train_Epoch_Reward                16620.39536
Running_Training_Average_Rewards  13515.05726
Explore_Time                      0.00094
Train___Time                      0.12848
Eval____Time                      0.09995
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12088.54569
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.84316     1.59502     94.58092     89.84419
alpha_0                           0.82036      0.00012     0.82053      0.82020
Alpha_loss                        -1.33324     0.00101     -1.33175     -1.33465
Training/policy_loss              -2.49285     0.00718     -2.48270     -2.50123
Training/qf1_loss                 9199.39160   1264.79512  10297.40234  6828.96094
Training/qf2_loss                 17380.64375  1440.57287  18835.66797  14935.15625
Training/pf_norm                  0.10751      0.01800     0.13518      0.07965
Training/qf1_norm                 4601.27036   622.39378   5252.52490   3493.95679
Training/qf2_norm                 350.37259    3.93356     354.78479    344.09381
log_std/mean                      -0.12901     0.00542     -0.12038     -0.13616
log_probs/mean                    -2.73670     0.00050     -2.73595     -2.73742
mean/mean                         0.00037      0.00061     0.00116      -0.00016
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01889824867248535
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70908
epoch first part time 2.86102294921875e-06
replay_buffer._size: [60450]
collect time 0.0008623600006103516
inner_dict_sum {'sac_diff0': 0.0002574920654296875, 'sac_diff1': 0.007816314697265625, 'sac_diff2': 0.00932168960571289, 'sac_diff3': 0.011926412582397461, 'sac_diff4': 0.008276224136352539, 'sac_diff5': 0.039147377014160156, 'sac_diff6': 0.0005095005035400391, 'all': 0.0772550106048584}
diff5_list [0.007851600646972656, 0.0076236724853515625, 0.00799870491027832, 0.00758814811706543, 0.008085250854492188]
time3 0
time4 0.07802462577819824
time5 0.07806921005249023
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0356)
policy weight change tensor(37.9851, grad_fn=<SumBackward0>)
time8 0.002057313919067383
train_time 0.08964157104492188
eval time 0.14178848266601562
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:34,147 MainThread INFO: EPOCH:396
2024-01-23 01:02:34,148 MainThread INFO: Time Consumed:0.23457622528076172s
2024-01-23 01:02:34,148 MainThread INFO: Total Frames:60300s
  4%|▍         | 397/10000 [04:01<40:53,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12205.63370
Train_Epoch_Reward                3707.97486
Running_Training_Average_Rewards  13405.40301
Explore_Time                      0.00086
Train___Time                      0.08964
Eval____Time                      0.14179
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12458.43845
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.75230     2.90506     96.28282     88.16776
alpha_0                           0.81995      0.00012     0.82012      0.81979
Alpha_loss                        -1.33594     0.00274     -1.33084     -1.33808
Training/policy_loss              -2.48253     0.00784     -2.46756     -2.48932
Training/qf1_loss                 8742.14453   848.21031   9412.84961   7152.53418
Training/qf2_loss                 17318.42793  1199.14064  18121.12891  14957.76074
Training/pf_norm                  0.10517      0.01245     0.12001      0.09133
Training/qf1_norm                 4338.79087   486.14493   4737.44238   3397.12524
Training/qf2_norm                 347.36383    10.42042    356.63055    327.35831
log_std/mean                      -0.12830     0.00003     -0.12825     -0.12833
log_probs/mean                    -2.73331     0.00990     -2.71440     -2.74151
mean/mean                         -0.00107     0.00012     -0.00092     -0.00123
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018405675888061523
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70908
epoch first part time 3.337860107421875e-06
replay_buffer._size: [60600]
collect time 0.0009379386901855469
inner_dict_sum {'sac_diff0': 0.0002665519714355469, 'sac_diff1': 0.007708311080932617, 'sac_diff2': 0.009086132049560547, 'sac_diff3': 0.011952400207519531, 'sac_diff4': 0.008008241653442383, 'sac_diff5': 0.03744983673095703, 'sac_diff6': 0.0004887580871582031, 'all': 0.07496023178100586}
diff5_list [0.007894039154052734, 0.0075626373291015625, 0.007534980773925781, 0.007135152816772461, 0.007323026657104492]
time3 0
time4 0.0756978988647461
time5 0.07574295997619629
time7 4.76837158203125e-07
gen_weight_change tensor(-23.0356)
policy weight change tensor(37.9300, grad_fn=<SumBackward0>)
time8 0.001897573471069336
train_time 0.08702850341796875
eval time 0.14144515991210938
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:34,401 MainThread INFO: EPOCH:397
2024-01-23 01:02:34,401 MainThread INFO: Time Consumed:0.2316749095916748s
2024-01-23 01:02:34,402 MainThread INFO: Total Frames:60450s
  4%|▍         | 398/10000 [04:02<40:48,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12222.10256
Train_Epoch_Reward                17616.34251
Running_Training_Average_Rewards  12898.87337
Explore_Time                      0.00093
Train___Time                      0.08703
Eval____Time                      0.14145
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12425.31224
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.03503     1.52052    93.39275     88.76515
alpha_0                           0.81954      0.00012    0.81971      0.81938
Alpha_loss                        -1.33904     0.00079    -1.33819     -1.34015
Training/policy_loss              -2.50242     0.00262    -2.49923     -2.50497
Training/qf1_loss                 7640.23506   715.78567  8922.49219   6815.31348
Training/qf2_loss                 15857.58379  817.96750  17114.69141  14706.07227
Training/pf_norm                  0.11720      0.03373    0.15439      0.06546
Training/qf1_norm                 3919.49336   239.80990  4306.51562   3584.36646
Training/qf2_norm                 358.71932    5.53748    367.31653    350.39450
log_std/mean                      -0.13403     0.00014    -0.13379     -0.13418
log_probs/mean                    -2.73198     0.00294    -2.72802     -2.73450
mean/mean                         -0.00051     0.00004    -0.00046     -0.00058
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01840806007385254
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70908
epoch first part time 2.86102294921875e-06
replay_buffer._size: [60750]
collect time 0.0009407997131347656
inner_dict_sum {'sac_diff0': 0.0002741813659667969, 'sac_diff1': 0.007441043853759766, 'sac_diff2': 0.008855342864990234, 'sac_diff3': 0.011305809020996094, 'sac_diff4': 0.007851362228393555, 'sac_diff5': 0.03752493858337402, 'sac_diff6': 0.0004687309265136719, 'all': 0.07372140884399414}
diff5_list [0.007498979568481445, 0.007469892501831055, 0.007480621337890625, 0.007255077362060547, 0.007820367813110352]
time3 0
time4 0.07444620132446289
time5 0.07448887825012207
time7 4.76837158203125e-07
gen_weight_change tensor(-23.0356)
policy weight change tensor(37.8261, grad_fn=<SumBackward0>)
time8 0.0018947124481201172
train_time 0.08565592765808105
eval time 0.14561867713928223
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:34,658 MainThread INFO: EPOCH:398
2024-01-23 01:02:34,658 MainThread INFO: Time Consumed:0.2344820499420166s
2024-01-23 01:02:34,658 MainThread INFO: Total Frames:60600s
  4%|▍         | 399/10000 [04:02<40:52,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12225.31241
Train_Epoch_Reward                12741.38906
Running_Training_Average_Rewards  13248.59812
Explore_Time                      0.00094
Train___Time                      0.08566
Eval____Time                      0.14562
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12318.01919
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.42042     1.14325    90.44362     87.25644
alpha_0                           0.81913      0.00012    0.81930      0.81897
Alpha_loss                        -1.34412     0.00058    -1.34333     -1.34478
Training/policy_loss              -2.51942     0.00348    -2.51610     -2.52488
Training/qf1_loss                 8185.83330   765.14694  9087.76367   7159.46729
Training/qf2_loss                 15346.47012  835.69752  16353.52832  14313.91211
Training/pf_norm                  0.10412      0.02646    0.14847      0.07508
Training/qf1_norm                 4901.17480   162.81161  5053.40479   4598.10938
Training/qf2_norm                 356.95372    4.44548    361.06366    348.47803
log_std/mean                      -0.13013     0.00025    -0.12975     -0.13044
log_probs/mean                    -2.74057     0.00398    -2.73711     -2.74706
mean/mean                         -0.00136     0.00002    -0.00132     -0.00138
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01843428611755371
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70908
epoch first part time 1.811981201171875e-05
replay_buffer._size: [60900]
collect time 0.0009427070617675781
inner_dict_sum {'sac_diff0': 0.000255584716796875, 'sac_diff1': 0.007810354232788086, 'sac_diff2': 0.008976459503173828, 'sac_diff3': 0.0117340087890625, 'sac_diff4': 0.007999658584594727, 'sac_diff5': 0.038202524185180664, 'sac_diff6': 0.00048089027404785156, 'all': 0.07545948028564453}
diff5_list [0.007677555084228516, 0.007630586624145508, 0.00775599479675293, 0.0074160099029541016, 0.007722377777099609]
time3 0
time4 0.07622289657592773
time5 0.07626652717590332
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0356)
policy weight change tensor(37.6412, grad_fn=<SumBackward0>)
time8 0.0019102096557617188
train_time 0.08757352828979492
eval time 0.13741493225097656
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:34,908 MainThread INFO: EPOCH:399
2024-01-23 01:02:34,908 MainThread INFO: Time Consumed:0.22818374633789062s
2024-01-23 01:02:34,909 MainThread INFO: Total Frames:60750s
  4%|▍         | 400/10000 [04:02<40:38,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12213.56925
Train_Epoch_Reward                12299.69889
Running_Training_Average_Rewards  13137.08228
Explore_Time                      0.00094
Train___Time                      0.08757
Eval____Time                      0.13741
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12246.67393
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.50072     1.17774    92.15981     89.20170
alpha_0                           0.81872      0.00012    0.81889      0.81856
Alpha_loss                        -1.34574     0.00157    -1.34411     -1.34868
Training/policy_loss              -2.51719     0.00376    -2.51337     -2.52331
Training/qf1_loss                 7459.98828   419.81851  8281.62012   7146.89990
Training/qf2_loss                 15404.20820  437.74269  16145.76270  14913.65918
Training/pf_norm                  0.11544      0.03387    0.15997      0.06139
Training/qf1_norm                 4282.96953   159.27343  4509.68652   4111.77002
Training/qf2_norm                 371.88956    4.69887    378.57568    366.72711
log_std/mean                      -0.15156     0.00045    -0.15086     -0.15208
log_probs/mean                    -2.73177     0.00469    -2.72698     -2.73973
mean/mean                         -0.00059     0.00003    -0.00055     -0.00062
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01871633529663086
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70908
epoch first part time 2.86102294921875e-06
replay_buffer._size: [61050]
collect time 0.0008511543273925781
inside mustsac before update, task 0, sumup 70908
inside mustsac after update, task 0, sumup 70507
inner_dict_sum {'sac_diff0': 0.00025153160095214844, 'sac_diff1': 0.007932186126708984, 'sac_diff2': 0.009341001510620117, 'sac_diff3': 0.012020349502563477, 'sac_diff4': 0.008530616760253906, 'sac_diff5': 0.0585172176361084, 'sac_diff6': 0.0005064010620117188, 'all': 0.09709930419921875}
diff5_list [0.011900186538696289, 0.011752843856811523, 0.011747598648071289, 0.01151585578918457, 0.011600732803344727]
time3 0.0011258125305175781
time4 0.09793376922607422
time5 0.09798717498779297
time7 0.009233713150024414
gen_weight_change tensor(-23.0511)
policy weight change tensor(37.6901, grad_fn=<SumBackward0>)
time8 0.002852201461791992
train_time 0.13006830215454102
eval time 0.1034250259399414
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:35,167 MainThread INFO: EPOCH:400
2024-01-23 01:02:35,167 MainThread INFO: Time Consumed:0.23653030395507812s
2024-01-23 01:02:35,167 MainThread INFO: Total Frames:60900s
  4%|▍         | 401/10000 [04:02<40:59,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12185.53650
Train_Epoch_Reward                8277.45277
Running_Training_Average_Rewards  13105.17256
Explore_Time                      0.00085
Train___Time                      0.13007
Eval____Time                      0.10343
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12194.11751
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.79673     2.85991     99.12627     91.19264
alpha_0                           0.81831      0.00012     0.81848      0.81815
Alpha_loss                        -1.34927     0.00160     -1.34635     -1.35101
Training/policy_loss              -2.50411     0.01369     -2.49178     -2.52857
Training/qf1_loss                 8963.63955   1080.42080  10559.82520  7795.09521
Training/qf2_loss                 17533.80039  1412.17920  20068.30078  16151.42188
Training/pf_norm                  0.13343      0.02942     0.17317      0.09506
Training/qf1_norm                 4398.52104   456.00958   5009.93457   3623.54956
Training/qf2_norm                 372.89193    27.47191    413.39331    345.52823
log_std/mean                      -0.12518     0.00881     -0.11523     -0.14029
log_probs/mean                    -2.73262     0.00620     -2.72137     -2.73821
mean/mean                         -0.00007     0.00034     0.00034      -0.00065
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018519163131713867
epoch last part time3 0.0026993751525878906
inside rlalgo, task 0, sumup 70507
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [61200]
collect time 0.0010161399841308594
inner_dict_sum {'sac_diff0': 0.0002772808074951172, 'sac_diff1': 0.008054733276367188, 'sac_diff2': 0.010890007019042969, 'sac_diff3': 0.01225137710571289, 'sac_diff4': 0.008365154266357422, 'sac_diff5': 0.038553714752197266, 'sac_diff6': 0.0004780292510986328, 'all': 0.07887029647827148}
diff5_list [0.008046388626098633, 0.007803916931152344, 0.0072667598724365234, 0.007375955581665039, 0.008060693740844727]
time3 0
time4 0.0796349048614502
time5 0.07968544960021973
time7 9.5367431640625e-07
gen_weight_change tensor(-23.0511)
policy weight change tensor(37.5221, grad_fn=<SumBackward0>)
time8 0.0019659996032714844
train_time 0.09138035774230957
eval time 0.13349413871765137
epoch last part time 8.344650268554688e-06
2024-01-23 01:02:35,420 MainThread INFO: EPOCH:401
2024-01-23 01:02:35,420 MainThread INFO: Time Consumed:0.22817254066467285s
2024-01-23 01:02:35,420 MainThread INFO: Total Frames:61050s
  4%|▍         | 402/10000 [04:03<40:42,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12244.17015
Train_Epoch_Reward                11890.28285
Running_Training_Average_Rewards  13353.40489
Explore_Time                      0.00101
Train___Time                      0.09138
Eval____Time                      0.13349
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12517.31998
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.15919     1.45351    90.93405     87.13741
alpha_0                           0.81791      0.00012    0.81807      0.81774
Alpha_loss                        -1.35232     0.00171    -1.34972     -1.35488
Training/policy_loss              -2.49434     0.00616    -2.48366     -2.50057
Training/qf1_loss                 7041.49453   498.50901  7628.70801   6455.34229
Training/qf2_loss                 15041.46504  594.67977  15832.47852  14168.54102
Training/pf_norm                  0.10252      0.00687    0.11317      0.09182
Training/qf1_norm                 3393.67549   240.43236  3699.34229   3071.82959
Training/qf2_norm                 353.86431    5.56247    360.97440    346.22339
log_std/mean                      -0.13227     0.00019    -0.13204     -0.13258
log_probs/mean                    -2.73103     0.00745    -2.71808     -2.73851
mean/mean                         -0.00083     0.00007    -0.00075     -0.00093
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018863677978515625
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70507
epoch first part time 2.86102294921875e-06
replay_buffer._size: [61350]
collect time 0.0009696483612060547
inner_dict_sum {'sac_diff0': 0.000270843505859375, 'sac_diff1': 0.00821995735168457, 'sac_diff2': 0.009210348129272461, 'sac_diff3': 0.011816740036010742, 'sac_diff4': 0.008115530014038086, 'sac_diff5': 0.03802156448364258, 'sac_diff6': 0.0004818439483642578, 'all': 0.07613682746887207}
diff5_list [0.007874488830566406, 0.00745081901550293, 0.0075266361236572266, 0.007593631744384766, 0.00757598876953125]
time3 0
time4 0.07688331604003906
time5 0.07692933082580566
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0511)
policy weight change tensor(37.4362, grad_fn=<SumBackward0>)
time8 0.0019102096557617188
train_time 0.08822131156921387
eval time 0.13938069343566895
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:35,673 MainThread INFO: EPOCH:402
2024-01-23 01:02:35,674 MainThread INFO: Time Consumed:0.23082709312438965s
2024-01-23 01:02:35,674 MainThread INFO: Total Frames:61200s
  4%|▍         | 403/10000 [04:03<40:38,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12286.14895
Train_Epoch_Reward                22254.71532
Running_Training_Average_Rewards  14024.67614
Explore_Time                      0.00096
Train___Time                      0.08822
Eval____Time                      0.13938
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12470.24038
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.43012     3.22418     92.06866     83.30476
alpha_0                           0.81750      0.00012     0.81766      0.81733
Alpha_loss                        -1.35658     0.00144     -1.35428     -1.35869
Training/policy_loss              -2.51266     0.00529     -2.50342     -2.51946
Training/qf1_loss                 8000.08047   1379.57137  9198.97852   5302.04785
Training/qf2_loss                 15752.89414  1805.51398  17312.92188  12233.42383
Training/pf_norm                  0.07976      0.01515     0.09449      0.05943
Training/qf1_norm                 4229.21929   484.48362   4626.06055   3317.44312
Training/qf2_norm                 360.86011    12.67985    371.26877    336.78207
log_std/mean                      -0.12605     0.00011     -0.12594     -0.12624
log_probs/mean                    -2.73548     0.00645     -2.72406     -2.74341
mean/mean                         -0.00116     0.00013     -0.00095     -0.00131
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018701553344726562
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70507
epoch first part time 2.86102294921875e-06
replay_buffer._size: [61500]
collect time 0.0009558200836181641
inner_dict_sum {'sac_diff0': 0.00024437904357910156, 'sac_diff1': 0.007462501525878906, 'sac_diff2': 0.008946418762207031, 'sac_diff3': 0.011536836624145508, 'sac_diff4': 0.007874727249145508, 'sac_diff5': 0.03805732727050781, 'sac_diff6': 0.0004820823669433594, 'all': 0.07460427284240723}
diff5_list [0.007890939712524414, 0.007357597351074219, 0.008019447326660156, 0.0075266361236572266, 0.007262706756591797]
time3 0
time4 0.07535839080810547
time5 0.07540440559387207
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0511)
policy weight change tensor(37.4010, grad_fn=<SumBackward0>)
time8 0.001997232437133789
train_time 0.08692240715026855
eval time 0.14627718925476074
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:35,932 MainThread INFO: EPOCH:403
2024-01-23 01:02:35,932 MainThread INFO: Time Consumed:0.23649144172668457s
2024-01-23 01:02:35,933 MainThread INFO: Total Frames:61350s
  4%|▍         | 404/10000 [04:03<40:53,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12319.42500
Train_Epoch_Reward                4771.14729
Running_Training_Average_Rewards  12915.80504
Explore_Time                      0.00095
Train___Time                      0.08692
Eval____Time                      0.14628
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12405.52281
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.77293     2.30849     94.86432     88.31557
alpha_0                           0.81709      0.00012     0.81725      0.81692
Alpha_loss                        -1.35950     0.00099     -1.35769     -1.36043
Training/policy_loss              -2.51013     0.00703     -2.49717     -2.51669
Training/qf1_loss                 8138.77080   1210.83162  9989.39844   7109.54395
Training/qf2_loss                 16095.89375  1460.50007  18049.04102  14830.40430
Training/pf_norm                  0.12601      0.01244     0.14901      0.11180
Training/qf1_norm                 4275.89634   339.09705   4867.23145   3918.27832
Training/qf2_norm                 365.28087    9.09643     381.44208    355.73206
log_std/mean                      -0.12576     0.00009     -0.12565     -0.12587
log_probs/mean                    -2.73327     0.00840     -2.71764     -2.74121
mean/mean                         -0.00003     0.00007     0.00003      -0.00015
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018911123275756836
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70507
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [61650]
collect time 0.0009520053863525391
inner_dict_sum {'sac_diff0': 0.00026226043701171875, 'sac_diff1': 0.0076751708984375, 'sac_diff2': 0.009288311004638672, 'sac_diff3': 0.012140750885009766, 'sac_diff4': 0.008117914199829102, 'sac_diff5': 0.03783416748046875, 'sac_diff6': 0.0004792213439941406, 'all': 0.07579779624938965}
diff5_list [0.007858037948608398, 0.007504940032958984, 0.007421016693115234, 0.0075151920318603516, 0.007534980773925781]
time3 0
time4 0.07656383514404297
time5 0.07660818099975586
time7 4.76837158203125e-07
gen_weight_change tensor(-23.0511)
policy weight change tensor(37.3614, grad_fn=<SumBackward0>)
time8 0.0018851757049560547
train_time 0.08790946006774902
eval time 0.14356160163879395
epoch last part time 5.9604644775390625e-06
2024-01-23 01:02:36,190 MainThread INFO: EPOCH:404
2024-01-23 01:02:36,190 MainThread INFO: Time Consumed:0.23469829559326172s
2024-01-23 01:02:36,190 MainThread INFO: Total Frames:61500s
  4%|▍         | 405/10000 [04:03<40:59,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12348.65874
Train_Epoch_Reward                22101.10813
Running_Training_Average_Rewards  13340.74258
Explore_Time                      0.00095
Train___Time                      0.08791
Eval____Time                      0.14356
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12362.39721
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.20857     1.54463     95.12028     90.97975
alpha_0                           0.81668      0.00012     0.81684      0.81652
Alpha_loss                        -1.36286     0.00144     -1.36062     -1.36508
Training/policy_loss              -2.51596     0.00206     -2.51279     -2.51891
Training/qf1_loss                 9510.16289   1480.59187  12354.11816  8281.10352
Training/qf2_loss                 17776.82148  1598.57649  20918.41602  16693.08984
Training/pf_norm                  0.12139      0.01610     0.13498      0.09123
Training/qf1_norm                 4835.72998   237.04821   5117.50146   4500.55420
Training/qf2_norm                 379.38315    6.06075     387.02908    370.67395
log_std/mean                      -0.13434     0.00012     -0.13413     -0.13445
log_probs/mean                    -2.73323     0.00281     -2.72881     -2.73751
mean/mean                         0.00027      0.00001     0.00030      0.00025
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019311189651489258
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70507
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [61800]
collect time 0.0009393692016601562
inside mustsac before update, task 0, sumup 70507
inside mustsac after update, task 0, sumup 71534
inner_dict_sum {'sac_diff0': 0.0002570152282714844, 'sac_diff1': 0.00792694091796875, 'sac_diff2': 0.00933694839477539, 'sac_diff3': 0.012104988098144531, 'sac_diff4': 0.00859212875366211, 'sac_diff5': 0.059772491455078125, 'sac_diff6': 0.0005078315734863281, 'all': 0.09849834442138672}
diff5_list [0.011838436126708984, 0.011923551559448242, 0.012284994125366211, 0.011944293975830078, 0.01178121566772461]
time3 0.0011267662048339844
time4 0.09932494163513184
time5 0.09937596321105957
time7 0.009824037551879883
gen_weight_change tensor(-22.8963)
policy weight change tensor(37.3243, grad_fn=<SumBackward0>)
time8 0.0019469261169433594
train_time 0.1310575008392334
eval time 0.09608769416809082
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:36,443 MainThread INFO: EPOCH:405
2024-01-23 01:02:36,443 MainThread INFO: Time Consumed:0.23026537895202637s
2024-01-23 01:02:36,443 MainThread INFO: Total Frames:61650s
  4%|▍         | 406/10000 [04:04<40:48,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12365.79068
Train_Epoch_Reward                10318.11899
Running_Training_Average_Rewards  13458.40696
Explore_Time                      0.00094
Train___Time                      0.13106
Eval____Time                      0.09609
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12259.86511
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.83561     1.96138     94.90006     89.41924
alpha_0                           0.81627      0.00012     0.81643      0.81611
Alpha_loss                        -1.36713     0.00147     -1.36525     -1.36919
Training/policy_loss              -2.51382     0.02280     -2.48790     -2.54607
Training/qf1_loss                 8817.05518   942.40418   9807.36133   7081.30322
Training/qf2_loss                 17316.55156  1161.92328  18636.50586  15183.46289
Training/pf_norm                  0.10452      0.02462     0.14886      0.07837
Training/qf1_norm                 3960.69272   745.15289   4913.48682   2924.11377
Training/qf2_norm                 375.48231    22.69493    412.70285    351.67911
log_std/mean                      -0.13096     0.00661     -0.12386     -0.14261
log_probs/mean                    -2.73767     0.00330     -2.73174     -2.74115
mean/mean                         -0.00027     0.00064     0.00016      -0.00154
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018749713897705078
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71534
epoch first part time 2.86102294921875e-06
replay_buffer._size: [61950]
collect time 0.0009348392486572266
inner_dict_sum {'sac_diff0': 0.0002579689025878906, 'sac_diff1': 0.007734060287475586, 'sac_diff2': 0.009305953979492188, 'sac_diff3': 0.011929750442504883, 'sac_diff4': 0.008272409439086914, 'sac_diff5': 0.03920245170593262, 'sac_diff6': 0.00048279762268066406, 'all': 0.07718539237976074}
diff5_list [0.008321285247802734, 0.007738351821899414, 0.008324384689331055, 0.007531881332397461, 0.007286548614501953]
time3 0
time4 0.07795262336730957
time5 0.07799887657165527
time7 7.152557373046875e-07
gen_weight_change tensor(-22.8963)
policy weight change tensor(37.2148, grad_fn=<SumBackward0>)
time8 0.0018858909606933594
train_time 0.08930158615112305
eval time 0.137253999710083
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:36,695 MainThread INFO: EPOCH:406
2024-01-23 01:02:36,695 MainThread INFO: Time Consumed:0.2297825813293457s
2024-01-23 01:02:36,695 MainThread INFO: Total Frames:61800s
  4%|▍         | 407/10000 [04:04<40:38,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12336.59151
Train_Epoch_Reward                11952.68679
Running_Training_Average_Rewards  13717.76804
Explore_Time                      0.00093
Train___Time                      0.08930
Eval____Time                      0.13725
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12166.44675
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.52731     1.71357    93.15314     88.33216
alpha_0                           0.81586      0.00012    0.81603      0.81570
Alpha_loss                        -1.36940     0.00152    -1.36704     -1.37083
Training/policy_loss              -2.51165     0.00419    -2.50652     -2.51789
Training/qf1_loss                 7198.41836   614.88266  8020.81104   6513.16211
Training/qf2_loss                 15469.78145  838.48816  16469.09375  14447.83594
Training/pf_norm                  0.12049      0.02723    0.15590      0.07853
Training/qf1_norm                 3281.43486   251.07762  3686.70874   2978.50293
Training/qf2_norm                 370.34362    6.88488    380.83267    361.69485
log_std/mean                      -0.13797     0.00023    -0.13765     -0.13828
log_probs/mean                    -2.73224     0.00504    -2.72625     -2.73977
mean/mean                         -0.00117     0.00005    -0.00111     -0.00124
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018626689910888672
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71534
epoch first part time 2.86102294921875e-06
replay_buffer._size: [62100]
collect time 0.0009713172912597656
inner_dict_sum {'sac_diff0': 0.00025916099548339844, 'sac_diff1': 0.008021354675292969, 'sac_diff2': 0.010094642639160156, 'sac_diff3': 0.012386798858642578, 'sac_diff4': 0.00830984115600586, 'sac_diff5': 0.04005122184753418, 'sac_diff6': 0.00048613548278808594, 'all': 0.07960915565490723}
diff5_list [0.008123159408569336, 0.008431434631347656, 0.008373498916625977, 0.007596492767333984, 0.0075266361236572266]
time3 0
time4 0.08037042617797852
time5 0.08041834831237793
time7 9.5367431640625e-07
gen_weight_change tensor(-22.8963)
policy weight change tensor(37.1688, grad_fn=<SumBackward0>)
time8 0.002105712890625
train_time 0.09224462509155273
eval time 0.13764691352844238
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:36,950 MainThread INFO: EPOCH:407
2024-01-23 01:02:36,951 MainThread INFO: Time Consumed:0.23315191268920898s
2024-01-23 01:02:36,951 MainThread INFO: Total Frames:61950s
  4%|▍         | 408/10000 [04:04<40:42,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12295.35862
Train_Epoch_Reward                2882.09915
Running_Training_Average_Rewards  13349.57973
Explore_Time                      0.00097
Train___Time                      0.09224
Eval____Time                      0.13765
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12012.98328
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.32758     1.42841    92.88820     89.22783
alpha_0                           0.81546      0.00012    0.81562      0.81529
Alpha_loss                        -1.37268     0.00083    -1.37131     -1.37379
Training/policy_loss              -2.52141     0.00220    -2.51751     -2.52398
Training/qf1_loss                 8447.46396   412.27033  8995.17188   7811.50537
Training/qf2_loss                 16754.19707  482.28446  17541.02734  16159.45117
Training/pf_norm                  0.10981      0.02269    0.13741      0.07027
Training/qf1_norm                 3775.20884   203.64117  4003.88086   3478.50537
Training/qf2_norm                 380.49587    5.86552    386.90433    371.76028
log_std/mean                      -0.12634     0.00006    -0.12628     -0.12645
log_probs/mean                    -2.73181     0.00256    -2.72769     -2.73495
mean/mean                         0.00010      0.00013    0.00030      -0.00004
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01854395866394043
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71534
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [62250]
collect time 0.0009524822235107422
inner_dict_sum {'sac_diff0': 0.0002689361572265625, 'sac_diff1': 0.007944583892822266, 'sac_diff2': 0.00927424430847168, 'sac_diff3': 0.011856317520141602, 'sac_diff4': 0.008255720138549805, 'sac_diff5': 0.03878426551818848, 'sac_diff6': 0.0005097389221191406, 'all': 0.07689380645751953}
diff5_list [0.008029937744140625, 0.008226156234741211, 0.00788426399230957, 0.0073320865631103516, 0.007311820983886719]
time3 0
time4 0.0776512622833252
time5 0.07769989967346191
time7 4.76837158203125e-07
gen_weight_change tensor(-22.8963)
policy weight change tensor(37.2317, grad_fn=<SumBackward0>)
time8 0.0019724369049072266
train_time 0.08955621719360352
eval time 0.14703965187072754
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:37,213 MainThread INFO: EPOCH:408
2024-01-23 01:02:37,213 MainThread INFO: Time Consumed:0.2398700714111328s
2024-01-23 01:02:37,213 MainThread INFO: Total Frames:62100s
  4%|▍         | 409/10000 [04:04<41:04,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12254.91780
Train_Epoch_Reward                5223.68439
Running_Training_Average_Rewards  13264.82607
Explore_Time                      0.00095
Train___Time                      0.08956
Eval____Time                      0.14704
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11913.61104
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.65822     1.81785    93.80556     88.63898
alpha_0                           0.81505      0.00012    0.81521      0.81488
Alpha_loss                        -1.37715     0.00161    -1.37441     -1.37926
Training/policy_loss              -2.51897     0.00588    -2.51389     -2.53048
Training/qf1_loss                 7614.15674   711.20257  8588.24707   6899.53418
Training/qf2_loss                 15887.20391  942.68902  17089.68750  14950.82812
Training/pf_norm                  0.12526      0.01119    0.14073      0.10716
Training/qf1_norm                 3376.34985   293.68130  3876.32129   3017.61450
Training/qf2_norm                 372.04020    7.29281    384.64905    364.02609
log_std/mean                      -0.10876     0.00013    -0.10863     -0.10897
log_probs/mean                    -2.73721     0.00705    -2.73039     -2.75086
mean/mean                         0.00028      0.00003    0.00034      0.00025
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018873214721679688
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71534
epoch first part time 2.86102294921875e-06
replay_buffer._size: [62400]
collect time 0.00096893310546875
inner_dict_sum {'sac_diff0': 0.000255584716796875, 'sac_diff1': 0.00800776481628418, 'sac_diff2': 0.009483814239501953, 'sac_diff3': 0.012552022933959961, 'sac_diff4': 0.008597612380981445, 'sac_diff5': 0.0389556884765625, 'sac_diff6': 0.0004863739013671875, 'all': 0.0783388614654541}
diff5_list [0.008014202117919922, 0.007712602615356445, 0.008330106735229492, 0.0073816776275634766, 0.007517099380493164]
time3 0
time4 0.07909774780273438
time5 0.0791468620300293
time7 9.5367431640625e-07
gen_weight_change tensor(-22.8963)
policy weight change tensor(37.3541, grad_fn=<SumBackward0>)
time8 0.0019974708557128906
train_time 0.09092283248901367
eval time 0.16767048835754395
epoch last part time 7.62939453125e-06
2024-01-23 01:02:37,497 MainThread INFO: EPOCH:409
2024-01-23 01:02:37,497 MainThread INFO: Time Consumed:0.2619626522064209s
2024-01-23 01:02:37,498 MainThread INFO: Total Frames:62250s
  4%|▍         | 410/10000 [04:05<42:24,  3.77it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12212.80910
Train_Epoch_Reward                13016.77472
Running_Training_Average_Rewards  13137.07201
Explore_Time                      0.00096
Train___Time                      0.09092
Eval____Time                      0.16767
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11825.58689
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.54200     1.23173     92.36653     89.11328
alpha_0                           0.81464      0.00012     0.81480      0.81448
Alpha_loss                        -1.37901     0.00106     -1.37781     -1.38070
Training/policy_loss              -2.50209     0.00533     -2.49440     -2.51090
Training/qf1_loss                 7753.59805   1167.73522  9733.62793   6327.04395
Training/qf2_loss                 16242.14258  1308.17783  18339.91797  14442.09668
Training/pf_norm                  0.15822      0.03696     0.21132      0.11677
Training/qf1_norm                 3427.46694   207.19744   3562.25195   3014.91772
Training/qf2_norm                 368.01020    4.76563     371.32565    358.66675
log_std/mean                      -0.12095     0.00015     -0.12075     -0.12118
log_probs/mean                    -2.72984     0.00626     -2.72075     -2.74001
mean/mean                         0.00066      0.00005     0.00075      0.00061
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019235849380493164
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71534
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [62550]
collect time 0.0009317398071289062
inside mustsac before update, task 0, sumup 71534
inside mustsac after update, task 0, sumup 71235
inner_dict_sum {'sac_diff0': 0.00025582313537597656, 'sac_diff1': 0.008013248443603516, 'sac_diff2': 0.009639501571655273, 'sac_diff3': 0.012401103973388672, 'sac_diff4': 0.008526325225830078, 'sac_diff5': 0.05930328369140625, 'sac_diff6': 0.0005049705505371094, 'all': 0.09864425659179688}
diff5_list [0.013103961944580078, 0.011754274368286133, 0.011349201202392578, 0.011581897735595703, 0.011513948440551758]
time3 0.0011086463928222656
time4 0.09946870803833008
time5 0.09952044486999512
time7 0.009315013885498047
gen_weight_change tensor(-22.7363)
policy weight change tensor(37.3607, grad_fn=<SumBackward0>)
time8 0.002798795700073242
train_time 0.1319894790649414
eval time 0.09093856811523438
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:37,746 MainThread INFO: EPOCH:410
2024-01-23 01:02:37,746 MainThread INFO: Time Consumed:0.22605490684509277s
2024-01-23 01:02:37,747 MainThread INFO: Total Frames:62400s
  4%|▍         | 411/10000 [04:05<41:45,  3.83it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12170.52642
Train_Epoch_Reward                15638.86932
Running_Training_Average_Rewards  13010.98052
Explore_Time                      0.00093
Train___Time                      0.13199
Eval____Time                      0.09094
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11771.29074
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.59442     2.21336    94.55441     88.88495
alpha_0                           0.81423      0.00012    0.81440      0.81407
Alpha_loss                        -1.38276     0.00216    -1.38051     -1.38639
Training/policy_loss              -2.51452     0.01039    -2.50239     -2.53202
Training/qf1_loss                 7927.67949   448.82444  8688.22852   7278.43604
Training/qf2_loss                 16258.68223  720.63075  17433.10938  15402.33887
Training/pf_norm                  0.12775      0.02826    0.16705      0.08753
Training/qf1_norm                 3816.48755   530.66278  4354.21729   2831.36279
Training/qf2_norm                 382.75039    15.75705   398.84692    362.34756
log_std/mean                      -0.12606     0.00865    -0.11680     -0.13637
log_probs/mean                    -2.73171     0.00749    -2.72078     -2.74283
mean/mean                         -0.00031     0.00072    0.00021      -0.00174
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01910567283630371
epoch last part time3 0.002875804901123047
inside rlalgo, task 0, sumup 71235
epoch first part time 2.86102294921875e-06
replay_buffer._size: [62700]
collect time 0.0010371208190917969
inner_dict_sum {'sac_diff0': 0.00024628639221191406, 'sac_diff1': 0.00818324089050293, 'sac_diff2': 0.009676933288574219, 'sac_diff3': 0.012425661087036133, 'sac_diff4': 0.008532524108886719, 'sac_diff5': 0.03970670700073242, 'sac_diff6': 0.00048470497131347656, 'all': 0.07925605773925781}
diff5_list [0.008427858352661133, 0.0077860355377197266, 0.007967233657836914, 0.007904529571533203, 0.007621049880981445]
time3 0
time4 0.08001136779785156
time5 0.08005809783935547
time7 7.152557373046875e-07
gen_weight_change tensor(-22.7363)
policy weight change tensor(37.4775, grad_fn=<SumBackward0>)
time8 0.0019943714141845703
train_time 0.09204363822937012
eval time 0.12455582618713379
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:37,992 MainThread INFO: EPOCH:411
2024-01-23 01:02:37,992 MainThread INFO: Time Consumed:0.21987628936767578s
2024-01-23 01:02:37,992 MainThread INFO: Total Frames:62550s
  4%|▍         | 412/10000 [04:05<40:48,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12084.13471
Train_Epoch_Reward                2499.17918
Running_Training_Average_Rewards  12723.75386
Explore_Time                      0.00103
Train___Time                      0.09204
Eval____Time                      0.12456
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11653.40291
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.39014     3.13589     95.88340     86.47548
alpha_0                           0.81383      0.00012     0.81399      0.81366
Alpha_loss                        -1.38662     0.00117     -1.38516     -1.38874
Training/policy_loss              -2.51569     0.00235     -2.51149     -2.51847
Training/qf1_loss                 7281.62432   822.11013   8650.35742   6233.60303
Training/qf2_loss                 15891.48789  1296.40345  17958.30273  14083.37988
Training/pf_norm                  0.13558      0.03112     0.17784      0.09599
Training/qf1_norm                 2817.25427   589.98616   3671.96289   1902.46667
Training/qf2_norm                 388.92886    12.75031    406.98151    368.75815
log_std/mean                      -0.12123     0.00007     -0.12115     -0.12136
log_probs/mean                    -2.73411     0.00288     -2.72923     -2.73790
mean/mean                         -0.00140     0.00006     -0.00130     -0.00148
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01834249496459961
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71235
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [62850]
collect time 0.0009458065032958984
inner_dict_sum {'sac_diff0': 0.00026535987854003906, 'sac_diff1': 0.007597208023071289, 'sac_diff2': 0.008833169937133789, 'sac_diff3': 0.011802911758422852, 'sac_diff4': 0.00796365737915039, 'sac_diff5': 0.03845095634460449, 'sac_diff6': 0.00047707557678222656, 'all': 0.07539033889770508}
diff5_list [0.0077855587005615234, 0.007599353790283203, 0.007369518280029297, 0.007802724838256836, 0.007893800735473633]
time3 0
time4 0.07615971565246582
time5 0.07620358467102051
time7 4.76837158203125e-07
gen_weight_change tensor(-22.7363)
policy weight change tensor(37.6655, grad_fn=<SumBackward0>)
time8 0.0020411014556884766
train_time 0.08768248558044434
eval time 0.1392381191253662
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:38,244 MainThread INFO: EPOCH:412
2024-01-23 01:02:38,244 MainThread INFO: Time Consumed:0.23015785217285156s
2024-01-23 01:02:38,244 MainThread INFO: Total Frames:62700s
  4%|▍         | 413/10000 [04:05<40:40,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11992.57108
Train_Epoch_Reward                16243.28218
Running_Training_Average_Rewards  12358.25006
Explore_Time                      0.00094
Train___Time                      0.08768
Eval____Time                      0.13924
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11554.60405
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.52766     2.41289    93.79987     87.02095
alpha_0                           0.81342      0.00012    0.81358      0.81326
Alpha_loss                        -1.38954     0.00166    -1.38725     -1.39222
Training/policy_loss              -2.47216     0.00292    -2.46890     -2.47695
Training/qf1_loss                 7730.75176   664.65408  8241.79199   6431.95801
Training/qf2_loss                 15903.07363  951.22183  16742.87500  14120.14258
Training/pf_norm                  0.11215      0.02099    0.13787      0.07750
Training/qf1_norm                 3797.73130   400.25798  4358.07275   3212.59082
Training/qf2_norm                 337.99535    8.76060    349.52802    325.16220
log_std/mean                      -0.12563     0.00035    -0.12519     -0.12617
log_probs/mean                    -2.73195     0.00391    -2.72742     -2.73844
mean/mean                         0.00025      0.00019    0.00054      0.00002
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018790483474731445
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71235
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [63000]
collect time 0.0009980201721191406
inner_dict_sum {'sac_diff0': 0.0002560615539550781, 'sac_diff1': 0.00831294059753418, 'sac_diff2': 0.009409666061401367, 'sac_diff3': 0.01212000846862793, 'sac_diff4': 0.008588075637817383, 'sac_diff5': 0.03924226760864258, 'sac_diff6': 0.0004901885986328125, 'all': 0.07841920852661133}
diff5_list [0.008108377456665039, 0.007674694061279297, 0.0076825618743896484, 0.007811307907104492, 0.007965326309204102]
time3 0
time4 0.07920455932617188
time5 0.0792543888092041
time7 4.76837158203125e-07
gen_weight_change tensor(-22.7363)
policy weight change tensor(37.8417, grad_fn=<SumBackward0>)
time8 0.002227306365966797
train_time 0.09118127822875977
eval time 0.1382889747619629
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:38,499 MainThread INFO: EPOCH:413
2024-01-23 01:02:38,500 MainThread INFO: Time Consumed:0.23293447494506836s
2024-01-23 01:02:38,500 MainThread INFO: Total Frames:62850s
  4%|▍         | 414/10000 [04:06<40:44,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11900.68888
Train_Epoch_Reward                14551.65833
Running_Training_Average_Rewards  12392.65564
Explore_Time                      0.00099
Train___Time                      0.09118
Eval____Time                      0.13829
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11486.70082
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.45975     1.75728    93.28703     88.02945
alpha_0                           0.81301      0.00011    0.81317      0.81285
Alpha_loss                        -1.39327     0.00113    -1.39151     -1.39462
Training/policy_loss              -2.48996     0.00457    -2.48342     -2.49524
Training/qf1_loss                 6911.38838   524.32573  7814.63574   6243.73486
Training/qf2_loss                 15392.08887  782.96093  16745.60938  14340.94238
Training/pf_norm                  0.08819      0.01746    0.10912      0.06114
Training/qf1_norm                 2595.80688   338.49099  3132.37305   2093.91748
Training/qf2_norm                 370.51326    6.95429    381.62741    361.03680
log_std/mean                      -0.12559     0.00030    -0.12517     -0.12599
log_probs/mean                    -2.73373     0.00546    -2.72620     -2.74023
mean/mean                         0.00011      0.00006    0.00019      0.00001
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01906895637512207
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71235
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [63150]
collect time 0.0009870529174804688
inner_dict_sum {'sac_diff0': 0.00026106834411621094, 'sac_diff1': 0.008500099182128906, 'sac_diff2': 0.009903192520141602, 'sac_diff3': 0.013151168823242188, 'sac_diff4': 0.008889198303222656, 'sac_diff5': 0.04022026062011719, 'sac_diff6': 0.0005011558532714844, 'all': 0.08142614364624023}
diff5_list [0.008070707321166992, 0.0077724456787109375, 0.0077517032623291016, 0.008194208145141602, 0.008431196212768555]
time3 0
time4 0.08227944374084473
time5 0.08233809471130371
time7 9.5367431640625e-07
gen_weight_change tensor(-22.7363)
policy weight change tensor(37.9733, grad_fn=<SumBackward0>)
time8 0.0019474029541015625
train_time 0.09411263465881348
eval time 0.1329360008239746
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:38,753 MainThread INFO: EPOCH:414
2024-01-23 01:02:38,753 MainThread INFO: Time Consumed:0.23043274879455566s
2024-01-23 01:02:38,753 MainThread INFO: Total Frames:63000s
  4%|▍         | 415/10000 [04:06<40:37,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11807.38805
Train_Epoch_Reward                7625.99796
Running_Training_Average_Rewards  12504.38934
Explore_Time                      0.00098
Train___Time                      0.09411
Eval____Time                      0.13294
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11429.38891
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.03888     1.54404    93.43131     88.59143
alpha_0                           0.81261      0.00011    0.81277      0.81244
Alpha_loss                        -1.39662     0.00093    -1.39560     -1.39802
Training/policy_loss              -2.51839     0.00475    -2.51014     -2.52394
Training/qf1_loss                 7873.59668   790.15780  8803.55273   6662.59668
Training/qf2_loss                 16302.69629  975.80326  17370.00391  14712.37207
Training/pf_norm                  0.10494      0.02514    0.14843      0.07020
Training/qf1_norm                 3200.46753   234.97412  3548.48901   2821.20239
Training/qf2_norm                 387.98375    6.38010    397.84998    377.77777
log_std/mean                      -0.13228     0.00019    -0.13199     -0.13253
log_probs/mean                    -2.73366     0.00559    -2.72404     -2.74039
mean/mean                         0.00013      0.00004    0.00019      0.00010
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01836562156677246
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71235
epoch first part time 3.337860107421875e-06
replay_buffer._size: [63300]
collect time 0.0010020732879638672
inside mustsac before update, task 0, sumup 71235
inside mustsac after update, task 0, sumup 70670
inner_dict_sum {'sac_diff0': 0.00027632713317871094, 'sac_diff1': 0.009105682373046875, 'sac_diff2': 0.011278629302978516, 'sac_diff3': 0.013758659362792969, 'sac_diff4': 0.009809494018554688, 'sac_diff5': 0.06481719017028809, 'sac_diff6': 0.0005486011505126953, 'all': 0.10959458351135254}
diff5_list [0.012803077697753906, 0.013468027114868164, 0.011663198471069336, 0.012166976928710938, 0.014715909957885742]
time3 0.0012385845184326172
time4 0.11053299903869629
time5 0.11059451103210449
time7 0.009717464447021484
gen_weight_change tensor(-22.6219)
policy weight change tensor(37.9920, grad_fn=<SumBackward0>)
time8 0.0020270347595214844
train_time 0.1435558795928955
eval time 0.09313106536865234
epoch last part time 7.867813110351562e-06
2024-01-23 01:02:39,015 MainThread INFO: EPOCH:415
2024-01-23 01:02:39,016 MainThread INFO: Time Consumed:0.240281343460083s
2024-01-23 01:02:39,016 MainThread INFO: Total Frames:63150s
  4%|▍         | 416/10000 [04:06<41:13,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11719.80698
Train_Epoch_Reward                18204.02437
Running_Training_Average_Rewards  12559.12754
Explore_Time                      0.00100
Train___Time                      0.14356
Eval____Time                      0.09313
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11384.05436
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.86602     2.08526     94.53341     89.45151
alpha_0                           0.81220      0.00011     0.81236      0.81204
Alpha_loss                        -1.39970     0.00151     -1.39790     -1.40152
Training/policy_loss              -2.51119     0.01020     -2.49868     -2.52191
Training/qf1_loss                 8494.79658   1059.85504  9924.94727   7516.03418
Training/qf2_loss                 17126.00020  1293.30751  18824.45703  15741.29492
Training/pf_norm                  0.09555      0.00562     0.09904      0.08438
Training/qf1_norm                 3004.68376   778.05507   3941.41895   1950.41467
Training/qf2_norm                 378.52148    12.22985    399.40024    366.02112
log_std/mean                      -0.12812     0.00434     -0.12509     -0.13662
log_probs/mean                    -2.73225     0.00513     -2.72682     -2.74100
mean/mean                         0.00062      0.00110     0.00227      -0.00069
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.022947072982788086
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70670
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [63450]
collect time 0.0008873939514160156
inner_dict_sum {'sac_diff0': 0.00020623207092285156, 'sac_diff1': 0.007100343704223633, 'sac_diff2': 0.008469820022583008, 'sac_diff3': 0.010924100875854492, 'sac_diff4': 0.007433414459228516, 'sac_diff5': 0.03299355506896973, 'sac_diff6': 0.000396728515625, 'all': 0.06752419471740723}
diff5_list [0.007319211959838867, 0.006478309631347656, 0.006354331970214844, 0.006367206573486328, 0.006474494934082031]
time3 0
time4 0.06833195686340332
time5 0.06838250160217285
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6219)
policy weight change tensor(38.0258, grad_fn=<SumBackward0>)
time8 0.0019271373748779297
train_time 0.08032822608947754
eval time 0.1563282012939453
epoch last part time 8.106231689453125e-06
2024-01-23 01:02:39,282 MainThread INFO: EPOCH:416
2024-01-23 01:02:39,282 MainThread INFO: Time Consumed:0.24012136459350586s
2024-01-23 01:02:39,282 MainThread INFO: Total Frames:63300s
  4%|▍         | 417/10000 [04:06<41:36,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11640.62011
Train_Epoch_Reward                2188.68981
Running_Training_Average_Rewards  12257.41231
Explore_Time                      0.00088
Train___Time                      0.08033
Eval____Time                      0.15633
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11374.57808
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.95351     1.55087    92.56792     87.70589
alpha_0                           0.81179      0.00011    0.81196      0.81163
Alpha_loss                        -1.40372     0.00210    -1.40129     -1.40652
Training/policy_loss              -2.49686     0.00433    -2.49168     -2.50195
Training/qf1_loss                 7330.93350   857.15310  8302.26953   6199.60010
Training/qf2_loss                 15758.53281  967.08767  16713.08984  14247.88574
Training/pf_norm                  0.11245      0.02045    0.14582      0.08378
Training/qf1_norm                 2326.74097   259.30216  2761.76074   1947.37988
Training/qf2_norm                 358.70328    5.91506    368.68704    350.19034
log_std/mean                      -0.13956     0.00003    -0.13950     -0.13958
log_probs/mean                    -2.73538     0.00569    -2.72858     -2.74234
mean/mean                         -0.00030     0.00005    -0.00023     -0.00037
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0229034423828125
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70670
epoch first part time 3.337860107421875e-06
replay_buffer._size: [63600]
collect time 0.0009946823120117188
inner_dict_sum {'sac_diff0': 0.00022840499877929688, 'sac_diff1': 0.008864879608154297, 'sac_diff2': 0.010788202285766602, 'sac_diff3': 0.011827707290649414, 'sac_diff4': 0.00917673110961914, 'sac_diff5': 0.04063916206359863, 'sac_diff6': 0.0004849433898925781, 'all': 0.08201003074645996}
diff5_list [0.00717473030090332, 0.006842851638793945, 0.00674891471862793, 0.013261795043945312, 0.006610870361328125]
time3 0
time4 0.08313465118408203
time5 0.08321022987365723
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6219)
policy weight change tensor(38.0017, grad_fn=<SumBackward0>)
time8 0.0025441646575927734
train_time 0.09710884094238281
eval time 0.13642191886901855
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:39,545 MainThread INFO: EPOCH:417
2024-01-23 01:02:39,546 MainThread INFO: Time Consumed:0.2371048927307129s
2024-01-23 01:02:39,546 MainThread INFO: Total Frames:63450s
  4%|▍         | 418/10000 [04:07<41:35,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11581.18427
Train_Epoch_Reward                8973.38022
Running_Training_Average_Rewards  11687.49527
Explore_Time                      0.00099
Train___Time                      0.09711
Eval____Time                      0.13642
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11418.62495
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.61113     2.31506    95.99082     89.57093
alpha_0                           0.81139      0.00011    0.81155      0.81123
Alpha_loss                        -1.40755     0.00146    -1.40572     -1.41007
Training/policy_loss              -2.52676     0.00420    -2.52106     -2.53110
Training/qf1_loss                 7616.28643   462.55427  8465.03125   7077.90088
Training/qf2_loss                 16385.30039  829.59938  17958.53711  15499.24023
Training/pf_norm                  0.10529      0.02083    0.13446      0.07591
Training/qf1_norm                 2043.80718   415.59424  2850.68286   1741.35425
Training/qf2_norm                 391.76992    9.76790    410.30197    383.26807
log_std/mean                      -0.12063     0.00005    -0.12055     -0.12070
log_probs/mean                    -2.73761     0.00512    -2.73090     -2.74320
mean/mean                         0.00069      0.00002    0.00072      0.00066
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01952505111694336
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70670
epoch first part time 4.76837158203125e-06
replay_buffer._size: [63750]
collect time 0.0009465217590332031
inner_dict_sum {'sac_diff0': 0.00020051002502441406, 'sac_diff1': 0.006811380386352539, 'sac_diff2': 0.008106470108032227, 'sac_diff3': 0.010458230972290039, 'sac_diff4': 0.00755620002746582, 'sac_diff5': 0.03227424621582031, 'sac_diff6': 0.0003876686096191406, 'all': 0.06579470634460449}
diff5_list [0.006581306457519531, 0.006302356719970703, 0.006513357162475586, 0.006438493728637695, 0.006438732147216797]
time3 0
time4 0.06657600402832031
time5 0.06662607192993164
time7 9.5367431640625e-07
gen_weight_change tensor(-22.6219)
policy weight change tensor(37.9703, grad_fn=<SumBackward0>)
time8 0.0018541812896728516
train_time 0.07845926284790039
eval time 0.15479159355163574
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:39,805 MainThread INFO: EPOCH:418
2024-01-23 01:02:39,805 MainThread INFO: Time Consumed:0.23653721809387207s
2024-01-23 01:02:39,806 MainThread INFO: Total Frames:63600s
  4%|▍         | 419/10000 [04:07<41:31,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11539.17202
Train_Epoch_Reward                10947.33721
Running_Training_Average_Rewards  11858.46258
Explore_Time                      0.00094
Train___Time                      0.07846
Eval____Time                      0.15479
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11493.48847
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.08470     1.13560     91.56538     88.11565
alpha_0                           0.81098      0.00011     0.81114      0.81082
Alpha_loss                        -1.41031     0.00207     -1.40733     -1.41278
Training/policy_loss              -2.53543     0.00536     -2.52646     -2.54243
Training/qf1_loss                 7871.52959   934.54382   9037.26074   6421.55713
Training/qf2_loss                 16461.66973  1011.45583  17573.33984  15050.54102
Training/pf_norm                  0.09875      0.01470     0.12259      0.08144
Training/qf1_norm                 1334.70862   179.75059   1561.11829   1008.01099
Training/qf2_norm                 399.02487    4.90831     405.48450    390.45724
log_std/mean                      -0.13672     0.00011     -0.13654     -0.13683
log_probs/mean                    -2.73469     0.00660     -2.72370     -2.74327
mean/mean                         0.00147      0.00002     0.00151      0.00144
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0185544490814209
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70670
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [63900]
collect time 0.0008411407470703125
inner_dict_sum {'sac_diff0': 0.00020933151245117188, 'sac_diff1': 0.007056713104248047, 'sac_diff2': 0.00829768180847168, 'sac_diff3': 0.01084136962890625, 'sac_diff4': 0.007105350494384766, 'sac_diff5': 0.03281688690185547, 'sac_diff6': 0.0003833770751953125, 'all': 0.0667107105255127}
diff5_list [0.0066070556640625, 0.006743669509887695, 0.006799221038818359, 0.006505250930786133, 0.006161689758300781]
time3 0
time4 0.06746554374694824
time5 0.06751370429992676
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6219)
policy weight change tensor(37.8459, grad_fn=<SumBackward0>)
time8 0.0018460750579833984
train_time 0.07905960083007812
eval time 0.15671515464782715
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:40,066 MainThread INFO: EPOCH:419
2024-01-23 01:02:40,067 MainThread INFO: Time Consumed:0.2389819622039795s
2024-01-23 01:02:40,067 MainThread INFO: Total Frames:63750s
  4%|▍         | 420/10000 [04:07<41:34,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11517.33230
Train_Epoch_Reward                32417.85821
Running_Training_Average_Rewards  12640.32647
Explore_Time                      0.00084
Train___Time                      0.07906
Eval____Time                      0.15672
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11607.18967
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.21794     2.77673     92.18985     83.50587
alpha_0                           0.81058      0.00011     0.81074      0.81041
Alpha_loss                        -1.41314     0.00212     -1.40984     -1.41550
Training/policy_loss              -2.49555     0.00630     -2.48866     -2.50381
Training/qf1_loss                 6496.03447   931.22951   8259.74023   5716.21436
Training/qf2_loss                 14663.22871  1310.03982  17051.93164  13115.49023
Training/pf_norm                  0.13648      0.02499     0.17772      0.10561
Training/qf1_norm                 1793.47418   483.81612   2540.70728   1020.44086
Training/qf2_norm                 360.07770    10.89644    375.20844    341.29443
log_std/mean                      -0.14320     0.00032     -0.14267     -0.14355
log_probs/mean                    -2.73213     0.00795     -2.72284     -2.74255
mean/mean                         0.00230      0.00004     0.00235      0.00224
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01852250099182129
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70670
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [64050]
collect time 0.0008730888366699219
inside mustsac before update, task 0, sumup 70670
inside mustsac after update, task 0, sumup 71199
inner_dict_sum {'sac_diff0': 0.0002510547637939453, 'sac_diff1': 0.007447242736816406, 'sac_diff2': 0.00902104377746582, 'sac_diff3': 0.011501312255859375, 'sac_diff4': 0.008102893829345703, 'sac_diff5': 0.05437040328979492, 'sac_diff6': 0.0004506111145019531, 'all': 0.09114456176757812}
diff5_list [0.0115509033203125, 0.01044917106628418, 0.011030435562133789, 0.011345386505126953, 0.0099945068359375]
time3 0.0009107589721679688
time4 0.09210491180419922
time5 0.0921633243560791
time7 0.00943303108215332
gen_weight_change tensor(-22.4936)
policy weight change tensor(37.8858, grad_fn=<SumBackward0>)
time8 0.002757549285888672
train_time 0.1237173080444336
eval time 0.11303973197937012
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:40,328 MainThread INFO: EPOCH:420
2024-01-23 01:02:40,328 MainThread INFO: Time Consumed:0.239854097366333s
2024-01-23 01:02:40,329 MainThread INFO: Total Frames:63900s
  4%|▍         | 421/10000 [04:07<41:44,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11509.35120
Train_Epoch_Reward                11146.17633
Running_Training_Average_Rewards  12261.39551
Explore_Time                      0.00087
Train___Time                      0.12372
Eval____Time                      0.11304
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11691.47977
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.62879     2.28171     93.28622     87.03428
alpha_0                           0.81017      0.00011     0.81033      0.81001
Alpha_loss                        -1.41621     0.00040     -1.41570     -1.41686
Training/policy_loss              -2.50189     0.02220     -2.48586     -2.54532
Training/qf1_loss                 7834.35811   838.47501   8965.27148   6488.81396
Training/qf2_loss                 16409.13711  1185.59606  18100.41211  14398.41895
Training/pf_norm                  0.11021      0.03087     0.16256      0.07028
Training/qf1_norm                 2217.63721   578.00889   3081.05444   1364.30286
Training/qf2_norm                 373.45402    10.55950    386.43866    354.37515
log_std/mean                      -0.12908     0.00646     -0.11793     -0.13764
log_probs/mean                    -2.73072     0.00537     -2.72508     -2.73702
mean/mean                         0.00069      0.00073     0.00198      0.00001
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018028736114501953
epoch last part time3 0.0026454925537109375
inside rlalgo, task 0, sumup 71199
epoch first part time 2.86102294921875e-06
replay_buffer._size: [64200]
collect time 0.0008585453033447266
inner_dict_sum {'sac_diff0': 0.00020074844360351562, 'sac_diff1': 0.006834983825683594, 'sac_diff2': 0.007850408554077148, 'sac_diff3': 0.01016545295715332, 'sac_diff4': 0.006749391555786133, 'sac_diff5': 0.031336069107055664, 'sac_diff6': 0.0004050731658935547, 'all': 0.06354212760925293}
diff5_list [0.006543636322021484, 0.006021738052368164, 0.006119489669799805, 0.0063517093658447266, 0.006299495697021484]
time3 0
time4 0.06429624557495117
time5 0.06433916091918945
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4936)
policy weight change tensor(37.7477, grad_fn=<SumBackward0>)
time8 0.0018007755279541016
train_time 0.07522940635681152
eval time 0.15634918212890625
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:40,587 MainThread INFO: EPOCH:421
2024-01-23 01:02:40,587 MainThread INFO: Time Consumed:0.23469805717468262s
2024-01-23 01:02:40,587 MainThread INFO: Total Frames:64050s
  4%|▍         | 422/10000 [04:08<41:28,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11520.06376
Train_Epoch_Reward                7795.23773
Running_Training_Average_Rewards  12055.40453
Explore_Time                      0.00085
Train___Time                      0.07523
Eval____Time                      0.15635
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11760.52852
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.45316     2.72450     95.35323     87.19237
alpha_0                           0.80977      0.00011     0.80993      0.80960
Alpha_loss                        -1.42011     0.00157     -1.41784     -1.42204
Training/policy_loss              -2.52376     0.00443     -2.51776     -2.52980
Training/qf1_loss                 7116.61104   1078.64637  8867.72852   5980.20850
Training/qf2_loss                 15788.84863  1490.28623  18404.10547  14237.79102
Training/pf_norm                  0.11244      0.02761     0.15186      0.08213
Training/qf1_norm                 1164.38221   533.67055   2097.07324   460.18149
Training/qf2_norm                 379.64116    11.06803    399.66266    366.54300
log_std/mean                      -0.13197     0.00027     -0.13163     -0.13237
log_probs/mean                    -2.73324     0.00549     -2.72569     -2.74077
mean/mean                         0.00135      0.00020     0.00160      0.00106
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018216371536254883
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71199
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [64350]
collect time 0.0009207725524902344
inner_dict_sum {'sac_diff0': 0.00020599365234375, 'sac_diff1': 0.0067250728607177734, 'sac_diff2': 0.007778167724609375, 'sac_diff3': 0.009868621826171875, 'sac_diff4': 0.006642341613769531, 'sac_diff5': 0.03127336502075195, 'sac_diff6': 0.00038361549377441406, 'all': 0.06287717819213867}
diff5_list [0.006327152252197266, 0.006220817565917969, 0.006618976593017578, 0.006180524826049805, 0.005925893783569336]
time3 0
time4 0.06362032890319824
time5 0.06366467475891113
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4936)
policy weight change tensor(37.7399, grad_fn=<SumBackward0>)
time8 0.0018312931060791016
train_time 0.07466769218444824
eval time 0.16101598739624023
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:40,847 MainThread INFO: EPOCH:422
2024-01-23 01:02:40,848 MainThread INFO: Time Consumed:0.2388300895690918s
2024-01-23 01:02:40,848 MainThread INFO: Total Frames:64200s
  4%|▍         | 423/10000 [04:08<41:31,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11540.67600
Train_Epoch_Reward                2440.68465
Running_Training_Average_Rewards  11755.97878
Explore_Time                      0.00092
Train___Time                      0.07467
Eval____Time                      0.16102
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11760.72648
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.75318     2.02440    92.65044     86.38010
alpha_0                           0.80936      0.00011    0.80952      0.80920
Alpha_loss                        -1.42278     0.00209    -1.42002     -1.42500
Training/policy_loss              -2.51532     0.00572    -2.50663     -2.52127
Training/qf1_loss                 7360.33066   397.13464  7655.05322   6607.97412
Training/qf2_loss                 15872.10195  510.50722  16655.47266  15153.56738
Training/pf_norm                  0.11184      0.01410    0.13140      0.08893
Training/qf1_norm                 1548.19740   392.61495  2123.70850   926.64301
Training/qf2_norm                 389.65253    8.64215    401.93875    375.20035
log_std/mean                      -0.13239     0.00008    -0.13231     -0.13251
log_probs/mean                    -2.72994     0.00709    -2.71964     -2.73656
mean/mean                         0.00011      0.00015    0.00031      -0.00011
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018517255783081055
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71199
epoch first part time 3.337860107421875e-06
replay_buffer._size: [64500]
collect time 0.0008327960968017578
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.00652623176574707, 'sac_diff2': 0.00775599479675293, 'sac_diff3': 0.010109424591064453, 'sac_diff4': 0.007502555847167969, 'sac_diff5': 0.0319972038269043, 'sac_diff6': 0.00040149688720703125, 'all': 0.0645139217376709}
diff5_list [0.006550788879394531, 0.006175994873046875, 0.0066645145416259766, 0.006326198577880859, 0.006279706954956055]
time3 0
time4 0.06526660919189453
time5 0.06531071662902832
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4936)
policy weight change tensor(37.7526, grad_fn=<SumBackward0>)
time8 0.0018520355224609375
train_time 0.07624340057373047
eval time 0.15888524055480957
epoch last part time 3.814697265625e-06
2024-01-23 01:02:41,108 MainThread INFO: EPOCH:423
2024-01-23 01:02:41,108 MainThread INFO: Time Consumed:0.23824501037597656s
2024-01-23 01:02:41,108 MainThread INFO: Total Frames:64350s
  4%|▍         | 424/10000 [04:08<41:31,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11563.91753
Train_Epoch_Reward                2234.13113
Running_Training_Average_Rewards  11566.82172
Explore_Time                      0.00083
Train___Time                      0.07624
Eval____Time                      0.15889
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11719.11609
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.13490     2.53924     92.96782     85.13087
alpha_0                           0.80896      0.00011     0.80912      0.80879
Alpha_loss                        -1.42623     0.00110     -1.42476     -1.42771
Training/policy_loss              -2.51298     0.00216     -2.50961     -2.51575
Training/qf1_loss                 7168.99443   1390.39496  9403.81934   5866.02148
Training/qf2_loss                 15428.20332  1757.05349  18282.70117  13488.23730
Training/pf_norm                  0.09132      0.02728     0.13648      0.06326
Training/qf1_norm                 2354.14045   474.28589   3015.46924   1573.98083
Training/qf2_norm                 383.72762    10.69805    400.04272    367.03958
log_std/mean                      -0.12474     0.00003     -0.12470     -0.12478
log_probs/mean                    -2.73037     0.00270     -2.72672     -2.73418
mean/mean                         -0.00089     0.00011     -0.00073     -0.00105
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018182754516601562
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71199
epoch first part time 2.86102294921875e-06
replay_buffer._size: [64650]
collect time 0.0008454322814941406
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.007424831390380859, 'sac_diff2': 0.00849771499633789, 'sac_diff3': 0.01097869873046875, 'sac_diff4': 0.007597208023071289, 'sac_diff5': 0.034482479095458984, 'sac_diff6': 0.0003998279571533203, 'all': 0.06959366798400879}
diff5_list [0.006746053695678711, 0.006076335906982422, 0.007101535797119141, 0.007266044616699219, 0.007292509078979492]
time3 0
time4 0.07038450241088867
time5 0.07043218612670898
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4936)
policy weight change tensor(37.7033, grad_fn=<SumBackward0>)
time8 0.0018901824951171875
train_time 0.08138012886047363
eval time 0.15509772300720215
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:41,369 MainThread INFO: EPOCH:424
2024-01-23 01:02:41,369 MainThread INFO: Time Consumed:0.23980069160461426s
2024-01-23 01:02:41,370 MainThread INFO: Total Frames:64500s
  4%|▍         | 425/10000 [04:08<41:37,  3.83it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11588.05465
Train_Epoch_Reward                1522.42612
Running_Training_Average_Rewards  11003.42679
Explore_Time                      0.00084
Train___Time                      0.08138
Eval____Time                      0.15510
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11670.76016
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.36699     2.14882    92.80135     86.85097
alpha_0                           0.80855      0.00011    0.80871      0.80839
Alpha_loss                        -1.43022     0.00096    -1.42836     -1.43087
Training/policy_loss              -2.50960     0.00310    -2.50631     -2.51464
Training/qf1_loss                 6964.68105   613.14403  7917.89209   6166.84375
Training/qf2_loss                 15643.69473  904.50354  16899.89062  14599.54883
Training/pf_norm                  0.10891      0.02601    0.14756      0.07688
Training/qf1_norm                 810.92003    349.28744  1192.88831   240.43774
Training/qf2_norm                 369.06636    8.35400    378.90115    355.33029
log_std/mean                      -0.13709     0.00014    -0.13688     -0.13725
log_probs/mean                    -2.73332     0.00377    -2.72989     -2.73954
mean/mean                         -0.00094     0.00013    -0.00078     -0.00114
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01903820037841797
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71199
epoch first part time 2.86102294921875e-06
replay_buffer._size: [64800]
collect time 0.0009372234344482422
inside mustsac before update, task 0, sumup 71199
inside mustsac after update, task 0, sumup 70197
inner_dict_sum {'sac_diff0': 0.0002086162567138672, 'sac_diff1': 0.006917238235473633, 'sac_diff2': 0.008320093154907227, 'sac_diff3': 0.010863542556762695, 'sac_diff4': 0.0072116851806640625, 'sac_diff5': 0.05128908157348633, 'sac_diff6': 0.0004029273986816406, 'all': 0.08521318435668945}
diff5_list [0.01279139518737793, 0.010109424591064453, 0.009601354598999023, 0.009531259536743164, 0.009255647659301758]
time3 0.0008656978607177734
time4 0.0860300064086914
time5 0.08608341217041016
time7 0.009395122528076172
gen_weight_change tensor(-22.4700)
policy weight change tensor(37.7425, grad_fn=<SumBackward0>)
time8 0.002116680145263672
train_time 0.11566019058227539
eval time 0.1125633716583252
epoch last part time 9.775161743164062e-06
2024-01-23 01:02:41,624 MainThread INFO: EPOCH:425
2024-01-23 01:02:41,624 MainThread INFO: Time Consumed:0.23159384727478027s
2024-01-23 01:02:41,624 MainThread INFO: Total Frames:64650s
  4%|▍         | 426/10000 [04:09<41:16,  3.87it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11614.67595
Train_Epoch_Reward                23165.35892
Running_Training_Average_Rewards  11221.59225
Explore_Time                      0.00093
Train___Time                      0.11566
Eval____Time                      0.11256
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11650.26733
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.23367     1.18549    94.25807     91.19123
alpha_0                           0.80815      0.00011    0.80831      0.80799
Alpha_loss                        -1.43433     0.00151    -1.43237     -1.43638
Training/policy_loss              -2.52736     0.02147    -2.49582     -2.55583
Training/qf1_loss                 7738.02949   692.42913  8640.93555   6983.19141
Training/qf2_loss                 16684.94727  853.48574  17908.03125  15736.98438
Training/pf_norm                  0.11038      0.03010    0.15482      0.07209
Training/qf1_norm                 1557.13846   818.00750  2477.00806   212.27602
Training/qf2_norm                 396.97663    15.47229   415.31158    373.38660
log_std/mean                      -0.12705     0.00230    -0.12407     -0.12990
log_probs/mean                    -2.73678     0.00555    -2.72759     -2.74321
mean/mean                         -0.00150     0.00076    -0.00026     -0.00263
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018570661544799805
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70197
epoch first part time 2.86102294921875e-06
replay_buffer._size: [64950]
collect time 0.0008809566497802734
inner_dict_sum {'sac_diff0': 0.0002315044403076172, 'sac_diff1': 0.006644248962402344, 'sac_diff2': 0.007887840270996094, 'sac_diff3': 0.01010751724243164, 'sac_diff4': 0.006742715835571289, 'sac_diff5': 0.031127214431762695, 'sac_diff6': 0.00039839744567871094, 'all': 0.06313943862915039}
diff5_list [0.006320953369140625, 0.006188392639160156, 0.006060361862182617, 0.006547689437866211, 0.006009817123413086]
time3 0
time4 0.06389641761779785
time5 0.06393933296203613
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4700)
policy weight change tensor(37.7541, grad_fn=<SumBackward0>)
time8 0.0019235610961914062
train_time 0.0748755931854248
eval time 0.15398001670837402
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:41,878 MainThread INFO: EPOCH:426
2024-01-23 01:02:41,878 MainThread INFO: Time Consumed:0.23213577270507812s
2024-01-23 01:02:41,878 MainThread INFO: Total Frames:64800s
  4%|▍         | 427/10000 [04:09<41:03,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11609.14966
Train_Epoch_Reward                11818.00665
Running_Training_Average_Rewards  11491.92664
Explore_Time                      0.00087
Train___Time                      0.07488
Eval____Time                      0.15398
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11319.31519
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.76100     2.35514     95.13731     88.08226
alpha_0                           0.80774      0.00011     0.80791      0.80758
Alpha_loss                        -1.43660     0.00204     -1.43417     -1.43953
Training/policy_loss              -2.54581     0.00473     -2.54175     -2.55178
Training/qf1_loss                 8372.13984   1181.03622  10384.16895  6991.03516
Training/qf2_loss                 17161.11504  1510.26160  19742.62109  15170.98926
Training/pf_norm                  0.12735      0.01815     0.14472      0.09353
Training/qf1_norm                 2203.62803   452.41550   2828.56323   1498.71765
Training/qf2_norm                 423.38391    10.51935    439.01590    407.28177
log_std/mean                      -0.12299     0.00006     -0.12293     -0.12308
log_probs/mean                    -2.73164     0.00615     -2.72602     -2.73924
mean/mean                         -0.00190     0.00008     -0.00178     -0.00202
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018498897552490234
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70197
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [65100]
collect time 0.0009665489196777344
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.007220268249511719, 'sac_diff2': 0.008458852767944336, 'sac_diff3': 0.010956048965454102, 'sac_diff4': 0.007066488265991211, 'sac_diff5': 0.032169342041015625, 'sac_diff6': 0.00040912628173828125, 'all': 0.06650114059448242}
diff5_list [0.007093191146850586, 0.006375312805175781, 0.006231546401977539, 0.00635981559753418, 0.006109476089477539]
time3 0
time4 0.06731200218200684
time5 0.06736326217651367
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4700)
policy weight change tensor(37.7838, grad_fn=<SumBackward0>)
time8 0.0019032955169677734
train_time 0.07864880561828613
eval time 0.14826130867004395
epoch last part time 7.867813110351562e-06
2024-01-23 01:02:42,131 MainThread INFO: EPOCH:427
2024-01-23 01:02:42,131 MainThread INFO: Time Consumed:0.2303907871246338s
2024-01-23 01:02:42,131 MainThread INFO: Total Frames:64950s
  4%|▍         | 428/10000 [04:09<40:52,  3.90it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11604.64929
Train_Epoch_Reward                14275.74186
Running_Training_Average_Rewards  11380.57328
Explore_Time                      0.00096
Train___Time                      0.07865
Eval____Time                      0.14826
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11373.62121
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.85917     0.95494    91.31715     88.74943
alpha_0                           0.80734      0.00011    0.80750      0.80718
Alpha_loss                        -1.44091     0.00106    -1.43981     -1.44256
Training/policy_loss              -2.52365     0.00219    -2.51945     -2.52557
Training/qf1_loss                 7412.54092   415.23353  7857.36328   6740.07666
Training/qf2_loss                 15940.71719  528.01794  16637.44727  15136.93164
Training/pf_norm                  0.14022      0.01884    0.16954      0.11355
Training/qf1_norm                 1404.92620   211.65529  1716.76440   1204.06775
Training/qf2_norm                 396.54761    4.06599    402.98093    392.19290
log_std/mean                      -0.13807     0.00005    -0.13802     -0.13815
log_probs/mean                    -2.73607     0.00263    -2.73094     -2.73816
mean/mean                         -0.00104     0.00006    -0.00097     -0.00111
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019261837005615234
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70197
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [65250]
collect time 0.0010104179382324219
inner_dict_sum {'sac_diff0': 0.00023126602172851562, 'sac_diff1': 0.007056713104248047, 'sac_diff2': 0.008109569549560547, 'sac_diff3': 0.010503768920898438, 'sac_diff4': 0.007031917572021484, 'sac_diff5': 0.0333399772644043, 'sac_diff6': 0.0003986358642578125, 'all': 0.06667184829711914}
diff5_list [0.0064814090728759766, 0.006356477737426758, 0.007472515106201172, 0.006662130355834961, 0.00636744499206543]
time3 0
time4 0.06747174263000488
time5 0.06751847267150879
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4700)
policy weight change tensor(37.6687, grad_fn=<SumBackward0>)
time8 0.002039194107055664
train_time 0.07880091667175293
eval time 0.15832185745239258
epoch last part time 7.3909759521484375e-06
2024-01-23 01:02:42,394 MainThread INFO: EPOCH:428
2024-01-23 01:02:42,394 MainThread INFO: Time Consumed:0.24056172370910645s
2024-01-23 01:02:42,395 MainThread INFO: Total Frames:65100s
  4%|▍         | 429/10000 [04:10<41:11,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11598.47624
Train_Epoch_Reward                4104.70451
Running_Training_Average_Rewards  11092.68380
Explore_Time                      0.00101
Train___Time                      0.07880
Eval____Time                      0.15832
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11431.75793
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.21530     1.71258     93.52042     88.90137
alpha_0                           0.80694      0.00011     0.80710      0.80678
Alpha_loss                        -1.44293     0.00085     -1.44152     -1.44403
Training/policy_loss              -2.53306     0.00208     -2.52959     -2.53554
Training/qf1_loss                 7743.02158   975.05537   9358.45898   6400.24658
Training/qf2_loss                 16460.84785  1236.86746  18441.47852  14745.90430
Training/pf_norm                  0.12444      0.02913     0.16727      0.09505
Training/qf1_norm                 2088.14768   332.81762   2554.35913   1613.73474
Training/qf2_norm                 407.19695    7.42109     416.35291    396.96716
log_std/mean                      -0.14211     0.00030     -0.14163     -0.14245
log_probs/mean                    -2.72975     0.00222     -2.72633     -2.73252
mean/mean                         -0.00242     0.00003     -0.00238     -0.00246
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018468379974365234
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70197
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [65400]
collect time 0.0009288787841796875
inner_dict_sum {'sac_diff0': 0.0002257823944091797, 'sac_diff1': 0.007039308547973633, 'sac_diff2': 0.00857996940612793, 'sac_diff3': 0.010869979858398438, 'sac_diff4': 0.007094144821166992, 'sac_diff5': 0.03244328498840332, 'sac_diff6': 0.0003993511199951172, 'all': 0.06665182113647461}
diff5_list [0.007523536682128906, 0.006277322769165039, 0.006273508071899414, 0.006261110305786133, 0.006107807159423828]
time3 0
time4 0.06744050979614258
time5 0.0674893856048584
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4700)
policy weight change tensor(37.5463, grad_fn=<SumBackward0>)
time8 0.0017838478088378906
train_time 0.07843732833862305
eval time 0.1498868465423584
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:42,648 MainThread INFO: EPOCH:429
2024-01-23 01:02:42,649 MainThread INFO: Time Consumed:0.23166728019714355s
2024-01-23 01:02:42,649 MainThread INFO: Total Frames:65250s
  4%|▍         | 430/10000 [04:10<41:00,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11583.12918
Train_Epoch_Reward                24429.51161
Running_Training_Average_Rewards  11497.01089
Explore_Time                      0.00092
Train___Time                      0.07844
Eval____Time                      0.14989
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11453.71911
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.78804     2.17554     94.72076     89.05009
alpha_0                           0.80653      0.00011     0.80669      0.80637
Alpha_loss                        -1.44693     0.00098     -1.44522     -1.44822
Training/policy_loss              -2.50800     0.00232     -2.50402     -2.51028
Training/qf1_loss                 7843.26895   760.86038   8959.09473   6798.64160
Training/qf2_loss                 16807.77031  1136.10897  18285.22852  15244.32617
Training/pf_norm                  0.10954      0.01098     0.11939      0.08875
Training/qf1_norm                 441.42444    263.30529   839.54919    127.87957
Training/qf2_norm                 382.35850    8.93563     394.52023    371.23627
log_std/mean                      -0.13129     0.00015     -0.13107     -0.13150
log_probs/mean                    -2.73273     0.00270     -2.72833     -2.73561
mean/mean                         -0.00201     0.00002     -0.00196     -0.00204
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019042253494262695
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70197
epoch first part time 3.814697265625e-06
replay_buffer._size: [65550]
collect time 0.00099945068359375
inside mustsac before update, task 0, sumup 70197
inside mustsac after update, task 0, sumup 70564
inner_dict_sum {'sac_diff0': 0.00020813941955566406, 'sac_diff1': 0.011222600936889648, 'sac_diff2': 0.010819435119628906, 'sac_diff3': 0.011771917343139648, 'sac_diff4': 0.008357524871826172, 'sac_diff5': 0.054517507553100586, 'sac_diff6': 0.0004227161407470703, 'all': 0.0973198413848877}
diff5_list [0.010298967361450195, 0.009723186492919922, 0.009992361068725586, 0.01182103157043457, 0.012681961059570312]
time3 0.0008838176727294922
time4 0.09829020500183105
time5 0.0983586311340332
time7 0.010268211364746094
gen_weight_change tensor(-22.4218)
policy weight change tensor(37.5358, grad_fn=<SumBackward0>)
time8 0.0027670860290527344
train_time 0.13018465042114258
eval time 0.09870457649230957
epoch last part time 7.152557373046875e-06
2024-01-23 01:02:42,904 MainThread INFO: EPOCH:430
2024-01-23 01:02:42,904 MainThread INFO: Time Consumed:0.23230409622192383s
2024-01-23 01:02:42,904 MainThread INFO: Total Frames:65400s
  4%|▍         | 431/10000 [04:10<41:05,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11559.14646
Train_Epoch_Reward                10061.03394
Running_Training_Average_Rewards  11556.46360
Explore_Time                      0.00099
Train___Time                      0.13018
Eval____Time                      0.09870
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11451.65258
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.74384     2.22179     92.57484     86.57179
alpha_0                           0.80613      0.00011     0.80629      0.80597
Alpha_loss                        -1.44981     0.00093     -1.44903     -1.45158
Training/policy_loss              -2.52716     0.01787     -2.50907     -2.55331
Training/qf1_loss                 7630.68867   886.37229   8644.29102   6379.38574
Training/qf2_loss                 16124.60703  1130.05246  17570.84766  14252.61133
Training/pf_norm                  0.12497      0.02252     0.14498      0.08472
Training/qf1_norm                 1588.30815   634.83991   2258.48511   800.44849
Training/qf2_norm                 391.10994    22.79669    423.12463    361.71158
log_std/mean                      -0.13186     0.00394     -0.12548     -0.13658
log_probs/mean                    -2.73047     0.00722     -2.72372     -2.74181
mean/mean                         -0.00194     0.00065     -0.00115     -0.00299
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019782543182373047
epoch last part time3 0.0027441978454589844
inside rlalgo, task 0, sumup 70564
epoch first part time 5.245208740234375e-06
replay_buffer._size: [65700]
collect time 0.0009343624114990234
inner_dict_sum {'sac_diff0': 0.00021886825561523438, 'sac_diff1': 0.0068988800048828125, 'sac_diff2': 0.0077667236328125, 'sac_diff3': 0.00997018814086914, 'sac_diff4': 0.00702977180480957, 'sac_diff5': 0.031080245971679688, 'sac_diff6': 0.00037741661071777344, 'all': 0.06334209442138672}
diff5_list [0.006712913513183594, 0.006193399429321289, 0.005949735641479492, 0.006146669387817383, 0.00607752799987793]
time3 0
time4 0.06408333778381348
time5 0.06412863731384277
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4218)
policy weight change tensor(37.4791, grad_fn=<SumBackward0>)
time8 0.0018374919891357422
train_time 0.07525515556335449
eval time 0.15002894401550293
epoch last part time 6.198883056640625e-06
2024-01-23 01:02:43,158 MainThread INFO: EPOCH:431
2024-01-23 01:02:43,158 MainThread INFO: Time Consumed:0.22855496406555176s
2024-01-23 01:02:43,158 MainThread INFO: Total Frames:65550s
  4%|▍         | 432/10000 [04:10<40:42,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11615.38527
Train_Epoch_Reward                12168.09018
Running_Training_Average_Rewards  11565.72384
Explore_Time                      0.00093
Train___Time                      0.07526
Eval____Time                      0.15003
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12322.91666
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.75696     1.61335    92.77187     88.41101
alpha_0                           0.80573      0.00011    0.80589      0.80557
Alpha_loss                        -1.45331     0.00106    -1.45157     -1.45451
Training/policy_loss              -2.50996     0.00676    -2.49798     -2.51734
Training/qf1_loss                 7331.82822   485.14030  7922.29443   6707.88770
Training/qf2_loss                 16026.23047  750.79228  16971.66016  15006.55957
Training/pf_norm                  0.10160      0.01843    0.12232      0.07395
Training/qf1_norm                 1738.44873   295.47920  2065.00952   1290.79590
Training/qf2_norm                 381.65073    6.58371    389.65201    371.90140
log_std/mean                      -0.12378     0.00005    -0.12374     -0.12387
log_probs/mean                    -2.73108     0.00798    -2.71682     -2.73975
mean/mean                         -0.00336     0.00014    -0.00316     -0.00354
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.017985105514526367
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70564
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [65850]
collect time 0.0009312629699707031
inner_dict_sum {'sac_diff0': 0.0002079010009765625, 'sac_diff1': 0.0070400238037109375, 'sac_diff2': 0.008399486541748047, 'sac_diff3': 0.010433673858642578, 'sac_diff4': 0.00684356689453125, 'sac_diff5': 0.03335928916931152, 'sac_diff6': 0.000392913818359375, 'all': 0.06667685508728027}
diff5_list [0.007911920547485352, 0.006540536880493164, 0.006322622299194336, 0.0062808990478515625, 0.006303310394287109]
time3 0
time4 0.06746888160705566
time5 0.0675206184387207
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4218)
policy weight change tensor(37.4200, grad_fn=<SumBackward0>)
time8 0.001867532730102539
train_time 0.07877874374389648
eval time 0.15621232986450195
epoch last part time 7.152557373046875e-06
2024-01-23 01:02:43,418 MainThread INFO: EPOCH:432
2024-01-23 01:02:43,418 MainThread INFO: Time Consumed:0.23832154273986816s
2024-01-23 01:02:43,418 MainThread INFO: Total Frames:65700s
  4%|▍         | 433/10000 [04:11<40:57,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11673.08087
Train_Epoch_Reward                13255.99222
Running_Training_Average_Rewards  11265.76640
Explore_Time                      0.00093
Train___Time                      0.07878
Eval____Time                      0.15621
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12337.68242
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.12655     2.89765     95.09233     86.42365
alpha_0                           0.80532      0.00011     0.80549      0.80516
Alpha_loss                        -1.45744     0.00114     -1.45541     -1.45892
Training/policy_loss              -2.54812     0.00666     -2.53507     -2.55367
Training/qf1_loss                 7621.21123   1068.81384  8815.56250   5890.83154
Training/qf2_loss                 16392.02676  1559.12011  18048.88867  13831.39844
Training/pf_norm                  0.11318      0.03049     0.15833      0.08006
Training/qf1_norm                 1451.95948   527.42012   2144.04785   605.78986
Training/qf2_norm                 426.16605    13.12672    444.19989    404.86078
log_std/mean                      -0.13682     0.00010     -0.13665     -0.13695
log_probs/mean                    -2.73465     0.00794     -2.71903     -2.74091
mean/mean                         -0.00224     0.00005     -0.00218     -0.00233
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018691062927246094
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70564
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [66000]
collect time 0.0008490085601806641
inner_dict_sum {'sac_diff0': 0.00024271011352539062, 'sac_diff1': 0.006935596466064453, 'sac_diff2': 0.007928609848022461, 'sac_diff3': 0.010737180709838867, 'sac_diff4': 0.006730556488037109, 'sac_diff5': 0.032117605209350586, 'sac_diff6': 0.00040078163146972656, 'all': 0.0650930404663086}
diff5_list [0.006499290466308594, 0.006272792816162109, 0.006227016448974609, 0.006336688995361328, 0.006781816482543945]
time3 0
time4 0.06591367721557617
time5 0.0659635066986084
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4218)
policy weight change tensor(37.3844, grad_fn=<SumBackward0>)
time8 0.0019528865814208984
train_time 0.07699322700500488
eval time 0.1508786678314209
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:43,671 MainThread INFO: EPOCH:433
2024-01-23 01:02:43,671 MainThread INFO: Time Consumed:0.23109984397888184s
2024-01-23 01:02:43,672 MainThread INFO: Total Frames:65850s
  4%|▍         | 434/10000 [04:11<40:45,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11733.06855
Train_Epoch_Reward                2834.54234
Running_Training_Average_Rewards  11201.21291
Explore_Time                      0.00084
Train___Time                      0.07699
Eval____Time                      0.15088
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12318.99287
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.61048     2.24328     93.44711     86.49923
alpha_0                           0.80492      0.00011     0.80508      0.80476
Alpha_loss                        -1.46056     0.00181     -1.45832     -1.46337
Training/policy_loss              -2.54723     0.00458     -2.54169     -2.55462
Training/qf1_loss                 7507.06279   1185.19858  8918.29297   5810.58203
Training/qf2_loss                 16027.70020  1509.53301  18135.61719  13767.18262
Training/pf_norm                  0.11000      0.02860     0.13556      0.07339
Training/qf1_norm                 743.25420    455.22058   1512.43018   96.39964
Training/qf2_norm                 422.80937    10.39171    440.52435    408.39255
log_std/mean                      -0.13266     0.00004     -0.13261     -0.13273
log_probs/mean                    -2.73349     0.00591     -2.72628     -2.74332
mean/mean                         -0.00214     0.00002     -0.00212     -0.00216
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018223285675048828
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70564
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [66150]
collect time 0.0008444786071777344
inner_dict_sum {'sac_diff0': 0.000202178955078125, 'sac_diff1': 0.007527589797973633, 'sac_diff2': 0.008664608001708984, 'sac_diff3': 0.010491609573364258, 'sac_diff4': 0.007030963897705078, 'sac_diff5': 0.032395362854003906, 'sac_diff6': 0.0003974437713623047, 'all': 0.06670975685119629}
diff5_list [0.0075380802154541016, 0.006282806396484375, 0.006197690963745117, 0.0063021183013916016, 0.006074666976928711]
time3 0
time4 0.0675044059753418
time5 0.06755375862121582
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4218)
policy weight change tensor(37.3551, grad_fn=<SumBackward0>)
time8 0.0019459724426269531
train_time 0.07885980606079102
eval time 0.15478897094726562
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:43,930 MainThread INFO: EPOCH:434
2024-01-23 01:02:43,930 MainThread INFO: Time Consumed:0.23690557479858398s
2024-01-23 01:02:43,930 MainThread INFO: Total Frames:66000s
  4%|▍         | 435/10000 [04:11<40:58,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11800.05201
Train_Epoch_Reward                17895.96985
Running_Training_Average_Rewards  11061.04163
Explore_Time                      0.00084
Train___Time                      0.07886
Eval____Time                      0.15479
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12340.59481
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.90507     1.87710     94.38081     88.96872
alpha_0                           0.80452      0.00011     0.80468      0.80436
Alpha_loss                        -1.46338     0.00135     -1.46217     -1.46586
Training/policy_loss              -2.49089     0.00436     -2.48624     -2.49721
Training/qf1_loss                 7903.24336   1205.91643  9544.54297   6580.18066
Training/qf2_loss                 16859.68008  1371.58024  18501.26562  15346.59863
Training/pf_norm                  0.11924      0.02333     0.15015      0.08707
Training/qf1_norm                 763.89969    323.01264   1210.33875   250.15213
Training/qf2_norm                 396.88091    7.83466     407.42755    384.94193
log_std/mean                      -0.13571     0.00008     -0.13563     -0.13582
log_probs/mean                    -2.73098     0.00532     -2.72540     -2.73841
mean/mean                         -0.00306     0.00008     -0.00293     -0.00315
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01977252960205078
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70564
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [66300]
collect time 0.0008895397186279297
inside mustsac before update, task 0, sumup 70564
inside mustsac after update, task 0, sumup 71338
inner_dict_sum {'sac_diff0': 0.00020456314086914062, 'sac_diff1': 0.006780862808227539, 'sac_diff2': 0.008132696151733398, 'sac_diff3': 0.010516881942749023, 'sac_diff4': 0.007152080535888672, 'sac_diff5': 0.049770355224609375, 'sac_diff6': 0.0004127025604248047, 'all': 0.08297014236450195}
diff5_list [0.01059412956237793, 0.010012388229370117, 0.009858131408691406, 0.009493827819824219, 0.009811878204345703]
time3 0.0008473396301269531
time4 0.0838782787322998
time5 0.08393144607543945
time7 0.009167909622192383
gen_weight_change tensor(-22.5816)
policy weight change tensor(37.3987, grad_fn=<SumBackward0>)
time8 0.002054452896118164
train_time 0.11342024803161621
eval time 0.11363506317138672
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:44,184 MainThread INFO: EPOCH:435
2024-01-23 01:02:44,184 MainThread INFO: Time Consumed:0.23036575317382812s
2024-01-23 01:02:44,184 MainThread INFO: Total Frames:66150s
  4%|▍         | 436/10000 [04:11<40:47,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11875.93893
Train_Epoch_Reward                11619.71516
Running_Training_Average_Rewards  11104.42817
Explore_Time                      0.00088
Train___Time                      0.11342
Eval____Time                      0.11364
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12409.13654
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.62604     2.39739    94.94945     88.03502
alpha_0                           0.80412      0.00011    0.80428      0.80396
Alpha_loss                        -1.46655     0.00184    -1.46406     -1.46981
Training/policy_loss              -2.53242     0.02162    -2.49190     -2.55021
Training/qf1_loss                 7362.47285   547.34181  7992.77979   6364.57910
Training/qf2_loss                 16265.72285  992.57554  17302.45898  14489.37500
Training/pf_norm                  0.11416      0.02611    0.14941      0.07857
Training/qf1_norm                 898.78378    589.85243  1822.01819   295.06177
Training/qf2_norm                 406.81617    24.61747   430.79190    371.59402
log_std/mean                      -0.12572     0.00731    -0.11832     -0.13663
log_probs/mean                    -2.73007     0.00500    -2.72480     -2.73883
mean/mean                         -0.00067     0.00055    0.00004      -0.00155
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0190274715423584
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71338
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [66450]
collect time 0.0008764266967773438
inner_dict_sum {'sac_diff0': 0.00021266937255859375, 'sac_diff1': 0.007373332977294922, 'sac_diff2': 0.008487462997436523, 'sac_diff3': 0.011161327362060547, 'sac_diff4': 0.0073719024658203125, 'sac_diff5': 0.033286094665527344, 'sac_diff6': 0.0003902912139892578, 'all': 0.0682830810546875}
diff5_list [0.0069427490234375, 0.007483959197998047, 0.006833076477050781, 0.006171703338623047, 0.005854606628417969]
time3 0
time4 0.06906747817993164
time5 0.06911611557006836
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5816)
policy weight change tensor(37.4049, grad_fn=<SumBackward0>)
time8 0.0038166046142578125
train_time 0.08229875564575195
eval time 0.15383696556091309
epoch last part time 1.2636184692382812e-05
2024-01-23 01:02:44,446 MainThread INFO: EPOCH:436
2024-01-23 01:02:44,446 MainThread INFO: Time Consumed:0.2394862174987793s
2024-01-23 01:02:44,446 MainThread INFO: Total Frames:66300s
  4%|▍         | 437/10000 [04:12<41:03,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11964.61955
Train_Epoch_Reward                23626.56601
Running_Training_Average_Rewards  11493.55748
Explore_Time                      0.00087
Train___Time                      0.08230
Eval____Time                      0.15384
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12206.12140
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.06792     2.44890     93.18512     85.83401
alpha_0                           0.80372      0.00011     0.80388      0.80355
Alpha_loss                        -1.46993     0.00099     -1.46881     -1.47149
Training/policy_loss              -2.54369     0.00311     -2.53791     -2.54660
Training/qf1_loss                 7582.53564   767.23075   8623.43359   6336.24219
Training/qf2_loss                 15818.60273  1031.97896  16752.19336  13947.89355
Training/pf_norm                  0.11121      0.02019     0.13628      0.07458
Training/qf1_norm                 3253.99702   400.42839   3761.99780   2549.36719
Training/qf2_norm                 429.76595    11.20066    443.67538    410.46951
log_std/mean                      -0.12799     0.00003     -0.12795     -0.12801
log_probs/mean                    -2.73017     0.00385     -2.72331     -2.73422
mean/mean                         -0.00032     0.00008     -0.00023     -0.00046
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018618345260620117
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71338
epoch first part time 7.152557373046875e-06
replay_buffer._size: [66600]
collect time 0.0008580684661865234
inner_dict_sum {'sac_diff0': 0.00020813941955566406, 'sac_diff1': 0.007608890533447266, 'sac_diff2': 0.008664369583129883, 'sac_diff3': 0.011089324951171875, 'sac_diff4': 0.0073163509368896484, 'sac_diff5': 0.033255815505981445, 'sac_diff6': 0.0003955364227294922, 'all': 0.06853842735290527}
diff5_list [0.0070116519927978516, 0.007417917251586914, 0.006539821624755859, 0.006175994873046875, 0.006110429763793945]
time3 0
time4 0.06936955451965332
time5 0.06942200660705566
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5816)
policy weight change tensor(37.4860, grad_fn=<SumBackward0>)
time8 0.0019445419311523438
train_time 0.08068275451660156
eval time 0.14292573928833008
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:44,695 MainThread INFO: EPOCH:437
2024-01-23 01:02:44,695 MainThread INFO: Time Consumed:0.22687458992004395s
2024-01-23 01:02:44,695 MainThread INFO: Total Frames:66450s
  4%|▍         | 438/10000 [04:12<40:38,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12058.24313
Train_Epoch_Reward                4113.33316
Running_Training_Average_Rewards  11534.59861
Explore_Time                      0.00085
Train___Time                      0.08068
Eval____Time                      0.14293
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12309.85694
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.28045     2.47654     93.45659     87.08148
alpha_0                           0.80331      0.00011     0.80347      0.80315
Alpha_loss                        -1.47490     0.00148     -1.47233     -1.47666
Training/policy_loss              -2.55692     0.00378     -2.55284     -2.56368
Training/qf1_loss                 7851.21357   1047.82671  9301.38672   6409.76465
Training/qf2_loss                 16704.55703  1501.49085  18458.28125  14433.51855
Training/pf_norm                  0.10214      0.02066     0.13006      0.07962
Training/qf1_norm                 736.25608    504.02493   1578.16260   306.75479
Training/qf2_norm                 420.87256    11.05035    430.63809    402.32123
log_std/mean                      -0.12953     0.00021     -0.12926     -0.12984
log_probs/mean                    -2.73749     0.00463     -2.73190     -2.74545
mean/mean                         -0.00064     0.00011     -0.00049     -0.00080
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01852273941040039
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71338
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [66750]
collect time 0.0009121894836425781
inner_dict_sum {'sac_diff0': 0.0002071857452392578, 'sac_diff1': 0.006752729415893555, 'sac_diff2': 0.008023977279663086, 'sac_diff3': 0.010662555694580078, 'sac_diff4': 0.006673336029052734, 'sac_diff5': 0.03280019760131836, 'sac_diff6': 0.00039649009704589844, 'all': 0.06551647186279297}
diff5_list [0.0077173709869384766, 0.006506681442260742, 0.0063571929931640625, 0.0060901641845703125, 0.006128787994384766]
time3 0
time4 0.06627774238586426
time5 0.06632375717163086
time7 7.152557373046875e-07
gen_weight_change tensor(-22.5816)
policy weight change tensor(37.4743, grad_fn=<SumBackward0>)
time8 0.0019145011901855469
train_time 0.07749247550964355
eval time 0.14889216423034668
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:44,946 MainThread INFO: EPOCH:438
2024-01-23 01:02:44,947 MainThread INFO: Time Consumed:0.2296900749206543s
2024-01-23 01:02:44,947 MainThread INFO: Total Frames:66600s
  4%|▍         | 439/10000 [04:12<40:28,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12156.14666
Train_Epoch_Reward                9763.48475
Running_Training_Average_Rewards  11685.92529
Explore_Time                      0.00091
Train___Time                      0.07749
Eval____Time                      0.14889
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12410.79326
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.60751     0.74164    90.78298     88.45345
alpha_0                           0.80291      0.00011    0.80307      0.80275
Alpha_loss                        -1.47698     0.00123    -1.47545     -1.47885
Training/policy_loss              -2.50427     0.00588    -2.49329     -2.50926
Training/qf1_loss                 6787.04639   872.83058  8465.94336   6042.67969
Training/qf2_loss                 15327.26016  986.60543  17237.14453  14602.28906
Training/pf_norm                  0.09467      0.01663    0.11434      0.06561
Training/qf1_norm                 395.84105    135.17502  619.20258    206.02530
Training/qf2_norm                 397.97179    3.27786    403.18924    392.83310
log_std/mean                      -0.13976     0.00009    -0.13959     -0.13985
log_probs/mean                    -2.73161     0.00708    -2.71850     -2.73732
mean/mean                         -0.00101     0.00009    -0.00089     -0.00114
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018434524536132812
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71338
epoch first part time 2.384185791015625e-06
replay_buffer._size: [66900]
collect time 0.0008511543273925781
inner_dict_sum {'sac_diff0': 0.00020766258239746094, 'sac_diff1': 0.0073468685150146484, 'sac_diff2': 0.008687496185302734, 'sac_diff3': 0.010757207870483398, 'sac_diff4': 0.007260322570800781, 'sac_diff5': 0.03279304504394531, 'sac_diff6': 0.0003998279571533203, 'all': 0.06745243072509766}
diff5_list [0.006602764129638672, 0.00736236572265625, 0.006531476974487305, 0.006198406219482422, 0.006098031997680664]
time3 0
time4 0.06826901435852051
time5 0.06832575798034668
time7 4.76837158203125e-07
gen_weight_change tensor(-22.5816)
policy weight change tensor(37.4403, grad_fn=<SumBackward0>)
time8 0.0018274784088134766
train_time 0.07927155494689941
eval time 0.14982032775878906
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:45,201 MainThread INFO: EPOCH:439
2024-01-23 01:02:45,201 MainThread INFO: Time Consumed:0.23243379592895508s
2024-01-23 01:02:45,201 MainThread INFO: Total Frames:66750s
  4%|▍         | 440/10000 [04:12<40:36,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12265.31330
Train_Epoch_Reward                2659.92003
Running_Training_Average_Rewards  11340.69680
Explore_Time                      0.00085
Train___Time                      0.07927
Eval____Time                      0.14982
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12545.38551
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.45975     2.71500     94.39271     86.46547
alpha_0                           0.80251      0.00011     0.80267      0.80235
Alpha_loss                        -1.48149     0.00170     -1.47825     -1.48308
Training/policy_loss              -2.52229     0.00489     -2.51325     -2.52737
Training/qf1_loss                 7138.27314   975.90446   8540.10547   5714.73340
Training/qf2_loss                 15692.42715  1417.12043  17750.48047  13610.03320
Training/pf_norm                  0.10274      0.02454     0.14341      0.07875
Training/qf1_norm                 2252.17505   485.94244   2974.99390   1535.74976
Training/qf2_norm                 393.96937    11.54264    410.60880    376.78729
log_std/mean                      -0.12049     0.00007     -0.12040     -0.12061
log_probs/mean                    -2.73681     0.00619     -2.72513     -2.74290
mean/mean                         0.00054      0.00007     0.00061      0.00042
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02087116241455078
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71338
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [67050]
collect time 0.000885009765625
inside mustsac before update, task 0, sumup 71338
inside mustsac after update, task 0, sumup 70606
inner_dict_sum {'sac_diff0': 0.00020694732666015625, 'sac_diff1': 0.007226228713989258, 'sac_diff2': 0.008614540100097656, 'sac_diff3': 0.010859489440917969, 'sac_diff4': 0.0073206424713134766, 'sac_diff5': 0.05192899703979492, 'sac_diff6': 0.0004096031188964844, 'all': 0.08656644821166992}
diff5_list [0.01131296157836914, 0.01120448112487793, 0.01030278205871582, 0.009803533554077148, 0.009305238723754883]
time3 0.0008554458618164062
time4 0.08744120597839355
time5 0.0874948501586914
time7 0.00951075553894043
gen_weight_change tensor(-22.7888)
policy weight change tensor(37.5298, grad_fn=<SumBackward0>)
time8 0.0028138160705566406
train_time 0.11866879463195801
eval time 0.1274876594543457
epoch last part time 7.3909759521484375e-06
2024-01-23 01:02:45,475 MainThread INFO: EPOCH:440
2024-01-23 01:02:45,475 MainThread INFO: Time Consumed:0.2495410442352295s
2024-01-23 01:02:45,476 MainThread INFO: Total Frames:66900s
  4%|▍         | 441/10000 [04:13<41:40,  3.82it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12392.50728
Train_Epoch_Reward                9164.25464
Running_Training_Average_Rewards  11124.87631
Explore_Time                      0.00088
Train___Time                      0.11867
Eval____Time                      0.12749
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12723.59234
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.37123     1.42192    91.30605     87.03274
alpha_0                           0.80211      0.00011    0.80227      0.80195
Alpha_loss                        -1.48452     0.00131    -1.48249     -1.48605
Training/policy_loss              -2.52690     0.02798    -2.49536     -2.57329
Training/qf1_loss                 7418.06816   694.53435  8710.38867   6765.32275
Training/qf2_loss                 15897.38457  886.13263  17514.93750  14920.45898
Training/pf_norm                  0.09416      0.00878    0.10739      0.08190
Training/qf1_norm                 621.91089    451.35926  1428.22534   255.04955
Training/qf2_norm                 400.18997    23.81336   444.00970    373.41589
log_std/mean                      -0.12657     0.00606    -0.11778     -0.13648
log_probs/mean                    -2.73528     0.00501    -2.72913     -2.74221
mean/mean                         -0.00026     0.00058    0.00030      -0.00138
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.021048545837402344
epoch last part time3 0.0029294490814208984
inside rlalgo, task 0, sumup 70606
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [67200]
collect time 0.0009250640869140625
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.007192373275756836, 'sac_diff2': 0.008645772933959961, 'sac_diff3': 0.010776758193969727, 'sac_diff4': 0.007311582565307617, 'sac_diff5': 0.03288912773132324, 'sac_diff6': 0.0003829002380371094, 'all': 0.06740903854370117}
diff5_list [0.007322549819946289, 0.006501197814941406, 0.006278038024902344, 0.006323337554931641, 0.0064640045166015625]
time3 0
time4 0.06817197799682617
time5 0.06821632385253906
time7 7.152557373046875e-07
gen_weight_change tensor(-22.7888)
policy weight change tensor(37.4521, grad_fn=<SumBackward0>)
time8 0.0019159317016601562
train_time 0.07943606376647949
eval time 0.1768045425415039
epoch last part time 7.867813110351562e-06
2024-01-23 01:02:45,762 MainThread INFO: EPOCH:441
2024-01-23 01:02:45,762 MainThread INFO: Time Consumed:0.2595977783203125s
2024-01-23 01:02:45,763 MainThread INFO: Total Frames:67050s
  4%|▍         | 442/10000 [04:13<42:38,  3.74it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12383.29827
Train_Epoch_Reward                3576.51784
Running_Training_Average_Rewards  11160.78760
Explore_Time                      0.00092
Train___Time                      0.07944
Eval____Time                      0.17680
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12230.82660
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.69971     1.60237    91.68147     87.34936
alpha_0                           0.80171      0.00011    0.80187      0.80155
Alpha_loss                        -1.48776     0.00151    -1.48640     -1.49061
Training/policy_loss              -2.54457     0.00328    -2.54009     -2.54973
Training/qf1_loss                 6524.53623   677.41717  7514.63770   5448.69336
Training/qf2_loss                 14885.46953  884.38350  16442.58594  13697.15625
Training/pf_norm                  0.10901      0.01438    0.12402      0.08875
Training/qf1_norm                 374.09440    103.39037  461.73245    185.00362
Training/qf2_norm                 433.51353    7.64231    447.76038    426.65372
log_std/mean                      -0.13160     0.00021    -0.13130     -0.13185
log_probs/mean                    -2.73467     0.00412    -2.72947     -2.74148
mean/mean                         -0.00079     0.00004    -0.00075     -0.00087
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018848180770874023
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70606
epoch first part time 2.86102294921875e-06
replay_buffer._size: [67350]
collect time 0.0008523464202880859
inner_dict_sum {'sac_diff0': 0.0002186298370361328, 'sac_diff1': 0.007485151290893555, 'sac_diff2': 0.008599042892456055, 'sac_diff3': 0.011040210723876953, 'sac_diff4': 0.007264614105224609, 'sac_diff5': 0.03458523750305176, 'sac_diff6': 0.0004165172576904297, 'all': 0.06960940361022949}
diff5_list [0.007932424545288086, 0.007287502288818359, 0.006532430648803711, 0.006321907043457031, 0.00651097297668457]
time3 0
time4 0.07048892974853516
time5 0.07054257392883301
time7 4.76837158203125e-07
gen_weight_change tensor(-22.7888)
policy weight change tensor(37.4024, grad_fn=<SumBackward0>)
time8 0.002008676528930664
train_time 0.08205676078796387
eval time 0.1829538345336914
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:46,053 MainThread INFO: EPOCH:442
2024-01-23 01:02:46,053 MainThread INFO: Time Consumed:0.2682464122772217s
2024-01-23 01:02:46,053 MainThread INFO: Total Frames:67200s
  4%|▍         | 443/10000 [04:13<43:44,  3.64it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12384.01231
Train_Epoch_Reward                19955.07189
Running_Training_Average_Rewards  11284.51392
Explore_Time                      0.00085
Train___Time                      0.08206
Eval____Time                      0.18295
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12344.82280
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.32107     1.96209     92.74521     87.48488
alpha_0                           0.80131      0.00011     0.80147      0.80115
Alpha_loss                        -1.49119     0.00125     -1.49010     -1.49358
Training/policy_loss              -2.50590     0.00505     -2.50050     -2.51448
Training/qf1_loss                 6955.93535   867.03321   8379.16016   5689.86475
Training/qf2_loss                 15538.42656  1181.94397  17376.33984  13790.31250
Training/pf_norm                  0.09957      0.02273     0.13530      0.06541
Training/qf1_norm                 1824.48513   366.07280   2279.65088   1276.20581
Training/qf2_norm                 382.61128    8.03469     392.29929    371.06116
log_std/mean                      -0.12451     0.00009     -0.12442     -0.12465
log_probs/mean                    -2.73497     0.00618     -2.72878     -2.74575
mean/mean                         0.00121      0.00003     0.00124      0.00116
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018873929977416992
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70606
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [67500]
collect time 0.0008025169372558594
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.007247447967529297, 'sac_diff2': 0.008805513381958008, 'sac_diff3': 0.01080012321472168, 'sac_diff4': 0.007327556610107422, 'sac_diff5': 0.03388643264770508, 'sac_diff6': 0.00039887428283691406, 'all': 0.06867337226867676}
diff5_list [0.006687164306640625, 0.006478786468505859, 0.007904529571533203, 0.006364583969116211, 0.00645136833190918]
time3 0
time4 0.0695042610168457
time5 0.06955838203430176
time7 4.76837158203125e-07
gen_weight_change tensor(-22.7888)
policy weight change tensor(37.4222, grad_fn=<SumBackward0>)
time8 0.0017840862274169922
train_time 0.08049392700195312
eval time 0.18851923942565918
epoch last part time 7.152557373046875e-06
2024-01-23 01:02:46,347 MainThread INFO: EPOCH:443
2024-01-23 01:02:46,348 MainThread INFO: Time Consumed:0.2722198963165283s
2024-01-23 01:02:46,348 MainThread INFO: Total Frames:67350s
  4%|▍         | 444/10000 [04:13<44:41,  3.56it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12389.33506
Train_Epoch_Reward                5462.89298
Running_Training_Average_Rewards  10981.55508
Explore_Time                      0.00080
Train___Time                      0.08049
Eval____Time                      0.18852
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12372.22035
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.01064     1.67456    91.84908     87.44994
alpha_0                           0.80091      0.00011    0.80107      0.80075
Alpha_loss                        -1.49472     0.00136    -1.49238     -1.49647
Training/policy_loss              -2.52235     0.00353    -2.51692     -2.52596
Training/qf1_loss                 7146.41064   239.10279  7582.40625   6907.61328
Training/qf2_loss                 15764.87344  462.19814  16546.06250  15239.19336
Training/pf_norm                  0.08860      0.01822    0.11579      0.06584
Training/qf1_norm                 344.58053    153.07526  531.69147    92.23444
Training/qf2_norm                 392.76868    6.95722    400.00671    382.01721
log_std/mean                      -0.13038     0.00006    -0.13031     -0.13048
log_probs/mean                    -2.73570     0.00447    -2.72952     -2.74055
mean/mean                         -0.00032     0.00005    -0.00028     -0.00042
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018871545791625977
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70606
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [67650]
collect time 0.000942230224609375
inner_dict_sum {'sac_diff0': 0.00022029876708984375, 'sac_diff1': 0.00750732421875, 'sac_diff2': 0.008933067321777344, 'sac_diff3': 0.011282205581665039, 'sac_diff4': 0.007496833801269531, 'sac_diff5': 0.03402280807495117, 'sac_diff6': 0.0004177093505859375, 'all': 0.06988024711608887}
diff5_list [0.006623029708862305, 0.00616765022277832, 0.0073528289794921875, 0.007604360580444336, 0.0062749385833740234]
time3 0
time4 0.07071304321289062
time5 0.07076525688171387
time7 9.5367431640625e-07
gen_weight_change tensor(-22.7888)
policy weight change tensor(37.5154, grad_fn=<SumBackward0>)
time8 0.0019378662109375
train_time 0.08208513259887695
eval time 0.19282293319702148
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:46,648 MainThread INFO: EPOCH:444
2024-01-23 01:02:46,649 MainThread INFO: Time Consumed:0.27832460403442383s
2024-01-23 01:02:46,649 MainThread INFO: Total Frames:67500s
  4%|▍         | 445/10000 [04:14<45:41,  3.49it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12386.69702
Train_Epoch_Reward                19252.12316
Running_Training_Average_Rewards  11369.09258
Explore_Time                      0.00094
Train___Time                      0.08209
Eval____Time                      0.19282
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12314.21447
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.85893     2.47722     92.90565     85.46832
alpha_0                           0.80051      0.00011     0.80067      0.80035
Alpha_loss                        -1.49761     0.00189     -1.49407     -1.49973
Training/policy_loss              -2.54357     0.00711     -2.53086     -2.55098
Training/qf1_loss                 7633.07109   949.44819   8719.59375   6165.45996
Training/qf2_loss                 16198.68105  1372.19411  17654.72070  13943.81543
Training/pf_norm                  0.09598      0.03218     0.13382      0.05490
Training/qf1_norm                 894.47295    422.91986   1414.21948   141.21333
Training/qf2_norm                 420.96717    11.40518    435.11734    400.80048
log_std/mean                      -0.12652     0.00016     -0.12632     -0.12677
log_probs/mean                    -2.73352     0.00877     -2.71762     -2.74215
mean/mean                         0.00098      0.00001     0.00100      0.00097
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019593000411987305
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70606
epoch first part time 3.814697265625e-06
replay_buffer._size: [67800]
collect time 0.001020669937133789
inside mustsac before update, task 0, sumup 70606
inside mustsac after update, task 0, sumup 71468
inner_dict_sum {'sac_diff0': 0.0002281665802001953, 'sac_diff1': 0.008189678192138672, 'sac_diff2': 0.009913921356201172, 'sac_diff3': 0.011700630187988281, 'sac_diff4': 0.008382320404052734, 'sac_diff5': 0.05964779853820801, 'sac_diff6': 0.000461578369140625, 'all': 0.09852409362792969}
diff5_list [0.010336160659790039, 0.009663105010986328, 0.010224342346191406, 0.017856597900390625, 0.01156759262084961]
time3 0.0008904933929443359
time4 0.09955358505249023
time5 0.09962201118469238
time7 0.010426044464111328
gen_weight_change tensor(-23.0451)
policy weight change tensor(37.5906, grad_fn=<SumBackward0>)
time8 0.0020558834075927734
train_time 0.13121509552001953
eval time 0.1284325122833252
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:46,935 MainThread INFO: EPOCH:445
2024-01-23 01:02:46,935 MainThread INFO: Time Consumed:0.2629091739654541s
2024-01-23 01:02:46,935 MainThread INFO: Total Frames:67650s
  4%|▍         | 446/10000 [04:14<45:39,  3.49it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12372.04778
Train_Epoch_Reward                12517.19368
Running_Training_Average_Rewards  11179.53156
Explore_Time                      0.00101
Train___Time                      0.13122
Eval____Time                      0.12843
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12262.64417
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.63917     2.24525     93.77353     86.89197
alpha_0                           0.80011      0.00011     0.80027      0.79995
Alpha_loss                        -1.50093     0.00231     -1.49886     -1.50532
Training/policy_loss              -2.53040     0.01667     -2.51489     -2.56157
Training/qf1_loss                 7283.81328   1480.92583  9553.61914   5201.25244
Training/qf2_loss                 16004.60684  1916.27657  18916.81641  13190.39062
Training/pf_norm                  0.12582      0.02325     0.14370      0.08039
Training/qf1_norm                 905.71885    381.45933   1393.16296   427.96506
Training/qf2_norm                 409.84000    15.92963    432.61981    394.11243
log_std/mean                      -0.12887     0.00586     -0.11874     -0.13360
log_probs/mean                    -2.73334     0.00758     -2.72404     -2.74696
mean/mean                         -0.00080     0.00104     0.00045      -0.00254
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019330739974975586
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71468
epoch first part time 5.0067901611328125e-06
replay_buffer._size: [67950]
collect time 0.0009486675262451172
inner_dict_sum {'sac_diff0': 0.0002541542053222656, 'sac_diff1': 0.00693511962890625, 'sac_diff2': 0.008324623107910156, 'sac_diff3': 0.010667085647583008, 'sac_diff4': 0.007007122039794922, 'sac_diff5': 0.032416343688964844, 'sac_diff6': 0.0003762245178222656, 'all': 0.06598067283630371}
diff5_list [0.007619142532348633, 0.006216764450073242, 0.006490468978881836, 0.006136417388916016, 0.005953550338745117]
time3 0
time4 0.06674981117248535
time5 0.06679439544677734
time7 4.76837158203125e-07
gen_weight_change tensor(-23.0451)
policy weight change tensor(37.8012, grad_fn=<SumBackward0>)
time8 0.0017843246459960938
train_time 0.078033447265625
eval time 0.16766738891601562
epoch last part time 7.152557373046875e-06
2024-01-23 01:02:47,207 MainThread INFO: EPOCH:446
2024-01-23 01:02:47,207 MainThread INFO: Time Consumed:0.24906563758850098s
2024-01-23 01:02:47,207 MainThread INFO: Total Frames:67800s
  4%|▍         | 447/10000 [04:14<44:55,  3.54it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12332.99451
Train_Epoch_Reward                17813.52078
Running_Training_Average_Rewards  11700.35926
Explore_Time                      0.00094
Train___Time                      0.07803
Eval____Time                      0.16767
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11815.58869
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.77083     2.16975     93.51064     88.13054
alpha_0                           0.79971      0.00011     0.79987      0.79955
Alpha_loss                        -1.50411     0.00268     -1.50098     -1.50722
Training/policy_loss              -2.53397     0.00708     -2.52699     -2.54483
Training/qf1_loss                 7292.31250   994.03709   8281.75879   5720.86865
Training/qf2_loss                 16015.21582  1426.38953  17516.18555  13952.01562
Training/pf_norm                  0.14032      0.01435     0.15597      0.11668
Training/qf1_norm                 1401.23160   460.06662   1976.12122   843.05701
Training/qf2_norm                 419.43782    9.57104     431.24222    407.75394
log_std/mean                      -0.12165     0.00036     -0.12122     -0.12222
log_probs/mean                    -2.73248     0.00910     -2.72360     -2.74615
mean/mean                         -0.00191     0.00013     -0.00175     -0.00210
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018718957901000977
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71468
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [68100]
collect time 0.0008518695831298828
inner_dict_sum {'sac_diff0': 0.00021839141845703125, 'sac_diff1': 0.007647037506103516, 'sac_diff2': 0.009128093719482422, 'sac_diff3': 0.011463165283203125, 'sac_diff4': 0.00750422477722168, 'sac_diff5': 0.03631925582885742, 'sac_diff6': 0.0004544258117675781, 'all': 0.07273459434509277}
diff5_list [0.0064983367919921875, 0.0065653324127197266, 0.007009267807006836, 0.008391380310058594, 0.007854938507080078]
time3 0
time4 0.07368326187133789
time5 0.07373857498168945
time7 9.5367431640625e-07
gen_weight_change tensor(-23.0451)
policy weight change tensor(37.9715, grad_fn=<SumBackward0>)
time8 0.002103567123413086
train_time 0.08533239364624023
eval time 0.17977309226989746
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:47,498 MainThread INFO: EPOCH:447
2024-01-23 01:02:47,498 MainThread INFO: Time Consumed:0.2684652805328369s
2024-01-23 01:02:47,498 MainThread INFO: Total Frames:67950s
  4%|▍         | 448/10000 [04:15<45:24,  3.51it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12277.69747
Train_Epoch_Reward                36542.32818
Running_Training_Average_Rewards  12619.32419
Explore_Time                      0.00085
Train___Time                      0.08533
Eval____Time                      0.17977
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11756.88655
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.56181     2.23626     91.13321     85.22282
alpha_0                           0.79931      0.00011     0.79947      0.79915
Alpha_loss                        -1.50804     0.00096     -1.50680     -1.50939
Training/policy_loss              -2.54964     0.00492     -2.54347     -2.55793
Training/qf1_loss                 6481.73086   1073.02328  8251.08594   5221.35010
Training/qf2_loss                 14835.81621  1474.22264  17095.93555  12944.14844
Training/pf_norm                  0.11366      0.02568     0.13877      0.06703
Training/qf1_norm                 423.19558    208.29131   770.37189    192.87032
Training/qf2_norm                 405.88091    10.08160    417.78369    390.97290
log_std/mean                      -0.13985     0.00023     -0.13954     -0.14021
log_probs/mean                    -2.73500     0.00574     -2.72831     -2.74462
mean/mean                         -0.00151     0.00002     -0.00148     -0.00153
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020179271697998047
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71468
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [68250]
collect time 0.0009691715240478516
inner_dict_sum {'sac_diff0': 0.0002110004425048828, 'sac_diff1': 0.007428884506225586, 'sac_diff2': 0.008907079696655273, 'sac_diff3': 0.01176762580871582, 'sac_diff4': 0.00780487060546875, 'sac_diff5': 0.03567647933959961, 'sac_diff6': 0.0004150867462158203, 'all': 0.07221102714538574}
diff5_list [0.007759571075439453, 0.006943941116333008, 0.007271289825439453, 0.006625652313232422, 0.0070760250091552734]
time3 0
time4 0.07303357124328613
time5 0.07309865951538086
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0451)
policy weight change tensor(38.0973, grad_fn=<SumBackward0>)
time8 0.002129077911376953
train_time 0.08519411087036133
eval time 0.1847693920135498
epoch last part time 7.152557373046875e-06
2024-01-23 01:02:47,795 MainThread INFO: EPOCH:448
2024-01-23 01:02:47,796 MainThread INFO: Time Consumed:0.27338218688964844s
2024-01-23 01:02:47,796 MainThread INFO: Total Frames:68100s
  4%|▍         | 449/10000 [04:15<45:55,  3.47it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12204.95780
Train_Epoch_Reward                5300.08860
Running_Training_Average_Rewards  12431.08257
Explore_Time                      0.00096
Train___Time                      0.08519
Eval____Time                      0.18477
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11683.39647
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.37268     3.28762     94.60298     86.21420
alpha_0                           0.79891      0.00011     0.79907      0.79875
Alpha_loss                        -1.51100     0.00148     -1.50885     -1.51330
Training/policy_loss              -2.55023     0.00271     -2.54544     -2.55279
Training/qf1_loss                 7036.67744   1170.08925  8391.75781   5408.60596
Training/qf2_loss                 15719.35273  1733.51299  17824.31055  13333.45508
Training/pf_norm                  0.14818      0.00501     0.15580      0.14227
Training/qf1_norm                 808.62805    595.47458   1603.32373   84.32463
Training/qf2_norm                 424.44149    15.01653    443.47998    404.91415
log_std/mean                      -0.13530     0.00022     -0.13498     -0.13555
log_probs/mean                    -2.73319     0.00369     -2.72659     -2.73744
mean/mean                         -0.00069     0.00014     -0.00054     -0.00091
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018500328063964844
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71468
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [68400]
collect time 0.0009732246398925781
inner_dict_sum {'sac_diff0': 0.0002224445343017578, 'sac_diff1': 0.008415937423706055, 'sac_diff2': 0.009952306747436523, 'sac_diff3': 0.012933492660522461, 'sac_diff4': 0.008594274520874023, 'sac_diff5': 0.03999066352844238, 'sac_diff6': 0.0004801750183105469, 'all': 0.08058929443359375}
diff5_list [0.007758140563964844, 0.007924556732177734, 0.008121728897094727, 0.008160114288330078, 0.008026123046875]
time3 0
time4 0.08157801628112793
time5 0.08164000511169434
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0451)
policy weight change tensor(38.1123, grad_fn=<SumBackward0>)
time8 0.0021114349365234375
train_time 0.0942847728729248
eval time 0.18227267265319824
epoch last part time 7.62939453125e-06
2024-01-23 01:02:48,098 MainThread INFO: EPOCH:449
2024-01-23 01:02:48,098 MainThread INFO: Time Consumed:0.27994608879089355s
2024-01-23 01:02:48,099 MainThread INFO: Total Frames:68250s
  4%|▍         | 450/10000 [04:15<46:38,  3.41it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12110.48459
Train_Epoch_Reward                18065.77997
Running_Training_Average_Rewards  11952.67996
Explore_Time                      0.00097
Train___Time                      0.09428
Eval____Time                      0.18227
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11600.65351
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.67820     2.15501     92.36977     86.25650
alpha_0                           0.79851      0.00011     0.79867      0.79835
Alpha_loss                        -1.51491     0.00170     -1.51280     -1.51733
Training/policy_loss              -2.53025     0.00374     -2.52536     -2.53626
Training/qf1_loss                 7302.50918   1026.84630  8363.41406   5847.95752
Training/qf2_loss                 15848.77852  1366.32322  17160.87109  13730.04883
Training/pf_norm                  0.10694      0.02441     0.13160      0.06286
Training/qf1_norm                 469.29873    344.17900   1075.99756   83.06856
Training/qf2_norm                 403.21985    9.55016     415.02673    388.08444
log_std/mean                      -0.13452     0.00003     -0.13448     -0.13456
log_probs/mean                    -2.73558     0.00482     -2.72921     -2.74335
mean/mean                         0.00037      0.00015     0.00059      0.00019
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019635677337646484
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71468
epoch first part time 3.337860107421875e-06
replay_buffer._size: [68550]
collect time 0.0009477138519287109
inside mustsac before update, task 0, sumup 71468
inside mustsac after update, task 0, sumup 69585
inner_dict_sum {'sac_diff0': 0.00025582313537597656, 'sac_diff1': 0.008018255233764648, 'sac_diff2': 0.009929656982421875, 'sac_diff3': 0.012415170669555664, 'sac_diff4': 0.008286476135253906, 'sac_diff5': 0.05581951141357422, 'sac_diff6': 0.00042629241943359375, 'all': 0.09515118598937988}
diff5_list [0.012362003326416016, 0.01089167594909668, 0.010818719863891602, 0.011002540588378906, 0.010744571685791016]
time3 0.0009717941284179688
time4 0.0960536003112793
time5 0.09610676765441895
time7 0.009341716766357422
gen_weight_change tensor(-23.1335)
policy weight change tensor(38.1407, grad_fn=<SumBackward0>)
time8 0.002666473388671875
train_time 0.12813282012939453
eval time 0.13489317893981934
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:48,388 MainThread INFO: EPOCH:450
2024-01-23 01:02:48,388 MainThread INFO: Time Consumed:0.2662842273712158s
2024-01-23 01:02:48,388 MainThread INFO: Total Frames:68400s
  5%|▍         | 451/10000 [04:16<46:33,  3.42it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11991.26787
Train_Epoch_Reward                6920.91870
Running_Training_Average_Rewards  11811.83804
Explore_Time                      0.00094
Train___Time                      0.12813
Eval____Time                      0.13489
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11531.42511
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.33795     1.70157     92.74510     88.09554
alpha_0                           0.79811      0.00011     0.79827      0.79795
Alpha_loss                        -1.51797     0.00210     -1.51576     -1.52198
Training/policy_loss              -2.54068     0.01715     -2.53037     -2.57476
Training/qf1_loss                 8156.81768   1301.70841  9695.44922   6251.17188
Training/qf2_loss                 16836.14961  1575.02518  18356.54492  14488.39648
Training/pf_norm                  0.10521      0.01476     0.11967      0.08097
Training/qf1_norm                 481.76997    226.82875   899.90387    207.40453
Training/qf2_norm                 418.87133    16.39171    449.85474    402.25906
log_std/mean                      -0.13568     0.00445     -0.12954     -0.14297
log_probs/mean                    -2.73423     0.00661     -2.72739     -2.74601
mean/mean                         -0.00186     0.00052     -0.00119     -0.00234
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018557310104370117
epoch last part time3 0.002703428268432617
inside rlalgo, task 0, sumup 69585
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [68700]
collect time 0.0008642673492431641
inner_dict_sum {'sac_diff0': 0.00020074844360351562, 'sac_diff1': 0.006764888763427734, 'sac_diff2': 0.007946252822875977, 'sac_diff3': 0.010240316390991211, 'sac_diff4': 0.006823539733886719, 'sac_diff5': 0.031621456146240234, 'sac_diff6': 0.0003809928894042969, 'all': 0.06397819519042969}
diff5_list [0.006591320037841797, 0.006575584411621094, 0.006331443786621094, 0.006062030792236328, 0.006061077117919922]
time3 0
time4 0.06475281715393066
time5 0.0648043155670166
time7 7.152557373046875e-07
gen_weight_change tensor(-23.1335)
policy weight change tensor(38.1167, grad_fn=<SumBackward0>)
time8 0.001963376998901367
train_time 0.07599639892578125
eval time 0.15633130073547363
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:48,648 MainThread INFO: EPOCH:451
2024-01-23 01:02:48,648 MainThread INFO: Time Consumed:0.23551511764526367s
2024-01-23 01:02:48,649 MainThread INFO: Total Frames:68550s
  5%|▍         | 452/10000 [04:16<44:52,  3.55it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11996.64719
Train_Epoch_Reward                3768.60046
Running_Training_Average_Rewards  11677.61680
Explore_Time                      0.00086
Train___Time                      0.07600
Eval____Time                      0.15633
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12284.61975
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.23646     3.47565     96.58136     86.49902
alpha_0                           0.79771      0.00011     0.79787      0.79755
Alpha_loss                        -1.52065     0.00201     -1.51816     -1.52431
Training/policy_loss              -2.57955     0.00421     -2.57593     -2.58723
Training/qf1_loss                 8450.39678   1312.87158  10279.95703  7045.09912
Training/qf2_loss                 17492.67676  1912.02575  19860.32227  14995.15137
Training/pf_norm                  0.10543      0.03354     0.16509      0.06282
Training/qf1_norm                 691.33114    386.93605   1239.87170   221.48331
Training/qf2_norm                 468.65120    16.92623    489.85992    440.71600
log_std/mean                      -0.13578     0.00005     -0.13569     -0.13582
log_probs/mean                    -2.73117     0.00552     -2.72612     -2.74143
mean/mean                         -0.00091     0.00006     -0.00082     -0.00097
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018718957901000977
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69585
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [68850]
collect time 0.0008370876312255859
inner_dict_sum {'sac_diff0': 0.00021696090698242188, 'sac_diff1': 0.006876468658447266, 'sac_diff2': 0.007772684097290039, 'sac_diff3': 0.010214090347290039, 'sac_diff4': 0.006657123565673828, 'sac_diff5': 0.03162670135498047, 'sac_diff6': 0.00038886070251464844, 'all': 0.06375288963317871}
diff5_list [0.00642704963684082, 0.006213665008544922, 0.0062999725341796875, 0.006291389465332031, 0.006394624710083008]
time3 0
time4 0.06451177597045898
time5 0.06455707550048828
time7 9.5367431640625e-07
gen_weight_change tensor(-23.1335)
policy weight change tensor(38.0486, grad_fn=<SumBackward0>)
time8 0.0018315315246582031
train_time 0.07532000541687012
eval time 0.15140247344970703
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:48,900 MainThread INFO: EPOCH:452
2024-01-23 01:02:48,900 MainThread INFO: Time Consumed:0.229935884475708s
2024-01-23 01:02:48,901 MainThread INFO: Total Frames:68700s
  5%|▍         | 453/10000 [04:16<43:28,  3.66it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11995.33894
Train_Epoch_Reward                18944.66204
Running_Training_Average_Rewards  12227.74938
Explore_Time                      0.00083
Train___Time                      0.07532
Eval____Time                      0.15140
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12331.74030
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.40817     1.95503     91.62816     86.23047
alpha_0                           0.79731      0.00011     0.79747      0.79715
Alpha_loss                        -1.52517     0.00076     -1.52402     -1.52600
Training/policy_loss              -2.56835     0.00426     -2.56182     -2.57371
Training/qf1_loss                 7452.13809   991.71151   9169.01172   6506.69531
Training/qf2_loss                 15903.00762  1227.34560  18078.58203  14410.33008
Training/pf_norm                  0.12663      0.02989     0.16282      0.08381
Training/qf1_norm                 1490.31748   410.55357   2190.58423   1038.24390
Training/qf2_norm                 441.00457    9.56769     451.86221    425.21024
log_std/mean                      -0.13354     0.00016     -0.13332     -0.13376
log_probs/mean                    -2.73628     0.00515     -2.72821     -2.74293
mean/mean                         -0.00238     0.00003     -0.00234     -0.00241
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01936507225036621
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69585
epoch first part time 3.337860107421875e-06
replay_buffer._size: [69000]
collect time 0.0009734630584716797
inner_dict_sum {'sac_diff0': 0.00021576881408691406, 'sac_diff1': 0.006936550140380859, 'sac_diff2': 0.008240699768066406, 'sac_diff3': 0.010550260543823242, 'sac_diff4': 0.006722450256347656, 'sac_diff5': 0.03300786018371582, 'sac_diff6': 0.0003783702850341797, 'all': 0.06605195999145508}
diff5_list [0.0071277618408203125, 0.00670933723449707, 0.006755828857421875, 0.006157636642456055, 0.006257295608520508]
time3 0
time4 0.06680870056152344
time5 0.06685471534729004
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1335)
policy weight change tensor(38.0694, grad_fn=<SumBackward0>)
time8 0.001795053482055664
train_time 0.07803821563720703
eval time 0.14961767196655273
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:49,154 MainThread INFO: EPOCH:453
2024-01-23 01:02:49,155 MainThread INFO: Time Consumed:0.23085308074951172s
2024-01-23 01:02:49,155 MainThread INFO: Total Frames:68850s
  5%|▍         | 454/10000 [04:16<42:32,  3.74it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11992.92676
Train_Epoch_Reward                10577.47484
Running_Training_Average_Rewards  12505.86084
Explore_Time                      0.00097
Train___Time                      0.07804
Eval____Time                      0.14962
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12348.09861
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.94984     2.68522     95.45432     87.28819
alpha_0                           0.79691      0.00011     0.79707      0.79675
Alpha_loss                        -1.52787     0.00180     -1.52584     -1.53100
Training/policy_loss              -2.54922     0.00453     -2.54481     -2.55752
Training/qf1_loss                 8356.51875   1542.35139  10619.42676  6263.67432
Training/qf2_loss                 17305.28945  1922.42904  19737.97656  14365.65039
Training/pf_norm                  0.10218      0.02476     0.12190      0.05403
Training/qf1_norm                 1450.36943   506.11598   2098.08984   576.29871
Training/qf2_norm                 428.65549    12.27116    444.66904    407.39285
log_std/mean                      -0.12347     0.00005     -0.12343     -0.12357
log_probs/mean                    -2.73332     0.00579     -2.72735     -2.74414
mean/mean                         -0.00115     0.00004     -0.00111     -0.00120
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018720626831054688
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69585
epoch first part time 2.86102294921875e-06
replay_buffer._size: [69150]
collect time 0.0009016990661621094
inner_dict_sum {'sac_diff0': 0.00022554397583007812, 'sac_diff1': 0.007216930389404297, 'sac_diff2': 0.008190155029296875, 'sac_diff3': 0.01046442985534668, 'sac_diff4': 0.006838083267211914, 'sac_diff5': 0.03531241416931152, 'sac_diff6': 0.0003991127014160156, 'all': 0.06864666938781738}
diff5_list [0.0067670345306396484, 0.008912801742553711, 0.0070383548736572266, 0.00623631477355957, 0.006357908248901367]
time3 0
time4 0.06942248344421387
time5 0.06947660446166992
time7 7.152557373046875e-07
gen_weight_change tensor(-23.1335)
policy weight change tensor(38.1606, grad_fn=<SumBackward0>)
time8 0.001974821090698242
train_time 0.08058333396911621
eval time 0.15960955619812012
epoch last part time 9.059906005859375e-06
2024-01-23 01:02:49,420 MainThread INFO: EPOCH:454
2024-01-23 01:02:49,421 MainThread INFO: Time Consumed:0.24362993240356445s
2024-01-23 01:02:49,421 MainThread INFO: Total Frames:69000s
  5%|▍         | 455/10000 [04:17<42:29,  3.74it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11998.32569
Train_Epoch_Reward                16416.52612
Running_Training_Average_Rewards  13002.33084
Explore_Time                      0.00090
Train___Time                      0.08058
Eval____Time                      0.15961
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12368.20373
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.51528     2.72546     96.78685     88.45454
alpha_0                           0.79651      0.00011     0.79667      0.79635
Alpha_loss                        -1.53220     0.00133     -1.53064     -1.53421
Training/policy_loss              -2.54416     0.00297     -2.53920     -2.54797
Training/qf1_loss                 8100.20898   952.47797   9713.66309   7109.06104
Training/qf2_loss                 17195.36211  1484.77227  19671.57812  15386.22656
Training/pf_norm                  0.11337      0.02019     0.13773      0.07857
Training/qf1_norm                 872.40383    498.16761   1681.00537   160.68161
Training/qf2_norm                 434.25199    12.22362    453.29459    415.75900
log_std/mean                      -0.13631     0.00019     -0.13604     -0.13657
log_probs/mean                    -2.73758     0.00358     -2.73159     -2.74190
mean/mean                         -0.00058     0.00009     -0.00045     -0.00071
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019212722778320312
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69585
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [69300]
collect time 0.0009479522705078125
inside mustsac before update, task 0, sumup 69585
inside mustsac after update, task 0, sumup 71128
inner_dict_sum {'sac_diff0': 0.0002079010009765625, 'sac_diff1': 0.007005929946899414, 'sac_diff2': 0.008309602737426758, 'sac_diff3': 0.010282754898071289, 'sac_diff4': 0.00730586051940918, 'sac_diff5': 0.04968857765197754, 'sac_diff6': 0.00039386749267578125, 'all': 0.08319449424743652}
diff5_list [0.010896444320678711, 0.009762763977050781, 0.009891986846923828, 0.009600162506103516, 0.009537220001220703]
time3 0.0008559226989746094
time4 0.08400368690490723
time5 0.08405900001525879
time7 0.008943796157836914
gen_weight_change tensor(-23.1377)
policy weight change tensor(38.1240, grad_fn=<SumBackward0>)
time8 0.0020208358764648438
train_time 0.11316061019897461
eval time 0.12431573867797852
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:49,684 MainThread INFO: EPOCH:455
2024-01-23 01:02:49,684 MainThread INFO: Time Consumed:0.2406754493713379s
2024-01-23 01:02:49,684 MainThread INFO: Total Frames:69150s
  5%|▍         | 456/10000 [04:17<42:16,  3.76it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12001.56282
Train_Epoch_Reward                11225.12160
Running_Training_Average_Rewards  12604.32293
Explore_Time                      0.00094
Train___Time                      0.11316
Eval____Time                      0.12432
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12295.01544
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.25212     1.66092     93.81679     89.42741
alpha_0                           0.79611      0.00011     0.79627      0.79596
Alpha_loss                        -1.53448     0.00132     -1.53263     -1.53675
Training/policy_loss              -2.54074     0.01267     -2.52301     -2.55664
Training/qf1_loss                 7628.82949   1221.30017  10047.16699  6772.96484
Training/qf2_loss                 16453.66914  1480.17593  19369.76758  15384.21875
Training/pf_norm                  0.11648      0.02019     0.13782      0.07873
Training/qf1_norm                 1136.26990   723.49805   1945.78833   182.69946
Training/qf2_norm                 426.55873    8.72584     443.62607    419.52032
log_std/mean                      -0.13038     0.00227     -0.12796     -0.13443
log_probs/mean                    -2.73278     0.00437     -2.72761     -2.73860
mean/mean                         -0.00186     0.00093     -0.00081     -0.00351
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01809215545654297
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71128
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [69450]
collect time 0.0008754730224609375
inner_dict_sum {'sac_diff0': 0.0002110004425048828, 'sac_diff1': 0.006891727447509766, 'sac_diff2': 0.008054018020629883, 'sac_diff3': 0.010312318801879883, 'sac_diff4': 0.00705718994140625, 'sac_diff5': 0.0322721004486084, 'sac_diff6': 0.0003857612609863281, 'all': 0.06518411636352539}
diff5_list [0.006478309631347656, 0.0073697566986083984, 0.0061070919036865234, 0.0060596466064453125, 0.006257295608520508]
time3 0
time4 0.06593179702758789
time5 0.0659792423248291
time7 9.5367431640625e-07
gen_weight_change tensor(-23.1377)
policy weight change tensor(38.2536, grad_fn=<SumBackward0>)
time8 0.0019121170043945312
train_time 0.07693052291870117
eval time 0.13772797584533691
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:49,937 MainThread INFO: EPOCH:456
2024-01-23 01:02:49,937 MainThread INFO: Time Consumed:0.21781277656555176s
2024-01-23 01:02:49,937 MainThread INFO: Total Frames:69300s
  5%|▍         | 457/10000 [04:18<1:28:05,  1.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12024.51271
Train_Epoch_Reward                13850.38310
Running_Training_Average_Rewards  12672.06881
Explore_Time                      0.00087
Train___Time                      0.07693
Eval____Time                      0.13773
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12045.08768
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.60530     2.39140     92.61133     86.71384
alpha_0                           0.79572      0.00011     0.79588      0.79556
Alpha_loss                        -1.53774     0.00105     -1.53627     -1.53949
Training/policy_loss              -2.57811     0.00214     -2.57434     -2.58093
Training/qf1_loss                 7741.51211   878.18133   9169.79102   6575.25635
Training/qf2_loss                 16290.32598  1279.06728  18048.43359  14560.77148
Training/pf_norm                  0.11883      0.02763     0.16142      0.08870
Training/qf1_norm                 524.82862    394.76927   1079.02197   125.94860
Training/qf2_norm                 447.43646    11.60786    462.18460    432.79831
log_std/mean                      -0.12812     0.00022     -0.12782     -0.12844
log_probs/mean                    -2.73231     0.00284     -2.72721     -2.73551
mean/mean                         -0.00244     0.00008     -0.00233     -0.00255
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.9913220405578613
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 71128
epoch first part time 2.86102294921875e-06
replay_buffer._size: [69600]
collect time 0.0009708404541015625
inner_dict_sum {'sac_diff0': 0.0002307891845703125, 'sac_diff1': 0.007551670074462891, 'sac_diff2': 0.007912874221801758, 'sac_diff3': 0.010219097137451172, 'sac_diff4': 0.0075740814208984375, 'sac_diff5': 0.03298473358154297, 'sac_diff6': 0.0003871917724609375, 'all': 0.06686043739318848}
diff5_list [0.006680965423583984, 0.006305694580078125, 0.006392240524291992, 0.006426095962524414, 0.007179737091064453]
time3 0
time4 0.06761550903320312
time5 0.06766271591186523
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1377)
policy weight change tensor(38.3226, grad_fn=<SumBackward0>)
time8 0.002002716064453125
train_time 0.07890534400939941
eval time 0.0006549358367919922
epoch last part time 4.0531158447265625e-06
2024-01-23 01:02:51,015 MainThread INFO: EPOCH:457
2024-01-23 01:02:51,015 MainThread INFO: Time Consumed:0.08273148536682129s
2024-01-23 01:02:51,015 MainThread INFO: Total Frames:69450s
  5%|▍         | 458/10000 [04:18<1:06:40,  2.38it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12047.15709
Train_Epoch_Reward                5412.98678
Running_Training_Average_Rewards  12376.64364
Explore_Time                      0.00096
Train___Time                      0.07891
Eval____Time                      0.00065
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11983.33031
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.42577     3.16399     95.88271     86.03213
alpha_0                           0.79532      0.00011     0.79548      0.79516
Alpha_loss                        -1.54189     0.00109     -1.54039     -1.54349
Training/policy_loss              -2.54085     0.00386     -2.53537     -2.54515
Training/qf1_loss                 6795.32451   837.02191   7924.06787   5740.14307
Training/qf2_loss                 15466.65801  1275.62499  16732.62305  13514.89844
Training/pf_norm                  0.12184      0.00900     0.13139      0.10898
Training/qf1_norm                 1387.86066   633.56496   2252.58350   305.06854
Training/qf2_norm                 415.97954    14.38017    440.76837    395.91324
log_std/mean                      -0.14115     0.00004     -0.14109     -0.14119
log_probs/mean                    -2.73574     0.00460     -2.72916     -2.74055
mean/mean                         -0.00213     0.00012     -0.00196     -0.00227
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018654346466064453
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71128
epoch first part time 2.384185791015625e-06
replay_buffer._size: [69706]
collect time 0.0008733272552490234
inner_dict_sum {'sac_diff0': 0.0001995563507080078, 'sac_diff1': 0.007425785064697266, 'sac_diff2': 0.008952856063842773, 'sac_diff3': 0.011693000793457031, 'sac_diff4': 0.007677316665649414, 'sac_diff5': 0.0339055061340332, 'sac_diff6': 0.0004153251647949219, 'all': 0.07026934623718262}
diff5_list [0.007309436798095703, 0.006887674331665039, 0.006920814514160156, 0.006365776062011719, 0.006421804428100586]
time3 0
time4 0.07105517387390137
time5 0.07110309600830078
time7 7.152557373046875e-07
gen_weight_change tensor(-23.1377)
policy weight change tensor(38.3123, grad_fn=<SumBackward0>)
time8 0.0019145011901855469
train_time 0.08210992813110352
eval time 0.0006024837493896484
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:51,123 MainThread INFO: EPOCH:458
2024-01-23 01:02:51,123 MainThread INFO: Time Consumed:0.0858001708984375s
2024-01-23 01:02:51,123 MainThread INFO: Total Frames:69600s
  5%|▍         | 459/10000 [04:18<51:48,  3.07it/s]  --------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12077.15047
Train_Epoch_Reward                10948.07398
Running_Training_Average_Rewards  12604.75595
Explore_Time                      0.00087
Train___Time                      0.08211
Eval____Time                      0.00060
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11983.33031
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.65284     2.88943     94.87052     86.89906
alpha_0                           0.79492      0.00011     0.79508      0.79476
Alpha_loss                        -1.54530     0.00093     -1.54422     -1.54678
Training/policy_loss              -2.53979     0.00522     -2.53588     -2.55005
Training/qf1_loss                 7589.94980   992.64408   9279.79102   6617.62500
Training/qf2_loss                 16147.58594  1446.50284  18812.35547  14661.25195
Training/pf_norm                  0.09326      0.02406     0.12146      0.04877
Training/qf1_norm                 504.95644    514.61935   1487.45715   109.95858
Training/qf2_norm                 419.86096    13.10368    443.62097    407.31479
log_std/mean                      -0.13905     0.00003     -0.13899     -0.13909
log_probs/mean                    -2.73591     0.00625     -2.73157     -2.74825
mean/mean                         -0.00254     0.00006     -0.00248     -0.00263
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01807999610900879
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71128
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [69814]
collect time 0.0008983612060546875
inner_dict_sum {'sac_diff0': 0.000194549560546875, 'sac_diff1': 0.007249593734741211, 'sac_diff2': 0.008537769317626953, 'sac_diff3': 0.01136326789855957, 'sac_diff4': 0.007487058639526367, 'sac_diff5': 0.03560471534729004, 'sac_diff6': 0.0004055500030517578, 'all': 0.07084250450134277}
diff5_list [0.0063059329986572266, 0.006671428680419922, 0.0071239471435546875, 0.00786137580871582, 0.007642030715942383]
time3 0
time4 0.0716543197631836
time5 0.07170319557189941
time7 7.152557373046875e-07
gen_weight_change tensor(-23.1377)
policy weight change tensor(38.2268, grad_fn=<SumBackward0>)
time8 0.002013683319091797
train_time 0.08290505409240723
eval time 0.0749213695526123
epoch last part time 5.9604644775390625e-06
2024-01-23 01:02:51,306 MainThread INFO: EPOCH:459
2024-01-23 01:02:51,306 MainThread INFO: Time Consumed:0.16116714477539062s
2024-01-23 01:02:51,306 MainThread INFO: Total Frames:69750s
  5%|▍         | 460/10000 [04:18<45:01,  3.53it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12114.29850
Train_Epoch_Reward                844.12818
Running_Training_Average_Rewards  11818.57651
Explore_Time                      0.00089
Train___Time                      0.08291
Eval____Time                      0.07492
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11972.13376
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.91992     2.13820     92.74290     86.54195
alpha_0                           0.79452      0.00011     0.79468      0.79436
Alpha_loss                        -1.54813     0.00152     -1.54561     -1.55018
Training/policy_loss              -2.55588     0.00612     -2.54983     -2.56768
Training/qf1_loss                 7529.52627   1294.81431  9454.32617   5687.98340
Training/qf2_loss                 16153.40742  1639.56389  18628.68359  13678.53613
Training/pf_norm                  0.08028      0.01201     0.09097      0.06526
Training/qf1_norm                 391.40795    228.86176   720.02753    102.35196
Training/qf2_norm                 441.03782    10.31962    454.64954    425.00525
log_std/mean                      -0.12937     0.00017     -0.12914     -0.12961
log_probs/mean                    -2.73358     0.00773     -2.72554     -2.74835
mean/mean                         -0.00098     0.00002     -0.00095     -0.00100
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018968582153320312
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71128
epoch first part time 2.86102294921875e-06
replay_buffer._size: [70018]
collect time 0.001010894775390625
inside mustsac before update, task 0, sumup 71128
inside mustsac after update, task 0, sumup 71008
inner_dict_sum {'sac_diff0': 0.00022149085998535156, 'sac_diff1': 0.008030414581298828, 'sac_diff2': 0.009508609771728516, 'sac_diff3': 0.012259483337402344, 'sac_diff4': 0.008866071701049805, 'sac_diff5': 0.058040618896484375, 'sac_diff6': 0.00046372413635253906, 'all': 0.09739041328430176}
diff5_list [0.012572288513183594, 0.011780738830566406, 0.009756803512573242, 0.011174678802490234, 0.012756109237670898]
time3 0.0009465217590332031
time4 0.09837126731872559
time5 0.09843015670776367
time7 0.009722232818603516
gen_weight_change tensor(-23.1235)
policy weight change tensor(38.1668, grad_fn=<SumBackward0>)
time8 0.0028874874114990234
train_time 0.13077354431152344
eval time 0.14006257057189941
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:51,603 MainThread INFO: EPOCH:460
2024-01-23 01:02:51,603 MainThread INFO: Time Consumed:0.2744126319885254s
2024-01-23 01:02:51,604 MainThread INFO: Total Frames:69900s
  5%|▍         | 461/10000 [04:19<45:58,  3.46it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12158.85847
Train_Epoch_Reward                33726.45778
Running_Training_Average_Rewards  12607.42397
Explore_Time                      0.00100
Train___Time                      0.13077
Eval____Time                      0.14006
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11977.02482
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.36890     1.35029    93.74633     89.68198
alpha_0                           0.79413      0.00011    0.79429      0.79397
Alpha_loss                        -1.55200     0.00251    -1.54817     -1.55471
Training/policy_loss              -2.56360     0.01238    -2.54558     -2.57669
Training/qf1_loss                 7817.99365   823.32478  9258.23242   6977.56250
Training/qf2_loss                 16633.42188  974.70113  18178.09961  15364.04980
Training/pf_norm                  0.09703      0.02356    0.13308      0.06693
Training/qf1_norm                 1440.44771   869.55031  3052.87939   693.41553
Training/qf2_norm                 445.22800    13.89273   464.03568    426.61578
log_std/mean                      -0.13656     0.00434    -0.12928     -0.14270
log_probs/mean                    -2.73575     0.00831    -2.72496     -2.74636
mean/mean                         -0.00132     0.00105    0.00036      -0.00288
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.021943092346191406
epoch last part time3 0.0031516551971435547
inside rlalgo, task 0, sumup 71008
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [70200]
collect time 0.001100301742553711
inner_dict_sum {'sac_diff0': 0.00022220611572265625, 'sac_diff1': 0.008502960205078125, 'sac_diff2': 0.010164499282836914, 'sac_diff3': 0.013268709182739258, 'sac_diff4': 0.00881505012512207, 'sac_diff5': 0.040221452713012695, 'sac_diff6': 0.0004992485046386719, 'all': 0.08169412612915039}
diff5_list [0.008551597595214844, 0.0077135562896728516, 0.008329391479492188, 0.007779121398925781, 0.007847785949707031]
time3 0
time4 0.08267831802368164
time5 0.08274006843566895
time7 9.5367431640625e-07
gen_weight_change tensor(-23.1235)
policy weight change tensor(38.1047, grad_fn=<SumBackward0>)
time8 0.0018916130065917969
train_time 0.09453868865966797
eval time 0.13574647903442383
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:51,866 MainThread INFO: EPOCH:461
2024-01-23 01:02:51,866 MainThread INFO: Time Consumed:0.23398542404174805s
2024-01-23 01:02:51,866 MainThread INFO: Total Frames:70050s
  5%|▍         | 462/10000 [04:19<44:29,  3.57it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12128.43197
Train_Epoch_Reward                6025.81852
Running_Training_Average_Rewards  12402.68158
Explore_Time                      0.00109
Train___Time                      0.09454
Eval____Time                      0.13575
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11980.35470
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.15672     2.92486     95.73530     89.50426
alpha_0                           0.79373      0.00011     0.79389      0.79357
Alpha_loss                        -1.55339     0.00130     -1.55152     -1.55550
Training/policy_loss              -2.54632     0.00422     -2.54184     -2.55391
Training/qf1_loss                 8062.36895   1163.60523  9705.96289   6640.38379
Training/qf2_loss                 17016.50234  1667.17145  19297.49219  15226.89648
Training/pf_norm                  0.11864      0.04314     0.16878      0.06248
Training/qf1_norm                 1034.96198   567.34924   1748.53906   476.15372
Training/qf2_norm                 437.62692    13.39481    454.17688    425.29169
log_std/mean                      -0.14371     0.00009     -0.14357     -0.14384
log_probs/mean                    -2.72718     0.00514     -2.72208     -2.73631
mean/mean                         -0.00178     0.00002     -0.00176     -0.00182
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02077794075012207
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71008
epoch first part time 2.86102294921875e-06
replay_buffer._size: [70350]
collect time 0.0008566379547119141
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.007757663726806641, 'sac_diff2': 0.00912165641784668, 'sac_diff3': 0.011785507202148438, 'sac_diff4': 0.007615804672241211, 'sac_diff5': 0.035404205322265625, 'sac_diff6': 0.00043582916259765625, 'all': 0.07233834266662598}
diff5_list [0.007416248321533203, 0.0067822933197021484, 0.007258176803588867, 0.0070953369140625, 0.006852149963378906]
time3 0
time4 0.07315397262573242
time5 0.07320523262023926
time7 9.5367431640625e-07
gen_weight_change tensor(-23.1235)
policy weight change tensor(38.0823, grad_fn=<SumBackward0>)
time8 0.0018458366394042969
train_time 0.08484387397766113
eval time 0.14827299118041992
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:52,126 MainThread INFO: EPOCH:462
2024-01-23 01:02:52,127 MainThread INFO: Time Consumed:0.2364346981048584s
2024-01-23 01:02:52,127 MainThread INFO: Total Frames:70200s
  5%|▍         | 463/10000 [04:19<43:32,  3.65it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12091.41107
Train_Epoch_Reward                24651.54419
Running_Training_Average_Rewards  12782.53331
Explore_Time                      0.00085
Train___Time                      0.08484
Eval____Time                      0.14827
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11961.53137
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.82935     2.43553     95.55959     88.55090
alpha_0                           0.79333      0.00011     0.79349      0.79317
Alpha_loss                        -1.55896     0.00068     -1.55833     -1.56004
Training/policy_loss              -2.52752     0.00335     -2.52104     -2.53039
Training/qf1_loss                 7469.04404   837.28068   8961.09375   6553.82422
Training/qf2_loss                 16379.15527  1258.91399  18532.04688  14896.26270
Training/pf_norm                  0.13030      0.02587     0.17763      0.10891
Training/qf1_norm                 1427.60835   453.02043   2182.17017   875.98108
Training/qf2_norm                 414.81298    10.70843    431.33853    400.43060
log_std/mean                      -0.12440     0.00004     -0.12435     -0.12445
log_probs/mean                    -2.73673     0.00386     -2.72926     -2.74003
mean/mean                         -0.00151     0.00002     -0.00147     -0.00153
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01995539665222168
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71008
epoch first part time 3.337860107421875e-06
replay_buffer._size: [70500]
collect time 0.0009469985961914062
inner_dict_sum {'sac_diff0': 0.00021195411682128906, 'sac_diff1': 0.007920026779174805, 'sac_diff2': 0.009887218475341797, 'sac_diff3': 0.012766838073730469, 'sac_diff4': 0.00847172737121582, 'sac_diff5': 0.03700900077819824, 'sac_diff6': 0.0004324913024902344, 'all': 0.07669925689697266}
diff5_list [0.007074832916259766, 0.006394863128662109, 0.00823664665222168, 0.00798177719116211, 0.007320880889892578]
time3 0
time4 0.07758688926696777
time5 0.07764339447021484
time7 9.5367431640625e-07
gen_weight_change tensor(-23.1235)
policy weight change tensor(38.1347, grad_fn=<SumBackward0>)
time8 0.0019979476928710938
train_time 0.08942532539367676
eval time 0.1557776927947998
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:52,399 MainThread INFO: EPOCH:463
2024-01-23 01:02:52,399 MainThread INFO: Time Consumed:0.24869561195373535s
2024-01-23 01:02:52,399 MainThread INFO: Total Frames:70350s
  5%|▍         | 464/10000 [04:20<43:23,  3.66it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12053.19844
Train_Epoch_Reward                10040.78764
Running_Training_Average_Rewards  13022.74149
Explore_Time                      0.00094
Train___Time                      0.08943
Eval____Time                      0.15578
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11965.97227
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.53281     0.73641    91.68790     89.38921
alpha_0                           0.79294      0.00011    0.79309      0.79278
Alpha_loss                        -1.55997     0.00099    -1.55882     -1.56169
Training/policy_loss              -2.54633     0.00306    -2.54244     -2.54960
Training/qf1_loss                 7222.88115   577.37494  7938.60449   6280.09180
Training/qf2_loss                 15698.37070  685.10424  16581.12109  14566.83887
Training/pf_norm                  0.09067      0.02890    0.13666      0.05456
Training/qf1_norm                 2753.48506   152.42928  2999.80200   2546.41162
Training/qf2_norm                 437.88558    3.31158    442.82172    432.54205
log_std/mean                      -0.13008     0.00012    -0.12990     -0.13023
log_probs/mean                    -2.72655     0.00370    -2.72158     -2.73086
mean/mean                         -0.00311     0.00003    -0.00307     -0.00314
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0184481143951416
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71008
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [70650]
collect time 0.0008528232574462891
inner_dict_sum {'sac_diff0': 0.0002193450927734375, 'sac_diff1': 0.00824284553527832, 'sac_diff2': 0.009799003601074219, 'sac_diff3': 0.012528419494628906, 'sac_diff4': 0.008492231369018555, 'sac_diff5': 0.03807640075683594, 'sac_diff6': 0.0004611015319824219, 'all': 0.0778193473815918}
diff5_list [0.008242607116699219, 0.0075151920318603516, 0.0073435306549072266, 0.0064830780029296875, 0.008491992950439453]
time3 0
time4 0.07877874374389648
time5 0.07884049415588379
time7 7.152557373046875e-07
gen_weight_change tensor(-23.1235)
policy weight change tensor(38.1465, grad_fn=<SumBackward0>)
time8 0.002210378646850586
train_time 0.0909721851348877
eval time 0.15204238891601562
epoch last part time 6.67572021484375e-06
2024-01-23 01:02:52,667 MainThread INFO: EPOCH:464
2024-01-23 01:02:52,668 MainThread INFO: Time Consumed:0.24631357192993164s
2024-01-23 01:02:52,668 MainThread INFO: Total Frames:70500s
  5%|▍         | 465/10000 [04:20<43:13,  3.68it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12016.29379
Train_Epoch_Reward                12035.91108
Running_Training_Average_Rewards  12827.40620
Explore_Time                      0.00085
Train___Time                      0.09097
Eval____Time                      0.15204
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11999.15723
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.45171     1.98599     91.62090     86.01753
alpha_0                           0.79254      0.00011     0.79270      0.79238
Alpha_loss                        -1.56446     0.00099     -1.56282     -1.56556
Training/policy_loss              -2.57260     0.00523     -2.56460     -2.57974
Training/qf1_loss                 6968.46016   985.91604   8566.27051   5897.42578
Training/qf2_loss                 15303.98926  1288.91118  17503.17188  14048.95605
Training/pf_norm                  0.09812      0.01551     0.12383      0.08092
Training/qf1_norm                 376.41817    187.94292   702.67499    210.85323
Training/qf2_norm                 439.68904    9.47398     455.12628    428.06903
log_std/mean                      -0.13434     0.00003     -0.13431     -0.13439
log_probs/mean                    -2.73141     0.00641     -2.72144     -2.74006
mean/mean                         -0.00281     0.00005     -0.00274     -0.00286
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019614696502685547
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71008
epoch first part time 3.337860107421875e-06
replay_buffer._size: [70800]
collect time 0.0008647441864013672
inside mustsac before update, task 0, sumup 71008
inside mustsac after update, task 0, sumup 70997
inner_dict_sum {'sac_diff0': 0.0002162456512451172, 'sac_diff1': 0.007286548614501953, 'sac_diff2': 0.008754491806030273, 'sac_diff3': 0.011530160903930664, 'sac_diff4': 0.007976055145263672, 'sac_diff5': 0.05215191841125488, 'sac_diff6': 0.0004284381866455078, 'all': 0.08834385871887207}
diff5_list [0.01034855842590332, 0.009763240814208984, 0.010710000991821289, 0.010724306106567383, 0.010605812072753906]
time3 0.0008654594421386719
time4 0.08925247192382812
time5 0.08930635452270508
time7 0.009200572967529297
gen_weight_change tensor(-23.2011)
policy weight change tensor(38.1301, grad_fn=<SumBackward0>)
time8 0.0020704269409179688
train_time 0.11938214302062988
eval time 0.13379812240600586
epoch last part time 5.9604644775390625e-06
2024-01-23 01:02:52,947 MainThread INFO: EPOCH:465
2024-01-23 01:02:52,947 MainThread INFO: Time Consumed:0.25635194778442383s
2024-01-23 01:02:52,948 MainThread INFO: Total Frames:70650s
  5%|▍         | 466/10000 [04:20<43:33,  3.65it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11987.76229
Train_Epoch_Reward                15653.43986
Running_Training_Average_Rewards  12961.86369
Explore_Time                      0.00086
Train___Time                      0.11938
Eval____Time                      0.13380
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12009.70043
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.80928     2.30405     94.06359     87.98994
alpha_0                           0.79214      0.00011     0.79230      0.79199
Alpha_loss                        -1.56775     0.00158     -1.56491     -1.56946
Training/policy_loss              -2.57436     0.01716     -2.55464     -2.59723
Training/qf1_loss                 6812.70703   1122.60277  8479.34961   5744.29590
Training/qf2_loss                 15558.12734  1564.98139  17878.12109  13941.85938
Training/pf_norm                  0.10978      0.02632     0.14255      0.07327
Training/qf1_norm                 984.12198    284.31920   1302.29285   472.64182
Training/qf2_norm                 455.63636    8.00162     468.59259    447.88672
log_std/mean                      -0.13201     0.00425     -0.12774     -0.13975
log_probs/mean                    -2.73107     0.00579     -2.72339     -2.73653
mean/mean                         -0.00218     0.00056     -0.00124     -0.00268
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01917409896850586
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70997
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [70950]
collect time 0.0009164810180664062
inner_dict_sum {'sac_diff0': 0.0002231597900390625, 'sac_diff1': 0.007603168487548828, 'sac_diff2': 0.008985757827758789, 'sac_diff3': 0.011812925338745117, 'sac_diff4': 0.008266925811767578, 'sac_diff5': 0.0365595817565918, 'sac_diff6': 0.0004596710205078125, 'all': 0.07391119003295898}
diff5_list [0.008104562759399414, 0.007615089416503906, 0.007209062576293945, 0.007391929626464844, 0.0062389373779296875]
time3 0
time4 0.07474470138549805
time5 0.07479667663574219
time7 4.76837158203125e-07
gen_weight_change tensor(-23.2011)
policy weight change tensor(38.0705, grad_fn=<SumBackward0>)
time8 0.0019140243530273438
train_time 0.08587980270385742
eval time 0.19295406341552734
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:53,252 MainThread INFO: EPOCH:466
2024-01-23 01:02:53,252 MainThread INFO: Time Consumed:0.2821462154388428s
2024-01-23 01:02:53,252 MainThread INFO: Total Frames:70800s
  5%|▍         | 467/10000 [04:20<45:01,  3.53it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11972.78794
Train_Epoch_Reward                20607.34211
Running_Training_Average_Rewards  12861.22289
Explore_Time                      0.00091
Train___Time                      0.08588
Eval____Time                      0.19295
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11895.34418
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.79278     3.28011     96.29653     87.20732
alpha_0                           0.79175      0.00011     0.79191      0.79159
Alpha_loss                        -1.57208     0.00145     -1.57054     -1.57430
Training/policy_loss              -2.61281     0.00318     -2.60771     -2.61676
Training/qf1_loss                 7460.06074   1069.17908  9324.53906   6420.67432
Training/qf2_loss                 16154.09082  1624.57071  18970.34766  14644.40918
Training/pf_norm                  0.12792      0.03586     0.18705      0.08730
Training/qf1_norm                 1577.41769   609.87471   2596.41479   948.61749
Training/qf2_norm                 480.66702    16.89834    508.86624    462.17850
log_std/mean                      -0.14588     0.00017     -0.14559     -0.14604
log_probs/mean                    -2.73521     0.00404     -2.72900     -2.74023
mean/mean                         -0.00102     0.00007     -0.00093     -0.00113
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01893329620361328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70997
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [71100]
collect time 0.000949859619140625
inner_dict_sum {'sac_diff0': 0.00022363662719726562, 'sac_diff1': 0.012474298477172852, 'sac_diff2': 0.011201143264770508, 'sac_diff3': 0.013083696365356445, 'sac_diff4': 0.009711980819702148, 'sac_diff5': 0.03765082359313965, 'sac_diff6': 0.00041794776916503906, 'all': 0.0847635269165039}
diff5_list [0.008064985275268555, 0.007322072982788086, 0.007959604263305664, 0.007059574127197266, 0.007244586944580078]
time3 0
time4 0.0855865478515625
time5 0.08565139770507812
time7 4.76837158203125e-07
gen_weight_change tensor(-23.2011)
policy weight change tensor(37.9897, grad_fn=<SumBackward0>)
time8 0.0022394657135009766
train_time 0.09847712516784668
eval time 0.1797938346862793
epoch last part time 9.298324584960938e-06
2024-01-23 01:02:53,557 MainThread INFO: EPOCH:467
2024-01-23 01:02:53,557 MainThread INFO: Time Consumed:0.281876802444458s
2024-01-23 01:02:53,557 MainThread INFO: Total Frames:70950s
  5%|▍         | 468/10000 [04:21<46:05,  3.45it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11966.44182
Train_Epoch_Reward                11484.05305
Running_Training_Average_Rewards  13106.91355
Explore_Time                      0.00094
Train___Time                      0.09848
Eval____Time                      0.17979
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11919.86910
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.47455     2.09339     94.00858     88.76917
alpha_0                           0.79135      0.00011     0.79151      0.79119
Alpha_loss                        -1.57576     0.00151     -1.57303     -1.57712
Training/policy_loss              -2.51600     0.00398     -2.50932     -2.52043
Training/qf1_loss                 7220.65703   835.64823   8336.62109   5777.04199
Training/qf2_loss                 16136.69121  1176.54973  17447.24414  14135.88574
Training/pf_norm                  0.08992      0.02777     0.13773      0.06140
Training/qf1_norm                 632.54319    426.73787   1198.98108   117.84575
Training/qf2_norm                 419.95287    9.15064     430.94681    408.06964
log_std/mean                      -0.12888     0.00009     -0.12880     -0.12905
log_probs/mean                    -2.73654     0.00507     -2.72777     -2.74235
mean/mean                         -0.00200     0.00011     -0.00186     -0.00215
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020327329635620117
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70997
epoch first part time 4.76837158203125e-06
replay_buffer._size: [71250]
collect time 0.0009567737579345703
inner_dict_sum {'sac_diff0': 0.0002238750457763672, 'sac_diff1': 0.007791280746459961, 'sac_diff2': 0.009102106094360352, 'sac_diff3': 0.012266159057617188, 'sac_diff4': 0.00826883316040039, 'sac_diff5': 0.03635859489440918, 'sac_diff6': 0.00045180320739746094, 'all': 0.0744626522064209}
diff5_list [0.007079362869262695, 0.0069370269775390625, 0.006996870040893555, 0.007515430450439453, 0.007829904556274414]
time3 0
time4 0.0753927230834961
time5 0.0754539966583252
time7 7.152557373046875e-07
gen_weight_change tensor(-23.2011)
policy weight change tensor(37.9429, grad_fn=<SumBackward0>)
time8 0.0021066665649414062
train_time 0.08767509460449219
eval time 0.1770493984222412
epoch last part time 1.6927719116210938e-05
2024-01-23 01:02:53,849 MainThread INFO: EPOCH:468
2024-01-23 01:02:53,849 MainThread INFO: Time Consumed:0.2681560516357422s
2024-01-23 01:02:53,850 MainThread INFO: Total Frames:71100s
  5%|▍         | 469/10000 [04:21<46:10,  3.44it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11954.96382
Train_Epoch_Reward                10525.12121
Running_Training_Average_Rewards  13132.30143
Explore_Time                      0.00095
Train___Time                      0.08768
Eval____Time                      0.17705
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11868.55034
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.08135     2.60885     92.25682     86.21494
alpha_0                           0.79096      0.00011     0.79111      0.79080
Alpha_loss                        -1.57862     0.00093     -1.57749     -1.58021
Training/policy_loss              -2.60439     0.00363     -2.60011     -2.60986
Training/qf1_loss                 6723.28398   1080.87267  7699.89355   5012.76074
Training/qf2_loss                 15157.29883  1513.17885  16713.71680  12907.04492
Training/pf_norm                  0.09567      0.01908     0.12322      0.06625
Training/qf1_norm                 554.28301    441.49322   1111.82520   88.36642
Training/qf2_norm                 466.62798    13.55578    483.19537    451.38150
log_std/mean                      -0.12839     0.00010     -0.12827     -0.12853
log_probs/mean                    -2.73437     0.00440     -2.72899     -2.74116
mean/mean                         -0.00406     0.00004     -0.00401     -0.00412
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019563913345336914
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70997
epoch first part time 3.337860107421875e-06
replay_buffer._size: [71400]
collect time 0.0009894371032714844
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.00793910026550293, 'sac_diff2': 0.00919651985168457, 'sac_diff3': 0.012142658233642578, 'sac_diff4': 0.008026361465454102, 'sac_diff5': 0.03714418411254883, 'sac_diff6': 0.0004429817199707031, 'all': 0.07511281967163086}
diff5_list [0.007535219192504883, 0.007893562316894531, 0.0072748661041259766, 0.007307529449462891, 0.007133007049560547]
time3 0
time4 0.07598185539245605
time5 0.07603836059570312
time7 4.76837158203125e-07
gen_weight_change tensor(-23.2011)
policy weight change tensor(37.8789, grad_fn=<SumBackward0>)
time8 0.0019986629486083984
train_time 0.08796548843383789
eval time 0.1796412467956543
epoch last part time 6.4373016357421875e-06
2024-01-23 01:02:54,143 MainThread INFO: EPOCH:469
2024-01-23 01:02:54,144 MainThread INFO: Time Consumed:0.27089595794677734s
2024-01-23 01:02:54,144 MainThread INFO: Total Frames:71250s
  5%|▍         | 470/10000 [04:21<46:17,  3.43it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11939.54050
Train_Epoch_Reward                13404.99834
Running_Training_Average_Rewards  13490.47071
Explore_Time                      0.00098
Train___Time                      0.08797
Eval____Time                      0.17964
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11817.90062
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.28634     1.88817    90.85233     85.24966
alpha_0                           0.79056      0.00011    0.79072      0.79040
Alpha_loss                        -1.58182     0.00171    -1.57978     -1.58485
Training/policy_loss              -2.59087     0.00677    -2.58351     -2.60370
Training/qf1_loss                 6201.26221   626.56956  7347.39062   5599.68262
Training/qf2_loss                 14279.25156  929.86351  16108.54980  13578.30176
Training/pf_norm                  0.12611      0.02107    0.15821      0.10527
Training/qf1_norm                 890.59710    354.84424  1265.16101   215.12752
Training/qf2_norm                 444.51012    9.34487    462.15201    434.43533
log_std/mean                      -0.13873     0.00016    -0.13845     -0.13888
log_probs/mean                    -2.73366     0.00828    -2.72510     -2.74944
mean/mean                         -0.00335     0.00002    -0.00333     -0.00337
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018719911575317383
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70997
epoch first part time 2.86102294921875e-06
replay_buffer._size: [71550]
collect time 0.0009779930114746094
inside mustsac before update, task 0, sumup 70997
inside mustsac after update, task 0, sumup 71194
inner_dict_sum {'sac_diff0': 0.00020384788513183594, 'sac_diff1': 0.006854534149169922, 'sac_diff2': 0.008389949798583984, 'sac_diff3': 0.01065373420715332, 'sac_diff4': 0.007203340530395508, 'sac_diff5': 0.04980778694152832, 'sac_diff6': 0.0004010200500488281, 'all': 0.08351421356201172}
diff5_list [0.011430501937866211, 0.010219097137451172, 0.009421825408935547, 0.009395360946655273, 0.009341001510620117]
time3 0.0008676052093505859
time4 0.08433246612548828
time5 0.0843806266784668
time7 0.009087562561035156
gen_weight_change tensor(-23.2606)
policy weight change tensor(37.9121, grad_fn=<SumBackward0>)
time8 0.0024466514587402344
train_time 0.11414599418640137
eval time 0.11587667465209961
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:54,399 MainThread INFO: EPOCH:470
2024-01-23 01:02:54,399 MainThread INFO: Time Consumed:0.23316287994384766s
2024-01-23 01:02:54,399 MainThread INFO: Total Frames:71400s
  5%|▍         | 471/10000 [04:22<44:45,  3.55it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11918.07045
Train_Epoch_Reward                22060.58484
Running_Training_Average_Rewards  13920.34838
Explore_Time                      0.00097
Train___Time                      0.11415
Eval____Time                      0.11588
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11762.32427
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.74137     1.04575     90.61119     87.99757
alpha_0                           0.79017      0.00011     0.79032      0.79001
Alpha_loss                        -1.58470     0.00044     -1.58429     -1.58548
Training/policy_loss              -2.57113     0.02850     -2.52994     -2.61519
Training/qf1_loss                 7789.07881   1140.60276  9829.86328   6531.04150
Training/qf2_loss                 16332.62070  1217.60109  18497.41016  15113.49805
Training/pf_norm                  0.09531      0.03551     0.13759      0.05251
Training/qf1_norm                 1322.19211   381.37734   1901.74146   742.24670
Training/qf2_norm                 450.29902    21.17428    486.54019    421.95801
log_std/mean                      -0.12952     0.00540     -0.12113     -0.13468
log_probs/mean                    -2.73158     0.00375     -2.72699     -2.73801
mean/mean                         -0.00219     0.00088     -0.00110     -0.00364
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01981973648071289
epoch last part time3 0.002532958984375
inside rlalgo, task 0, sumup 71194
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [71700]
collect time 0.0008027553558349609
inner_dict_sum {'sac_diff0': 0.0002536773681640625, 'sac_diff1': 0.006773948669433594, 'sac_diff2': 0.007998943328857422, 'sac_diff3': 0.010056257247924805, 'sac_diff4': 0.0067310333251953125, 'sac_diff5': 0.03150582313537598, 'sac_diff6': 0.00037980079650878906, 'all': 0.06369948387145996}
diff5_list [0.006507158279418945, 0.006112575531005859, 0.006461143493652344, 0.006344318389892578, 0.00608062744140625]
time3 0
time4 0.06443381309509277
time5 0.06447768211364746
time7 4.76837158203125e-07
gen_weight_change tensor(-23.2606)
policy weight change tensor(37.8736, grad_fn=<SumBackward0>)
time8 0.0018093585968017578
train_time 0.07549929618835449
eval time 0.1466057300567627
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:54,649 MainThread INFO: EPOCH:471
2024-01-23 01:02:54,650 MainThread INFO: Time Consumed:0.22514963150024414s
2024-01-23 01:02:54,650 MainThread INFO: Total Frames:71550s
  5%|▍         | 472/10000 [04:22<43:04,  3.69it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11915.74494
Train_Epoch_Reward                13394.30586
Running_Training_Average_Rewards  14247.60798
Explore_Time                      0.00080
Train___Time                      0.07550
Eval____Time                      0.14661
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11957.09956
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.74859     2.01095     94.39278     88.49117
alpha_0                           0.78977      0.00011     0.78993      0.78961
Alpha_loss                        -1.58958     0.00180     -1.58782     -1.59257
Training/policy_loss              -2.55921     0.00278     -2.55658     -2.56417
Training/qf1_loss                 7594.62383   1109.81722  9230.20117   6098.84766
Training/qf2_loss                 16562.07910  1443.75034  18705.45898  14437.00293
Training/pf_norm                  0.10919      0.01735     0.12922      0.08749
Training/qf1_norm                 436.43781    210.32873   772.26447    121.67744
Training/qf2_norm                 451.96478    9.49250     464.35226    436.48975
log_std/mean                      -0.12532     0.00006     -0.12525     -0.12542
log_probs/mean                    -2.73801     0.00397     -2.73402     -2.74498
mean/mean                         -0.00207     0.00004     -0.00202     -0.00211
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018397808074951172
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71194
epoch first part time 2.86102294921875e-06
replay_buffer._size: [71850]
collect time 0.0008409023284912109
inner_dict_sum {'sac_diff0': 0.0002205371856689453, 'sac_diff1': 0.0066051483154296875, 'sac_diff2': 0.0077745914459228516, 'sac_diff3': 0.009724855422973633, 'sac_diff4': 0.006430149078369141, 'sac_diff5': 0.031083106994628906, 'sac_diff6': 0.00038313865661621094, 'all': 0.062221527099609375}
diff5_list [0.0063359737396240234, 0.006059408187866211, 0.00622248649597168, 0.006448507308959961, 0.006016731262207031]
time3 0
time4 0.06296920776367188
time5 0.06301307678222656
time7 9.5367431640625e-07
gen_weight_change tensor(-23.2606)
policy weight change tensor(37.8804, grad_fn=<SumBackward0>)
time8 0.0018427371978759766
train_time 0.0737614631652832
eval time 0.15230321884155273
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:54,900 MainThread INFO: EPOCH:472
2024-01-23 01:02:54,901 MainThread INFO: Time Consumed:0.22916340827941895s
2024-01-23 01:02:54,901 MainThread INFO: Total Frames:71700s
  5%|▍         | 473/10000 [04:22<42:06,  3.77it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11914.53915
Train_Epoch_Reward                13865.53081
Running_Training_Average_Rewards  14044.62328
Explore_Time                      0.00084
Train___Time                      0.07376
Eval____Time                      0.15230
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11949.47348
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.03836     1.84426     93.47438     88.36084
alpha_0                           0.78938      0.00011     0.78953      0.78922
Alpha_loss                        -1.59247     0.00245     -1.58962     -1.59591
Training/policy_loss              -2.58844     0.00663     -2.57880     -2.59763
Training/qf1_loss                 6893.31328   1104.18912  8709.74414   5531.18994
Training/qf2_loss                 15489.64961  1414.58565  17972.51953  13888.89844
Training/pf_norm                  0.09464      0.02212     0.12148      0.05654
Training/qf1_norm                 406.56458    142.80641   554.95502    154.57632
Training/qf2_norm                 466.30710    9.19095     483.42883    457.72205
log_std/mean                      -0.13097     0.00005     -0.13092     -0.13104
log_probs/mean                    -2.73596     0.00845     -2.72393     -2.74765
mean/mean                         -0.00232     0.00006     -0.00226     -0.00240
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018690109252929688
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71194
epoch first part time 2.384185791015625e-06
replay_buffer._size: [72000]
collect time 0.0008280277252197266
inner_dict_sum {'sac_diff0': 0.0002276897430419922, 'sac_diff1': 0.006667375564575195, 'sac_diff2': 0.00786280632019043, 'sac_diff3': 0.010042190551757812, 'sac_diff4': 0.006636857986450195, 'sac_diff5': 0.0309598445892334, 'sac_diff6': 0.0003795623779296875, 'all': 0.06277632713317871}
diff5_list [0.006441593170166016, 0.006137371063232422, 0.006308317184448242, 0.0061397552490234375, 0.005932807922363281]
time3 0
time4 0.0635080337524414
time5 0.06355142593383789
time7 4.76837158203125e-07
gen_weight_change tensor(-23.2606)
policy weight change tensor(38.0024, grad_fn=<SumBackward0>)
time8 0.0018341541290283203
train_time 0.07433176040649414
eval time 0.1530146598815918
epoch last part time 5.245208740234375e-06
2024-01-23 01:02:55,153 MainThread INFO: EPOCH:473
2024-01-23 01:02:55,153 MainThread INFO: Time Consumed:0.23041749000549316s
2024-01-23 01:02:55,153 MainThread INFO: Total Frames:71850s
  5%|▍         | 474/10000 [04:22<41:29,  3.83it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11908.70180
Train_Epoch_Reward                23136.38384
Running_Training_Average_Rewards  14633.73964
Explore_Time                      0.00082
Train___Time                      0.07433
Eval____Time                      0.15301
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11907.59878
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.88693     1.82971    93.38504     88.12487
alpha_0                           0.78898      0.00011    0.78914      0.78882
Alpha_loss                        -1.59434     0.00086    -1.59319     -1.59578
Training/policy_loss              -2.57090     0.00567    -2.56207     -2.57805
Training/qf1_loss                 7196.65811   261.70332  7600.65527   6905.58447
Training/qf2_loss                 15757.80391  448.45161  16367.25488  15117.18945
Training/pf_norm                  0.11229      0.02337    0.14151      0.07032
Training/qf1_norm                 1008.44840   344.38048  1405.82910   383.05255
Training/qf2_norm                 456.83140    8.80400    473.71762    448.27283
log_std/mean                      -0.12530     0.00033    -0.12491     -0.12580
log_probs/mean                    -2.72967     0.00673    -2.71911     -2.73855
mean/mean                         -0.00288     0.00004    -0.00285     -0.00296
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018550634384155273
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71194
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [72150]
collect time 0.0008330345153808594
inner_dict_sum {'sac_diff0': 0.000270843505859375, 'sac_diff1': 0.006643772125244141, 'sac_diff2': 0.007853031158447266, 'sac_diff3': 0.009790182113647461, 'sac_diff4': 0.006818532943725586, 'sac_diff5': 0.03170895576477051, 'sac_diff6': 0.00039005279541015625, 'all': 0.06347537040710449}
diff5_list [0.0063250064849853516, 0.00670170783996582, 0.006254434585571289, 0.0063512325286865234, 0.0060765743255615234]
time3 0
time4 0.0642251968383789
time5 0.0642690658569336
time7 4.76837158203125e-07
gen_weight_change tensor(-23.2606)
policy weight change tensor(38.0750, grad_fn=<SumBackward0>)
time8 0.0018079280853271484
train_time 0.07485699653625488
eval time 0.15466690063476562
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:55,407 MainThread INFO: EPOCH:474
2024-01-23 01:02:55,408 MainThread INFO: Time Consumed:0.23269033432006836s
2024-01-23 01:02:55,408 MainThread INFO: Total Frames:72000s
  5%|▍         | 475/10000 [04:23<41:11,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11899.76107
Train_Epoch_Reward                7698.91280
Running_Training_Average_Rewards  14248.63263
Explore_Time                      0.00083
Train___Time                      0.07486
Eval____Time                      0.15467
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11909.74996
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.77654     2.45750     92.77116     86.30854
alpha_0                           0.78859      0.00011     0.78874      0.78843
Alpha_loss                        -1.59833     0.00224     -1.59482     -1.60126
Training/policy_loss              -2.55975     0.00598     -2.54799     -2.56439
Training/qf1_loss                 6426.10840   1039.39924  8434.26367   5598.96582
Training/qf2_loss                 15149.30723  1356.15391  17538.10938  13573.20117
Training/pf_norm                  0.11223      0.02074     0.13561      0.08196
Training/qf1_norm                 954.59677    451.31695   1347.57727   153.42027
Training/qf2_norm                 444.17316    11.64797    454.29453    423.19498
log_std/mean                      -0.13839     0.00010     -0.13822     -0.13850
log_probs/mean                    -2.73226     0.00761     -2.71751     -2.73895
mean/mean                         -0.00430     0.00002     -0.00428     -0.00433
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018867015838623047
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71194
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [72300]
collect time 0.0009489059448242188
inside mustsac before update, task 0, sumup 71194
inside mustsac after update, task 0, sumup 70590
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.00734710693359375, 'sac_diff2': 0.008827447891235352, 'sac_diff3': 0.011218070983886719, 'sac_diff4': 0.007649421691894531, 'sac_diff5': 0.05199456214904785, 'sac_diff6': 0.0004074573516845703, 'all': 0.08765459060668945}
diff5_list [0.011261701583862305, 0.009717226028442383, 0.010811805725097656, 0.010592937469482422, 0.009610891342163086]
time3 0.0008881092071533203
time4 0.0885317325592041
time5 0.0885932445526123
time7 0.009034872055053711
gen_weight_change tensor(-23.1090)
policy weight change tensor(38.0311, grad_fn=<SumBackward0>)
time8 0.001886129379272461
train_time 0.11853289604187012
eval time 0.12159872055053711
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:55,674 MainThread INFO: EPOCH:475
2024-01-23 01:02:55,674 MainThread INFO: Time Consumed:0.24333477020263672s
2024-01-23 01:02:55,674 MainThread INFO: Total Frames:72150s
  5%|▍         | 476/10000 [04:23<41:30,  3.82it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11889.16914
Train_Epoch_Reward                15887.83771
Running_Training_Average_Rewards  14360.98743
Explore_Time                      0.00094
Train___Time                      0.11853
Eval____Time                      0.12160
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11903.78112
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.24398     1.36926    91.49306     87.58833
alpha_0                           0.78819      0.00011    0.78835      0.78803
Alpha_loss                        -1.60331     0.00159    -1.60152     -1.60556
Training/policy_loss              -2.56749     0.01934    -2.53775     -2.59631
Training/qf1_loss                 6426.78779   402.09583  6959.85645   5798.11719
Training/qf2_loss                 15061.62109  629.61025  15729.43945  13915.30176
Training/pf_norm                  0.10048      0.02184    0.12618      0.06820
Training/qf1_norm                 679.01788    431.50464  1188.77307   141.13928
Training/qf2_norm                 446.66833    18.83289   473.43704    424.38098
log_std/mean                      -0.13860     0.00652    -0.13028     -0.14786
log_probs/mean                    -2.73906     0.00616    -2.73155     -2.74820
mean/mean                         -0.00210     0.00066    -0.00116     -0.00285
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01898479461669922
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70590
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [72450]
collect time 0.0009150505065917969
inner_dict_sum {'sac_diff0': 0.00023508071899414062, 'sac_diff1': 0.006794929504394531, 'sac_diff2': 0.008064031600952148, 'sac_diff3': 0.010277986526489258, 'sac_diff4': 0.00713348388671875, 'sac_diff5': 0.032607316970825195, 'sac_diff6': 0.0004031658172607422, 'all': 0.06551599502563477}
diff5_list [0.0065267086029052734, 0.00606846809387207, 0.006852626800537109, 0.0068361759185791016, 0.006323337554931641]
time3 0
time4 0.06628632545471191
time5 0.06633329391479492
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1090)
policy weight change tensor(38.0069, grad_fn=<SumBackward0>)
time8 0.001844644546508789
train_time 0.07706403732299805
eval time 0.16173648834228516
epoch last part time 5.7220458984375e-06
2024-01-23 01:02:55,938 MainThread INFO: EPOCH:476
2024-01-23 01:02:55,938 MainThread INFO: Time Consumed:0.24196815490722656s
2024-01-23 01:02:55,938 MainThread INFO: Total Frames:72300s
  5%|▍         | 477/10000 [04:23<41:36,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11839.67542
Train_Epoch_Reward                15927.59918
Running_Training_Average_Rewards  14298.12338
Explore_Time                      0.00091
Train___Time                      0.07706
Eval____Time                      0.16174
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11400.40692
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.97990     2.05793     93.91831     87.80166
alpha_0                           0.78780      0.00011     0.78796      0.78764
Alpha_loss                        -1.60486     0.00073     -1.60382     -1.60570
Training/policy_loss              -2.51687     0.00251     -2.51343     -2.52029
Training/qf1_loss                 7865.55869   1090.60956  9658.37793   6346.25537
Training/qf2_loss                 16651.54551  1381.44763  18971.49609  15003.74707
Training/pf_norm                  0.10508      0.04054     0.18319      0.06551
Training/qf1_norm                 946.80597    379.50936   1482.46008   381.23773
Training/qf2_norm                 414.51501    9.24903     427.63901    400.15500
log_std/mean                      -0.13826     0.00009     -0.13810     -0.13835
log_probs/mean                    -2.73142     0.00294     -2.72705     -2.73523
mean/mean                         -0.00176     0.00005     -0.00167     -0.00180
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01847529411315918
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70590
epoch first part time 2.86102294921875e-06
replay_buffer._size: [72600]
collect time 0.0009195804595947266
inner_dict_sum {'sac_diff0': 0.00023293495178222656, 'sac_diff1': 0.00676274299621582, 'sac_diff2': 0.007979154586791992, 'sac_diff3': 0.010385990142822266, 'sac_diff4': 0.006805419921875, 'sac_diff5': 0.03191852569580078, 'sac_diff6': 0.0004067420959472656, 'all': 0.06449151039123535}
diff5_list [0.00644683837890625, 0.006143808364868164, 0.006337642669677734, 0.006540536880493164, 0.006449699401855469]
time3 0
time4 0.06522941589355469
time5 0.0652766227722168
time7 7.152557373046875e-07
gen_weight_change tensor(-23.1090)
policy weight change tensor(38.0113, grad_fn=<SumBackward0>)
time8 0.0018270015716552734
train_time 0.07606792449951172
eval time 0.16254019737243652
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:56,202 MainThread INFO: EPOCH:477
2024-01-23 01:02:56,202 MainThread INFO: Time Consumed:0.24184393882751465s
2024-01-23 01:02:56,202 MainThread INFO: Total Frames:72450s
  5%|▍         | 478/10000 [04:23<41:42,  3.80it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11782.33888
Train_Epoch_Reward                5162.22305
Running_Training_Average_Rewards  13252.11988
Explore_Time                      0.00092
Train___Time                      0.07607
Eval____Time                      0.16254
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11346.50379
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.37877     2.10547    91.66678     85.47008
alpha_0                           0.78740      0.00011    0.78756      0.78725
Alpha_loss                        -1.60908     0.00088    -1.60828     -1.61069
Training/policy_loss              -2.57525     0.00300    -2.57054     -2.58003
Training/qf1_loss                 7038.85811   553.58225  7546.43750   6316.71533
Training/qf2_loss                 15526.79707  849.70221  16378.27539  14055.35352
Training/pf_norm                  0.08348      0.02125    0.10787      0.05359
Training/qf1_norm                 455.61305    374.89318  1172.81165   114.08136
Training/qf2_norm                 448.19059    10.10062   458.79449    429.26328
log_std/mean                      -0.12705     0.00005    -0.12697     -0.12713
log_probs/mean                    -2.73500     0.00382    -2.72885     -2.74081
mean/mean                         -0.00312     0.00004    -0.00307     -0.00319
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01914215087890625
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70590
epoch first part time 2.86102294921875e-06
replay_buffer._size: [72750]
collect time 0.0008535385131835938
inner_dict_sum {'sac_diff0': 0.0002300739288330078, 'sac_diff1': 0.0068471431732177734, 'sac_diff2': 0.008152484893798828, 'sac_diff3': 0.010338544845581055, 'sac_diff4': 0.0070569515228271484, 'sac_diff5': 0.031685590744018555, 'sac_diff6': 0.00040650367736816406, 'all': 0.06471729278564453}
diff5_list [0.006432533264160156, 0.006239414215087891, 0.006314992904663086, 0.0062427520751953125, 0.006455898284912109]
time3 0
time4 0.0654904842376709
time5 0.06553483009338379
time7 4.76837158203125e-07
gen_weight_change tensor(-23.1090)
policy weight change tensor(37.9779, grad_fn=<SumBackward0>)
time8 0.0019097328186035156
train_time 0.0766153335571289
eval time 0.1655735969543457
epoch last part time 5.9604644775390625e-06
2024-01-23 01:02:56,470 MainThread INFO: EPOCH:478
2024-01-23 01:02:56,470 MainThread INFO: Time Consumed:0.24537134170532227s
2024-01-23 01:02:56,470 MainThread INFO: Total Frames:72600s
  5%|▍         | 479/10000 [04:24<41:56,  3.78it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11729.64000
Train_Epoch_Reward                17861.76372
Running_Training_Average_Rewards  13670.84238
Explore_Time                      0.00085
Train___Time                      0.07662
Eval____Time                      0.16557
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11341.56145
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.53944     3.26037     94.90826     85.65936
alpha_0                           0.78701      0.00011     0.78717      0.78685
Alpha_loss                        -1.61190     0.00104     -1.61054     -1.61321
Training/policy_loss              -2.56474     0.00573     -2.55552     -2.57228
Training/qf1_loss                 6972.36787   634.86204   7772.17969   5852.21191
Training/qf2_loss                 15381.84785  1142.03232  16595.91602  13471.83887
Training/pf_norm                  0.12549      0.04249     0.18237      0.06018
Training/qf1_norm                 2052.15297   657.69527   2829.19263   933.09576
Training/qf2_norm                 442.40015    15.74258    468.08911    423.57928
log_std/mean                      -0.13249     0.00004     -0.13242     -0.13252
log_probs/mean                    -2.73272     0.00700     -2.72142     -2.74179
mean/mean                         -0.00292     0.00008     -0.00282     -0.00302
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018963336944580078
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70590
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [72900]
collect time 0.0008668899536132812
inner_dict_sum {'sac_diff0': 0.00021004676818847656, 'sac_diff1': 0.006971597671508789, 'sac_diff2': 0.008176088333129883, 'sac_diff3': 0.011148691177368164, 'sac_diff4': 0.007035017013549805, 'sac_diff5': 0.032372236251831055, 'sac_diff6': 0.00039005279541015625, 'all': 0.06630373001098633}
diff5_list [0.006522178649902344, 0.006803035736083984, 0.006191253662109375, 0.00642704963684082, 0.006428718566894531]
time3 0
time4 0.06707620620727539
time5 0.0671226978302002
time7 7.152557373046875e-07
gen_weight_change tensor(-23.1090)
policy weight change tensor(37.9461, grad_fn=<SumBackward0>)
time8 0.0021219253540039062
train_time 0.07850170135498047
eval time 0.16332054138183594
epoch last part time 5.0067901611328125e-06
2024-01-23 01:02:56,737 MainThread INFO: EPOCH:479
2024-01-23 01:02:56,737 MainThread INFO: Time Consumed:0.24495339393615723s
2024-01-23 01:02:56,737 MainThread INFO: Total Frames:72750s
  5%|▍         | 480/10000 [04:24<42:04,  3.77it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11683.43821
Train_Epoch_Reward                6231.23522
Running_Training_Average_Rewards  13276.35755
Explore_Time                      0.00086
Train___Time                      0.07850
Eval____Time                      0.16332
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11355.88279
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.43279     2.30517     94.49187     87.81669
alpha_0                           0.78662      0.00011     0.78677      0.78646
Alpha_loss                        -1.61635     0.00066     -1.61553     -1.61733
Training/policy_loss              -2.60583     0.00384     -2.60059     -2.61237
Training/qf1_loss                 7711.22061   1598.84954  10157.11523  6111.71045
Training/qf2_loss                 16425.19883  2004.10543  19637.45312  14642.66406
Training/pf_norm                  0.09390      0.01617     0.11656      0.06691
Training/qf1_norm                 568.58984    459.20134   1383.53064   81.50594
Training/qf2_norm                 486.82936    12.02658    508.01367    473.14493
log_std/mean                      -0.12015     0.00005     -0.12009     -0.12024
log_probs/mean                    -2.73724     0.00455     -2.73099     -2.74505
mean/mean                         -0.00238     0.00002     -0.00236     -0.00242
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018680810928344727
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70590
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [73050]
collect time 0.0008852481842041016
inside mustsac before update, task 0, sumup 70590
inside mustsac after update, task 0, sumup 70500
inner_dict_sum {'sac_diff0': 0.00020551681518554688, 'sac_diff1': 0.006932258605957031, 'sac_diff2': 0.008055925369262695, 'sac_diff3': 0.010613679885864258, 'sac_diff4': 0.007142066955566406, 'sac_diff5': 0.04883694648742676, 'sac_diff6': 0.0004189014434814453, 'all': 0.08220529556274414}
diff5_list [0.01001882553100586, 0.009767770767211914, 0.010100126266479492, 0.009593486785888672, 0.00935673713684082]
time3 0.0008370876312255859
time4 0.08304882049560547
time5 0.08310079574584961
time7 0.009049415588378906
gen_weight_change tensor(-23.0124)
policy weight change tensor(37.9171, grad_fn=<SumBackward0>)
time8 0.0025627613067626953
train_time 0.11286091804504395
eval time 0.1234588623046875
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:56,998 MainThread INFO: EPOCH:480
2024-01-23 01:02:56,999 MainThread INFO: Time Consumed:0.2393965721130371s
2024-01-23 01:02:56,999 MainThread INFO: Total Frames:72900s
  5%|▍         | 481/10000 [04:24<41:59,  3.78it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11647.01088
Train_Epoch_Reward                14447.53187
Running_Training_Average_Rewards  13527.24466
Explore_Time                      0.00088
Train___Time                      0.11286
Eval____Time                      0.12346
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11398.05093
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.50680     0.59984    92.45866     90.97896
alpha_0                           0.78622      0.00011    0.78638      0.78607
Alpha_loss                        -1.61824     0.00115    -1.61662     -1.61987
Training/policy_loss              -2.58253     0.03390    -2.53461     -2.63596
Training/qf1_loss                 7086.26074   648.49648  8255.13184   6354.92480
Training/qf2_loss                 15961.55449  750.00019  17271.01758  15087.88574
Training/pf_norm                  0.12868      0.01746    0.15673      0.10576
Training/qf1_norm                 890.46975    554.11636  1804.73352   329.36633
Training/qf2_norm                 468.20975    25.62444   514.71716    436.20413
log_std/mean                      -0.12672     0.00379    -0.12321     -0.13303
log_probs/mean                    -2.73106     0.00717    -2.71875     -2.73785
mean/mean                         -0.00247     0.00114    -0.00089     -0.00419
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018451929092407227
epoch last part time3 0.0024580955505371094
inside rlalgo, task 0, sumup 70500
epoch first part time 2.86102294921875e-06
replay_buffer._size: [73200]
collect time 0.0008301734924316406
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.0068759918212890625, 'sac_diff2': 0.008138895034790039, 'sac_diff3': 0.01044607162475586, 'sac_diff4': 0.006868600845336914, 'sac_diff5': 0.033396005630493164, 'sac_diff6': 0.00040268898010253906, 'all': 0.06633901596069336}
diff5_list [0.0067272186279296875, 0.0062253475189208984, 0.006023406982421875, 0.008197307586669922, 0.006222724914550781]
time3 0
time4 0.06710004806518555
time5 0.06715106964111328
time7 9.5367431640625e-07
gen_weight_change tensor(-23.0124)
policy weight change tensor(37.9891, grad_fn=<SumBackward0>)
time8 0.0019142627716064453
train_time 0.07804369926452637
eval time 0.16111326217651367
epoch last part time 7.152557373046875e-06
2024-01-23 01:02:57,265 MainThread INFO: EPOCH:481
2024-01-23 01:02:57,265 MainThread INFO: Time Consumed:0.24226093292236328s
2024-01-23 01:02:57,265 MainThread INFO: Total Frames:73050s
  5%|▍         | 482/10000 [04:24<41:57,  3.78it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11614.96816
Train_Epoch_Reward                6304.27676
Running_Training_Average_Rewards  13611.76720
Explore_Time                      0.00083
Train___Time                      0.07804
Eval____Time                      0.16111
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11636.67241
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.22124     1.50897     92.36335     88.06134
alpha_0                           0.78583      0.00011     0.78599      0.78567
Alpha_loss                        -1.62262     0.00218     -1.61992     -1.62643
Training/policy_loss              -2.60460     0.00493     -2.59834     -2.61213
Training/qf1_loss                 7419.24443   1044.17049  9060.12793   6149.78125
Training/qf2_loss                 16088.31035  1200.40781  17899.70703  14599.51367
Training/pf_norm                  0.11106      0.01500     0.13303      0.09196
Training/qf1_norm                 357.70151    223.83383   720.94727    117.17871
Training/qf2_norm                 481.36347    8.02355     493.30667    469.76660
log_std/mean                      -0.12787     0.00017     -0.12765     -0.12813
log_probs/mean                    -2.73529     0.00665     -2.72687     -2.74550
mean/mean                         -0.00187     0.00002     -0.00186     -0.00189
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018709897994995117
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70500
epoch first part time 2.86102294921875e-06
replay_buffer._size: [73350]
collect time 0.0008904933929443359
inner_dict_sum {'sac_diff0': 0.00022172927856445312, 'sac_diff1': 0.00704503059387207, 'sac_diff2': 0.00789952278137207, 'sac_diff3': 0.011024713516235352, 'sac_diff4': 0.007173061370849609, 'sac_diff5': 0.032485246658325195, 'sac_diff6': 0.00038695335388183594, 'all': 0.06623625755310059}
diff5_list [0.006870269775390625, 0.00639033317565918, 0.0065038204193115234, 0.006334543228149414, 0.006386280059814453]
time3 0
time4 0.06702899932861328
time5 0.0670771598815918
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0124)
policy weight change tensor(38.0434, grad_fn=<SumBackward0>)
time8 0.00203704833984375
train_time 0.0787043571472168
eval time 0.155073881149292
epoch last part time 5.4836273193359375e-06
2024-01-23 01:02:57,524 MainThread INFO: EPOCH:482
2024-01-23 01:02:57,524 MainThread INFO: Time Consumed:0.2369678020477295s
2024-01-23 01:02:57,525 MainThread INFO: Total Frames:73200s
  5%|▍         | 483/10000 [04:25<41:40,  3.81it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11590.15782
Train_Epoch_Reward                7697.56828
Running_Training_Average_Rewards  13236.86408
Explore_Time                      0.00088
Train___Time                      0.07870
Eval____Time                      0.15507
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11701.37003
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.37209     0.42844    91.96712     90.72264
alpha_0                           0.78544      0.00011    0.78560      0.78528
Alpha_loss                        -1.62530     0.00216    -1.62341     -1.62944
Training/policy_loss              -2.60319     0.00445    -2.59947     -2.61175
Training/qf1_loss                 7017.52842   599.38127  7498.28760   5947.86328
Training/qf2_loss                 15899.91113  636.21018  16501.69336  14758.06250
Training/pf_norm                  0.10395      0.01498    0.12179      0.08113
Training/qf1_norm                 129.90680    34.89159   193.48778    94.84530
Training/qf2_norm                 490.17823    2.15099    493.35641    487.09000
log_std/mean                      -0.13512     0.00011    -0.13494     -0.13525
log_probs/mean                    -2.73243     0.00597    -2.72738     -2.74400
mean/mean                         -0.00250     0.00007    -0.00240     -0.00259
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018059968948364258
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70500
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [73500]
collect time 0.0008273124694824219
inner_dict_sum {'sac_diff0': 0.00024056434631347656, 'sac_diff1': 0.0067691802978515625, 'sac_diff2': 0.007721900939941406, 'sac_diff3': 0.009339094161987305, 'sac_diff4': 0.00607752799987793, 'sac_diff5': 0.03127121925354004, 'sac_diff6': 0.00037407875061035156, 'all': 0.06179356575012207}
diff5_list [0.006489992141723633, 0.006445407867431641, 0.006017208099365234, 0.006092548370361328, 0.006226062774658203]
time3 0
time4 0.06252074241638184
time5 0.06256365776062012
time7 4.76837158203125e-07
gen_weight_change tensor(-23.0124)
policy weight change tensor(37.9882, grad_fn=<SumBackward0>)
time8 0.0018310546875
train_time 0.07320237159729004
eval time 0.15679168701171875
epoch last part time 4.76837158203125e-06
2024-01-23 01:02:57,779 MainThread INFO: EPOCH:483
2024-01-23 01:02:57,779 MainThread INFO: Time Consumed:0.23304438591003418s
2024-01-23 01:02:57,779 MainThread INFO: Total Frames:73350s
  5%|▍         | 484/10000 [04:25<41:18,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11573.61956
Train_Epoch_Reward                3913.10414
Running_Training_Average_Rewards  13014.71839
Explore_Time                      0.00082
Train___Time                      0.07320
Eval____Time                      0.15679
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11742.21617
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.87773     2.61493     96.59964     89.17776
alpha_0                           0.78505      0.00011     0.78520      0.78489
Alpha_loss                        -1.62869     0.00079     -1.62754     -1.62990
Training/policy_loss              -2.60837     0.00361     -2.60457     -2.61462
Training/qf1_loss                 7899.17031   1078.08843  9660.69434   6711.78271
Training/qf2_loss                 16854.63105  1575.74506  19538.16406  15143.50586
Training/pf_norm                  0.10667      0.01596     0.12941      0.08416
Training/qf1_norm                 576.16475    227.01479   811.48181    262.12119
Training/qf2_norm                 509.53480    14.06883    534.86713    494.92545
log_std/mean                      -0.13973     0.00012     -0.13950     -0.13984
log_probs/mean                    -2.73255     0.00436     -2.72779     -2.74001
mean/mean                         -0.00323     0.00013     -0.00303     -0.00340
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01860499382019043
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70500
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [73650]
collect time 0.0008349418640136719
inner_dict_sum {'sac_diff0': 0.00023627281188964844, 'sac_diff1': 0.006973981857299805, 'sac_diff2': 0.008119821548461914, 'sac_diff3': 0.010664701461791992, 'sac_diff4': 0.007179737091064453, 'sac_diff5': 0.03186917304992676, 'sac_diff6': 0.000400543212890625, 'all': 0.0654442310333252}
diff5_list [0.006518840789794922, 0.006234169006347656, 0.006418943405151367, 0.006402492523193359, 0.006294727325439453]
time3 0
time4 0.06619930267333984
time5 0.06624341011047363
time7 7.152557373046875e-07
gen_weight_change tensor(-23.0124)
policy weight change tensor(37.9513, grad_fn=<SumBackward0>)
time8 0.0018887519836425781
train_time 0.0773000717163086
eval time 0.1609194278717041
epoch last part time 4.291534423828125e-06
2024-01-23 01:02:58,042 MainThread INFO: EPOCH:484
2024-01-23 01:02:58,042 MainThread INFO: Time Consumed:0.24130582809448242s
2024-01-23 01:02:58,042 MainThread INFO: Total Frames:73500s
  5%|▍         | 485/10000 [04:25<41:26,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11565.01016
Train_Epoch_Reward                8686.07258
Running_Training_Average_Rewards  12757.03660
Explore_Time                      0.00083
Train___Time                      0.07730
Eval____Time                      0.16092
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11823.65603
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.62364     2.22545     95.27208     88.74956
alpha_0                           0.78465      0.00011     0.78481      0.78450
Alpha_loss                        -1.63239     0.00121     -1.63088     -1.63450
Training/policy_loss              -2.56844     0.00126     -2.56663     -2.57024
Training/qf1_loss                 8411.76719   1932.01360  10748.53125  5459.00000
Training/qf2_loss                 17518.69629  2369.84345  20377.90234  13782.66113
Training/pf_norm                  0.09139      0.01742     0.11133      0.07030
Training/qf1_norm                 483.80224    426.80889   1273.17139   119.66908
Training/qf2_norm                 462.40596    10.86472    474.88028    443.42270
log_std/mean                      -0.13636     0.00002     -0.13634     -0.13640
log_probs/mean                    -2.73391     0.00167     -2.73238     -2.73704
mean/mean                         -0.00185     0.00004     -0.00179     -0.00192
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018508434295654297
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70500
epoch first part time 3.337860107421875e-06
replay_buffer._size: [73800]
collect time 0.0007817745208740234
inside mustsac before update, task 0, sumup 70500
inside mustsac after update, task 0, sumup 71194
inner_dict_sum {'sac_diff0': 0.00020122528076171875, 'sac_diff1': 0.006978273391723633, 'sac_diff2': 0.008370637893676758, 'sac_diff3': 0.010788202285766602, 'sac_diff4': 0.007262706756591797, 'sac_diff5': 0.05043292045593262, 'sac_diff6': 0.0003998279571533203, 'all': 0.08443379402160645}
diff5_list [0.010519266128540039, 0.01005244255065918, 0.009800910949707031, 0.009759187698364258, 0.01030111312866211]
time3 0.0008492469787597656
time4 0.08528470993041992
time5 0.08533549308776855
time7 0.008997917175292969
gen_weight_change tensor(-22.8749)
policy weight change tensor(37.8867, grad_fn=<SumBackward0>)
time8 0.0017733573913574219
train_time 0.11416864395141602
eval time 0.12671613693237305
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:58,308 MainThread INFO: EPOCH:485
2024-01-23 01:02:58,308 MainThread INFO: Time Consumed:0.2439723014831543s
2024-01-23 01:02:58,308 MainThread INFO: Total Frames:73650s
  5%|▍         | 486/10000 [04:25<41:41,  3.80it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11566.33370
Train_Epoch_Reward                14499.49304
Running_Training_Average_Rewards  12866.18232
Explore_Time                      0.00078
Train___Time                      0.11417
Eval____Time                      0.12672
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11917.01647
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.03492     1.08079    90.53516     87.54195
alpha_0                           0.78426      0.00011    0.78442      0.78410
Alpha_loss                        -1.63566     0.00049    -1.63471     -1.63608
Training/policy_loss              -2.62217     0.02697    -2.58474     -2.66349
Training/qf1_loss                 6704.12217   708.19400  7880.77100   5732.81348
Training/qf2_loss                 15102.38750  548.24716  15945.27832  14258.63574
Training/pf_norm                  0.12123      0.01561    0.14905      0.10315
Training/qf1_norm                 1020.78453   382.72501  1698.56384   627.42902
Training/qf2_norm                 485.79399    21.02984   510.02237    452.87000
log_std/mean                      -0.13118     0.00761    -0.12274     -0.14530
log_probs/mean                    -2.73349     0.00452    -2.72944     -2.74077
mean/mean                         -0.00142     0.00072    -0.00028     -0.00217
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01945042610168457
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71194
epoch first part time 2.86102294921875e-06
replay_buffer._size: [73950]
collect time 0.0008504390716552734
inner_dict_sum {'sac_diff0': 0.00019931793212890625, 'sac_diff1': 0.007098197937011719, 'sac_diff2': 0.008498191833496094, 'sac_diff3': 0.010648250579833984, 'sac_diff4': 0.007145881652832031, 'sac_diff5': 0.0322575569152832, 'sac_diff6': 0.00040340423583984375, 'all': 0.06625080108642578}
diff5_list [0.006589412689208984, 0.006573677062988281, 0.0064389705657958984, 0.006239891052246094, 0.006415605545043945]
time3 0
time4 0.06703400611877441
time5 0.06708455085754395
time7 7.152557373046875e-07
gen_weight_change tensor(-22.8749)
policy weight change tensor(37.9091, grad_fn=<SumBackward0>)
time8 0.0018401145935058594
train_time 0.0781397819519043
eval time 0.16471576690673828
epoch last part time 5.9604644775390625e-06
2024-01-23 01:02:58,576 MainThread INFO: EPOCH:486
2024-01-23 01:02:58,577 MainThread INFO: Time Consumed:0.2460918426513672s
2024-01-23 01:02:58,577 MainThread INFO: Total Frames:73800s
  5%|▍         | 487/10000 [04:26<41:55,  3.78it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11686.13574
Train_Epoch_Reward                23935.43446
Running_Training_Average_Rewards  13202.35070
Explore_Time                      0.00085
Train___Time                      0.07814
Eval____Time                      0.16472
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12598.42729
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.22637     1.88020    91.65602     86.01766
alpha_0                           0.78387      0.00011    0.78403      0.78371
Alpha_loss                        -1.63874     0.00087    -1.63779     -1.64013
Training/policy_loss              -2.58915     0.00133    -2.58748     -2.59116
Training/qf1_loss                 6301.34336   641.95718  7336.10840   5537.68359
Training/qf2_loss                 14764.80684  891.12035  15988.08105  13395.57227
Training/pf_norm                  0.09247      0.02582    0.13551      0.06526
Training/qf1_norm                 336.19246    273.01481  840.49127    84.26429
Training/qf2_norm                 462.80226    9.98109    475.48575    445.74573
log_std/mean                      -0.12961     0.00008    -0.12953     -0.12974
log_probs/mean                    -2.73230     0.00146    -2.73018     -2.73415
mean/mean                         -0.00296     0.00010    -0.00281     -0.00309
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01864480972290039
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71194
epoch first part time 3.337860107421875e-06
replay_buffer._size: [74100]
collect time 0.0009043216705322266
inner_dict_sum {'sac_diff0': 0.00021314620971679688, 'sac_diff1': 0.008079767227172852, 'sac_diff2': 0.00943756103515625, 'sac_diff3': 0.012147188186645508, 'sac_diff4': 0.008081674575805664, 'sac_diff5': 0.036690473556518555, 'sac_diff6': 0.0004417896270751953, 'all': 0.07509160041809082}
diff5_list [0.007273674011230469, 0.007239580154418945, 0.007661342620849609, 0.0071032047271728516, 0.00741267204284668]
time3 0
time4 0.07596969604492188
time5 0.07602548599243164
time7 9.5367431640625e-07
gen_weight_change tensor(-22.8749)
policy weight change tensor(37.9911, grad_fn=<SumBackward0>)
time8 0.0019485950469970703
train_time 0.08763933181762695
eval time 0.15706491470336914
epoch last part time 6.9141387939453125e-06
2024-01-23 01:02:58,847 MainThread INFO: EPOCH:487
2024-01-23 01:02:58,847 MainThread INFO: Time Consumed:0.24793696403503418s
2024-01-23 01:02:58,847 MainThread INFO: Total Frames:73950s
  5%|▍         | 488/10000 [04:26<42:11,  3.76it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11824.25208
Train_Epoch_Reward                7652.22456
Running_Training_Average_Rewards  13276.99196
Explore_Time                      0.00090
Train___Time                      0.08764
Eval____Time                      0.15706
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12727.66728
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.84169     2.37413     92.34377     86.06622
alpha_0                           0.78348      0.00011     0.78363      0.78332
Alpha_loss                        -1.64203     0.00141     -1.64091     -1.64479
Training/policy_loss              -2.62390     0.00521     -2.61783     -2.63282
Training/qf1_loss                 6658.39268   823.77725   8249.52539   6093.98193
Training/qf2_loss                 15060.31426  1178.55089  17297.68945  13977.20312
Training/pf_norm                  0.11693      0.02393     0.15823      0.08778
Training/qf1_norm                 443.87711    305.51917   947.24689    143.78636
Training/qf2_norm                 481.90471    12.19681    499.91541    467.55423
log_std/mean                      -0.13278     0.00016     -0.13261     -0.13304
log_probs/mean                    -2.73199     0.00635     -2.72506     -2.74334
mean/mean                         -0.00190     0.00007     -0.00182     -0.00199
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018393516540527344
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71194
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [74250]
collect time 0.0010118484497070312
inner_dict_sum {'sac_diff0': 0.00021719932556152344, 'sac_diff1': 0.008132457733154297, 'sac_diff2': 0.010009527206420898, 'sac_diff3': 0.01298213005065918, 'sac_diff4': 0.008812427520751953, 'sac_diff5': 0.03860974311828613, 'sac_diff6': 0.00043964385986328125, 'all': 0.07920312881469727}
diff5_list [0.007390022277832031, 0.00858759880065918, 0.008093118667602539, 0.007354021072387695, 0.0071849822998046875]
time3 0
time4 0.0801091194152832
time5 0.08016729354858398
time7 7.152557373046875e-07
gen_weight_change tensor(-22.8749)
policy weight change tensor(38.0969, grad_fn=<SumBackward0>)
time8 0.0020689964294433594
train_time 0.09194707870483398
eval time 0.16319823265075684
epoch last part time 7.867813110351562e-06
2024-01-23 01:02:59,127 MainThread INFO: EPOCH:488
2024-01-23 01:02:59,128 MainThread INFO: Time Consumed:0.2586185932159424s
2024-01-23 01:02:59,128 MainThread INFO: Total Frames:74100s
  5%|▍         | 489/10000 [04:26<42:53,  3.70it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11980.39583
Train_Epoch_Reward                12723.61851
Running_Training_Average_Rewards  13336.17677
Explore_Time                      0.00101
Train___Time                      0.09195
Eval____Time                      0.16320
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12902.99886
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.88281     2.12884    93.37415     86.97826
alpha_0                           0.78309      0.00011    0.78324      0.78293
Alpha_loss                        -1.64481     0.00184    -1.64345     -1.64832
Training/policy_loss              -2.60579     0.00649    -2.59636     -2.61438
Training/qf1_loss                 7008.18223   687.04736  8347.41211   6509.03223
Training/qf2_loss                 15581.65449  760.47831  16838.42383  14676.61230
Training/pf_norm                  0.11666      0.01791    0.14124      0.08761
Training/qf1_norm                 1245.60741   380.80093  1875.05151   705.66290
Training/qf2_norm                 482.57083    11.49923   501.61591    466.65924
log_std/mean                      -0.12401     0.00016    -0.12379     -0.12424
log_probs/mean                    -2.72961     0.00803    -2.71855     -2.74122
mean/mean                         -0.00099     0.00006    -0.00094     -0.00110
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018944263458251953
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71194
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [74400]
collect time 0.0010259151458740234
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.007380247116088867, 'sac_diff2': 0.008649587631225586, 'sac_diff3': 0.011122465133666992, 'sac_diff4': 0.007517337799072266, 'sac_diff5': 0.03400778770446777, 'sac_diff6': 0.0004215240478515625, 'all': 0.0693063735961914}
diff5_list [0.007250785827636719, 0.006662845611572266, 0.0066356658935546875, 0.006948232650756836, 0.006510257720947266]
time3 0
time4 0.07012438774108887
time5 0.07018685340881348
time7 7.152557373046875e-07
gen_weight_change tensor(-22.8749)
policy weight change tensor(38.2160, grad_fn=<SumBackward0>)
time8 0.002036571502685547
train_time 0.08163928985595703
eval time 0.15889930725097656
epoch last part time 7.3909759521484375e-06
2024-01-23 01:02:59,394 MainThread INFO: EPOCH:489
2024-01-23 01:02:59,394 MainThread INFO: Time Consumed:0.24402594566345215s
2024-01-23 01:02:59,395 MainThread INFO: Total Frames:74250s
  5%|▍         | 490/10000 [04:27<42:42,  3.71it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12151.50527
Train_Epoch_Reward                6725.58347
Running_Training_Average_Rewards  13532.22528
Explore_Time                      0.00102
Train___Time                      0.08164
Eval____Time                      0.15890
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13066.97719
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.84594     1.30943    93.25494     89.59190
alpha_0                           0.78269      0.00011    0.78285      0.78254
Alpha_loss                        -1.64920     0.00123    -1.64745     -1.65047
Training/policy_loss              -2.59490     0.00484    -2.58688     -2.60091
Training/qf1_loss                 7971.47256   781.87037  8776.30273   6928.35693
Training/qf2_loss                 16729.33848  902.30538  17592.99023  15474.25000
Training/pf_norm                  0.08328      0.01547    0.10352      0.06534
Training/qf1_norm                 1122.42222   277.26232  1625.01086   827.61768
Training/qf2_norm                 481.01572    6.56566    493.08542    474.89944
log_std/mean                      -0.12771     0.00020    -0.12744     -0.12801
log_probs/mean                    -2.73379     0.00587    -2.72389     -2.74050
mean/mean                         -0.00327     0.00006    -0.00321     -0.00338
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018754243850708008
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71194
epoch first part time 3.337860107421875e-06
replay_buffer._size: [74550]
collect time 0.0009677410125732422
inside mustsac before update, task 0, sumup 71194
inside mustsac after update, task 0, sumup 71265
inner_dict_sum {'sac_diff0': 0.00022292137145996094, 'sac_diff1': 0.007741451263427734, 'sac_diff2': 0.009126901626586914, 'sac_diff3': 0.011886358261108398, 'sac_diff4': 0.008590936660766602, 'sac_diff5': 0.0546417236328125, 'sac_diff6': 0.0004558563232421875, 'all': 0.0926661491394043}
diff5_list [0.011148214340209961, 0.01013040542602539, 0.010992765426635742, 0.010369062423706055, 0.012001276016235352]
time3 0.0009012222290039062
time4 0.0935666561126709
time5 0.09363079071044922
time7 0.009166240692138672
gen_weight_change tensor(-22.6132)
policy weight change tensor(38.1426, grad_fn=<SumBackward0>)
time8 0.0026297569274902344
train_time 0.12477993965148926
eval time 0.10812854766845703
epoch last part time 4.5299530029296875e-06
2024-01-23 01:02:59,653 MainThread INFO: EPOCH:490
2024-01-23 01:02:59,653 MainThread INFO: Time Consumed:0.23602914810180664s
2024-01-23 01:02:59,653 MainThread INFO: Total Frames:74400s
  5%|▍         | 491/10000 [04:27<42:17,  3.75it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12332.71970
Train_Epoch_Reward                5009.65230
Running_Training_Average_Rewards  12574.99843
Explore_Time                      0.00096
Train___Time                      0.12478
Eval____Time                      0.10813
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13210.19527
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.81200     2.56247     93.81987     86.14552
alpha_0                           0.78230      0.00011     0.78246      0.78215
Alpha_loss                        -1.65331     0.00126     -1.65187     -1.65550
Training/policy_loss              -2.62207     0.05423     -2.52912     -2.68266
Training/qf1_loss                 7342.15830   1150.97789  9484.31152   6088.43311
Training/qf2_loss                 15790.31445  1346.89715  18419.08203  14628.74414
Training/pf_norm                  0.11148      0.01400     0.13419      0.09377
Training/qf1_norm                 2011.59457   1053.85536  3680.01050   720.65070
Training/qf2_norm                 492.02156    46.30270    545.01318    412.99509
log_std/mean                      -0.13461     0.00961     -0.12472     -0.15171
log_probs/mean                    -2.73682     0.00327     -2.73098     -2.74024
mean/mean                         -0.00082     0.00056     -0.00005     -0.00177
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018364429473876953
epoch last part time3 0.0025708675384521484
inside rlalgo, task 0, sumup 71265
epoch first part time 2.384185791015625e-06
replay_buffer._size: [74700]
collect time 0.0009219646453857422
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.006975650787353516, 'sac_diff2': 0.008330821990966797, 'sac_diff3': 0.010947704315185547, 'sac_diff4': 0.007029294967651367, 'sac_diff5': 0.03322339057922363, 'sac_diff6': 0.00039958953857421875, 'all': 0.0671241283416748}
diff5_list [0.00693511962890625, 0.007086277008056641, 0.006635427474975586, 0.006204366683959961, 0.006362199783325195]
time3 0
time4 0.06787395477294922
time5 0.0679175853729248
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6132)
policy weight change tensor(38.1606, grad_fn=<SumBackward0>)
time8 0.0018687248229980469
train_time 0.07882332801818848
eval time 0.14883017539978027
epoch last part time 4.0531158447265625e-06
2024-01-23 01:02:59,907 MainThread INFO: EPOCH:491
2024-01-23 01:02:59,908 MainThread INFO: Time Consumed:0.23078656196594238s
2024-01-23 01:02:59,908 MainThread INFO: Total Frames:74550s
  5%|▍         | 492/10000 [04:27<41:35,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12447.72861
Train_Epoch_Reward                4622.64358
Running_Training_Average_Rewards  12528.22594
Explore_Time                      0.00092
Train___Time                      0.07882
Eval____Time                      0.14883
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12786.76150
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.80501     3.42895     96.35225     86.76631
alpha_0                           0.78191      0.00011     0.78207      0.78176
Alpha_loss                        -1.65728     0.00197     -1.65400     -1.65947
Training/policy_loss              -2.66392     0.00535     -2.65389     -2.66985
Training/qf1_loss                 7773.33037   2508.09983  12568.93164  5457.93457
Training/qf2_loss                 16324.75684  3200.96041  22464.52148  13576.25488
Training/pf_norm                  0.12452      0.03256     0.16132      0.07121
Training/qf1_norm                 1427.17336   680.05241   2045.09998   141.65840
Training/qf2_norm                 537.86383    19.90950    575.69434    519.66302
log_std/mean                      -0.13959     0.00010     -0.13942     -0.13970
log_probs/mean                    -2.73929     0.00705     -2.72593     -2.74671
mean/mean                         -0.00125     0.00006     -0.00118     -0.00135
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01853656768798828
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71265
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [74850]
collect time 0.0008175373077392578
inner_dict_sum {'sac_diff0': 0.00020575523376464844, 'sac_diff1': 0.007179737091064453, 'sac_diff2': 0.008064746856689453, 'sac_diff3': 0.010999441146850586, 'sac_diff4': 0.006989955902099609, 'sac_diff5': 0.03301095962524414, 'sac_diff6': 0.00040268898010253906, 'all': 0.06685328483581543}
diff5_list [0.006372928619384766, 0.007075071334838867, 0.006894350051879883, 0.006468772888183594, 0.006199836730957031]
time3 0
time4 0.06762552261352539
time5 0.06767511367797852
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6132)
policy weight change tensor(38.1655, grad_fn=<SumBackward0>)
time8 0.001955747604370117
train_time 0.07866597175598145
eval time 0.1569206714630127
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:00,168 MainThread INFO: EPOCH:492
2024-01-23 01:03:00,169 MainThread INFO: Time Consumed:0.23885393142700195s
2024-01-23 01:03:00,169 MainThread INFO: Total Frames:74700s
  5%|▍         | 493/10000 [04:27<41:31,  3.82it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12561.07986
Train_Epoch_Reward                29012.42543
Running_Training_Average_Rewards  12673.58864
Explore_Time                      0.00081
Train___Time                      0.07867
Eval____Time                      0.15692
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12834.88254
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.92271     1.82771    91.78178     87.18639
alpha_0                           0.78152      0.00011    0.78168      0.78136
Alpha_loss                        -1.65980     0.00094    -1.65842     -1.66103
Training/policy_loss              -2.60438     0.00290    -2.60066     -2.60824
Training/qf1_loss                 6542.70107   416.70955  6988.67578   5905.56055
Training/qf2_loss                 14922.83125  614.60569  15821.15918  13957.21094
Training/pf_norm                  0.08321      0.01880    0.11155      0.05510
Training/qf1_norm                 890.23651    342.03432  1455.15295   561.16034
Training/qf2_norm                 467.20704    9.37349    481.96185    458.24304
log_std/mean                      -0.12587     0.00003    -0.12583     -0.12591
log_probs/mean                    -2.73581     0.00340    -2.73142     -2.74013
mean/mean                         -0.00094     0.00005    -0.00087     -0.00099
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018683671951293945
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71265
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [75000]
collect time 0.0012364387512207031
inner_dict_sum {'sac_diff0': 0.00020694732666015625, 'sac_diff1': 0.0072841644287109375, 'sac_diff2': 0.009067058563232422, 'sac_diff3': 0.011816024780273438, 'sac_diff4': 0.00759577751159668, 'sac_diff5': 0.035294532775878906, 'sac_diff6': 0.00041747093200683594, 'all': 0.07168197631835938}
diff5_list [0.007456064224243164, 0.006679058074951172, 0.007154703140258789, 0.006772756576538086, 0.007231950759887695]
time3 0
time4 0.07247567176818848
time5 0.07253456115722656
time7 7.152557373046875e-07
gen_weight_change tensor(-22.6132)
policy weight change tensor(38.0842, grad_fn=<SumBackward0>)
time8 0.0018191337585449219
train_time 0.0840764045715332
eval time 0.14931511878967285
epoch last part time 7.867813110351562e-06
2024-01-23 01:03:00,428 MainThread INFO: EPOCH:493
2024-01-23 01:03:00,428 MainThread INFO: Time Consumed:0.23704075813293457s
2024-01-23 01:03:00,428 MainThread INFO: Total Frames:74850s
  5%|▍         | 494/10000 [04:28<41:23,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12675.06749
Train_Epoch_Reward                10084.39106
Running_Training_Average_Rewards  12675.04209
Explore_Time                      0.00123
Train___Time                      0.08408
Eval____Time                      0.14932
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12882.09247
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.46539     2.12975     94.06789     88.36108
alpha_0                           0.78113      0.00011     0.78129      0.78097
Alpha_loss                        -1.66234     0.00127     -1.66077     -1.66406
Training/policy_loss              -2.62241     0.00278     -2.61794     -2.62666
Training/qf1_loss                 8048.21582   1362.28637  9436.64062   5602.82520
Training/qf2_loss                 16933.06328  1728.46013  18436.94727  13858.30664
Training/pf_norm                  0.14126      0.02763     0.17723      0.09859
Training/qf1_norm                 598.85121    421.69234   1231.25073   127.00280
Training/qf2_norm                 515.30331    11.72584    530.19336    498.37350
log_std/mean                      -0.13381     0.00022     -0.13350     -0.13408
log_probs/mean                    -2.73247     0.00364     -2.72643     -2.73764
mean/mean                         -0.00447     0.00009     -0.00434     -0.00459
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018485069274902344
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71265
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [75150]
collect time 0.0009691715240478516
inner_dict_sum {'sac_diff0': 0.00021529197692871094, 'sac_diff1': 0.007423877716064453, 'sac_diff2': 0.008575677871704102, 'sac_diff3': 0.011249780654907227, 'sac_diff4': 0.007424354553222656, 'sac_diff5': 0.03374457359313965, 'sac_diff6': 0.0004055500030517578, 'all': 0.06903910636901855}
diff5_list [0.0066564083099365234, 0.0064623355865478516, 0.006136894226074219, 0.0072405338287353516, 0.007248401641845703]
time3 0
time4 0.06987380981445312
time5 0.06993293762207031
time7 4.76837158203125e-07
gen_weight_change tensor(-22.6132)
policy weight change tensor(37.9037, grad_fn=<SumBackward0>)
time8 0.0019354820251464844
train_time 0.08121800422668457
eval time 0.1602156162261963
epoch last part time 8.58306884765625e-06
2024-01-23 01:03:00,695 MainThread INFO: EPOCH:494
2024-01-23 01:03:00,695 MainThread INFO: Time Consumed:0.24482417106628418s
2024-01-23 01:03:00,695 MainThread INFO: Total Frames:75000s
  5%|▍         | 495/10000 [04:28<41:39,  3.80it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12780.81635
Train_Epoch_Reward                9806.87053
Running_Training_Average_Rewards  12600.74074
Explore_Time                      0.00096
Train___Time                      0.08122
Eval____Time                      0.16022
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12881.14466
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.76786     2.32692     92.58743     86.99968
alpha_0                           0.78074      0.00011     0.78090      0.78058
Alpha_loss                        -1.66606     0.00144     -1.66457     -1.66796
Training/policy_loss              -2.61568     0.00193     -2.61324     -2.61855
Training/qf1_loss                 6966.05498   798.89139   7979.23584   5939.07617
Training/qf2_loss                 15535.04941  1148.98408  17053.64453  14309.92188
Training/pf_norm                  0.09665      0.01456     0.11758      0.07335
Training/qf1_norm                 791.42145    439.92897   1339.40137   296.78683
Training/qf2_norm                 497.15801    12.15217    511.74625    482.69272
log_std/mean                      -0.14476     0.00037     -0.14420     -0.14521
log_probs/mean                    -2.73390     0.00246     -2.73102     -2.73732
mean/mean                         -0.00286     0.00002     -0.00284     -0.00288
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018540382385253906
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71265
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [75300]
collect time 0.0008587837219238281
inside mustsac before update, task 0, sumup 71265
inside mustsac after update, task 0, sumup 70330
inner_dict_sum {'sac_diff0': 0.0002346038818359375, 'sac_diff1': 0.007268190383911133, 'sac_diff2': 0.008895397186279297, 'sac_diff3': 0.011522531509399414, 'sac_diff4': 0.007948160171508789, 'sac_diff5': 0.051223039627075195, 'sac_diff6': 0.0004291534423828125, 'all': 0.08752107620239258}
diff5_list [0.010587692260742188, 0.010543584823608398, 0.009748697280883789, 0.010736227035522461, 0.00960683822631836]
time3 0.0009276866912841797
time4 0.08843159675598145
time5 0.08848404884338379
time7 0.009277105331420898
gen_weight_change tensor(-22.4791)
policy weight change tensor(37.9146, grad_fn=<SumBackward0>)
time8 0.0018792152404785156
train_time 0.11830925941467285
eval time 0.11086010932922363
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:00,949 MainThread INFO: EPOCH:495
2024-01-23 01:03:00,950 MainThread INFO: Time Consumed:0.23243474960327148s
2024-01-23 01:03:00,950 MainThread INFO: Total Frames:75150s
  5%|▍         | 496/10000 [04:28<41:14,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12875.44507
Train_Epoch_Reward                18670.28395
Running_Training_Average_Rewards  12701.30221
Explore_Time                      0.00085
Train___Time                      0.11831
Eval____Time                      0.11086
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12863.30369
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.93188     1.66762    91.11269     86.53972
alpha_0                           0.78035      0.00011    0.78050      0.78019
Alpha_loss                        -1.67014     0.00295    -1.66523     -1.67384
Training/policy_loss              -2.60607     0.02175    -2.58249     -2.64300
Training/qf1_loss                 6619.50801   551.00813  7613.35645   5939.55664
Training/qf2_loss                 14818.29082  836.13595  16429.16602  13995.01270
Training/pf_norm                  0.10992      0.02730    0.14981      0.06565
Training/qf1_norm                 1120.34476   440.18133  1539.82092   411.39462
Training/qf2_norm                 476.17466    18.24255   498.86182    448.40918
log_std/mean                      -0.12897     0.00740    -0.12028     -0.14090
log_probs/mean                    -2.73675     0.00942    -2.72238     -2.75168
mean/mean                         -0.00156     0.00068    -0.00046     -0.00226
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018138885498046875
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70330
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [75450]
collect time 0.0008759498596191406
inner_dict_sum {'sac_diff0': 0.0002155303955078125, 'sac_diff1': 0.006932735443115234, 'sac_diff2': 0.008199214935302734, 'sac_diff3': 0.010484457015991211, 'sac_diff4': 0.007315158843994141, 'sac_diff5': 0.03287458419799805, 'sac_diff6': 0.0003952980041503906, 'all': 0.06641697883605957}
diff5_list [0.006590366363525391, 0.0062944889068603516, 0.006217479705810547, 0.0065729618072509766, 0.007199287414550781]
time3 0
time4 0.06723546981811523
time5 0.06728339195251465
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4791)
policy weight change tensor(37.7472, grad_fn=<SumBackward0>)
time8 0.0019788742065429688
train_time 0.07867240905761719
eval time 0.14827299118041992
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:01,202 MainThread INFO: EPOCH:496
2024-01-23 01:03:01,202 MainThread INFO: Time Consumed:0.23040986061096191s
2024-01-23 01:03:01,202 MainThread INFO: Total Frames:75300s
  5%|▍         | 497/10000 [04:28<40:50,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12899.81697
Train_Epoch_Reward                7033.29725
Running_Training_Average_Rewards  12248.83405
Explore_Time                      0.00087
Train___Time                      0.07867
Eval____Time                      0.14827
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12842.14624
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.06096     1.68296    91.19847     86.95435
alpha_0                           0.77996      0.00011    0.78011      0.77980
Alpha_loss                        -1.67323     0.00184    -1.67007     -1.67542
Training/policy_loss              -2.59677     0.00375    -2.58946     -2.59971
Training/qf1_loss                 7106.76328   385.35441  7517.31494   6429.73291
Training/qf2_loss                 15537.61543  590.39534  16174.90332  14437.46582
Training/pf_norm                  0.08853      0.00961    0.10672      0.08013
Training/qf1_norm                 1037.86427   332.07765  1475.02930   632.11957
Training/qf2_norm                 463.32614    8.78166    474.45795    452.14325
log_std/mean                      -0.13530     0.00024    -0.13498     -0.13566
log_probs/mean                    -2.73565     0.00511    -2.72561     -2.73910
mean/mean                         -0.00180     0.00002    -0.00176     -0.00182
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018184185028076172
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70330
epoch first part time 2.86102294921875e-06
replay_buffer._size: [75600]
collect time 0.0008597373962402344
inner_dict_sum {'sac_diff0': 0.00021123886108398438, 'sac_diff1': 0.007135152816772461, 'sac_diff2': 0.008326530456542969, 'sac_diff3': 0.010854244232177734, 'sac_diff4': 0.0071926116943359375, 'sac_diff5': 0.03351879119873047, 'sac_diff6': 0.0004138946533203125, 'all': 0.06765246391296387}
diff5_list [0.007456779479980469, 0.00640106201171875, 0.0063076019287109375, 0.0065953731536865234, 0.006757974624633789]
time3 0
time4 0.06845211982727051
time5 0.06850314140319824
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4791)
policy weight change tensor(37.7022, grad_fn=<SumBackward0>)
time8 0.001978635787963867
train_time 0.0799248218536377
eval time 0.1546015739440918
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:01,461 MainThread INFO: EPOCH:497
2024-01-23 01:03:01,461 MainThread INFO: Time Consumed:0.2378520965576172s
2024-01-23 01:03:01,462 MainThread INFO: Total Frames:75450s
  5%|▍         | 498/10000 [04:29<40:55,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12912.45173
Train_Epoch_Reward                13558.07973
Running_Training_Average_Rewards  12317.96827
Explore_Time                      0.00085
Train___Time                      0.07992
Eval____Time                      0.15460
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12854.01492
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.93562     2.84095     92.88225     86.05387
alpha_0                           0.77957      0.00011     0.77972      0.77941
Alpha_loss                        -1.67552     0.00219     -1.67243     -1.67844
Training/policy_loss              -2.57698     0.00358     -2.57216     -2.58146
Training/qf1_loss                 6819.79824   908.27188   8292.39648   5566.70605
Training/qf2_loss                 15325.75449  1349.98444  17295.86328  13411.31543
Training/pf_norm                  0.09960      0.02756     0.13690      0.06240
Training/qf1_norm                 1778.63618   512.34559   2293.24512   1098.76648
Training/qf2_norm                 465.25818    14.25850    480.50430    445.80179
log_std/mean                      -0.13347     0.00005     -0.13341     -0.13353
log_probs/mean                    -2.73131     0.00503     -2.72431     -2.73762
mean/mean                         -0.00012     0.00007     -0.00006     -0.00024
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018513917922973633
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70330
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [75750]
collect time 0.000858306884765625
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.006996631622314453, 'sac_diff2': 0.00834035873413086, 'sac_diff3': 0.011019706726074219, 'sac_diff4': 0.007553815841674805, 'sac_diff5': 0.034432172775268555, 'sac_diff6': 0.0004100799560546875, 'all': 0.06896519660949707}
diff5_list [0.006744384765625, 0.0062961578369140625, 0.0064830780029296875, 0.00619959831237793, 0.008708953857421875]
time3 0
time4 0.06981253623962402
time5 0.06986117362976074
time7 7.152557373046875e-07
gen_weight_change tensor(-22.4791)
policy weight change tensor(37.6836, grad_fn=<SumBackward0>)
time8 0.0020799636840820312
train_time 0.08151769638061523
eval time 0.14996576309204102
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:01,718 MainThread INFO: EPOCH:498
2024-01-23 01:03:01,718 MainThread INFO: Time Consumed:0.234755277633667s
2024-01-23 01:03:01,719 MainThread INFO: Total Frames:75600s
  5%|▍         | 499/10000 [04:29<40:50,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12905.66823
Train_Epoch_Reward                7522.09548
Running_Training_Average_Rewards  12217.86741
Explore_Time                      0.00085
Train___Time                      0.08152
Eval____Time                      0.14997
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12835.16379
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.99859     2.61393     93.78622     86.69938
alpha_0                           0.77918      0.00011     0.77933      0.77902
Alpha_loss                        -1.67901     0.00177     -1.67703     -1.68170
Training/policy_loss              -2.66341     0.00520     -2.65729     -2.67117
Training/qf1_loss                 7960.40000   861.53048   9413.68164   6897.86572
Training/qf2_loss                 16545.90352  1308.71927  18783.34766  14818.15625
Training/pf_norm                  0.12296      0.00509     0.12901      0.11410
Training/qf1_norm                 1292.71830   537.80562   1939.21216   472.21863
Training/qf2_norm                 538.92107    15.45054    561.50531    519.43146
log_std/mean                      -0.13041     0.00002     -0.13039     -0.13044
log_probs/mean                    -2.73179     0.00646     -2.72388     -2.74124
mean/mean                         -0.00061     0.00009     -0.00046     -0.00072
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018269062042236328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70330
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [75900]
collect time 0.0008330345153808594
inner_dict_sum {'sac_diff0': 0.0002243518829345703, 'sac_diff1': 0.007077693939208984, 'sac_diff2': 0.008476734161376953, 'sac_diff3': 0.01082611083984375, 'sac_diff4': 0.0070343017578125, 'sac_diff5': 0.032514095306396484, 'sac_diff6': 0.00038814544677734375, 'all': 0.06654143333435059}
diff5_list [0.007289409637451172, 0.006333827972412109, 0.00651097297668457, 0.006169795989990234, 0.0062100887298583984]
time3 0
time4 0.06732892990112305
time5 0.06737971305847168
time7 4.76837158203125e-07
gen_weight_change tensor(-22.4791)
policy weight change tensor(37.6010, grad_fn=<SumBackward0>)
time8 0.001958608627319336
train_time 0.0788118839263916
eval time 0.1550588607788086
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:01,977 MainThread INFO: EPOCH:499
2024-01-23 01:03:01,977 MainThread INFO: Time Consumed:0.2371079921722412s
2024-01-23 01:03:01,977 MainThread INFO: Total Frames:75750s
  5%|▌         | 500/10000 [04:29<40:52,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12875.94323
Train_Epoch_Reward                4206.21230
Running_Training_Average_Rewards  11911.24121
Explore_Time                      0.00083
Train___Time                      0.07881
Eval____Time                      0.15506
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12769.72723
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.96550     3.18709     95.02647     85.90012
alpha_0                           0.77879      0.00011     0.77895      0.77863
Alpha_loss                        -1.68277     0.00058     -1.68180     -1.68349
Training/policy_loss              -2.62330     0.00289     -2.61945     -2.62840
Training/qf1_loss                 6363.35186   779.17377   7360.40479   5480.87891
Training/qf2_loss                 14599.33262  1303.36712  16864.04688  13065.17383
Training/pf_norm                  0.09962      0.03114     0.15282      0.05990
Training/qf1_norm                 2854.95559   675.27689   3427.11011   1542.86194
Training/qf2_norm                 491.49894    17.05586    524.00018    475.09583
log_std/mean                      -0.13401     0.00017     -0.13372     -0.13420
log_probs/mean                    -2.73336     0.00311     -2.72955     -2.73900
mean/mean                         -0.00123     0.00007     -0.00114     -0.00134
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01803731918334961
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70330
epoch first part time 3.814697265625e-06
replay_buffer._size: [76050]
collect time 0.0008447170257568359
inside mustsac before update, task 0, sumup 70330
inside mustsac after update, task 0, sumup 71315
inner_dict_sum {'sac_diff0': 0.0002410411834716797, 'sac_diff1': 0.007780790328979492, 'sac_diff2': 0.009519100189208984, 'sac_diff3': 0.012377262115478516, 'sac_diff4': 0.00865936279296875, 'sac_diff5': 0.059351205825805664, 'sac_diff6': 0.0004761219024658203, 'all': 0.0984048843383789}
diff5_list [0.011296272277832031, 0.010668277740478516, 0.012237787246704102, 0.01245737075805664, 0.012691497802734375]
time3 0.0009641647338867188
time4 0.09941220283508301
time5 0.09947395324707031
time7 0.009337902069091797
gen_weight_change tensor(-22.3979)
policy weight change tensor(37.5995, grad_fn=<SumBackward0>)
time8 0.0027968883514404297
train_time 0.13187265396118164
eval time 0.10537600517272949
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:02,239 MainThread INFO: EPOCH:500
2024-01-23 01:03:02,239 MainThread INFO: Time Consumed:0.24035859107971191s
2024-01-23 01:03:02,239 MainThread INFO: Total Frames:75900s
  5%|▌         | 501/10000 [04:29<41:15,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12825.61291
Train_Epoch_Reward                16841.88368
Running_Training_Average_Rewards  11737.28451
Explore_Time                      0.00084
Train___Time                      0.13187
Eval____Time                      0.10538
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12706.89203
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.94865     1.50946     90.38409     86.24980
alpha_0                           0.77840      0.00011     0.77856      0.77824
Alpha_loss                        -1.68602     0.00223     -1.68303     -1.68988
Training/policy_loss              -2.60771     0.03815     -2.56489     -2.67784
Training/qf1_loss                 6830.25596   908.32909   8054.12207   5289.88672
Training/qf2_loss                 15234.60098  1112.14365  16538.12891  13175.77344
Training/pf_norm                  0.11727      0.00694     0.12600      0.10561
Training/qf1_norm                 848.73324    440.74958   1499.66174   123.82954
Training/qf2_norm                 476.29389    33.36039    539.70044    443.87131
log_std/mean                      -0.13512     0.00370     -0.12864     -0.13989
log_probs/mean                    -2.73291     0.00858     -2.71827     -2.74294
mean/mean                         -0.00225     0.00110     -0.00030     -0.00347
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01918816566467285
epoch last part time3 0.003087282180786133
inside rlalgo, task 0, sumup 71315
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [76200]
collect time 0.0008840560913085938
inner_dict_sum {'sac_diff0': 0.0002028942108154297, 'sac_diff1': 0.006766796112060547, 'sac_diff2': 0.008351564407348633, 'sac_diff3': 0.010948657989501953, 'sac_diff4': 0.007100582122802734, 'sac_diff5': 0.03292536735534668, 'sac_diff6': 0.00044918060302734375, 'all': 0.06674504280090332}
diff5_list [0.006555318832397461, 0.007015228271484375, 0.0065190792083740234, 0.006577730178833008, 0.0062580108642578125]
time3 0
time4 0.06752324104309082
time5 0.06757164001464844
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3979)
policy weight change tensor(37.5486, grad_fn=<SumBackward0>)
time8 0.0017790794372558594
train_time 0.07876276969909668
eval time 0.14270710945129395
epoch last part time 7.867813110351562e-06
2024-01-23 01:03:02,489 MainThread INFO: EPOCH:501
2024-01-23 01:03:02,489 MainThread INFO: Time Consumed:0.22481298446655273s
2024-01-23 01:03:02,490 MainThread INFO: Total Frames:76050s
  5%|▌         | 502/10000 [04:30<40:34,  3.90it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12784.81171
Train_Epoch_Reward                11167.15197
Running_Training_Average_Rewards  11663.04604
Explore_Time                      0.00088
Train___Time                      0.07876
Eval____Time                      0.14271
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12378.74950
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.86481     1.28564    90.18526     86.46434
alpha_0                           0.77801      0.00011    0.77817      0.77786
Alpha_loss                        -1.69011     0.00126    -1.68879     -1.69225
Training/policy_loss              -2.72948     0.00214    -2.72562     -2.73163
Training/qf1_loss                 6564.47002   343.67742  6947.19385   6005.14844
Training/qf2_loss                 14962.27012  566.65860  15606.34863  13920.54980
Training/pf_norm                  0.09373      0.01691    0.11455      0.07233
Training/qf1_norm                 524.99795    299.49985  1095.14307   297.64246
Training/qf2_norm                 575.53103    8.38870    584.46539    560.02313
log_std/mean                      -0.13100     0.00003    -0.13095     -0.13102
log_probs/mean                    -2.73578     0.00284    -2.73051     -2.73894
mean/mean                         -0.00015     0.00005    -0.00010     -0.00023
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01850414276123047
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71315
epoch first part time 2.86102294921875e-06
replay_buffer._size: [76350]
collect time 0.0008528232574462891
inner_dict_sum {'sac_diff0': 0.00021791458129882812, 'sac_diff1': 0.007352590560913086, 'sac_diff2': 0.009125471115112305, 'sac_diff3': 0.011861801147460938, 'sac_diff4': 0.007569789886474609, 'sac_diff5': 0.034401655197143555, 'sac_diff6': 0.0004329681396484375, 'all': 0.07096219062805176}
diff5_list [0.006615161895751953, 0.006277799606323242, 0.007132530212402344, 0.007452964782714844, 0.006923198699951172]
time3 0
time4 0.07181453704833984
time5 0.071868896484375
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3979)
policy weight change tensor(37.5549, grad_fn=<SumBackward0>)
time8 0.0018310546875
train_time 0.08311986923217773
eval time 0.14827704429626465
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:02,746 MainThread INFO: EPOCH:502
2024-01-23 01:03:02,746 MainThread INFO: Time Consumed:0.23467230796813965s
2024-01-23 01:03:02,747 MainThread INFO: Total Frames:76200s
  5%|▌         | 503/10000 [04:30<40:38,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12737.09609
Train_Epoch_Reward                12340.13065
Running_Training_Average_Rewards  11612.19937
Explore_Time                      0.00085
Train___Time                      0.08312
Eval____Time                      0.14828
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12357.72635
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.63117     2.55110     94.01299     86.94913
alpha_0                           0.77762      0.00011     0.77778      0.77747
Alpha_loss                        -1.69350     0.00239     -1.68966     -1.69630
Training/policy_loss              -2.58967     0.00433     -2.58249     -2.59444
Training/qf1_loss                 7419.69268   1364.63858  10019.62891  6055.08496
Training/qf2_loss                 15823.76719  1766.29163  19140.14062  14025.44824
Training/pf_norm                  0.10068      0.03356     0.16143      0.07007
Training/qf1_norm                 2310.45154   469.92834   3113.16357   1804.44897
Training/qf2_norm                 473.08120    13.08867    495.34622    459.27240
log_std/mean                      -0.12612     0.00007     -0.12606     -0.12624
log_probs/mean                    -2.73589     0.00594     -2.72595     -2.74221
mean/mean                         -0.00191     0.00008     -0.00178     -0.00198
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01929640769958496
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71315
epoch first part time 3.337860107421875e-06
replay_buffer._size: [76500]
collect time 0.0008547306060791016
inner_dict_sum {'sac_diff0': 0.0002307891845703125, 'sac_diff1': 0.006510257720947266, 'sac_diff2': 0.0078012943267822266, 'sac_diff3': 0.009932756423950195, 'sac_diff4': 0.006623506546020508, 'sac_diff5': 0.031417131423950195, 'sac_diff6': 0.0003783702850341797, 'all': 0.06289410591125488}
diff5_list [0.006676912307739258, 0.0061681270599365234, 0.006201028823852539, 0.0061931610107421875, 0.0061779022216796875]
time3 0
time4 0.06362724304199219
time5 0.06367230415344238
time7 7.152557373046875e-07
gen_weight_change tensor(-22.3979)
policy weight change tensor(37.5929, grad_fn=<SumBackward0>)
time8 0.0018949508666992188
train_time 0.07488393783569336
eval time 0.15848398208618164
epoch last part time 5.9604644775390625e-06
2024-01-23 01:03:03,005 MainThread INFO: EPOCH:503
2024-01-23 01:03:03,006 MainThread INFO: Time Consumed:0.23653674125671387s
2024-01-23 01:03:03,006 MainThread INFO: Total Frames:76350s
  5%|▌         | 504/10000 [04:30<40:43,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12677.19718
Train_Epoch_Reward                38270.37884
Running_Training_Average_Rewards  12116.66587
Explore_Time                      0.00085
Train___Time                      0.07488
Eval____Time                      0.15848
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12283.10338
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.36652     2.22668     94.35487     87.54932
alpha_0                           0.77723      0.00011     0.77739      0.77708
Alpha_loss                        -1.69745     0.00236     -1.69304     -1.70010
Training/policy_loss              -2.63214     0.00494     -2.62334     -2.63762
Training/qf1_loss                 7566.05928   1300.98809  9700.78125   5813.67578
Training/qf2_loss                 16212.08477  1693.13104  19094.80469  13924.66406
Training/pf_norm                  0.08668      0.02378     0.12608      0.06538
Training/qf1_norm                 363.88981    322.75438   943.75983    100.40153
Training/qf2_norm                 513.35715    12.30027    535.65137    498.16058
log_std/mean                      -0.12623     0.00004     -0.12616     -0.12628
log_probs/mean                    -2.73819     0.00670     -2.72602     -2.74513
mean/mean                         -0.00140     0.00003     -0.00137     -0.00145
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01844191551208496
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71315
epoch first part time 2.86102294921875e-06
replay_buffer._size: [76650]
collect time 0.0007872581481933594
inner_dict_sum {'sac_diff0': 0.00021839141845703125, 'sac_diff1': 0.0068247318267822266, 'sac_diff2': 0.007804393768310547, 'sac_diff3': 0.009989261627197266, 'sac_diff4': 0.006625175476074219, 'sac_diff5': 0.03133058547973633, 'sac_diff6': 0.00038123130798339844, 'all': 0.06317377090454102}
diff5_list [0.0065326690673828125, 0.006127595901489258, 0.006204366683959961, 0.006318330764770508, 0.006147623062133789]
time3 0
time4 0.06391143798828125
time5 0.06395506858825684
time7 4.76837158203125e-07
gen_weight_change tensor(-22.3979)
policy weight change tensor(37.7116, grad_fn=<SumBackward0>)
time8 0.0018129348754882812
train_time 0.07489228248596191
eval time 0.1560688018798828
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:03,261 MainThread INFO: EPOCH:504
2024-01-23 01:03:03,262 MainThread INFO: Time Consumed:0.23412203788757324s
2024-01-23 01:03:03,262 MainThread INFO: Total Frames:76500s
  5%|▌         | 505/10000 [04:30<40:47,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12612.57634
Train_Epoch_Reward                11398.00108
Running_Training_Average_Rewards  12239.96881
Explore_Time                      0.00078
Train___Time                      0.07489
Eval____Time                      0.15607
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12234.93624
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.30810     3.38944     94.40648     85.48830
alpha_0                           0.77684      0.00011     0.77700      0.77669
Alpha_loss                        -1.69963     0.00185     -1.69700     -1.70258
Training/policy_loss              -2.64273     0.00588     -2.63227     -2.64743
Training/qf1_loss                 6929.49678   755.94210   8337.97852   6097.48535
Training/qf2_loss                 15581.60508  1296.13577  17689.14062  13769.73730
Training/pf_norm                  0.10398      0.01556     0.12949      0.08398
Training/qf1_norm                 1211.32997   698.05855   2198.25879   419.95816
Training/qf2_norm                 511.81324    18.58257    534.04980    485.26297
log_std/mean                      -0.12146     0.00022     -0.12122     -0.12182
log_probs/mean                    -2.73347     0.00748     -2.72040     -2.73983
mean/mean                         -0.00083     0.00001     -0.00082     -0.00085
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021584272384643555
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71315
epoch first part time 7.152557373046875e-06
replay_buffer._size: [76800]
collect time 0.0011839866638183594
inside mustsac before update, task 0, sumup 71315
inside mustsac after update, task 0, sumup 71813
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.008072614669799805, 'sac_diff2': 0.010249853134155273, 'sac_diff3': 0.012450218200683594, 'sac_diff4': 0.00939631462097168, 'sac_diff5': 0.05266380310058594, 'sac_diff6': 0.0004229545593261719, 'all': 0.09346985816955566}
diff5_list [0.012530326843261719, 0.009608268737792969, 0.010766267776489258, 0.009902000427246094, 0.009856939315795898]
time3 0.0009136199951171875
time4 0.09439969062805176
time5 0.09445858001708984
time7 0.009049177169799805
gen_weight_change tensor(-22.2606)
policy weight change tensor(37.6470, grad_fn=<SumBackward0>)
time8 0.0019254684448242188
train_time 0.124847412109375
eval time 0.10361337661743164
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:03,519 MainThread INFO: EPOCH:505
2024-01-23 01:03:03,520 MainThread INFO: Time Consumed:0.2320387363433838s
2024-01-23 01:03:03,520 MainThread INFO: Total Frames:76650s
  5%|▌         | 506/10000 [04:31<40:42,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12547.94227
Train_Epoch_Reward                15034.51330
Running_Training_Average_Rewards  12211.52467
Explore_Time                      0.00118
Train___Time                      0.12485
Eval____Time                      0.10361
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12216.96304
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.82008     2.21244     91.95738     85.89333
alpha_0                           0.77646      0.00011     0.77661      0.77630
Alpha_loss                        -1.70281     0.00114     -1.70180     -1.70498
Training/policy_loss              -2.63047     0.06325     -2.55229     -2.72064
Training/qf1_loss                 6489.34326   1459.83491  9338.12109   5501.50098
Training/qf2_loss                 14637.17266  1869.22908  18304.10547  13378.15039
Training/pf_norm                  0.13445      0.02991     0.17439      0.09718
Training/qf1_norm                 1563.15022   522.66811   2394.60498   776.09192
Training/qf2_norm                 486.53622    38.64663    538.37823    448.43073
log_std/mean                      -0.13575     0.00530     -0.12977     -0.14474
log_probs/mean                    -2.73271     0.00533     -2.72693     -2.74130
mean/mean                         -0.00243     0.00059     -0.00196     -0.00355
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019405126571655273
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71813
epoch first part time 2.86102294921875e-06
replay_buffer._size: [76950]
collect time 0.0009121894836425781
inner_dict_sum {'sac_diff0': 0.0002167224884033203, 'sac_diff1': 0.00734710693359375, 'sac_diff2': 0.008758544921875, 'sac_diff3': 0.011093854904174805, 'sac_diff4': 0.007503032684326172, 'sac_diff5': 0.03295016288757324, 'sac_diff6': 0.0003948211669921875, 'all': 0.06826424598693848}
diff5_list [0.0069119930267333984, 0.00630497932434082, 0.00661778450012207, 0.006993770599365234, 0.006121635437011719]
time3 0
time4 0.06904149055480957
time5 0.06909298896789551
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2606)
policy weight change tensor(37.8160, grad_fn=<SumBackward0>)
time8 0.0018312931060791016
train_time 0.08017969131469727
eval time 0.15419673919677734
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:03,780 MainThread INFO: EPOCH:506
2024-01-23 01:03:03,780 MainThread INFO: Time Consumed:0.23757123947143555s
2024-01-23 01:03:03,780 MainThread INFO: Total Frames:76800s
  5%|▌         | 507/10000 [04:31<40:48,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12483.83574
Train_Epoch_Reward                6275.62815
Running_Training_Average_Rewards  11889.79230
Explore_Time                      0.00091
Train___Time                      0.08018
Eval____Time                      0.15420
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12201.08095
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.91993     2.66095     95.87137     87.74971
alpha_0                           0.77607      0.00011     0.77622      0.77591
Alpha_loss                        -1.70537     0.00212     -1.70261     -1.70902
Training/policy_loss              -2.66481     0.00381     -2.66114     -2.67126
Training/qf1_loss                 8156.61211   1038.46880  9192.22949   6184.26172
Training/qf2_loss                 16927.10098  1448.50761  18617.58203  14267.50488
Training/pf_norm                  0.10772      0.02925     0.14607      0.06398
Training/qf1_norm                 2566.95002   497.86434   3313.77393   1802.47717
Training/qf2_norm                 555.94739    15.57921    578.98328    531.51959
log_std/mean                      -0.12654     0.00029     -0.12617     -0.12697
log_probs/mean                    -2.72955     0.00537     -2.72396     -2.73864
mean/mean                         -0.00139     0.00003     -0.00136     -0.00144
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018299579620361328
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71813
epoch first part time 3.337860107421875e-06
replay_buffer._size: [77100]
collect time 0.0008225440979003906
inner_dict_sum {'sac_diff0': 0.00023221969604492188, 'sac_diff1': 0.006659984588623047, 'sac_diff2': 0.008033514022827148, 'sac_diff3': 0.010049819946289062, 'sac_diff4': 0.006854057312011719, 'sac_diff5': 0.032472848892211914, 'sac_diff6': 0.0003895759582519531, 'all': 0.06469202041625977}
diff5_list [0.006346940994262695, 0.006159543991088867, 0.006163597106933594, 0.006523609161376953, 0.007279157638549805]
time3 0
time4 0.06549453735351562
time5 0.06554079055786133
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2606)
policy weight change tensor(37.9614, grad_fn=<SumBackward0>)
time8 0.0019237995147705078
train_time 0.07655644416809082
eval time 0.1620008945465088
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:04,043 MainThread INFO: EPOCH:507
2024-01-23 01:03:04,044 MainThread INFO: Time Consumed:0.24177169799804688s
2024-01-23 01:03:04,044 MainThread INFO: Total Frames:76950s
  5%|▌         | 508/10000 [04:31<41:06,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12413.69917
Train_Epoch_Reward                16719.83259
Running_Training_Average_Rewards  12275.04595
Explore_Time                      0.00082
Train___Time                      0.07656
Eval____Time                      0.16200
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12152.64919
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.34958     3.26843     96.62506     87.24347
alpha_0                           0.77568      0.00011     0.77583      0.77552
Alpha_loss                        -1.70982     0.00228     -1.70645     -1.71283
Training/policy_loss              -2.57812     0.00546     -2.57082     -2.58400
Training/qf1_loss                 7382.21611   1141.56117  9285.17285   6053.83057
Training/qf2_loss                 16051.00586  1701.07626  19099.40234  14154.92969
Training/pf_norm                  0.11120      0.02423     0.14201      0.07595
Training/qf1_norm                 758.93664    645.69814   2005.43066   141.08864
Training/qf2_norm                 487.18248    17.09475    520.24744    471.74307
log_std/mean                      -0.13447     0.00019     -0.13420     -0.13474
log_probs/mean                    -2.73380     0.00732     -2.72410     -2.74108
mean/mean                         -0.00173     0.00004     -0.00169     -0.00180
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01914072036743164
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71813
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [77250]
collect time 0.0009648799896240234
inner_dict_sum {'sac_diff0': 0.00023484230041503906, 'sac_diff1': 0.0068242549896240234, 'sac_diff2': 0.007815837860107422, 'sac_diff3': 0.010297536849975586, 'sac_diff4': 0.006928205490112305, 'sac_diff5': 0.03154349327087402, 'sac_diff6': 0.00037789344787597656, 'all': 0.06402206420898438}
diff5_list [0.006618976593017578, 0.006283283233642578, 0.006204843521118164, 0.006217241287231445, 0.006219148635864258]
time3 0
time4 0.0647590160369873
time5 0.06480216979980469
time7 7.152557373046875e-07
gen_weight_change tensor(-22.2606)
policy weight change tensor(38.0545, grad_fn=<SumBackward0>)
time8 0.0018215179443359375
train_time 0.07597708702087402
eval time 0.15564298629760742
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:04,301 MainThread INFO: EPOCH:508
2024-01-23 01:03:04,302 MainThread INFO: Time Consumed:0.23494458198547363s
2024-01-23 01:03:04,302 MainThread INFO: Total Frames:77100s
  5%|▌         | 509/10000 [04:31<40:59,  3.86it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12341.63866
Train_Epoch_Reward                3596.02657
Running_Training_Average_Rewards  11799.52138
Explore_Time                      0.00096
Train___Time                      0.07598
Eval____Time                      0.15564
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12114.55871
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.37265     1.03875    92.00823     89.11517
alpha_0                           0.77529      0.00011    0.77545      0.77514
Alpha_loss                        -1.71297     0.00098    -1.71190     -1.71437
Training/policy_loss              -2.63096     0.00270    -2.62677     -2.63412
Training/qf1_loss                 7188.59336   524.41615  7828.68115   6436.77441
Training/qf2_loss                 15783.55039  633.20911  16705.59766  14813.44043
Training/pf_norm                  0.10277      0.02913    0.12975      0.05312
Training/qf1_norm                 1692.95398   151.52680  1958.84229   1520.57996
Training/qf2_norm                 513.37941    5.54009    521.49384    506.57635
log_std/mean                      -0.13190     0.00011    -0.13174     -0.13207
log_probs/mean                    -2.73296     0.00344    -2.72728     -2.73663
mean/mean                         -0.00219     0.00015    -0.00200     -0.00241
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018720626831054688
epoch last part time3 1.430511474609375e-06
inside rlalgo, task 0, sumup 71813
epoch first part time 2.86102294921875e-06
replay_buffer._size: [77400]
collect time 0.0008270740509033203
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.007269859313964844, 'sac_diff2': 0.00876927375793457, 'sac_diff3': 0.011181116104125977, 'sac_diff4': 0.007366657257080078, 'sac_diff5': 0.03486275672912598, 'sac_diff6': 0.0004153251647949219, 'all': 0.07007741928100586}
diff5_list [0.0067636966705322266, 0.0072515010833740234, 0.007365226745605469, 0.006978034973144531, 0.0065042972564697266]
time3 0
time4 0.07089900970458984
time5 0.07094573974609375
time7 4.76837158203125e-07
gen_weight_change tensor(-22.2606)
policy weight change tensor(38.0372, grad_fn=<SumBackward0>)
time8 0.0019724369049072266
train_time 0.08208799362182617
eval time 0.14507246017456055
epoch last part time 8.58306884765625e-06
2024-01-23 01:03:04,554 MainThread INFO: EPOCH:509
2024-01-23 01:03:04,554 MainThread INFO: Time Consumed:0.23043489456176758s
2024-01-23 01:03:04,555 MainThread INFO: Total Frames:77250s
  5%|▌         | 510/10000 [04:32<40:40,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12265.45730
Train_Epoch_Reward                5014.95002
Running_Training_Average_Rewards  11758.97854
Explore_Time                      0.00082
Train___Time                      0.08209
Eval____Time                      0.14507
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12007.91364
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.48474     1.14751    89.07153     85.48920
alpha_0                           0.77490      0.00011    0.77506      0.77475
Alpha_loss                        -1.71632     0.00130    -1.71470     -1.71821
Training/policy_loss              -2.62436     0.00281    -2.61982     -2.62781
Training/qf1_loss                 5905.88691   508.85683  6737.93848   5306.00586
Training/qf2_loss                 14037.16230  709.76815  15176.39062  13049.05176
Training/pf_norm                  0.12217      0.02000    0.14356      0.09495
Training/qf1_norm                 802.01441    238.70277  1213.36731   471.82297
Training/qf2_norm                 488.41793    6.38921    497.13766    477.20178
log_std/mean                      -0.13559     0.00010    -0.13543     -0.13569
log_probs/mean                    -2.73288     0.00354    -2.72807     -2.73768
mean/mean                         -0.00212     0.00020    -0.00185     -0.00242
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018420934677124023
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71813
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [77550]
collect time 0.0008544921875
inside mustsac before update, task 0, sumup 71813
inside mustsac after update, task 0, sumup 70440
inner_dict_sum {'sac_diff0': 0.00022459030151367188, 'sac_diff1': 0.007720232009887695, 'sac_diff2': 0.009258508682250977, 'sac_diff3': 0.011554718017578125, 'sac_diff4': 0.008064508438110352, 'sac_diff5': 0.053691864013671875, 'sac_diff6': 0.0004627704620361328, 'all': 0.09097719192504883}
diff5_list [0.01105642318725586, 0.011706352233886719, 0.010034322738647461, 0.010829925537109375, 0.010064840316772461]
time3 0.0009288787841796875
time4 0.09189677238464355
time5 0.09195303916931152
time7 0.008808135986328125
gen_weight_change tensor(-22.1087)
policy weight change tensor(37.9550, grad_fn=<SumBackward0>)
time8 0.0025665760040283203
train_time 0.12237668037414551
eval time 0.10892415046691895
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:04,811 MainThread INFO: EPOCH:510
2024-01-23 01:03:04,811 MainThread INFO: Time Consumed:0.23452043533325195s
2024-01-23 01:03:04,811 MainThread INFO: Total Frames:77400s
  5%|▌         | 511/10000 [04:32<40:46,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12177.43264
Train_Epoch_Reward                9023.44864
Running_Training_Average_Rewards  11578.17576
Explore_Time                      0.00085
Train___Time                      0.12238
Eval____Time                      0.10892
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11826.64535
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.49435     1.14394    89.84731     86.51221
alpha_0                           0.77452      0.00011    0.77467      0.77436
Alpha_loss                        -1.72038     0.00210    -1.71689     -1.72270
Training/policy_loss              -2.62658     0.02461    -2.58331     -2.65857
Training/qf1_loss                 6675.26553   731.88083  7838.37988   5972.84521
Training/qf2_loss                 14960.59902  872.91659  16293.97656  14104.70605
Training/pf_norm                  0.08821      0.03602    0.12116      0.03147
Training/qf1_norm                 1230.49804   679.95359  2254.07324   145.46519
Training/qf2_norm                 502.19268    22.45913   523.61444    460.25351
log_std/mean                      -0.12882     0.00451    -0.12021     -0.13318
log_probs/mean                    -2.73561     0.00777    -2.72723     -2.74735
mean/mean                         -0.00204     0.00045    -0.00118     -0.00242
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018427610397338867
epoch last part time3 0.0027642250061035156
inside rlalgo, task 0, sumup 70440
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [77700]
collect time 0.0009570121765136719
inner_dict_sum {'sac_diff0': 0.00023984909057617188, 'sac_diff1': 0.006791591644287109, 'sac_diff2': 0.00804448127746582, 'sac_diff3': 0.010443687438964844, 'sac_diff4': 0.0070116519927978516, 'sac_diff5': 0.032628774642944336, 'sac_diff6': 0.00038433074951171875, 'all': 0.06554436683654785}
diff5_list [0.00727081298828125, 0.0065271854400634766, 0.006605386734008789, 0.006098270416259766, 0.006127119064331055]
time3 0
time4 0.06631875038146973
time5 0.06636834144592285
time7 4.76837158203125e-07
gen_weight_change tensor(-22.1087)
policy weight change tensor(37.9728, grad_fn=<SumBackward0>)
time8 0.0018734931945800781
train_time 0.07755804061889648
eval time 0.15476155281066895
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:05,071 MainThread INFO: EPOCH:511
2024-01-23 01:03:05,071 MainThread INFO: Time Consumed:0.23569083213806152s
2024-01-23 01:03:05,071 MainThread INFO: Total Frames:77550s
  5%|▌         | 512/10000 [04:33<1:05:10,  2.43it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12152.68111
Train_Epoch_Reward                7985.52299
Running_Training_Average_Rewards  11634.21730
Explore_Time                      0.00095
Train___Time                      0.07756
Eval____Time                      0.15476
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12131.23428
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.62602     2.03166     92.22030     86.93384
alpha_0                           0.77413      0.00011     0.77428      0.77398
Alpha_loss                        -1.72336     0.00115     -1.72239     -1.72550
Training/policy_loss              -2.60018     0.00391     -2.59280     -2.60444
Training/qf1_loss                 7554.93594   1088.17961  9198.60059   6120.59277
Training/qf2_loss                 16137.92246  1455.91449  18294.29688  14182.34961
Training/pf_norm                  0.09993      0.02576     0.13137      0.07401
Training/qf1_norm                 395.66021    162.33930   620.93549    142.04683
Training/qf2_norm                 489.68782    10.90963    503.93027    475.29395
log_std/mean                      -0.13273     0.00009     -0.13263     -0.13287
log_probs/mean                    -2.73409     0.00487     -2.72504     -2.73981
mean/mean                         -0.00184     0.00012     -0.00167     -0.00201
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.5335800647735596
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70440
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [77850]
collect time 0.0009188652038574219
inner_dict_sum {'sac_diff0': 0.0002117156982421875, 'sac_diff1': 0.007520437240600586, 'sac_diff2': 0.008994579315185547, 'sac_diff3': 0.011649370193481445, 'sac_diff4': 0.007965326309204102, 'sac_diff5': 0.03586745262145996, 'sac_diff6': 0.0004253387451171875, 'all': 0.07263422012329102}
diff5_list [0.008282661437988281, 0.00759577751159668, 0.006615638732910156, 0.006802082061767578, 0.006571292877197266]
time3 0
time4 0.07351446151733398
time5 0.07356834411621094
time7 7.152557373046875e-07
gen_weight_change tensor(-22.1087)
policy weight change tensor(37.9095, grad_fn=<SumBackward0>)
time8 0.0018503665924072266
train_time 0.08481097221374512
eval time 0.0005691051483154297
epoch last part time 4.291534423828125e-06
2024-01-23 01:03:05,696 MainThread INFO: EPOCH:512
2024-01-23 01:03:05,697 MainThread INFO: Time Consumed:0.08844208717346191s
2024-01-23 01:03:05,697 MainThread INFO: Total Frames:77700s
  5%|▌         | 513/10000 [04:33<50:52,  3.11it/s]  --------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12113.72421
Train_Epoch_Reward                17967.78992
Running_Training_Average_Rewards  11976.55803
Explore_Time                      0.00091
Train___Time                      0.08481
Eval____Time                      0.00057
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11968.15731
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.59931     2.38122    92.84209     86.33697
alpha_0                           0.77374      0.00011    0.77390      0.77359
Alpha_loss                        -1.72756     0.00109    -1.72597     -1.72878
Training/policy_loss              -2.68978     0.00460    -2.68270     -2.69569
Training/qf1_loss                 6867.55029   519.16255  7721.36230   6278.25488
Training/qf2_loss                 15367.62441  911.56531  16884.70703  14401.70312
Training/pf_norm                  0.10131      0.02129    0.13309      0.06741
Training/qf1_norm                 1357.63693   507.67860  2075.50757   711.51074
Training/qf2_norm                 553.79081    14.45115   573.12177    533.59784
log_std/mean                      -0.13505     0.00021    -0.13469     -0.13528
log_probs/mean                    -2.73733     0.00564    -2.72852     -2.74458
mean/mean                         -0.00216     0.00002    -0.00212     -0.00219
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018550395965576172
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70440
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [77976]
collect time 0.0008192062377929688
inner_dict_sum {'sac_diff0': 0.00022935867309570312, 'sac_diff1': 0.006650447845458984, 'sac_diff2': 0.007939577102661133, 'sac_diff3': 0.01001739501953125, 'sac_diff4': 0.006984710693359375, 'sac_diff5': 0.033138275146484375, 'sac_diff6': 0.00039315223693847656, 'all': 0.0653529167175293}
diff5_list [0.0062770843505859375, 0.006296873092651367, 0.006127834320068359, 0.006598711013793945, 0.007837772369384766]
time3 0
time4 0.06613421440124512
time5 0.06618165969848633
time7 7.152557373046875e-07
gen_weight_change tensor(-22.1087)
policy weight change tensor(37.8305, grad_fn=<SumBackward0>)
time8 0.0020422935485839844
train_time 0.07721614837646484
eval time 0.0006325244903564453
epoch last part time 5.7220458984375e-06
2024-01-23 01:03:05,799 MainThread INFO: EPOCH:513
2024-01-23 01:03:05,800 MainThread INFO: Time Consumed:0.0810096263885498s
2024-01-23 01:03:05,800 MainThread INFO: Total Frames:77850s
  5%|▌         | 514/10000 [04:33<40:35,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12082.22960
Train_Epoch_Reward                3855.35022
Running_Training_Average_Rewards  11974.63289
Explore_Time                      0.00081
Train___Time                      0.07722
Eval____Time                      0.00063
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11968.15731
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.31465     0.84451    92.10242     89.72166
alpha_0                           0.77336      0.00011    0.77351      0.77320
Alpha_loss                        -1.72906     0.00241    -1.72534     -1.73266
Training/policy_loss              -2.66129     0.00423    -2.65521     -2.66783
Training/qf1_loss                 7045.86875   568.37633  7870.56152   6416.67676
Training/qf2_loss                 15915.07402  572.34267  16897.50977  15267.45117
Training/pf_norm                  0.10328      0.01325    0.12421      0.08421
Training/qf1_norm                 469.87017    183.96746  781.80505    225.99788
Training/qf2_norm                 546.14293    5.02133    550.55798    536.61627
log_std/mean                      -0.12835     0.00010    -0.12819     -0.12849
log_probs/mean                    -2.73007     0.00612    -2.72081     -2.73884
mean/mean                         -0.00319     0.00004    -0.00314     -0.00325
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.020559072494506836
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70440
epoch first part time 2.86102294921875e-06
replay_buffer._size: [78096]
collect time 0.0009648799896240234
inner_dict_sum {'sac_diff0': 0.00021576881408691406, 'sac_diff1': 0.006864309310913086, 'sac_diff2': 0.00820016860961914, 'sac_diff3': 0.010459661483764648, 'sac_diff4': 0.007122039794921875, 'sac_diff5': 0.03249096870422363, 'sac_diff6': 0.00041031837463378906, 'all': 0.06576323509216309}
diff5_list [0.007050991058349609, 0.00631260871887207, 0.006237506866455078, 0.006655693054199219, 0.006234169006347656]
time3 0
time4 0.06652998924255371
time5 0.06657671928405762
time7 4.76837158203125e-07
gen_weight_change tensor(-22.1087)
policy weight change tensor(37.7241, grad_fn=<SumBackward0>)
time8 0.0018837451934814453
train_time 0.07750058174133301
eval time 0.05362391471862793
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:05,958 MainThread INFO: EPOCH:514
2024-01-23 01:03:05,959 MainThread INFO: Time Consumed:0.1345062255859375s
2024-01-23 01:03:05,959 MainThread INFO: Total Frames:78000s
  5%|▌         | 515/10000 [04:33<35:50,  4.41it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12042.97914
Train_Epoch_Reward                12069.92797
Running_Training_Average_Rewards  12087.42807
Explore_Time                      0.00096
Train___Time                      0.07750
Eval____Time                      0.05362
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11842.43158
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.51318     1.68144     91.22173     86.01960
alpha_0                           0.77297      0.00011     0.77312      0.77281
Alpha_loss                        -1.73344     0.00102     -1.73200     -1.73505
Training/policy_loss              -2.67854     0.00482     -2.67201     -2.68340
Training/qf1_loss                 6956.08184   996.08984   8387.31348   5730.36621
Training/qf2_loss                 15281.31738  1063.17442  16704.29102  13571.81445
Training/pf_norm                  0.10801      0.02311     0.14246      0.07453
Training/qf1_norm                 453.42321    262.65867   958.66443    202.16969
Training/qf2_norm                 539.25930    10.09906    555.53253    524.22504
log_std/mean                      -0.13573     0.00022     -0.13542     -0.13603
log_probs/mean                    -2.73401     0.00597     -2.72578     -2.74027
mean/mean                         -0.00191     0.00002     -0.00190     -0.00195
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01840829849243164
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70440
epoch first part time 3.337860107421875e-06
replay_buffer._size: [78277]
collect time 0.0009405612945556641
inside mustsac before update, task 0, sumup 70440
inside mustsac after update, task 0, sumup 70599
inner_dict_sum {'sac_diff0': 0.00022149085998535156, 'sac_diff1': 0.0070874691009521484, 'sac_diff2': 0.008453130722045898, 'sac_diff3': 0.01098322868347168, 'sac_diff4': 0.007542133331298828, 'sac_diff5': 0.050667524337768555, 'sac_diff6': 0.00041294097900390625, 'all': 0.08536791801452637}
diff5_list [0.010673761367797852, 0.010887384414672852, 0.009974241256713867, 0.009621143341064453, 0.009510993957519531]
time3 0.0008831024169921875
time4 0.08625912666320801
time5 0.08631443977355957
time7 0.009112358093261719
gen_weight_change tensor(-21.9295)
policy weight change tensor(37.6325, grad_fn=<SumBackward0>)
time8 0.0018770694732666016
train_time 0.11552834510803223
eval time 0.11434793472290039
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:06,214 MainThread INFO: EPOCH:515
2024-01-23 01:03:06,214 MainThread INFO: Time Consumed:0.2332744598388672s
2024-01-23 01:03:06,214 MainThread INFO: Total Frames:78150s
  5%|▌         | 516/10000 [04:33<37:12,  4.25it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11989.86929
Train_Epoch_Reward                4683.25121
Running_Training_Average_Rewards  11760.22001
Explore_Time                      0.00094
Train___Time                      0.11553
Eval____Time                      0.11435
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11685.86452
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.10988     2.24542     92.20840     85.63036
alpha_0                           0.77258      0.00011     0.77274      0.77243
Alpha_loss                        -1.73663     0.00153     -1.73410     -1.73844
Training/policy_loss              -2.63716     0.02642     -2.59781     -2.67407
Training/qf1_loss                 7121.62002   895.47372   8816.08398   6357.65234
Training/qf2_loss                 15541.09980  1187.72856  17789.46680  14338.84668
Training/pf_norm                  0.11985      0.02281     0.16014      0.08934
Training/qf1_norm                 1036.97723   547.11721   1592.36865   155.05894
Training/qf2_norm                 512.60134    18.21907    544.04156    497.56015
log_std/mean                      -0.13725     0.00316     -0.13140     -0.14024
log_probs/mean                    -2.73331     0.00502     -2.72612     -2.74096
mean/mean                         -0.00297     0.00039     -0.00219     -0.00325
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018817663192749023
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70599
epoch first part time 2.86102294921875e-06
replay_buffer._size: [78450]
collect time 0.0010259151458740234
inner_dict_sum {'sac_diff0': 0.00021648406982421875, 'sac_diff1': 0.007048606872558594, 'sac_diff2': 0.008305549621582031, 'sac_diff3': 0.010547399520874023, 'sac_diff4': 0.0071337223052978516, 'sac_diff5': 0.032759904861450195, 'sac_diff6': 0.0003955364227294922, 'all': 0.0664072036743164}
diff5_list [0.007090091705322266, 0.006107330322265625, 0.0064008235931396484, 0.0062944889068603516, 0.006867170333862305]
time3 0
time4 0.0671989917755127
time5 0.06724429130554199
time7 7.152557373046875e-07
gen_weight_change tensor(-21.9295)
policy weight change tensor(37.5415, grad_fn=<SumBackward0>)
time8 0.0019495487213134766
train_time 0.07881593704223633
eval time 0.14865779876708984
epoch last part time 7.867813110351562e-06
2024-01-23 01:03:06,467 MainThread INFO: EPOCH:516
2024-01-23 01:03:06,468 MainThread INFO: Time Consumed:0.23099565505981445s
2024-01-23 01:03:06,468 MainThread INFO: Total Frames:78300s
  5%|▌         | 517/10000 [04:34<38:03,  4.15it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11904.52359
Train_Epoch_Reward                5497.25688
Running_Training_Average_Rewards  11145.61409
Explore_Time                      0.00102
Train___Time                      0.07882
Eval____Time                      0.14866
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11347.62401
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.81294     2.06231     93.06429     87.20270
alpha_0                           0.77220      0.00011     0.77235      0.77204
Alpha_loss                        -1.73987     0.00290     -1.73552     -1.74433
Training/policy_loss              -2.68830     0.00546     -2.68030     -2.69707
Training/qf1_loss                 6960.22061   715.88886   8056.57959   5844.09033
Training/qf2_loss                 15653.74902  1028.93235  17145.76953  13909.58789
Training/pf_norm                  0.12582      0.00849     0.13688      0.11322
Training/qf1_norm                 1386.24390   373.90645   1769.11084   718.81250
Training/qf2_norm                 575.83446    12.41905    589.27332    554.05280
log_std/mean                      -0.13272     0.00014     -0.13252     -0.13288
log_probs/mean                    -2.73284     0.00776     -2.72120     -2.74487
mean/mean                         -0.00193     0.00005     -0.00188     -0.00202
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018701791763305664
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70599
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [78600]
collect time 0.0009658336639404297
inner_dict_sum {'sac_diff0': 0.0002205371856689453, 'sac_diff1': 0.0072748661041259766, 'sac_diff2': 0.008428812026977539, 'sac_diff3': 0.010536909103393555, 'sac_diff4': 0.007215023040771484, 'sac_diff5': 0.03307485580444336, 'sac_diff6': 0.00040531158447265625, 'all': 0.06715631484985352}
diff5_list [0.006613731384277344, 0.006268501281738281, 0.006222963333129883, 0.0071392059326171875, 0.006830453872680664]
time3 0
time4 0.06803774833679199
time5 0.06808876991271973
time7 7.152557373046875e-07
gen_weight_change tensor(-21.9295)
policy weight change tensor(37.5143, grad_fn=<SumBackward0>)
time8 0.001959085464477539
train_time 0.0793302059173584
eval time 0.15650081634521484
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:06,729 MainThread INFO: EPOCH:517
2024-01-23 01:03:06,729 MainThread INFO: Time Consumed:0.23922038078308105s
2024-01-23 01:03:06,729 MainThread INFO: Total Frames:78450s
  5%|▌         | 518/10000 [04:34<39:01,  4.05it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11833.95833
Train_Epoch_Reward                14189.87313
Running_Training_Average_Rewards  11363.53571
Explore_Time                      0.00096
Train___Time                      0.07933
Eval____Time                      0.15650
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11446.99658
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.98018     1.70243    91.34988     86.65314
alpha_0                           0.77181      0.00011    0.77197      0.77166
Alpha_loss                        -1.74375     0.00111    -1.74216     -1.74502
Training/policy_loss              -2.68165     0.00432    -2.67653     -2.68745
Training/qf1_loss                 6591.55703   471.92332  7132.95166   5897.81152
Training/qf2_loss                 14974.70137  736.26084  15635.05664  14045.25488
Training/pf_norm                  0.09192      0.02534    0.12687      0.05997
Training/qf1_norm                 1279.42742   354.57277  1774.83386   838.92615
Training/qf2_norm                 536.70808    10.01170   550.36877    523.19879
log_std/mean                      -0.13510     0.00005    -0.13505     -0.13518
log_probs/mean                    -2.73481     0.00533    -2.72900     -2.74232
mean/mean                         -0.00286     0.00004    -0.00281     -0.00291
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01829075813293457
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70599
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [78750]
collect time 0.0009157657623291016
inner_dict_sum {'sac_diff0': 0.00024127960205078125, 'sac_diff1': 0.007348537445068359, 'sac_diff2': 0.008877992630004883, 'sac_diff3': 0.010811328887939453, 'sac_diff4': 0.007408618927001953, 'sac_diff5': 0.03248929977416992, 'sac_diff6': 0.0003993511199951172, 'all': 0.06757640838623047}
diff5_list [0.006987094879150391, 0.006890773773193359, 0.0063436031341552734, 0.006178855895996094, 0.006088972091674805]
time3 0
time4 0.06839346885681152
time5 0.06844520568847656
time7 7.152557373046875e-07
gen_weight_change tensor(-21.9295)
policy weight change tensor(37.4780, grad_fn=<SumBackward0>)
time8 0.0018069744110107422
train_time 0.07956528663635254
eval time 0.1480112075805664
epoch last part time 7.867813110351562e-06
2024-01-23 01:03:06,982 MainThread INFO: EPOCH:518
2024-01-23 01:03:06,982 MainThread INFO: Time Consumed:0.2308666706085205s
2024-01-23 01:03:06,982 MainThread INFO: Total Frames:78600s
  5%|▌         | 519/10000 [04:34<39:18,  4.02it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11771.80005
Train_Epoch_Reward                13804.03701
Running_Training_Average_Rewards  11399.54966
Explore_Time                      0.00091
Train___Time                      0.07957
Eval____Time                      0.14801
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11492.97595
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.46199     2.03266     93.20893     88.68311
alpha_0                           0.77142      0.00011     0.77158      0.77127
Alpha_loss                        -1.74859     0.00231     -1.74681     -1.75307
Training/policy_loss              -2.64685     0.00591     -2.64160     -2.65804
Training/qf1_loss                 6807.62822   763.66442   8074.09131   5993.13184
Training/qf2_loss                 15498.74160  1095.00473  17220.16211  14378.90625
Training/pf_norm                  0.08586      0.02136     0.12598      0.06710
Training/qf1_norm                 950.09658    388.54822   1347.71448   460.66769
Training/qf2_norm                 526.18550    11.60704    541.66766    516.00323
log_std/mean                      -0.13803     0.00010     -0.13786     -0.13813
log_probs/mean                    -2.74048     0.00764     -2.73418     -2.75514
mean/mean                         -0.00168     0.00005     -0.00160     -0.00173
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018596649169921875
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70599
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [78900]
collect time 0.0009446144104003906
inner_dict_sum {'sac_diff0': 0.0002117156982421875, 'sac_diff1': 0.007743358612060547, 'sac_diff2': 0.008898019790649414, 'sac_diff3': 0.011198043823242188, 'sac_diff4': 0.0072901248931884766, 'sac_diff5': 0.033208608627319336, 'sac_diff6': 0.00043392181396484375, 'all': 0.06898379325866699}
diff5_list [0.007091999053955078, 0.0073511600494384766, 0.006258487701416016, 0.006231546401977539, 0.0062754154205322266]
time3 0
time4 0.06988263130187988
time5 0.06993722915649414
time7 4.76837158203125e-07
gen_weight_change tensor(-21.9295)
policy weight change tensor(37.3900, grad_fn=<SumBackward0>)
time8 0.0018651485443115234
train_time 0.08123993873596191
eval time 0.15852856636047363
epoch last part time 7.62939453125e-06
2024-01-23 01:03:07,247 MainThread INFO: EPOCH:519
2024-01-23 01:03:07,247 MainThread INFO: Time Consumed:0.24314165115356445s
2024-01-23 01:03:07,248 MainThread INFO: Total Frames:78750s
  5%|▌         | 520/10000 [04:34<40:08,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11719.92206
Train_Epoch_Reward                37797.23319
Running_Training_Average_Rewards  12435.27132
Explore_Time                      0.00094
Train___Time                      0.08124
Eval____Time                      0.15853
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11489.13370
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.84437     3.66127     95.64574     85.39990
alpha_0                           0.77104      0.00011     0.77119      0.77088
Alpha_loss                        -1.75119     0.00266     -1.74684     -1.75391
Training/policy_loss              -2.66608     0.00929     -2.65307     -2.67465
Training/qf1_loss                 7624.33184   1236.21892  9244.81934   6045.04004
Training/qf2_loss                 16349.67344  1858.89987  18800.74609  13823.38086
Training/pf_norm                  0.07960      0.02264     0.10798      0.04397
Training/qf1_norm                 1457.07258   673.67716   2340.60449   446.43472
Training/qf2_norm                 534.26959    20.97645    561.53992    503.15268
log_std/mean                      -0.13819     0.00019     -0.13793     -0.13844
log_probs/mean                    -2.73753     0.01180     -2.72076     -2.74830
mean/mean                         -0.00247     0.00005     -0.00240     -0.00255
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01970696449279785
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70599
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [79050]
collect time 0.0009713172912597656
inside mustsac before update, task 0, sumup 70599
inside mustsac after update, task 0, sumup 71164
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.008129596710205078, 'sac_diff2': 0.009610891342163086, 'sac_diff3': 0.01222372055053711, 'sac_diff4': 0.00848245620727539, 'sac_diff5': 0.05579113960266113, 'sac_diff6': 0.0004603862762451172, 'all': 0.09491920471191406}
diff5_list [0.011559724807739258, 0.01105809211730957, 0.010481119155883789, 0.01137399673461914, 0.011318206787109375]
time3 0.0008945465087890625
time4 0.09590888023376465
time5 0.09596705436706543
time7 0.009268760681152344
gen_weight_change tensor(-21.6781)
policy weight change tensor(37.2989, grad_fn=<SumBackward0>)
time8 0.002712726593017578
train_time 0.12757420539855957
eval time 0.10323739051818848
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:07,505 MainThread INFO: EPOCH:520
2024-01-23 01:03:07,505 MainThread INFO: Time Consumed:0.23431777954101562s
2024-01-23 01:03:07,506 MainThread INFO: Total Frames:78900s
  5%|▌         | 521/10000 [04:35<40:24,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11687.17396
Train_Epoch_Reward                11025.39615
Running_Training_Average_Rewards  12635.79612
Explore_Time                      0.00097
Train___Time                      0.12757
Eval____Time                      0.10324
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11499.16437
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.38316     1.95724     91.76048     85.97115
alpha_0                           0.77065      0.00011     0.77081      0.77050
Alpha_loss                        -1.75423     0.00081     -1.75298     -1.75512
Training/policy_loss              -2.68815     0.02300     -2.66560     -2.72492
Training/qf1_loss                 6524.78037   901.55672   8293.41211   5843.03174
Training/qf2_loss                 15016.47812  1137.26628  17240.57227  14164.69336
Training/pf_norm                  0.11102      0.02515     0.15720      0.08578
Training/qf1_norm                 558.61100    445.12612   1370.53943   133.62013
Training/qf2_norm                 546.47040    18.70531    570.28833    521.65094
log_std/mean                      -0.12673     0.00474     -0.12142     -0.13538
log_probs/mean                    -2.73624     0.00377     -2.73144     -2.74128
mean/mean                         -0.00335     0.00102     -0.00180     -0.00491
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018456220626831055
epoch last part time3 0.0028629302978515625
inside rlalgo, task 0, sumup 71164
epoch first part time 2.86102294921875e-06
replay_buffer._size: [79200]
collect time 0.0009615421295166016
inner_dict_sum {'sac_diff0': 0.00022149085998535156, 'sac_diff1': 0.007382392883300781, 'sac_diff2': 0.00870966911315918, 'sac_diff3': 0.01055765151977539, 'sac_diff4': 0.007251262664794922, 'sac_diff5': 0.03316521644592285, 'sac_diff6': 0.0004239082336425781, 'all': 0.06771159172058105}
diff5_list [0.0068359375, 0.007593631744384766, 0.006345510482788086, 0.006175041198730469, 0.006215095520019531]
time3 0
time4 0.06854963302612305
time5 0.0686044692993164
time7 7.152557373046875e-07
gen_weight_change tensor(-21.6781)
policy weight change tensor(37.3399, grad_fn=<SumBackward0>)
time8 0.001961946487426758
train_time 0.08020949363708496
eval time 0.14857840538024902
epoch last part time 7.62939453125e-06
2024-01-23 01:03:07,762 MainThread INFO: EPOCH:521
2024-01-23 01:03:07,762 MainThread INFO: Time Consumed:0.23216652870178223s
2024-01-23 01:03:07,762 MainThread INFO: Total Frames:79050s
  5%|▌         | 522/10000 [04:35<40:20,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11687.00746
Train_Epoch_Reward                15397.46346
Running_Training_Average_Rewards  12994.95678
Explore_Time                      0.00096
Train___Time                      0.08021
Eval____Time                      0.14858
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12129.56931
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.08370     1.55155     90.74226     86.52626
alpha_0                           0.77027      0.00011     0.77042      0.77011
Alpha_loss                        -1.75634     0.00089     -1.75506     -1.75769
Training/policy_loss              -2.64570     0.00442     -2.64050     -2.65179
Training/qf1_loss                 6908.20029   1007.61233  8249.00684   5697.08984
Training/qf2_loss                 15354.20078  1192.50318  16780.92969  13661.61523
Training/pf_norm                  0.12107      0.04058     0.16188      0.04600
Training/qf1_norm                 481.76683    247.49855   738.83978    74.33717
Training/qf2_norm                 525.55443    8.78147     534.31494    511.31314
log_std/mean                      -0.11642     0.00016     -0.11626     -0.11670
log_probs/mean                    -2.73143     0.00600     -2.72394     -2.73882
mean/mean                         -0.00387     0.00005     -0.00381     -0.00394
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01909351348876953
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71164
epoch first part time 2.86102294921875e-06
replay_buffer._size: [79350]
collect time 0.0010166168212890625
inner_dict_sum {'sac_diff0': 0.00023317337036132812, 'sac_diff1': 0.006840229034423828, 'sac_diff2': 0.007977962493896484, 'sac_diff3': 0.010489940643310547, 'sac_diff4': 0.006880998611450195, 'sac_diff5': 0.03163862228393555, 'sac_diff6': 0.0003781318664550781, 'all': 0.06443905830383301}
diff5_list [0.006937980651855469, 0.0063018798828125, 0.006144285202026367, 0.0061757564544677734, 0.0060787200927734375]
time3 0
time4 0.06520223617553711
time5 0.0652472972869873
time7 7.152557373046875e-07
gen_weight_change tensor(-21.6781)
policy weight change tensor(37.4411, grad_fn=<SumBackward0>)
time8 0.0018029212951660156
train_time 0.07635283470153809
eval time 0.15622663497924805
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:08,021 MainThread INFO: EPOCH:522
2024-01-23 01:03:08,021 MainThread INFO: Time Consumed:0.23598027229309082s
2024-01-23 01:03:08,021 MainThread INFO: Total Frames:79200s
  5%|▌         | 523/10000 [04:35<40:28,  3.90it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11701.89138
Train_Epoch_Reward                17464.81890
Running_Training_Average_Rewards  12610.03656
Explore_Time                      0.00101
Train___Time                      0.07635
Eval____Time                      0.15623
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12116.99649
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.91300     1.59438    92.79797     88.04710
alpha_0                           0.76988      0.00011    0.77004      0.76973
Alpha_loss                        -1.75917     0.00300    -1.75462     -1.76339
Training/policy_loss              -2.69058     0.00636    -2.68145     -2.70038
Training/qf1_loss                 6718.96797   513.76165  7180.88965   5755.49805
Training/qf2_loss                 15234.75352  715.38393  16009.31836  13941.79688
Training/pf_norm                  0.11522      0.01956    0.14867      0.09652
Training/qf1_norm                 1625.21384   294.18146  2170.36304   1326.81641
Training/qf2_norm                 553.97150    9.41645    571.10388    542.96674
log_std/mean                      -0.13533     0.00018    -0.13508     -0.13556
log_probs/mean                    -2.72936     0.00883    -2.71710     -2.74292
mean/mean                         -0.00245     0.00003    -0.00243     -0.00251
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018183469772338867
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71164
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [79500]
collect time 0.001010894775390625
inner_dict_sum {'sac_diff0': 0.00021338462829589844, 'sac_diff1': 0.006864309310913086, 'sac_diff2': 0.008191347122192383, 'sac_diff3': 0.010472774505615234, 'sac_diff4': 0.007129192352294922, 'sac_diff5': 0.03320193290710449, 'sac_diff6': 0.00040912628173828125, 'all': 0.0664820671081543}
diff5_list [0.006544351577758789, 0.006284236907958984, 0.00617218017578125, 0.006483793258666992, 0.0077173709869384766]
time3 0
time4 0.06732439994812012
time5 0.06737184524536133
time7 1.1920928955078125e-06
gen_weight_change tensor(-21.6781)
policy weight change tensor(37.4923, grad_fn=<SumBackward0>)
time8 0.0020155906677246094
train_time 0.07888627052307129
eval time 0.14957213401794434
epoch last part time 7.62939453125e-06
2024-01-23 01:03:08,275 MainThread INFO: EPOCH:523
2024-01-23 01:03:08,275 MainThread INFO: Time Consumed:0.23194050788879395s
2024-01-23 01:03:08,275 MainThread INFO: Total Frames:79350s
  5%|▌         | 524/10000 [04:35<40:23,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11714.24277
Train_Epoch_Reward                11941.76108
Running_Training_Average_Rewards  12671.94889
Explore_Time                      0.00101
Train___Time                      0.07889
Eval____Time                      0.14957
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12091.67115
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.42539     1.26531     89.75908     86.68621
alpha_0                           0.76950      0.00011     0.76965      0.76934
Alpha_loss                        -1.76318     0.00186     -1.76088     -1.76550
Training/policy_loss              -2.71247     0.00315     -2.70897     -2.71717
Training/qf1_loss                 6775.44492   946.41502   7971.68457   5661.95020
Training/qf2_loss                 15074.80293  1167.52963  16516.25586  13648.19531
Training/pf_norm                  0.10567      0.01927     0.14030      0.08899
Training/qf1_norm                 572.02116    278.43678   868.64697    170.71721
Training/qf2_norm                 570.35668    7.61100     578.42523    560.16309
log_std/mean                      -0.14107     0.00014     -0.14089     -0.14123
log_probs/mean                    -2.73184     0.00428     -2.72693     -2.73804
mean/mean                         -0.00296     0.00010     -0.00282     -0.00309
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019132375717163086
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71164
epoch first part time 2.86102294921875e-06
replay_buffer._size: [79650]
collect time 0.0009474754333496094
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.007005453109741211, 'sac_diff2': 0.008768081665039062, 'sac_diff3': 0.011029481887817383, 'sac_diff4': 0.0073089599609375, 'sac_diff5': 0.033051490783691406, 'sac_diff6': 0.00040149688720703125, 'all': 0.06777572631835938}
diff5_list [0.0069391727447509766, 0.006219625473022461, 0.007265567779541016, 0.00634312629699707, 0.006283998489379883]
time3 0
time4 0.06861257553100586
time5 0.0686643123626709
time7 9.5367431640625e-07
gen_weight_change tensor(-21.6781)
policy weight change tensor(37.5065, grad_fn=<SumBackward0>)
time8 0.0018432140350341797
train_time 0.07989811897277832
eval time 0.14975714683532715
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:08,530 MainThread INFO: EPOCH:524
2024-01-23 01:03:08,531 MainThread INFO: Time Consumed:0.23299121856689453s
2024-01-23 01:03:08,531 MainThread INFO: Total Frames:79500s
  5%|▌         | 525/10000 [04:36<40:22,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11741.60369
Train_Epoch_Reward                6428.05401
Running_Training_Average_Rewards  12559.32168
Explore_Time                      0.00094
Train___Time                      0.07990
Eval____Time                      0.14976
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12116.04081
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.86865     1.83048    93.30418     88.52771
alpha_0                           0.76911      0.00011    0.76927      0.76896
Alpha_loss                        -1.76766     0.00193    -1.76551     -1.77103
Training/policy_loss              -2.69208     0.00576    -2.68798     -2.70343
Training/qf1_loss                 6859.05498   532.12517  7607.79932   6108.82178
Training/qf2_loss                 15444.06699  673.96617  16350.03418  14436.73438
Training/pf_norm                  0.11297      0.02620    0.13224      0.06100
Training/qf1_norm                 382.27478    138.62149  542.23834    127.72209
Training/qf2_norm                 560.17104    10.98254   580.70264    551.72186
log_std/mean                      -0.13120     0.00005    -0.13115     -0.13128
log_probs/mean                    -2.73608     0.00779    -2.73083     -2.75151
mean/mean                         -0.00506     0.00005    -0.00498     -0.00511
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018639564514160156
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71164
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [79800]
collect time 0.0008640289306640625
inside mustsac before update, task 0, sumup 71164
inside mustsac after update, task 0, sumup 71053
inner_dict_sum {'sac_diff0': 0.00022482872009277344, 'sac_diff1': 0.007061004638671875, 'sac_diff2': 0.008407831192016602, 'sac_diff3': 0.010590553283691406, 'sac_diff4': 0.0075109004974365234, 'sac_diff5': 0.049111127853393555, 'sac_diff6': 0.0004127025604248047, 'all': 0.08331894874572754}
diff5_list [0.0106658935546875, 0.010228633880615234, 0.009494781494140625, 0.009348869323730469, 0.009372949600219727]
time3 0.0009343624114990234
time4 0.08421611785888672
time5 0.08428740501403809
time7 0.009110450744628906
gen_weight_change tensor(-21.5119)
policy weight change tensor(37.4739, grad_fn=<SumBackward0>)
time8 0.0018050670623779297
train_time 0.1135702133178711
eval time 0.1205606460571289
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:08,790 MainThread INFO: EPOCH:525
2024-01-23 01:03:08,790 MainThread INFO: Time Consumed:0.2374265193939209s
2024-01-23 01:03:08,791 MainThread INFO: Total Frames:79650s
  5%|▌         | 526/10000 [04:36<40:31,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11798.46728
Train_Epoch_Reward                12986.83493
Running_Training_Average_Rewards  12369.87338
Explore_Time                      0.00086
Train___Time                      0.11357
Eval____Time                      0.12056
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12254.50049
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.82292     2.76863     95.65585     87.96617
alpha_0                           0.76873      0.00011     0.76888      0.76858
Alpha_loss                        -1.77081     0.00259     -1.76794     -1.77541
Training/policy_loss              -2.69390     0.04378     -2.63309     -2.73774
Training/qf1_loss                 7829.05840   1703.38638  10918.56836  6065.71680
Training/qf2_loss                 16567.47578  2232.32004  20636.21094  14277.42773
Training/pf_norm                  0.11210      0.03074     0.16602      0.07611
Training/qf1_norm                 1334.91385   424.74717   2009.88977   731.94391
Training/qf2_norm                 566.26471    32.59605    612.98151    518.95740
log_std/mean                      -0.13277     0.00491     -0.12365     -0.13785
log_probs/mean                    -2.73524     0.00759     -2.72690     -2.74762
mean/mean                         -0.00330     0.00095     -0.00229     -0.00464
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.017985820770263672
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71053
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [79950]
collect time 0.0009505748748779297
inner_dict_sum {'sac_diff0': 0.00020956993103027344, 'sac_diff1': 0.007089138031005859, 'sac_diff2': 0.008627653121948242, 'sac_diff3': 0.010945796966552734, 'sac_diff4': 0.0074269771575927734, 'sac_diff5': 0.033661842346191406, 'sac_diff6': 0.0004086494445800781, 'all': 0.06836962699890137}
diff5_list [0.0066699981689453125, 0.006944179534912109, 0.006590366363525391, 0.006669044494628906, 0.0067882537841796875]
time3 0
time4 0.0691981315612793
time5 0.06925630569458008
time7 7.152557373046875e-07
gen_weight_change tensor(-21.5119)
policy weight change tensor(37.4930, grad_fn=<SumBackward0>)
time8 0.001980304718017578
train_time 0.08054137229919434
eval time 0.1605081558227539
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:09,056 MainThread INFO: EPOCH:526
2024-01-23 01:03:09,057 MainThread INFO: Time Consumed:0.24440574645996094s
2024-01-23 01:03:09,057 MainThread INFO: Total Frames:79800s
  5%|▌         | 527/10000 [04:36<40:58,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11843.46981
Train_Epoch_Reward                1731.57333
Running_Training_Average_Rewards  12193.14925
Explore_Time                      0.00095
Train___Time                      0.08054
Eval____Time                      0.16051
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11797.64929
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.04598     3.06756     94.30317     85.13657
alpha_0                           0.76834      0.00011     0.76850      0.76819
Alpha_loss                        -1.77551     0.00174     -1.77261     -1.77777
Training/policy_loss              -2.70796     0.00392     -2.70401     -2.71479
Training/qf1_loss                 6933.81494   1296.22933  9204.24219   5620.15967
Training/qf2_loss                 15574.44160  1872.36036  18691.94922  13291.79492
Training/pf_norm                  0.09403      0.02259     0.12535      0.06492
Training/qf1_norm                 677.61974    458.61401   1503.80798   264.01392
Training/qf2_norm                 556.42198    18.47180    582.36139    526.95239
log_std/mean                      -0.12003     0.00006     -0.11994     -0.12010
log_probs/mean                    -2.74029     0.00517     -2.73439     -2.74889
mean/mean                         -0.00178     0.00003     -0.00174     -0.00182
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0182039737701416
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71053
epoch first part time 2.86102294921875e-06
replay_buffer._size: [80100]
collect time 0.000978231430053711
inner_dict_sum {'sac_diff0': 0.000213623046875, 'sac_diff1': 0.007320880889892578, 'sac_diff2': 0.008756875991821289, 'sac_diff3': 0.010929346084594727, 'sac_diff4': 0.0073070526123046875, 'sac_diff5': 0.03265237808227539, 'sac_diff6': 0.00039768218994140625, 'all': 0.06757783889770508}
diff5_list [0.006611824035644531, 0.006523609161376953, 0.0070688724517822266, 0.006308078765869141, 0.006139993667602539]
time3 0
time4 0.06840705871582031
time5 0.06846022605895996
time7 7.152557373046875e-07
gen_weight_change tensor(-21.5119)
policy weight change tensor(37.5985, grad_fn=<SumBackward0>)
time8 0.0018582344055175781
train_time 0.07953333854675293
eval time 0.16242527961730957
epoch last part time 8.106231689453125e-06
2024-01-23 01:03:09,324 MainThread INFO: EPOCH:527
2024-01-23 01:03:09,324 MainThread INFO: Time Consumed:0.24535536766052246s
2024-01-23 01:03:09,324 MainThread INFO: Total Frames:79950s
  5%|▌         | 528/10000 [04:36<41:20,  3.82it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11881.01187
Train_Epoch_Reward                37719.44167
Running_Training_Average_Rewards  12998.52798
Explore_Time                      0.00097
Train___Time                      0.07953
Eval____Time                      0.16243
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11822.41711
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.74876     1.60839    91.56337     86.85012
alpha_0                           0.76796      0.00011    0.76811      0.76781
Alpha_loss                        -1.77726     0.00160    -1.77611     -1.78039
Training/policy_loss              -2.68310     0.00541    -2.67741     -2.69243
Training/qf1_loss                 6802.48809   385.55030  7290.26855   6403.06738
Training/qf2_loss                 15309.61113  553.19143  16060.94824  14569.83789
Training/pf_norm                  0.10866      0.02922    0.15679      0.06677
Training/qf1_norm                 1721.28528   304.41539  2066.33179   1171.18408
Training/qf2_norm                 554.44222    9.98391    565.61060    536.55884
log_std/mean                      -0.12081     0.00024    -0.12054     -0.12118
log_probs/mean                    -2.73416     0.00697    -2.72671     -2.74604
mean/mean                         -0.00229     0.00004    -0.00221     -0.00232
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01833319664001465
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71053
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [80250]
collect time 0.0008919239044189453
inner_dict_sum {'sac_diff0': 0.0002384185791015625, 'sac_diff1': 0.0069310665130615234, 'sac_diff2': 0.008115768432617188, 'sac_diff3': 0.010200738906860352, 'sac_diff4': 0.006853818893432617, 'sac_diff5': 0.03387188911437988, 'sac_diff6': 0.0004050731658935547, 'all': 0.06661677360534668}
diff5_list [0.00660395622253418, 0.006737470626831055, 0.007912874221801758, 0.006296634674072266, 0.006320953369140625]
time3 0
time4 0.06743049621582031
time5 0.06747865676879883
time7 4.76837158203125e-07
gen_weight_change tensor(-21.5119)
policy weight change tensor(37.7236, grad_fn=<SumBackward0>)
time8 0.0018858909606933594
train_time 0.0787651538848877
eval time 0.15945911407470703
epoch last part time 1.7881393432617188e-05
2024-01-23 01:03:09,587 MainThread INFO: EPOCH:528
2024-01-23 01:03:09,587 MainThread INFO: Time Consumed:0.24138689041137695s
2024-01-23 01:03:09,587 MainThread INFO: Total Frames:80100s
  5%|▌         | 529/10000 [04:37<41:23,  3.81it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11919.33289
Train_Epoch_Reward                17365.44231
Running_Training_Average_Rewards  13326.63954
Explore_Time                      0.00089
Train___Time                      0.07877
Eval____Time                      0.15946
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11876.18616
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.36028     1.95420    91.55737     86.01320
alpha_0                           0.76758      0.00011    0.76773      0.76742
Alpha_loss                        -1.77998     0.00134    -1.77854     -1.78180
Training/policy_loss              -2.69183     0.00324    -2.68708     -2.69575
Training/qf1_loss                 6917.10391   693.47236  7977.25781   6095.52637
Training/qf2_loss                 15185.12148  910.06756  16426.53516  13898.53418
Training/pf_norm                  0.10008      0.01666    0.12813      0.08263
Training/qf1_norm                 1296.21332   385.08838  1787.04773   669.03241
Training/qf2_norm                 542.45854    11.73951   561.50488    528.48975
log_std/mean                      -0.12842     0.00019    -0.12816     -0.12870
log_probs/mean                    -2.73171     0.00438    -2.72471     -2.73689
mean/mean                         -0.00108     0.00005    -0.00101     -0.00114
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018148422241210938
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71053
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [80400]
collect time 0.0009152889251708984
inner_dict_sum {'sac_diff0': 0.000213623046875, 'sac_diff1': 0.00669550895690918, 'sac_diff2': 0.007605314254760742, 'sac_diff3': 0.00966644287109375, 'sac_diff4': 0.006745100021362305, 'sac_diff5': 0.03150033950805664, 'sac_diff6': 0.00037598609924316406, 'all': 0.06280231475830078}
diff5_list [0.00615692138671875, 0.006285667419433594, 0.006698131561279297, 0.00637054443359375, 0.00598907470703125]
time3 0
time4 0.06352591514587402
time5 0.0635690689086914
time7 4.76837158203125e-07
gen_weight_change tensor(-21.5119)
policy weight change tensor(37.8041, grad_fn=<SumBackward0>)
time8 0.0019450187683105469
train_time 0.07438445091247559
eval time 0.16203570365905762
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:09,848 MainThread INFO: EPOCH:529
2024-01-23 01:03:09,848 MainThread INFO: Time Consumed:0.23958492279052734s
2024-01-23 01:03:09,848 MainThread INFO: Total Frames:80250s
  5%|▌         | 530/10000 [04:37<41:20,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11959.05931
Train_Epoch_Reward                12745.45609
Running_Training_Average_Rewards  13611.28100
Explore_Time                      0.00090
Train___Time                      0.07438
Eval____Time                      0.16204
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11886.39792
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.12092     3.10902     93.98584     85.22337
alpha_0                           0.76719      0.00011     0.76735      0.76704
Alpha_loss                        -1.78382     0.00134     -1.78203     -1.78570
Training/policy_loss              -2.63207     0.00424     -2.62616     -2.63817
Training/qf1_loss                 7256.91025   685.72250   8163.51953   6379.35303
Training/qf2_loss                 15890.95742  1198.66210  17289.53125  14112.51270
Training/pf_norm                  0.10876      0.00843     0.11740      0.09640
Training/qf1_norm                 699.87665    385.49442   1266.12292   135.49210
Training/qf2_norm                 504.82041    17.07982    526.13354    477.97046
log_std/mean                      -0.12865     0.00010     -0.12846     -0.12876
log_probs/mean                    -2.73348     0.00514     -2.72615     -2.74065
mean/mean                         -0.00285     0.00011     -0.00274     -0.00304
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018018484115600586
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71053
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [80550]
collect time 0.0009317398071289062
inside mustsac before update, task 0, sumup 71053
inside mustsac after update, task 0, sumup 70651
inner_dict_sum {'sac_diff0': 0.00020551681518554688, 'sac_diff1': 0.006729602813720703, 'sac_diff2': 0.008124828338623047, 'sac_diff3': 0.010474681854248047, 'sac_diff4': 0.007232666015625, 'sac_diff5': 0.04932856559753418, 'sac_diff6': 0.0003883838653564453, 'all': 0.08248424530029297}
diff5_list [0.010263204574584961, 0.009848356246948242, 0.009962320327758789, 0.009632349014282227, 0.009622335433959961]
time3 0.0008697509765625
time4 0.08328819274902344
time5 0.08333468437194824
time7 0.009307384490966797
gen_weight_change tensor(-21.2702)
policy weight change tensor(37.7119, grad_fn=<SumBackward0>)
time8 0.0025968551635742188
train_time 0.1134798526763916
eval time 0.11955118179321289
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:10,106 MainThread INFO: EPOCH:530
2024-01-23 01:03:10,106 MainThread INFO: Time Consumed:0.23625516891479492s
2024-01-23 01:03:10,106 MainThread INFO: Total Frames:80400s
  5%|▌         | 531/10000 [04:37<41:18,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11997.02607
Train_Epoch_Reward                25149.77995
Running_Training_Average_Rewards  13888.21087
Explore_Time                      0.00093
Train___Time                      0.11348
Eval____Time                      0.11955
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11878.83201
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.14600     1.33428     93.58324     89.73320
alpha_0                           0.76681      0.00011     0.76696      0.76666
Alpha_loss                        -1.78727     0.00238     -1.78363     -1.79013
Training/policy_loss              -2.68512     0.05458     -2.59870     -2.75762
Training/qf1_loss                 7154.24834   1112.50701  8460.29199   5416.97217
Training/qf2_loss                 15959.01230  1258.28221  17256.88672  13946.05957
Training/pf_norm                  0.08953      0.02225     0.12146      0.06447
Training/qf1_norm                 958.44601    698.64766   1993.28870   114.21026
Training/qf2_norm                 558.26522    35.28756    588.96570    497.85818
log_std/mean                      -0.13539     0.00809     -0.12713     -0.14886
log_probs/mean                    -2.73381     0.00766     -2.72262     -2.74204
mean/mean                         -0.00258     0.00102     -0.00130     -0.00392
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018337726593017578
epoch last part time3 0.002808094024658203
inside rlalgo, task 0, sumup 70651
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [80700]
collect time 0.0010275840759277344
inner_dict_sum {'sac_diff0': 0.00021004676818847656, 'sac_diff1': 0.007173061370849609, 'sac_diff2': 0.008760929107666016, 'sac_diff3': 0.011558294296264648, 'sac_diff4': 0.007599353790283203, 'sac_diff5': 0.03564047813415527, 'sac_diff6': 0.0004124641418457031, 'all': 0.07135462760925293}
diff5_list [0.007038593292236328, 0.006353139877319336, 0.007144927978515625, 0.007255077362060547, 0.007848739624023438]
time3 0
time4 0.07219934463500977
time5 0.07225155830383301
time7 9.5367431640625e-07
gen_weight_change tensor(-21.2702)
policy weight change tensor(37.8006, grad_fn=<SumBackward0>)
time8 0.0019207000732421875
train_time 0.08358311653137207
eval time 0.15227890014648438
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:10,370 MainThread INFO: EPOCH:531
2024-01-23 01:03:10,370 MainThread INFO: Time Consumed:0.23932528495788574s
2024-01-23 01:03:10,370 MainThread INFO: Total Frames:80550s
  5%|▌         | 532/10000 [04:37<41:25,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12048.94665
Train_Epoch_Reward                5719.35761
Running_Training_Average_Rewards  13706.61773
Explore_Time                      0.00102
Train___Time                      0.08358
Eval____Time                      0.15228
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12648.77511
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.67994     2.02273     93.19771     87.68717
alpha_0                           0.76643      0.00011     0.76658      0.76627
Alpha_loss                        -1.79108     0.00130     -1.78979     -1.79282
Training/policy_loss              -2.77056     0.00243     -2.76724     -2.77454
Training/qf1_loss                 6767.53438   895.21247   8346.16113   5864.91650
Training/qf2_loss                 15300.75410  1288.58402  17565.48047  14006.37793
Training/pf_norm                  0.10962      0.02766     0.15949      0.08154
Training/qf1_norm                 493.93102    210.13370   756.21600    273.01880
Training/qf2_norm                 611.42825    13.51087    635.18292    598.23749
log_std/mean                      -0.12827     0.00016     -0.12807     -0.12852
log_probs/mean                    -2.73548     0.00334     -2.73081     -2.74060
mean/mean                         -0.00112     0.00002     -0.00109     -0.00115
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021511554718017578
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70651
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [80850]
collect time 0.0010170936584472656
inner_dict_sum {'sac_diff0': 0.00022125244140625, 'sac_diff1': 0.008161544799804688, 'sac_diff2': 0.009659528732299805, 'sac_diff3': 0.012914896011352539, 'sac_diff4': 0.008565425872802734, 'sac_diff5': 0.0390927791595459, 'sac_diff6': 0.0004477500915527344, 'all': 0.07906317710876465}
diff5_list [0.0077855587005615234, 0.008049249649047852, 0.007948875427246094, 0.007899284362792969, 0.007409811019897461]
time3 0
time4 0.0799553394317627
time5 0.08001852035522461
time7 9.5367431640625e-07
gen_weight_change tensor(-21.2702)
policy weight change tensor(37.9157, grad_fn=<SumBackward0>)
time8 0.0020570755004882812
train_time 0.0924062728881836
eval time 0.1768512725830078
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:10,668 MainThread INFO: EPOCH:532
2024-01-23 01:03:10,668 MainThread INFO: Time Consumed:0.2727391719818115s
2024-01-23 01:03:10,669 MainThread INFO: Total Frames:80700s
  5%|▌         | 533/10000 [04:38<43:04,  3.66it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12104.68591
Train_Epoch_Reward                10070.37621
Running_Training_Average_Rewards  13630.95925
Explore_Time                      0.00101
Train___Time                      0.09241
Eval____Time                      0.17685
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12674.38904
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.75920     3.03296     93.18999     84.38332
alpha_0                           0.76604      0.00011     0.76620      0.76589
Alpha_loss                        -1.79404     0.00108     -1.79193     -1.79498
Training/policy_loss              -2.75846     0.00250     -2.75558     -2.76251
Training/qf1_loss                 7314.69902   1290.40602  8883.44922   5369.72070
Training/qf2_loss                 15870.17461  1820.64972  18050.09961  12970.86523
Training/pf_norm                  0.09503      0.01843     0.12590      0.07091
Training/qf1_norm                 1070.28105   552.91999   1718.02783   106.72830
Training/qf2_norm                 597.02654    19.97469    618.41101    561.03729
log_std/mean                      -0.13461     0.00021     -0.13431     -0.13492
log_probs/mean                    -2.73393     0.00320     -2.73022     -2.73791
mean/mean                         -0.00161     0.00012     -0.00144     -0.00178
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020509719848632812
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70651
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [81000]
collect time 0.0010318756103515625
inner_dict_sum {'sac_diff0': 0.0002090930938720703, 'sac_diff1': 0.008460283279418945, 'sac_diff2': 0.010161876678466797, 'sac_diff3': 0.01290440559387207, 'sac_diff4': 0.008745908737182617, 'sac_diff5': 0.03957223892211914, 'sac_diff6': 0.000461578369140625, 'all': 0.08051538467407227}
diff5_list [0.007775306701660156, 0.008153438568115234, 0.007879018783569336, 0.007867097854614258, 0.007897377014160156]
time3 0
time4 0.08144903182983398
time5 0.08150553703308105
time7 7.152557373046875e-07
gen_weight_change tensor(-21.2702)
policy weight change tensor(38.0082, grad_fn=<SumBackward0>)
time8 0.002023458480834961
train_time 0.09361100196838379
eval time 0.18494296073913574
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:10,975 MainThread INFO: EPOCH:533
2024-01-23 01:03:10,975 MainThread INFO: Time Consumed:0.2821183204650879s
2024-01-23 01:03:10,976 MainThread INFO: Total Frames:80850s
  5%|▌         | 534/10000 [04:38<44:37,  3.54it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12170.49501
Train_Epoch_Reward                744.78848
Running_Training_Average_Rewards  12380.10623
Explore_Time                      0.00103
Train___Time                      0.09361
Eval____Time                      0.18494
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12749.76216
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.91534     1.18068    92.20002     88.91818
alpha_0                           0.76566      0.00011    0.76581      0.76551
Alpha_loss                        -1.79737     0.00099    -1.79601     -1.79892
Training/policy_loss              -2.64762     0.00364    -2.64114     -2.65113
Training/qf1_loss                 6668.80967   403.48582  7381.04785   6305.00000
Training/qf2_loss                 15299.22012  605.32355  16466.91016  14891.65820
Training/pf_norm                  0.14058      0.02197    0.17728      0.11161
Training/qf1_norm                 384.90999    104.80608  486.90198    197.88489
Training/qf2_norm                 536.24836    7.14549    550.05524    530.64789
log_std/mean                      -0.13315     0.00016    -0.13289     -0.13330
log_probs/mean                    -2.73381     0.00433    -2.72659     -2.73824
mean/mean                         -0.00302     0.00011    -0.00287     -0.00316
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019832134246826172
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70651
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [81150]
collect time 0.0008356571197509766
inner_dict_sum {'sac_diff0': 0.00022149085998535156, 'sac_diff1': 0.007170677185058594, 'sac_diff2': 0.008197784423828125, 'sac_diff3': 0.010538339614868164, 'sac_diff4': 0.00725102424621582, 'sac_diff5': 0.03277087211608887, 'sac_diff6': 0.0003879070281982422, 'all': 0.06653809547424316}
diff5_list [0.007937192916870117, 0.006386995315551758, 0.006254673004150391, 0.006124973297119141, 0.006067037582397461]
time3 0
time4 0.0673060417175293
time5 0.06734919548034668
time7 7.152557373046875e-07
gen_weight_change tensor(-21.2702)
policy weight change tensor(38.0173, grad_fn=<SumBackward0>)
time8 0.001796722412109375
train_time 0.07814455032348633
eval time 0.1596851348876953
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:11,240 MainThread INFO: EPOCH:534
2024-01-23 01:03:11,240 MainThread INFO: Time Consumed:0.24100732803344727s
2024-01-23 01:03:11,240 MainThread INFO: Total Frames:81000s
  5%|▌         | 535/10000 [04:38<43:41,  3.61it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12245.14643
Train_Epoch_Reward                2115.95599
Running_Training_Average_Rewards  12070.70473
Explore_Time                      0.00083
Train___Time                      0.07814
Eval____Time                      0.15969
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12862.55499
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.44151     2.00667     93.04585     86.90929
alpha_0                           0.76528      0.00011     0.76543      0.76512
Alpha_loss                        -1.80201     0.00082     -1.80066     -1.80321
Training/policy_loss              -2.60628     0.00346     -2.60105     -2.61158
Training/qf1_loss                 7716.66230   1506.69392  9453.70215   5081.41211
Training/qf2_loss                 16415.17051  1894.19751  18654.79102  13088.50293
Training/pf_norm                  0.10809      0.01182     0.11941      0.08632
Training/qf1_norm                 394.49106    294.39570   972.80054    174.62067
Training/qf2_norm                 511.46873    11.04607    526.37610    492.28922
log_std/mean                      -0.13263     0.00003     -0.13258     -0.13266
log_probs/mean                    -2.73855     0.00375     -2.73292     -2.74383
mean/mean                         -0.00224     0.00003     -0.00220     -0.00227
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01871943473815918
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70651
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [81300]
collect time 0.0008251667022705078
inside mustsac before update, task 0, sumup 70651
inside mustsac after update, task 0, sumup 69468
inner_dict_sum {'sac_diff0': 0.00019931793212890625, 'sac_diff1': 0.006345987319946289, 'sac_diff2': 0.007847309112548828, 'sac_diff3': 0.010045289993286133, 'sac_diff4': 0.0072438716888427734, 'sac_diff5': 0.048120975494384766, 'sac_diff6': 0.0003814697265625, 'all': 0.0801842212677002}
diff5_list [0.009893655776977539, 0.009377241134643555, 0.009735107421875, 0.009958028793334961, 0.009156942367553711]
time3 0.0008282661437988281
time4 0.0809781551361084
time5 0.0810239315032959
time7 0.008707046508789062
gen_weight_change tensor(-21.0247)
policy weight change tensor(37.9178, grad_fn=<SumBackward0>)
time8 0.0018603801727294922
train_time 0.10919833183288574
eval time 0.12340617179870605
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:11,497 MainThread INFO: EPOCH:535
2024-01-23 01:03:11,498 MainThread INFO: Time Consumed:0.2356579303741455s
2024-01-23 01:03:11,498 MainThread INFO: Total Frames:81150s
  5%|▌         | 536/10000 [04:39<42:45,  3.69it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12318.36077
Train_Epoch_Reward                12239.54308
Running_Training_Average_Rewards  11977.53906
Explore_Time                      0.00082
Train___Time                      0.10920
Eval____Time                      0.12341
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12986.64391
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.23750     2.12695     92.58673     87.59155
alpha_0                           0.76489      0.00011     0.76505      0.76474
Alpha_loss                        -1.80522     0.00178     -1.80330     -1.80754
Training/policy_loss              -2.72267     0.04754     -2.65956     -2.80389
Training/qf1_loss                 7031.18838   737.86348   8102.89844   6362.16504
Training/qf2_loss                 15680.61426  1044.91954  17104.08203  14489.57520
Training/pf_norm                  0.08736      0.03277     0.14808      0.05652
Training/qf1_norm                 837.51901    522.19031   1678.51440   264.24860
Training/qf2_norm                 584.76703    36.67465    648.15112    540.46747
log_std/mean                      -0.13363     0.00669     -0.12786     -0.14499
log_probs/mean                    -2.73797     0.00691     -2.72932     -2.74758
mean/mean                         -0.00407     0.00081     -0.00323     -0.00544
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0182952880859375
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69468
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [81450]
collect time 0.0008149147033691406
inner_dict_sum {'sac_diff0': 0.00021719932556152344, 'sac_diff1': 0.006758213043212891, 'sac_diff2': 0.00776219367980957, 'sac_diff3': 0.009840011596679688, 'sac_diff4': 0.006546497344970703, 'sac_diff5': 0.030920982360839844, 'sac_diff6': 0.000377655029296875, 'all': 0.062422752380371094}
diff5_list [0.0064051151275634766, 0.006179094314575195, 0.006296396255493164, 0.006103992462158203, 0.005936384201049805]
time3 0
time4 0.06316494941711426
time5 0.06320858001708984
time7 7.152557373046875e-07
gen_weight_change tensor(-21.0247)
policy weight change tensor(37.9291, grad_fn=<SumBackward0>)
time8 0.0017955303192138672
train_time 0.07382798194885254
eval time 0.15766000747680664
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:11,754 MainThread INFO: EPOCH:536
2024-01-23 01:03:11,754 MainThread INFO: Time Consumed:0.23469305038452148s
2024-01-23 01:03:11,754 MainThread INFO: Total Frames:81300s
  5%|▌         | 537/10000 [04:39<42:07,  3.74it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12423.79164
Train_Epoch_Reward                7082.54505
Running_Training_Average_Rewards  12004.43629
Explore_Time                      0.00081
Train___Time                      0.07383
Eval____Time                      0.15766
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12851.95804
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.22094     1.43778     90.80543     87.44855
alpha_0                           0.76451      0.00011     0.76466      0.76436
Alpha_loss                        -1.80768     0.00151     -1.80598     -1.80993
Training/policy_loss              -2.71651     0.00611     -2.70698     -2.72318
Training/qf1_loss                 6763.89023   780.11701   7631.95166   5626.69824
Training/qf2_loss                 15242.59023  1039.44077  16411.29492  13816.11914
Training/pf_norm                  0.10250      0.02310     0.13133      0.06181
Training/qf1_norm                 370.08951    230.07632   630.66156    86.04910
Training/qf2_norm                 573.63557    9.25716     584.03058    562.44257
log_std/mean                      -0.12990     0.00003     -0.12986     -0.12994
log_probs/mean                    -2.73455     0.00752     -2.72321     -2.74294
mean/mean                         -0.00409     0.00004     -0.00402     -0.00413
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019561052322387695
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69468
epoch first part time 2.86102294921875e-06
replay_buffer._size: [81600]
collect time 0.0008625984191894531
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.007395029067993164, 'sac_diff2': 0.008824825286865234, 'sac_diff3': 0.011118412017822266, 'sac_diff4': 0.007458686828613281, 'sac_diff5': 0.03311610221862793, 'sac_diff6': 0.00042891502380371094, 'all': 0.06855177879333496}
diff5_list [0.006891965866088867, 0.0072705745697021484, 0.006597757339477539, 0.0060787200927734375, 0.0062770843505859375]
time3 0
time4 0.0693509578704834
time5 0.06939935684204102
time7 4.76837158203125e-07
gen_weight_change tensor(-21.0247)
policy weight change tensor(37.8279, grad_fn=<SumBackward0>)
time8 0.002002716064453125
train_time 0.08085083961486816
eval time 0.15599966049194336
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:12,017 MainThread INFO: EPOCH:537
2024-01-23 01:03:12,017 MainThread INFO: Time Consumed:0.23998808860778809s
2024-01-23 01:03:12,017 MainThread INFO: Total Frames:81450s
  5%|▌         | 538/10000 [04:39<41:54,  3.76it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12537.40353
Train_Epoch_Reward                23152.68072
Running_Training_Average_Rewards  12218.86456
Explore_Time                      0.00086
Train___Time                      0.08085
Eval____Time                      0.15600
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12958.53596
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.12107     1.87658    89.97846     84.94410
alpha_0                           0.76413      0.00011    0.76428      0.76398
Alpha_loss                        -1.80946     0.00159    -1.80725     -1.81098
Training/policy_loss              -2.73643     0.00357    -2.73160     -2.74149
Training/qf1_loss                 6442.82783   550.56837  7245.51953   5682.99512
Training/qf2_loss                 14461.96543  762.97424  15453.16113  13338.48926
Training/pf_norm                  0.13233      0.02067    0.15963      0.11271
Training/qf1_norm                 1449.53970   343.15692  1962.56970   1076.89294
Training/qf2_norm                 576.37043    11.63732   593.13458    562.14081
log_std/mean                      -0.14026     0.00033    -0.13979     -0.14069
log_probs/mean                    -2.72865     0.00478    -2.72260     -2.73610
mean/mean                         -0.00271     0.00004    -0.00266     -0.00276
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01893901824951172
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69468
epoch first part time 2.86102294921875e-06
replay_buffer._size: [81750]
collect time 0.0008225440979003906
inner_dict_sum {'sac_diff0': 0.0002315044403076172, 'sac_diff1': 0.006489276885986328, 'sac_diff2': 0.007665395736694336, 'sac_diff3': 0.009637117385864258, 'sac_diff4': 0.006558656692504883, 'sac_diff5': 0.03053450584411621, 'sac_diff6': 0.0003788471221923828, 'all': 0.061495304107666016}
diff5_list [0.0063059329986572266, 0.0059528350830078125, 0.0062410831451416016, 0.006094217300415039, 0.005940437316894531]
time3 0
time4 0.0622105598449707
time5 0.06225442886352539
time7 4.76837158203125e-07
gen_weight_change tensor(-21.0247)
policy weight change tensor(37.7095, grad_fn=<SumBackward0>)
time8 0.001848459243774414
train_time 0.0731205940246582
eval time 0.15315461158752441
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:12,269 MainThread INFO: EPOCH:538
2024-01-23 01:03:12,269 MainThread INFO: Time Consumed:0.22933244705200195s
2024-01-23 01:03:12,269 MainThread INFO: Total Frames:81600s
  5%|▌         | 539/10000 [04:39<41:12,  3.83it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12654.61451
Train_Epoch_Reward                14504.74871
Running_Training_Average_Rewards  12582.48863
Explore_Time                      0.00082
Train___Time                      0.07312
Eval____Time                      0.15315
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13048.29598
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.98600     1.17908    90.68616     87.57271
alpha_0                           0.76375      0.00011    0.76390      0.76359
Alpha_loss                        -1.81446     0.00235    -1.81141     -1.81855
Training/policy_loss              -2.74371     0.00502    -2.73723     -2.75059
Training/qf1_loss                 7169.11592   579.58476  7836.27100   6114.37744
Training/qf2_loss                 15514.12754  502.21591  16236.24707  14730.35449
Training/pf_norm                  0.14141      0.01963    0.17373      0.11545
Training/qf1_norm                 1697.67920   189.75876  1951.76416   1445.02502
Training/qf2_norm                 607.67073    7.78887    618.93555    598.41846
log_std/mean                      -0.13169     0.00015    -0.13151     -0.13194
log_probs/mean                    -2.73473     0.00690    -2.72591     -2.74492
mean/mean                         -0.00355     0.00002    -0.00352     -0.00359
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018169164657592773
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69468
epoch first part time 2.384185791015625e-06
replay_buffer._size: [81900]
collect time 0.0007536411285400391
inner_dict_sum {'sac_diff0': 0.0002281665802001953, 'sac_diff1': 0.006443023681640625, 'sac_diff2': 0.007627964019775391, 'sac_diff3': 0.010038137435913086, 'sac_diff4': 0.006569385528564453, 'sac_diff5': 0.030752897262573242, 'sac_diff6': 0.0003662109375, 'all': 0.06202578544616699}
diff5_list [0.006030082702636719, 0.006203651428222656, 0.005821704864501953, 0.006812572479248047, 0.005884885787963867]
time3 0
time4 0.06274652481079102
time5 0.06278753280639648
time7 7.152557373046875e-07
gen_weight_change tensor(-21.0247)
policy weight change tensor(37.5700, grad_fn=<SumBackward0>)
time8 0.0017762184143066406
train_time 0.07332944869995117
eval time 0.15158843994140625
epoch last part time 4.0531158447265625e-06
2024-01-23 01:03:12,518 MainThread INFO: EPOCH:539
2024-01-23 01:03:12,518 MainThread INFO: Time Consumed:0.2278881072998047s
2024-01-23 01:03:12,518 MainThread INFO: Total Frames:81750s
  5%|▌         | 540/10000 [04:40<40:38,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12768.40047
Train_Epoch_Reward                14950.73496
Running_Training_Average_Rewards  12913.68146
Explore_Time                      0.00075
Train___Time                      0.07333
Eval____Time                      0.15159
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13024.25752
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.09450     3.89281     96.51109     86.24512
alpha_0                           0.76337      0.00011     0.76352      0.76321
Alpha_loss                        -1.81803     0.00169     -1.81621     -1.82047
Training/policy_loss              -2.71319     0.00489     -2.70711     -2.71942
Training/qf1_loss                 7107.14492   1703.52459  10305.23535  5658.67676
Training/qf2_loss                 15735.38145  2406.60740  20133.17773  13686.86133
Training/pf_norm                  0.12446      0.02259     0.16197      0.09429
Training/qf1_norm                 638.48594    531.77503   1596.11938   99.33348
Training/qf2_norm                 601.15258    25.64301    643.22784    575.99390
log_std/mean                      -0.12863     0.00021     -0.12834     -0.12896
log_probs/mean                    -2.73549     0.00628     -2.72812     -2.74361
mean/mean                         -0.00355     0.00014     -0.00332     -0.00371
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018309831619262695
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69468
epoch first part time 3.814697265625e-06
replay_buffer._size: [82050]
collect time 0.0008149147033691406
inside mustsac before update, task 0, sumup 69468
inside mustsac after update, task 0, sumup 70852
inner_dict_sum {'sac_diff0': 0.00019502639770507812, 'sac_diff1': 0.0063550472259521484, 'sac_diff2': 0.0076024532318115234, 'sac_diff3': 0.009830236434936523, 'sac_diff4': 0.006787776947021484, 'sac_diff5': 0.04737091064453125, 'sac_diff6': 0.00039267539978027344, 'all': 0.07853412628173828}
diff5_list [0.009798526763916016, 0.009433507919311523, 0.00973367691040039, 0.0090789794921875, 0.00932621955871582]
time3 0.0008175373077392578
time4 0.07931709289550781
time5 0.07936429977416992
time7 0.008896589279174805
gen_weight_change tensor(-20.6773)
policy weight change tensor(37.4328, grad_fn=<SumBackward0>)
time8 0.0025207996368408203
train_time 0.10820341110229492
eval time 0.11832380294799805
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:12,769 MainThread INFO: EPOCH:540
2024-01-23 01:03:12,770 MainThread INFO: Time Consumed:0.2296140193939209s
2024-01-23 01:03:12,770 MainThread INFO: Total Frames:81900s
  5%|▌         | 541/10000 [04:40<40:31,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12879.92418
Train_Epoch_Reward                22863.55511
Running_Training_Average_Rewards  13375.01834
Explore_Time                      0.00081
Train___Time                      0.10820
Eval____Time                      0.11832
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12994.06907
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.17237     1.52759     91.90207     87.84052
alpha_0                           0.76298      0.00011     0.76314      0.76283
Alpha_loss                        -1.82115     0.00052     -1.82049     -1.82188
Training/policy_loss              -2.69041     0.02960     -2.64413     -2.73358
Training/qf1_loss                 7784.28457   1233.02675  10008.28516  6848.61475
Training/qf2_loss                 16401.54102  1448.99148  18936.45703  15158.93555
Training/pf_norm                  0.08729      0.02205     0.12154      0.06382
Training/qf1_norm                 1164.16770   734.15228   2367.43262   289.30804
Training/qf2_norm                 559.84687    22.51404    594.18170    533.88947
log_std/mean                      -0.13548     0.00779     -0.12157     -0.14386
log_probs/mean                    -2.73457     0.00361     -2.73007     -2.74077
mean/mean                         -0.00254     0.00057     -0.00193     -0.00339
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01937413215637207
epoch last part time3 0.002660036087036133
inside rlalgo, task 0, sumup 70852
epoch first part time 2.86102294921875e-06
replay_buffer._size: [82200]
collect time 0.0008063316345214844
inner_dict_sum {'sac_diff0': 0.0002372264862060547, 'sac_diff1': 0.00647425651550293, 'sac_diff2': 0.007552623748779297, 'sac_diff3': 0.009686946868896484, 'sac_diff4': 0.006494760513305664, 'sac_diff5': 0.030167102813720703, 'sac_diff6': 0.0003676414489746094, 'all': 0.06098055839538574}
diff5_list [0.006269931793212891, 0.006006002426147461, 0.00590205192565918, 0.005876302719116211, 0.006112813949584961]
time3 0
time4 0.06169724464416504
time5 0.06173872947692871
time7 4.76837158203125e-07
gen_weight_change tensor(-20.6773)
policy weight change tensor(37.4213, grad_fn=<SumBackward0>)
time8 0.0017592906951904297
train_time 0.07214522361755371
eval time 0.15638232231140137
epoch last part time 4.291534423828125e-06
2024-01-23 01:03:13,026 MainThread INFO: EPOCH:541
2024-01-23 01:03:13,027 MainThread INFO: Time Consumed:0.23150944709777832s
2024-01-23 01:03:13,027 MainThread INFO: Total Frames:82050s
  5%|▌         | 542/10000 [04:40<40:18,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12830.83343
Train_Epoch_Reward                12638.33944
Running_Training_Average_Rewards  13530.11223
Explore_Time                      0.00080
Train___Time                      0.07215
Eval____Time                      0.15638
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12157.86765
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.99965     1.27567     90.85433     86.89492
alpha_0                           0.76260      0.00011     0.76275      0.76245
Alpha_loss                        -1.82430     0.00222     -1.82088     -1.82741
Training/policy_loss              -2.69257     0.00635     -2.68517     -2.70137
Training/qf1_loss                 6175.16748   1088.38835  7528.14355   4497.93311
Training/qf2_loss                 14587.29785  1307.46068  16044.28125  12498.64258
Training/pf_norm                  0.10418      0.02505     0.13625      0.07220
Training/qf1_norm                 253.37849    171.69221   574.87018    88.24986
Training/qf2_norm                 553.21638    7.82511     564.25372    540.05505
log_std/mean                      -0.12430     0.00005     -0.12424     -0.12438
log_probs/mean                    -2.73376     0.00835     -2.72363     -2.74524
mean/mean                         -0.00201     0.00003     -0.00197     -0.00204
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018111228942871094
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70852
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [82350]
collect time 0.0008013248443603516
inner_dict_sum {'sac_diff0': 0.00023174285888671875, 'sac_diff1': 0.0060884952545166016, 'sac_diff2': 0.0072269439697265625, 'sac_diff3': 0.009494304656982422, 'sac_diff4': 0.006395578384399414, 'sac_diff5': 0.03023552894592285, 'sac_diff6': 0.0003771781921386719, 'all': 0.06004977226257324}
diff5_list [0.0061435699462890625, 0.0059299468994140625, 0.00588226318359375, 0.006253957748413086, 0.006025791168212891]
time3 0
time4 0.06076955795288086
time5 0.06081032752990723
time7 4.76837158203125e-07
gen_weight_change tensor(-20.6773)
policy weight change tensor(37.4738, grad_fn=<SumBackward0>)
time8 0.0017423629760742188
train_time 0.07122039794921875
eval time 0.15880393981933594
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:13,281 MainThread INFO: EPOCH:542
2024-01-23 01:03:13,281 MainThread INFO: Time Consumed:0.23306632041931152s
2024-01-23 01:03:13,281 MainThread INFO: Total Frames:82200s
  5%|▌         | 543/10000 [04:40<40:15,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12775.75545
Train_Epoch_Reward                11963.18387
Running_Training_Average_Rewards  13329.95869
Explore_Time                      0.00080
Train___Time                      0.07122
Eval____Time                      0.15880
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12123.60923
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.32430     1.19522    89.26076     85.51884
alpha_0                           0.76222      0.00011    0.76237      0.76207
Alpha_loss                        -1.82611     0.00106    -1.82453     -1.82786
Training/policy_loss              -2.67606     0.00336    -2.67195     -2.68151
Training/qf1_loss                 6001.94424   151.23875  6178.60645   5739.12354
Training/qf2_loss                 13988.80996  305.49835  14493.84668  13615.39062
Training/pf_norm                  0.10198      0.03420    0.16531      0.06721
Training/qf1_norm                 2381.78315   233.53607  2710.82935   1992.53149
Training/qf2_norm                 529.55342    6.80852    540.65631    519.51874
log_std/mean                      -0.13029     0.00011    -0.13012     -0.13043
log_probs/mean                    -2.72802     0.00397    -2.72320     -2.73446
mean/mean                         -0.00340     0.00005    -0.00335     -0.00350
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018184423446655273
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70852
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [82500]
collect time 0.0008182525634765625
inner_dict_sum {'sac_diff0': 0.0002257823944091797, 'sac_diff1': 0.006397247314453125, 'sac_diff2': 0.007517576217651367, 'sac_diff3': 0.009911298751831055, 'sac_diff4': 0.006680727005004883, 'sac_diff5': 0.031324148178100586, 'sac_diff6': 0.0003705024719238281, 'all': 0.06242728233337402}
diff5_list [0.006899595260620117, 0.005938053131103516, 0.006002187728881836, 0.006532192230224609, 0.005952119827270508]
time3 0
time4 0.06316208839416504
time5 0.06320524215698242
time7 7.152557373046875e-07
gen_weight_change tensor(-20.6773)
policy weight change tensor(37.5417, grad_fn=<SumBackward0>)
time8 0.001795053482055664
train_time 0.07370877265930176
eval time 0.14889907836914062
epoch last part time 4.291534423828125e-06
2024-01-23 01:03:13,528 MainThread INFO: EPOCH:543
2024-01-23 01:03:13,528 MainThread INFO: Time Consumed:0.22564172744750977s
2024-01-23 01:03:13,529 MainThread INFO: Total Frames:82350s
  5%|▌         | 544/10000 [04:41<39:51,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12710.40150
Train_Epoch_Reward                9506.83642
Running_Training_Average_Rewards  13518.34156
Explore_Time                      0.00081
Train___Time                      0.07371
Eval____Time                      0.14890
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12096.22268
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.52928     0.80189     90.93454     88.57527
alpha_0                           0.76184      0.00011     0.76199      0.76169
Alpha_loss                        -1.83093     0.00120     -1.82907     -1.83246
Training/policy_loss              -2.67812     0.00188     -2.67539     -2.68127
Training/qf1_loss                 6584.82441   1238.16999  7887.14453   4824.68896
Training/qf2_loss                 15146.96543  1375.52753  16712.72266  13191.42676
Training/pf_norm                  0.09881      0.01042     0.11511      0.08465
Training/qf1_norm                 192.70025    92.91371    359.69983    108.59861
Training/qf2_norm                 552.84088    4.92159     561.15515    546.44025
log_std/mean                      -0.13330     0.00017     -0.13303     -0.13349
log_probs/mean                    -2.73336     0.00259     -2.73019     -2.73777
mean/mean                         -0.00140     0.00001     -0.00139     -0.00142
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01785445213317871
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70852
epoch first part time 2.384185791015625e-06
replay_buffer._size: [82650]
collect time 0.0008070468902587891
inner_dict_sum {'sac_diff0': 0.00025153160095214844, 'sac_diff1': 0.006430387496948242, 'sac_diff2': 0.007439851760864258, 'sac_diff3': 0.009464502334594727, 'sac_diff4': 0.006403923034667969, 'sac_diff5': 0.030367612838745117, 'sac_diff6': 0.0003688335418701172, 'all': 0.06072664260864258}
diff5_list [0.006302356719970703, 0.006032705307006836, 0.005963563919067383, 0.006238698959350586, 0.005830287933349609]
time3 0
time4 0.06144070625305176
time5 0.06148242950439453
time7 4.76837158203125e-07
gen_weight_change tensor(-20.6773)
policy weight change tensor(37.6654, grad_fn=<SumBackward0>)
time8 0.0018248558044433594
train_time 0.0721139907836914
eval time 0.1531078815460205
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:13,778 MainThread INFO: EPOCH:544
2024-01-23 01:03:13,778 MainThread INFO: Time Consumed:0.22833538055419922s
2024-01-23 01:03:13,778 MainThread INFO: Total Frames:82500s
  5%|▌         | 545/10000 [04:41<39:47,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12628.74048
Train_Epoch_Reward                10394.73442
Running_Training_Average_Rewards  13462.50178
Explore_Time                      0.00080
Train___Time                      0.07211
Eval____Time                      0.15311
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12045.94474
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.44259     0.88726    90.83629     88.42143
alpha_0                           0.76146      0.00011    0.76161      0.76131
Alpha_loss                        -1.83366     0.00213    -1.83154     -1.83629
Training/policy_loss              -2.73256     0.00350    -2.72804     -2.73744
Training/qf1_loss                 6566.10049   384.80438  7031.99902   6058.08496
Training/qf2_loss                 15052.25879  490.60891  15802.62207  14466.96387
Training/pf_norm                  0.09263      0.01507    0.10528      0.06364
Training/qf1_norm                 1337.85874   196.40334  1578.11401   1047.10718
Training/qf2_norm                 594.60405    5.86658    603.93158    587.92841
log_std/mean                      -0.12771     0.00026    -0.12739     -0.12810
log_probs/mean                    -2.73103     0.00471    -2.72571     -2.73756
mean/mean                         -0.00239     0.00009    -0.00228     -0.00252
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01976180076599121
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70852
epoch first part time 2.86102294921875e-06
replay_buffer._size: [82800]
collect time 0.00081634521484375
inside mustsac before update, task 0, sumup 70852
inside mustsac after update, task 0, sumup 70773
inner_dict_sum {'sac_diff0': 0.00019621849060058594, 'sac_diff1': 0.006418943405151367, 'sac_diff2': 0.00754547119140625, 'sac_diff3': 0.009605169296264648, 'sac_diff4': 0.006910800933837891, 'sac_diff5': 0.047452449798583984, 'sac_diff6': 0.00038695335388183594, 'all': 0.07851600646972656}
diff5_list [0.010094404220581055, 0.00920724868774414, 0.009898662567138672, 0.009061813354492188, 0.00919032096862793]
time3 0.0008347034454345703
time4 0.0793464183807373
time5 0.07939434051513672
time7 0.008971691131591797
gen_weight_change tensor(-20.4244)
policy weight change tensor(37.5995, grad_fn=<SumBackward0>)
time8 0.0018324851989746094
train_time 0.1079103946685791
eval time 0.12704110145568848
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:14,039 MainThread INFO: EPOCH:545
2024-01-23 01:03:14,039 MainThread INFO: Time Consumed:0.23799824714660645s
2024-01-23 01:03:14,040 MainThread INFO: Total Frames:82650s
  5%|▌         | 546/10000 [04:41<40:07,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12528.65163
Train_Epoch_Reward                7787.59817
Running_Training_Average_Rewards  13565.98001
Explore_Time                      0.00081
Train___Time                      0.10791
Eval____Time                      0.12704
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11985.75544
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.14413     1.53389     92.43414     88.03923
alpha_0                           0.76108      0.00011     0.76123      0.76093
Alpha_loss                        -1.83813     0.00142     -1.83652     -1.83994
Training/policy_loss              -2.71745     0.06445     -2.63621     -2.80467
Training/qf1_loss                 6792.35820   1118.14826  8379.60156   5149.71436
Training/qf2_loss                 15361.82305  1480.23312  17475.64062  13278.65918
Training/pf_norm                  0.09963      0.01624     0.12596      0.08156
Training/qf1_norm                 1392.37866   898.51861   2678.52881   385.36169
Training/qf2_norm                 585.90785    47.75648    636.99982    512.48932
log_std/mean                      -0.12600     0.00672     -0.12070     -0.13917
log_probs/mean                    -2.73508     0.00377     -2.72919     -2.73924
mean/mean                         -0.00312     0.00074     -0.00209     -0.00386
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018255233764648438
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70773
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [82950]
collect time 0.0009725093841552734
inner_dict_sum {'sac_diff0': 0.00021886825561523438, 'sac_diff1': 0.0072154998779296875, 'sac_diff2': 0.008474111557006836, 'sac_diff3': 0.01094365119934082, 'sac_diff4': 0.0071718692779541016, 'sac_diff5': 0.032869577407836914, 'sac_diff6': 0.0003979206085205078, 'all': 0.0672914981842041}
diff5_list [0.007329225540161133, 0.006958723068237305, 0.006354331970214844, 0.00614476203918457, 0.0060825347900390625]
time3 0
time4 0.06803703308105469
time5 0.06808280944824219
time7 4.76837158203125e-07
gen_weight_change tensor(-20.4244)
policy weight change tensor(37.6293, grad_fn=<SumBackward0>)
time8 0.0018036365509033203
train_time 0.07902240753173828
eval time 0.1742105484008789
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:14,318 MainThread INFO: EPOCH:546
2024-01-23 01:03:14,318 MainThread INFO: Time Consumed:0.25664258003234863s
2024-01-23 01:03:14,318 MainThread INFO: Total Frames:82800s
  5%|▌         | 547/10000 [04:41<41:15,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12517.24118
Train_Epoch_Reward                8149.30808
Running_Training_Average_Rewards  13654.38172
Explore_Time                      0.00097
Train___Time                      0.07902
Eval____Time                      0.17421
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12737.85349
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.56734     1.74092     91.55041     86.68651
alpha_0                           0.76070      0.00011     0.76085      0.76055
Alpha_loss                        -1.84121     0.00220     -1.83879     -1.84499
Training/policy_loss              -2.71772     0.00395     -2.71113     -2.72346
Training/qf1_loss                 7053.62832   817.29000   7873.84473   5768.80127
Training/qf2_loss                 15506.04043  1046.97535  16584.72656  14089.58105
Training/pf_norm                  0.11338      0.01626     0.14022      0.08972
Training/qf1_norm                 1432.14504   334.03482   1813.08875   885.30139
Training/qf2_norm                 595.54635    11.54116    608.90820    576.47150
log_std/mean                      -0.14035     0.00004     -0.14027     -0.14040
log_probs/mean                    -2.73405     0.00568     -2.72519     -2.74292
mean/mean                         -0.00344     0.00001     -0.00343     -0.00346
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01853346824645996
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70773
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [83100]
collect time 0.0008075237274169922
inner_dict_sum {'sac_diff0': 0.00020551681518554688, 'sac_diff1': 0.00725102424621582, 'sac_diff2': 0.008627176284790039, 'sac_diff3': 0.011056184768676758, 'sac_diff4': 0.00748443603515625, 'sac_diff5': 0.032787322998046875, 'sac_diff6': 0.0004165172576904297, 'all': 0.06782817840576172}
diff5_list [0.007287263870239258, 0.0066280364990234375, 0.006494045257568359, 0.0062160491943359375, 0.006161928176879883]
time3 0
time4 0.06862449645996094
time5 0.06867027282714844
time7 4.76837158203125e-07
gen_weight_change tensor(-20.4244)
policy weight change tensor(37.6963, grad_fn=<SumBackward0>)
time8 0.0020067691802978516
train_time 0.08007478713989258
eval time 0.16972827911376953
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:14,593 MainThread INFO: EPOCH:547
2024-01-23 01:03:14,593 MainThread INFO: Time Consumed:0.2528820037841797s
2024-01-23 01:03:14,593 MainThread INFO: Total Frames:82950s
  5%|▌         | 548/10000 [04:42<41:51,  3.76it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12484.11743
Train_Epoch_Reward                20241.35624
Running_Training_Average_Rewards  13856.09782
Explore_Time                      0.00080
Train___Time                      0.08007
Eval____Time                      0.16973
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12627.29849
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.08307     2.23336     93.74032     87.18768
alpha_0                           0.76032      0.00011     0.76047      0.76017
Alpha_loss                        -1.84420     0.00084     -1.84326     -1.84554
Training/policy_loss              -2.75535     0.00160     -2.75225     -2.75680
Training/qf1_loss                 7033.23691   882.32374   8050.96875   5741.45215
Training/qf2_loss                 15796.88809  1256.26069  17280.08203  13831.17676
Training/pf_norm                  0.12020      0.02197     0.14873      0.08996
Training/qf1_norm                 1650.35212   436.06082   2152.14185   862.21985
Training/qf2_norm                 600.41394    13.70593    616.51489    576.40527
log_std/mean                      -0.12805     0.00017     -0.12783     -0.12830
log_probs/mean                    -2.73268     0.00188     -2.72973     -2.73488
mean/mean                         -0.00420     0.00000     -0.00420     -0.00420
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018174409866333008
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70773
epoch first part time 2.384185791015625e-06
replay_buffer._size: [83250]
collect time 0.0007565021514892578
inner_dict_sum {'sac_diff0': 0.0002300739288330078, 'sac_diff1': 0.00661778450012207, 'sac_diff2': 0.007669925689697266, 'sac_diff3': 0.009745359420776367, 'sac_diff4': 0.0065495967864990234, 'sac_diff5': 0.03103184700012207, 'sac_diff6': 0.00039649009704589844, 'all': 0.0622410774230957}
diff5_list [0.006436824798583984, 0.006218671798706055, 0.006085634231567383, 0.006220340728759766, 0.006070375442504883]
time3 0
time4 0.06297469139099121
time5 0.0630197525024414
time7 7.152557373046875e-07
gen_weight_change tensor(-20.4244)
policy weight change tensor(37.8661, grad_fn=<SumBackward0>)
time8 0.0017960071563720703
train_time 0.07369685173034668
eval time 0.1621685028076172
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:14,853 MainThread INFO: EPOCH:548
2024-01-23 01:03:14,853 MainThread INFO: Time Consumed:0.23903322219848633s
2024-01-23 01:03:14,854 MainThread INFO: Total Frames:83100s
  5%|▌         | 549/10000 [04:42<41:36,  3.79it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12432.24841
Train_Epoch_Reward                6630.87881
Running_Training_Average_Rewards  13616.99255
Explore_Time                      0.00075
Train___Time                      0.07370
Eval____Time                      0.16217
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12529.60580
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.39189     2.48118     92.17401     84.68221
alpha_0                           0.75994      0.00011     0.76009      0.75979
Alpha_loss                        -1.84668     0.00232     -1.84236     -1.84877
Training/policy_loss              -2.69242     0.00579     -2.68434     -2.70034
Training/qf1_loss                 6517.80020   708.55573   7353.08203   5601.37109
Training/qf2_loss                 14710.18477  1048.34213  15682.31738  13190.91602
Training/pf_norm                  0.12429      0.01704     0.14813      0.09489
Training/qf1_norm                 1927.95249   440.84472   2625.23486   1314.75183
Training/qf2_norm                 566.42045    15.25684    589.00854    543.13245
log_std/mean                      -0.12330     0.00031     -0.12289     -0.12377
log_probs/mean                    -2.72943     0.00743     -2.71858     -2.73952
mean/mean                         -0.00175     0.00001     -0.00174     -0.00176
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018393993377685547
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70773
epoch first part time 3.337860107421875e-06
replay_buffer._size: [83400]
collect time 0.0009868144989013672
inner_dict_sum {'sac_diff0': 0.00022482872009277344, 'sac_diff1': 0.006924629211425781, 'sac_diff2': 0.008072376251220703, 'sac_diff3': 0.010616064071655273, 'sac_diff4': 0.007090330123901367, 'sac_diff5': 0.03197669982910156, 'sac_diff6': 0.00039958953857421875, 'all': 0.06530451774597168}
diff5_list [0.0066187381744384766, 0.006745576858520508, 0.0062029361724853516, 0.0062656402587890625, 0.006143808364868164]
time3 0
time4 0.06612563133239746
time5 0.0661764144897461
time7 4.76837158203125e-07
gen_weight_change tensor(-20.4244)
policy weight change tensor(38.0785, grad_fn=<SumBackward0>)
time8 0.001850128173828125
train_time 0.07740545272827148
eval time 0.15031671524047852
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:15,106 MainThread INFO: EPOCH:549
2024-01-23 01:03:15,106 MainThread INFO: Time Consumed:0.23095107078552246s
2024-01-23 01:03:15,106 MainThread INFO: Total Frames:83250s
  6%|▌         | 550/10000 [04:42<44:54,  3.51it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12382.27036
Train_Epoch_Reward                8894.39378
Running_Training_Average_Rewards  12653.56457
Explore_Time                      0.00098
Train___Time                      0.07741
Eval____Time                      0.15032
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12524.47699
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.73009     2.42375     92.11202     85.34425
alpha_0                           0.75956      0.00011     0.75971      0.75941
Alpha_loss                        -1.85077     0.00184     -1.84802     -1.85295
Training/policy_loss              -2.74727     0.00424     -2.73967     -2.75142
Training/qf1_loss                 6753.49932   853.59452   8104.59619   5905.08887
Training/qf2_loss                 15100.32207  1242.01698  17041.51758  13668.25293
Training/pf_norm                  0.10221      0.01941     0.13246      0.08215
Training/qf1_norm                 1081.93485   437.65404   1760.85449   518.60767
Training/qf2_norm                 602.62793    15.92856    624.67615    580.19006
log_std/mean                      -0.11849     0.00038     -0.11800     -0.11906
log_probs/mean                    -2.73209     0.00565     -2.72206     -2.73757
mean/mean                         -0.00286     0.00004     -0.00281     -0.00292
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.09959220886230469
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70773
epoch first part time 2.86102294921875e-06
replay_buffer._size: [83550]
collect time 0.0008478164672851562
inside mustsac before update, task 0, sumup 70773
inside mustsac after update, task 0, sumup 71199
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.006648063659667969, 'sac_diff2': 0.007893562316894531, 'sac_diff3': 0.009891748428344727, 'sac_diff4': 0.007029533386230469, 'sac_diff5': 0.04769539833068848, 'sac_diff6': 0.0003848075866699219, 'all': 0.07974815368652344}
diff5_list [0.010031461715698242, 0.009684085845947266, 0.009593009948730469, 0.009199857711791992, 0.009186983108520508]
time3 0.000827789306640625
time4 0.08057093620300293
time5 0.08062577247619629
time7 0.009414434432983398
gen_weight_change tensor(-20.3269)
policy weight change tensor(38.0549, grad_fn=<SumBackward0>)
time8 0.0025796890258789062
train_time 0.1106269359588623
eval time 0.035871267318725586
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:15,359 MainThread INFO: EPOCH:550
2024-01-23 01:03:15,359 MainThread INFO: Time Consumed:0.149505615234375s
2024-01-23 01:03:15,359 MainThread INFO: Total Frames:83400s
  6%|▌         | 551/10000 [04:42<39:42,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12331.97810
Train_Epoch_Reward                19046.66711
Running_Training_Average_Rewards  12920.94027
Explore_Time                      0.00084
Train___Time                      0.11063
Eval____Time                      0.03587
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12491.14654
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.58070     1.72412     90.16842     86.18580
alpha_0                           0.75918      0.00011     0.75933      0.75903
Alpha_loss                        -1.85470     0.00270     -1.85173     -1.85862
Training/policy_loss              -2.75432     0.07621     -2.68592     -2.87669
Training/qf1_loss                 6802.08018   1013.07910  8115.18506   5116.89160
Training/qf2_loss                 15134.77441  1315.97028  16775.18164  13006.94727
Training/pf_norm                  0.08511      0.02976     0.12171      0.05151
Training/qf1_norm                 695.35761    465.01921   1324.87512   127.74378
Training/qf2_norm                 599.29448    56.09533    700.11871    547.63269
log_std/mean                      -0.13735     0.00298     -0.13251     -0.14081
log_probs/mean                    -2.73413     0.00956     -2.72076     -2.74836
mean/mean                         -0.00341     0.00097     -0.00242     -0.00504
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019125699996948242
epoch last part time3 0.0027472972869873047
inside rlalgo, task 0, sumup 71199
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [83700]
collect time 0.0008242130279541016
inner_dict_sum {'sac_diff0': 0.00022220611572265625, 'sac_diff1': 0.006368875503540039, 'sac_diff2': 0.007594108581542969, 'sac_diff3': 0.010131597518920898, 'sac_diff4': 0.006883382797241211, 'sac_diff5': 0.03133821487426758, 'sac_diff6': 0.00038623809814453125, 'all': 0.06292462348937988}
diff5_list [0.0066564083099365234, 0.006117105484008789, 0.006059885025024414, 0.0062749385833740234, 0.006229877471923828]
time3 0
time4 0.06366252899169922
time5 0.06370687484741211
time7 7.152557373046875e-07
gen_weight_change tensor(-20.3269)
policy weight change tensor(38.2949, grad_fn=<SumBackward0>)
time8 0.0019235610961914062
train_time 0.07449126243591309
eval time 0.14635252952575684
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:15,607 MainThread INFO: EPOCH:551
2024-01-23 01:03:15,608 MainThread INFO: Time Consumed:0.2238905429840088s
2024-01-23 01:03:15,608 MainThread INFO: Total Frames:83550s
  6%|▌         | 552/10000 [04:43<39:21,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12329.51038
Train_Epoch_Reward                24169.45519
Running_Training_Average_Rewards  13213.33999
Explore_Time                      0.00082
Train___Time                      0.07449
Eval____Time                      0.14635
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12133.19044
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.69056     2.87362     93.77672     85.03642
alpha_0                           0.75880      0.00011     0.75895      0.75865
Alpha_loss                        -1.85790     0.00231     -1.85397     -1.86040
Training/policy_loss              -2.66644     0.00567     -2.65765     -2.67325
Training/qf1_loss                 6780.97471   1072.94673  8163.06787   4928.55664
Training/qf2_loss                 15349.61641  1638.19443  17525.58008  12563.25000
Training/pf_norm                  0.10221      0.02395     0.13034      0.07470
Training/qf1_norm                 672.49396    462.23084   1523.79211   271.00809
Training/qf2_norm                 544.73601    16.86442    569.13824    517.66925
log_std/mean                      -0.12115     0.00038     -0.12066     -0.12173
log_probs/mean                    -2.73353     0.00759     -2.72172     -2.74258
mean/mean                         -0.00378     0.00004     -0.00373     -0.00384
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0179898738861084
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71199
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [83850]
collect time 0.0008046627044677734
inner_dict_sum {'sac_diff0': 0.00023126602172851562, 'sac_diff1': 0.006483793258666992, 'sac_diff2': 0.00762486457824707, 'sac_diff3': 0.009864091873168945, 'sac_diff4': 0.0066225528717041016, 'sac_diff5': 0.03058338165283203, 'sac_diff6': 0.00038814544677734375, 'all': 0.061798095703125}
diff5_list [0.0062673091888427734, 0.0061910152435302734, 0.005869150161743164, 0.0061492919921875, 0.00610661506652832]
time3 0
time4 0.0625464916229248
time5 0.06259036064147949
time7 4.76837158203125e-07
gen_weight_change tensor(-20.3269)
policy weight change tensor(38.4413, grad_fn=<SumBackward0>)
time8 0.0018799304962158203
train_time 0.0732734203338623
eval time 0.15996170043945312
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:15,865 MainThread INFO: EPOCH:552
2024-01-23 01:03:15,865 MainThread INFO: Time Consumed:0.23629403114318848s
2024-01-23 01:03:15,866 MainThread INFO: Total Frames:83700s
  6%|▌         | 553/10000 [04:43<39:43,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12320.19616
Train_Epoch_Reward                7357.29080
Running_Training_Average_Rewards  12876.42239
Explore_Time                      0.00080
Train___Time                      0.07327
Eval____Time                      0.15996
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12030.46704
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.04557     0.78214    90.54546     88.42416
alpha_0                           0.75842      0.00011    0.75857      0.75827
Alpha_loss                        -1.86180     0.00075    -1.86080     -1.86267
Training/policy_loss              -2.77552     0.00201    -2.77204     -2.77747
Training/qf1_loss                 6538.79531   589.65078  7503.43066   5689.07080
Training/qf2_loss                 14892.33477  531.50603  15856.77637  14333.48633
Training/pf_norm                  0.10762      0.01866    0.13612      0.08232
Training/qf1_norm                 2111.08176   127.62314  2249.86499   1869.73279
Training/qf2_norm                 617.42174    5.20740    627.46179    613.12897
log_std/mean                      -0.13265     0.00013    -0.13245     -0.13281
log_probs/mean                    -2.73544     0.00241    -2.73136     -2.73782
mean/mean                         -0.00413     0.00002    -0.00411     -0.00415
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01810145378112793
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71199
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [84000]
collect time 0.0009253025054931641
inner_dict_sum {'sac_diff0': 0.0002357959747314453, 'sac_diff1': 0.006627082824707031, 'sac_diff2': 0.007525920867919922, 'sac_diff3': 0.009762287139892578, 'sac_diff4': 0.006828784942626953, 'sac_diff5': 0.0315701961517334, 'sac_diff6': 0.0003764629364013672, 'all': 0.0629265308380127}
diff5_list [0.00652623176574707, 0.0061817169189453125, 0.006304502487182617, 0.006411075592041016, 0.006146669387817383]
time3 0
time4 0.06368851661682129
time5 0.06373143196105957
time7 4.76837158203125e-07
gen_weight_change tensor(-20.3269)
policy weight change tensor(38.5296, grad_fn=<SumBackward0>)
time8 0.0017685890197753906
train_time 0.07447052001953125
eval time 0.14942669868469238
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:16,114 MainThread INFO: EPOCH:553
2024-01-23 01:03:16,114 MainThread INFO: Time Consumed:0.22705578804016113s
2024-01-23 01:03:16,114 MainThread INFO: Total Frames:83850s
  6%|▌         | 554/10000 [04:43<39:33,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12312.76819
Train_Epoch_Reward                11744.84197
Running_Training_Average_Rewards  12869.85842
Explore_Time                      0.00092
Train___Time                      0.07447
Eval____Time                      0.14943
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12021.94295
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.26222     1.72188     92.42861     87.67094
alpha_0                           0.75804      0.00011     0.75819      0.75789
Alpha_loss                        -1.86583     0.00124     -1.86477     -1.86799
Training/policy_loss              -2.68586     0.00547     -2.67902     -2.69211
Training/qf1_loss                 7256.91914   925.29704   8311.26758   5775.67969
Training/qf2_loss                 15935.42988  1238.46066  17401.31445  13974.42773
Training/pf_norm                  0.09116      0.02626     0.13829      0.05906
Training/qf1_norm                 576.81378    329.69228   1019.59369   94.28650
Training/qf2_norm                 567.45109    11.07745    581.16211    550.46301
log_std/mean                      -0.13472     0.00016     -0.13457     -0.13498
log_probs/mean                    -2.73786     0.00671     -2.72920     -2.74564
mean/mean                         -0.00101     0.00003     -0.00099     -0.00105
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01822352409362793
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71199
epoch first part time 3.337860107421875e-06
replay_buffer._size: [84150]
collect time 0.0009720325469970703
inner_dict_sum {'sac_diff0': 0.00023317337036132812, 'sac_diff1': 0.006805419921875, 'sac_diff2': 0.0076143741607666016, 'sac_diff3': 0.009812116622924805, 'sac_diff4': 0.0065839290618896484, 'sac_diff5': 0.03274106979370117, 'sac_diff6': 0.0004055500030517578, 'all': 0.06419563293457031}
diff5_list [0.006632566452026367, 0.005879640579223633, 0.006257534027099609, 0.007362842559814453, 0.006608486175537109]
time3 0
time4 0.06497788429260254
time5 0.06502485275268555
time7 7.152557373046875e-07
gen_weight_change tensor(-20.3269)
policy weight change tensor(38.5965, grad_fn=<SumBackward0>)
time8 0.0019752979278564453
train_time 0.07646441459655762
eval time 0.15870094299316406
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:16,374 MainThread INFO: EPOCH:554
2024-01-23 01:03:16,374 MainThread INFO: Time Consumed:0.2385098934173584s
2024-01-23 01:03:16,375 MainThread INFO: Total Frames:84000s
  6%|▌         | 555/10000 [04:43<40:01,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12311.86721
Train_Epoch_Reward                21869.53758
Running_Training_Average_Rewards  13384.57454
Explore_Time                      0.00097
Train___Time                      0.07646
Eval____Time                      0.15870
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12036.93490
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.84461     3.08144     95.94192     87.49728
alpha_0                           0.75766      0.00011     0.75781      0.75751
Alpha_loss                        -1.86843     0.00132     -1.86709     -1.87070
Training/policy_loss              -2.75013     0.00194     -2.74710     -2.75204
Training/qf1_loss                 6712.45664   1174.64848  8883.66699   5642.09766
Training/qf2_loss                 15266.26445  1706.62913  18521.05859  13841.01855
Training/pf_norm                  0.12839      0.01018     0.14102      0.11540
Training/qf1_norm                 1300.88973   564.49322   2416.69971   878.44421
Training/qf2_norm                 602.52813    20.37209    642.90466    587.45319
log_std/mean                      -0.13223     0.00013     -0.13208     -0.13239
log_probs/mean                    -2.73505     0.00290     -2.73070     -2.73838
mean/mean                         -0.00315     0.00006     -0.00308     -0.00324
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019445419311523438
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71199
epoch first part time 2.86102294921875e-06
replay_buffer._size: [84300]
collect time 0.0009546279907226562
inside mustsac before update, task 0, sumup 71199
inside mustsac after update, task 0, sumup 70697
inner_dict_sum {'sac_diff0': 0.00021147727966308594, 'sac_diff1': 0.006970643997192383, 'sac_diff2': 0.008295774459838867, 'sac_diff3': 0.010135412216186523, 'sac_diff4': 0.006952047348022461, 'sac_diff5': 0.048255205154418945, 'sac_diff6': 0.0004138946533203125, 'all': 0.08123445510864258}
diff5_list [0.01043391227722168, 0.009332418441772461, 0.0097198486328125, 0.009429693222045898, 0.009339332580566406]
time3 0.0008170604705810547
time4 0.08204841613769531
time5 0.08209609985351562
time7 0.00905919075012207
gen_weight_change tensor(-20.1917)
policy weight change tensor(38.4950, grad_fn=<SumBackward0>)
time8 0.001802682876586914
train_time 0.1108555793762207
eval time 0.1148533821105957
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:16,626 MainThread INFO: EPOCH:555
2024-01-23 01:03:16,626 MainThread INFO: Time Consumed:0.22884559631347656s
2024-01-23 01:03:16,626 MainThread INFO: Total Frames:84150s
  6%|▌         | 556/10000 [04:44<39:50,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12316.08210
Train_Epoch_Reward                12193.42748
Running_Training_Average_Rewards  13358.12762
Explore_Time                      0.00095
Train___Time                      0.11086
Eval____Time                      0.11485
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12027.90438
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.76441     2.71556     94.85996     86.87708
alpha_0                           0.75728      0.00011     0.75743      0.75713
Alpha_loss                        -1.87179     0.00227     -1.86754     -1.87383
Training/policy_loss              -2.70539     0.02010     -2.66918     -2.73113
Training/qf1_loss                 7306.12275   1706.58700  9917.17480   5549.33691
Training/qf2_loss                 15859.46992  2118.13483  19461.93359  13574.09375
Training/pf_norm                  0.12796      0.01396     0.14183      0.10345
Training/qf1_norm                 973.43408    583.87604   1632.10144   129.15314
Training/qf2_norm                 571.03315    14.32914    591.76160    552.11896
log_std/mean                      -0.13983     0.00594     -0.13356     -0.15019
log_probs/mean                    -2.73503     0.00721     -2.72217     -2.74166
mean/mean                         -0.00270     0.00094     -0.00118     -0.00358
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01779770851135254
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70697
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [84450]
collect time 0.0008699893951416016
inner_dict_sum {'sac_diff0': 0.0002319812774658203, 'sac_diff1': 0.006520986557006836, 'sac_diff2': 0.007859468460083008, 'sac_diff3': 0.010261297225952148, 'sac_diff4': 0.006705522537231445, 'sac_diff5': 0.030977487564086914, 'sac_diff6': 0.00037741661071777344, 'all': 0.06293416023254395}
diff5_list [0.006512880325317383, 0.00611114501953125, 0.005961894989013672, 0.006343364715576172, 0.0060482025146484375]
time3 0
time4 0.06365847587585449
time5 0.06370377540588379
time7 7.152557373046875e-07
gen_weight_change tensor(-20.1917)
policy weight change tensor(38.4734, grad_fn=<SumBackward0>)
time8 0.0017910003662109375
train_time 0.07429766654968262
eval time 0.15810489654541016
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:16,883 MainThread INFO: EPOCH:556
2024-01-23 01:03:16,883 MainThread INFO: Time Consumed:0.2355039119720459s
2024-01-23 01:03:16,883 MainThread INFO: Total Frames:84300s
  6%|▌         | 557/10000 [04:44<40:00,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12262.26780
Train_Epoch_Reward                2346.07760
Running_Training_Average_Rewards  13378.61110
Explore_Time                      0.00087
Train___Time                      0.07430
Eval____Time                      0.15810
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12199.71045
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.09251     0.39556    89.53343     88.37267
alpha_0                           0.75690      0.00011    0.75706      0.75675
Alpha_loss                        -1.87610     0.00094    -1.87510     -1.87774
Training/policy_loss              -2.82712     0.00322    -2.82250     -2.83168
Training/qf1_loss                 6585.11670   453.36425  7464.00635   6219.94580
Training/qf2_loss                 15029.85938  483.27714  15989.62793  14692.49414
Training/pf_norm                  0.07729      0.01375    0.09692      0.05570
Training/qf1_norm                 242.42496    87.08253   375.40427    118.50433
Training/qf2_norm                 670.21611    3.51444    672.94519    663.34491
log_std/mean                      -0.13336     0.00004    -0.13328     -0.13340
log_probs/mean                    -2.73841     0.00405    -2.73329     -2.74419
mean/mean                         -0.00197     0.00005    -0.00192     -0.00204
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.017946243286132812
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70697
epoch first part time 2.384185791015625e-06
replay_buffer._size: [84600]
collect time 0.00087738037109375
inner_dict_sum {'sac_diff0': 0.0002243518829345703, 'sac_diff1': 0.006501674652099609, 'sac_diff2': 0.0077114105224609375, 'sac_diff3': 0.009636878967285156, 'sac_diff4': 0.006748676300048828, 'sac_diff5': 0.030853271484375, 'sac_diff6': 0.0003924369812011719, 'all': 0.06206870079040527}
diff5_list [0.006696224212646484, 0.0060346126556396484, 0.006156206130981445, 0.006103992462158203, 0.005862236022949219]
time3 0
time4 0.06282162666320801
time5 0.06286406517028809
time7 4.76837158203125e-07
gen_weight_change tensor(-20.1917)
policy weight change tensor(38.4136, grad_fn=<SumBackward0>)
time8 0.0018241405487060547
train_time 0.07352995872497559
eval time 0.15044760704040527
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:17,131 MainThread INFO: EPOCH:557
2024-01-23 01:03:17,132 MainThread INFO: Time Consumed:0.22713160514831543s
2024-01-23 01:03:17,132 MainThread INFO: Total Frames:84450s
  6%|▌         | 558/10000 [04:44<39:44,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12214.53681
Train_Epoch_Reward                8776.96715
Running_Training_Average_Rewards  12413.86195
Explore_Time                      0.00087
Train___Time                      0.07353
Eval____Time                      0.15045
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12149.98865
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.67402     1.29766     90.29329     86.81670
alpha_0                           0.75653      0.00011     0.75668      0.75637
Alpha_loss                        -1.87739     0.00144     -1.87516     -1.87895
Training/policy_loss              -2.66408     0.00450     -2.65573     -2.66792
Training/qf1_loss                 6563.24834   884.69729   7787.97803   5128.91309
Training/qf2_loss                 14946.42813  1070.67706  16343.34570  13293.19336
Training/pf_norm                  0.09830      0.02469     0.13396      0.05843
Training/qf1_norm                 450.68207    248.64357   804.47589    151.02606
Training/qf2_norm                 560.35299    8.00140     570.03809    549.47235
log_std/mean                      -0.12978     0.00011     -0.12964     -0.12992
log_probs/mean                    -2.73096     0.00556     -2.72056     -2.73655
mean/mean                         -0.00368     0.00005     -0.00362     -0.00376
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.017948389053344727
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70697
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [84750]
collect time 0.0008804798126220703
inner_dict_sum {'sac_diff0': 0.0002493858337402344, 'sac_diff1': 0.006653308868408203, 'sac_diff2': 0.0076901912689208984, 'sac_diff3': 0.009772539138793945, 'sac_diff4': 0.006575107574462891, 'sac_diff5': 0.03208041191101074, 'sac_diff6': 0.0004029273986816406, 'all': 0.06342387199401855}
diff5_list [0.006384849548339844, 0.006351947784423828, 0.006102800369262695, 0.0069196224212646484, 0.0063211917877197266]
time3 0
time4 0.06417250633239746
time5 0.06421566009521484
time7 4.76837158203125e-07
gen_weight_change tensor(-20.1917)
policy weight change tensor(38.2588, grad_fn=<SumBackward0>)
time8 0.0018320083618164062
train_time 0.07489657402038574
eval time 0.15039467811584473
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:17,381 MainThread INFO: EPOCH:558
2024-01-23 01:03:17,381 MainThread INFO: Time Consumed:0.2285306453704834s
2024-01-23 01:03:17,382 MainThread INFO: Total Frames:84600s
  6%|▌         | 559/10000 [04:44<39:38,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12168.06292
Train_Epoch_Reward                29511.70295
Running_Training_Average_Rewards  12818.73730
Explore_Time                      0.00088
Train___Time                      0.07490
Eval____Time                      0.15039
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12064.86683
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.72986     1.08990    91.04577     87.99141
alpha_0                           0.75615      0.00011    0.75630      0.75600
Alpha_loss                        -1.88181     0.00141    -1.87956     -1.88316
Training/policy_loss              -2.79251     0.00246    -2.78881     -2.79609
Training/qf1_loss                 6641.15186   564.21167  7616.79688   5942.61426
Training/qf2_loss                 15166.70039  552.45711  16089.58789  14555.30664
Training/pf_norm                  0.11423      0.00790    0.12554      0.10115
Training/qf1_norm                 1298.07661   233.94397  1602.38843   902.61084
Training/qf2_norm                 637.22177    7.31654    645.68079    625.78778
log_std/mean                      -0.14835     0.00037    -0.14780     -0.14882
log_probs/mean                    -2.73473     0.00341    -2.72910     -2.73955
mean/mean                         -0.00338     0.00007    -0.00326     -0.00345
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018638134002685547
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70697
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [84900]
collect time 0.0007779598236083984
inner_dict_sum {'sac_diff0': 0.00022745132446289062, 'sac_diff1': 0.006425619125366211, 'sac_diff2': 0.007727146148681641, 'sac_diff3': 0.009552001953125, 'sac_diff4': 0.00653529167175293, 'sac_diff5': 0.03060173988342285, 'sac_diff6': 0.00038814544677734375, 'all': 0.06145739555358887}
diff5_list [0.006295204162597656, 0.005967140197753906, 0.0060577392578125, 0.006333351135253906, 0.005948305130004883]
time3 0
time4 0.06217360496520996
time5 0.06221652030944824
time7 7.152557373046875e-07
gen_weight_change tensor(-20.1917)
policy weight change tensor(38.1346, grad_fn=<SumBackward0>)
time8 0.0018491744995117188
train_time 0.07286310195922852
eval time 0.14917993545532227
epoch last part time 3.814697265625e-06
2024-01-23 01:03:17,628 MainThread INFO: EPOCH:559
2024-01-23 01:03:17,628 MainThread INFO: Time Consumed:0.22502398490905762s
2024-01-23 01:03:17,628 MainThread INFO: Total Frames:84750s
  6%|▌         | 560/10000 [04:45<39:23,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12108.40073
Train_Epoch_Reward                1290.42830
Running_Training_Average_Rewards  12436.90304
Explore_Time                      0.00077
Train___Time                      0.07286
Eval____Time                      0.14918
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11927.85515
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.30389     1.98876    92.59660     87.88770
alpha_0                           0.75577      0.00011    0.75592      0.75562
Alpha_loss                        -1.88586     0.00260    -1.88325     -1.89021
Training/policy_loss              -2.76359     0.00579    -2.75422     -2.77124
Training/qf1_loss                 7038.01465   487.90896  7633.09668   6165.79834
Training/qf2_loss                 15681.71211  708.82182  16335.76855  14349.75000
Training/pf_norm                  0.10153      0.02962    0.13796      0.06409
Training/qf1_norm                 377.56550    146.07907  517.76111    105.97482
Training/qf2_norm                 611.82383    13.41080   627.24225    595.48657
log_std/mean                      -0.12361     0.00017    -0.12336     -0.12385
log_probs/mean                    -2.73715     0.00763    -2.72544     -2.74787
mean/mean                         -0.00305     0.00005    -0.00297     -0.00311
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018416881561279297
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70697
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [85050]
collect time 0.0007688999176025391
inside mustsac before update, task 0, sumup 70697
inside mustsac after update, task 0, sumup 70274
inner_dict_sum {'sac_diff0': 0.00019788742065429688, 'sac_diff1': 0.00634765625, 'sac_diff2': 0.007725238800048828, 'sac_diff3': 0.009894132614135742, 'sac_diff4': 0.0068399906158447266, 'sac_diff5': 0.04733633995056152, 'sac_diff6': 0.00038504600524902344, 'all': 0.07872629165649414}
diff5_list [0.0099334716796875, 0.00920724868774414, 0.009515523910522461, 0.009478330612182617, 0.009201765060424805]
time3 0.0008392333984375
time4 0.07950830459594727
time5 0.07956337928771973
time7 0.008945941925048828
gen_weight_change tensor(-19.9451)
policy weight change tensor(37.9805, grad_fn=<SumBackward0>)
time8 0.0024938583374023438
train_time 0.1085357666015625
eval time 0.11701607704162598
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:17,878 MainThread INFO: EPOCH:560
2024-01-23 01:03:17,879 MainThread INFO: Time Consumed:0.2285010814666748s
2024-01-23 01:03:17,879 MainThread INFO: Total Frames:84900s
  6%|▌         | 561/10000 [04:45<39:30,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12048.39973
Train_Epoch_Reward                29700.28090
Running_Training_Average_Rewards  12588.58641
Explore_Time                      0.00076
Train___Time                      0.10854
Eval____Time                      0.11702
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11891.13653
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.81200     2.51905     93.83974     86.95390
alpha_0                           0.75539      0.00011     0.75554      0.75524
Alpha_loss                        -1.88721     0.00186     -1.88471     -1.88960
Training/policy_loss              -2.75189     0.04396     -2.71050     -2.83116
Training/qf1_loss                 6733.43506   668.73254   7268.74756   5451.38086
Training/qf2_loss                 15422.79570  1054.26314  16474.96875  13494.93164
Training/pf_norm                  0.10623      0.02492     0.13865      0.06454
Training/qf1_norm                 1836.00787   1372.44114  4179.38330   471.22827
Training/qf2_norm                 622.79600    22.26938    660.50641    594.74957
log_std/mean                      -0.13718     0.01062     -0.11652     -0.14686
log_probs/mean                    -2.72996     0.00525     -2.72345     -2.73608
mean/mean                         -0.00208     0.00049     -0.00135     -0.00256
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018123149871826172
epoch last part time3 0.0024940967559814453
inside rlalgo, task 0, sumup 70274
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [85200]
collect time 0.0008237361907958984
inner_dict_sum {'sac_diff0': 0.00023818016052246094, 'sac_diff1': 0.006365776062011719, 'sac_diff2': 0.007672548294067383, 'sac_diff3': 0.010087728500366211, 'sac_diff4': 0.006657123565673828, 'sac_diff5': 0.030558109283447266, 'sac_diff6': 0.0003762245178222656, 'all': 0.06195569038391113}
diff5_list [0.0063648223876953125, 0.005895376205444336, 0.005909919738769531, 0.006256103515625, 0.006131887435913086]
time3 0
time4 0.06268525123596191
time5 0.0627279281616211
time7 4.76837158203125e-07
gen_weight_change tensor(-19.9451)
policy weight change tensor(37.9282, grad_fn=<SumBackward0>)
time8 0.0018749237060546875
train_time 0.07354736328125
eval time 0.1477501392364502
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:18,127 MainThread INFO: EPOCH:561
2024-01-23 01:03:18,127 MainThread INFO: Time Consumed:0.2243356704711914s
2024-01-23 01:03:18,127 MainThread INFO: Total Frames:85050s
  6%|▌         | 562/10000 [04:45<39:14,  4.01it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12049.39078
Train_Epoch_Reward                33026.52857
Running_Training_Average_Rewards  13498.82544
Explore_Time                      0.00082
Train___Time                      0.07355
Eval____Time                      0.14775
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12143.10090
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.39469     1.44388    93.21104     89.16033
alpha_0                           0.75501      0.00011    0.75516      0.75486
Alpha_loss                        -1.89237     0.00080    -1.89126     -1.89377
Training/policy_loss              -2.75838     0.00185    -2.75570     -2.76130
Training/qf1_loss                 7112.88594   596.08959  7793.62207   6332.26660
Training/qf2_loss                 15931.94844  784.30922  16888.67578  15110.81836
Training/pf_norm                  0.09951      0.02811    0.12873      0.05820
Training/qf1_norm                 1777.21108   309.20372  2233.29590   1434.13806
Training/qf2_norm                 625.61589    9.60593    638.20990    611.06659
log_std/mean                      -0.13154     0.00007    -0.13148     -0.13167
log_probs/mean                    -2.73635     0.00255    -2.73343     -2.74097
mean/mean                         -0.00249     0.00005    -0.00242     -0.00257
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01820993423461914
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70274
epoch first part time 4.291534423828125e-06
replay_buffer._size: [85350]
collect time 0.0011076927185058594
inner_dict_sum {'sac_diff0': 0.00021600723266601562, 'sac_diff1': 0.007126808166503906, 'sac_diff2': 0.00857996940612793, 'sac_diff3': 0.010508060455322266, 'sac_diff4': 0.00715327262878418, 'sac_diff5': 0.03193855285644531, 'sac_diff6': 0.00039005279541015625, 'all': 0.06591272354125977}
diff5_list [0.0070726871490478516, 0.0063555240631103516, 0.006117105484008789, 0.006267070770263672, 0.0061261653900146484]
time3 0
time4 0.06670713424682617
time5 0.06676864624023438
time7 7.152557373046875e-07
gen_weight_change tensor(-19.9451)
policy weight change tensor(37.9978, grad_fn=<SumBackward0>)
time8 0.0018856525421142578
train_time 0.07867622375488281
eval time 0.14713144302368164
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:18,378 MainThread INFO: EPOCH:562
2024-01-23 01:03:18,378 MainThread INFO: Time Consumed:0.22923588752746582s
2024-01-23 01:03:18,379 MainThread INFO: Total Frames:85200s
  6%|▌         | 563/10000 [04:45<39:34,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12052.12478
Train_Epoch_Reward                19063.60559
Running_Training_Average_Rewards  13798.59975
Explore_Time                      0.00110
Train___Time                      0.07868
Eval____Time                      0.14713
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12057.80708
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.63196     3.14061     96.46941     88.02156
alpha_0                           0.75464      0.00011     0.75479      0.75449
Alpha_loss                        -1.89412     0.00171     -1.89208     -1.89684
Training/policy_loss              -2.77262     0.00653     -2.76559     -2.78047
Training/qf1_loss                 7010.18086   1554.12983  9144.02051   4918.21582
Training/qf2_loss                 15756.37539  2092.59809  19004.76562  13150.20020
Training/pf_norm                  0.13468      0.02262     0.16718      0.10361
Training/qf1_norm                 570.48593    351.28829   1246.58142   270.87329
Training/qf2_norm                 624.07821    20.46662    661.91754    607.47455
log_std/mean                      -0.11874     0.00015     -0.11860     -0.11901
log_probs/mean                    -2.73060     0.00865     -2.72094     -2.74266
mean/mean                         -0.00224     0.00005     -0.00219     -0.00231
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0233914852142334
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70274
epoch first part time 3.337860107421875e-06
replay_buffer._size: [85500]
collect time 0.0009729862213134766
inner_dict_sum {'sac_diff0': 0.00020265579223632812, 'sac_diff1': 0.006873130798339844, 'sac_diff2': 0.008185148239135742, 'sac_diff3': 0.010261297225952148, 'sac_diff4': 0.0070836544036865234, 'sac_diff5': 0.03145432472229004, 'sac_diff6': 0.00041556358337402344, 'all': 0.06447577476501465}
diff5_list [0.00669407844543457, 0.0060710906982421875, 0.006031036376953125, 0.006338596343994141, 0.006319522857666016]
time3 0
time4 0.06527972221374512
time5 0.06532645225524902
time7 9.5367431640625e-07
gen_weight_change tensor(-19.9451)
policy weight change tensor(38.1511, grad_fn=<SumBackward0>)
time8 0.0018877983093261719
train_time 0.07707095146179199
eval time 0.1463489532470703
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:18,632 MainThread INFO: EPOCH:563
2024-01-23 01:03:18,632 MainThread INFO: Time Consumed:0.22664737701416016s
2024-01-23 01:03:18,632 MainThread INFO: Total Frames:85350s
  6%|▌         | 564/10000 [04:46<39:38,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12053.91479
Train_Epoch_Reward                34046.05516
Running_Training_Average_Rewards  14908.64197
Explore_Time                      0.00097
Train___Time                      0.07707
Eval____Time                      0.14635
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12039.84299
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.00217     2.70145    94.01447     85.84197
alpha_0                           0.75426      0.00011    0.75441      0.75411
Alpha_loss                        -1.89705     0.00183    -1.89399     -1.89952
Training/policy_loss              -2.78255     0.00422    -2.77749     -2.78749
Training/qf1_loss                 7056.12832   514.99855  7699.38281   6281.14160
Training/qf2_loss                 15328.84199  766.25259  16534.62891  14354.94238
Training/pf_norm                  0.10065      0.01353    0.11244      0.07557
Training/qf1_norm                 2813.50254   569.71477  3511.78003   1776.28418
Training/qf2_norm                 643.83429    19.00334   679.21777    622.14478
log_std/mean                      -0.12740     0.00025    -0.12703     -0.12773
log_probs/mean                    -2.72905     0.00560    -2.72248     -2.73542
mean/mean                         -0.00356     0.00005    -0.00349     -0.00361
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02306389808654785
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70274
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [85650]
collect time 0.0008404254913330078
inner_dict_sum {'sac_diff0': 0.00019931793212890625, 'sac_diff1': 0.006811380386352539, 'sac_diff2': 0.00822591781616211, 'sac_diff3': 0.010352611541748047, 'sac_diff4': 0.007070779800415039, 'sac_diff5': 0.03139090538024902, 'sac_diff6': 0.0003905296325683594, 'all': 0.06444144248962402}
diff5_list [0.006706714630126953, 0.0061931610107421875, 0.006192684173583984, 0.0062122344970703125, 0.006086111068725586]
time3 0
time4 0.06520891189575195
time5 0.06526017189025879
time7 4.76837158203125e-07
gen_weight_change tensor(-19.9451)
policy weight change tensor(38.2853, grad_fn=<SumBackward0>)
time8 0.001874685287475586
train_time 0.07706189155578613
eval time 0.15018177032470703
epoch last part time 5.7220458984375e-06
2024-01-23 01:03:18,888 MainThread INFO: EPOCH:564
2024-01-23 01:03:18,889 MainThread INFO: Time Consumed:0.23045587539672852s
2024-01-23 01:03:18,889 MainThread INFO: Total Frames:85500s
  6%|▌         | 565/10000 [04:46<39:50,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12052.24584
Train_Epoch_Reward                16553.60780
Running_Training_Average_Rewards  15389.89703
Explore_Time                      0.00084
Train___Time                      0.07706
Eval____Time                      0.15018
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12020.24543
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.20413     1.89039    91.60188     86.14606
alpha_0                           0.75388      0.00011    0.75403      0.75373
Alpha_loss                        -1.90201     0.00203    -1.89903     -1.90381
Training/policy_loss              -2.75262     0.00315    -2.74817     -2.75618
Training/qf1_loss                 6816.42129   585.04489  7928.98584   6267.76807
Training/qf2_loss                 15258.15957  745.48746  16533.85938  14325.80762
Training/pf_norm                  0.10985      0.01042    0.12473      0.09262
Training/qf1_norm                 1359.15319   366.33222  1943.54724   930.31696
Training/qf2_norm                 602.70912    12.55049   618.80267    582.30719
log_std/mean                      -0.13511     0.00020    -0.13483     -0.13540
log_probs/mean                    -2.73470     0.00435    -2.72893     -2.73957
mean/mean                         -0.00344     0.00003    -0.00340     -0.00347
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.022777080535888672
epoch last part time3 1.430511474609375e-06
inside rlalgo, task 0, sumup 70274
epoch first part time 3.814697265625e-06
replay_buffer._size: [85800]
collect time 0.0008292198181152344
inside mustsac before update, task 0, sumup 70274
inside mustsac after update, task 0, sumup 71500
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.0072863101959228516, 'sac_diff2': 0.008844614028930664, 'sac_diff3': 0.011131525039672852, 'sac_diff4': 0.00841069221496582, 'sac_diff5': 0.053734540939331055, 'sac_diff6': 0.0004127025604248047, 'all': 0.09003281593322754}
diff5_list [0.012904644012451172, 0.010765314102172852, 0.010126590728759766, 0.009968757629394531, 0.009969234466552734]
time3 0.0009140968322753906
time4 0.09093260765075684
time5 0.0909883975982666
time7 0.009124040603637695
gen_weight_change tensor(-19.6750)
policy weight change tensor(38.1868, grad_fn=<SumBackward0>)
time8 0.0068624019622802734
train_time 0.12621521949768066
eval time 0.12190723419189453
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:19,166 MainThread INFO: EPOCH:565
2024-01-23 01:03:19,167 MainThread INFO: Time Consumed:0.25156307220458984s
2024-01-23 01:03:19,167 MainThread INFO: Total Frames:85650s
  6%|▌         | 566/10000 [04:46<40:50,  3.85it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12053.85692
Train_Epoch_Reward                20921.95913
Running_Training_Average_Rewards  15679.31090
Explore_Time                      0.00082
Train___Time                      0.12622
Eval____Time                      0.12191
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12044.01519
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.58498     0.39156    89.06524     87.89774
alpha_0                           0.75351      0.00011    0.75366      0.75335
Alpha_loss                        -1.90553     0.00168    -1.90269     -1.90725
Training/policy_loss              -2.79253     0.03838    -2.73868     -2.83981
Training/qf1_loss                 6464.32549   226.13889  6674.53760   6035.32227
Training/qf2_loss                 14807.50254  311.49074  15074.75488  14221.06250
Training/pf_norm                  0.09971      0.01422    0.12612      0.08388
Training/qf1_norm                 540.72541    388.64311  1205.92712   185.73428
Training/qf2_norm                 634.04476    27.95115   664.49170    592.96722
log_std/mean                      -0.13211     0.00332    -0.12595     -0.13558
log_probs/mean                    -2.73523     0.00564    -2.72817     -2.74345
mean/mean                         -0.00298     0.00084    -0.00179     -0.00432
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019741535186767578
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71500
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [85950]
collect time 0.0008904933929443359
inner_dict_sum {'sac_diff0': 0.00024819374084472656, 'sac_diff1': 0.009025096893310547, 'sac_diff2': 0.010814666748046875, 'sac_diff3': 0.013054847717285156, 'sac_diff4': 0.009332656860351562, 'sac_diff5': 0.039765119552612305, 'sac_diff6': 0.0004811286926269531, 'all': 0.08272171020507812}
diff5_list [0.009208917617797852, 0.007494688034057617, 0.007323503494262695, 0.008137226104736328, 0.0076007843017578125]
time3 0
time4 0.08351635932922363
time5 0.08356428146362305
time7 7.152557373046875e-07
gen_weight_change tensor(-19.6750)
policy weight change tensor(38.2495, grad_fn=<SumBackward0>)
time8 0.0019466876983642578
train_time 0.09705567359924316
eval time 0.13118386268615723
epoch last part time 5.9604644775390625e-06
2024-01-23 01:03:19,421 MainThread INFO: EPOCH:566
2024-01-23 01:03:19,421 MainThread INFO: Time Consumed:0.23145246505737305s
2024-01-23 01:03:19,421 MainThread INFO: Total Frames:85800s
  6%|▌         | 567/10000 [04:47<40:33,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12058.58472
Train_Epoch_Reward                10270.13357
Running_Training_Average_Rewards  15785.56385
Explore_Time                      0.00089
Train___Time                      0.09706
Eval____Time                      0.13118
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12246.98844
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.17659     3.06209     94.56331     85.98537
alpha_0                           0.75313      0.00011     0.75328      0.75298
Alpha_loss                        -1.90897     0.00088     -1.90762     -1.91003
Training/policy_loss              -2.77499     0.00225     -2.77112     -2.77726
Training/qf1_loss                 6288.17012   914.73698   7502.50000   5244.02734
Training/qf2_loss                 14740.28184  1339.69015  16509.27148  13247.68164
Training/pf_norm                  0.11658      0.02561     0.13997      0.06868
Training/qf1_norm                 503.07280    433.82265   1348.87463   166.11945
Training/qf2_norm                 654.74341    22.58949    694.37256    631.09741
log_std/mean                      -0.13261     0.00007     -0.13252     -0.13273
log_probs/mean                    -2.73552     0.00269     -2.73107     -2.73843
mean/mean                         -0.00253     0.00006     -0.00246     -0.00262
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018750429153442383
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71500
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [86100]
collect time 0.0008726119995117188
inner_dict_sum {'sac_diff0': 0.0002503395080566406, 'sac_diff1': 0.007825374603271484, 'sac_diff2': 0.008967161178588867, 'sac_diff3': 0.011281251907348633, 'sac_diff4': 0.007772684097290039, 'sac_diff5': 0.03756093978881836, 'sac_diff6': 0.0004925727844238281, 'all': 0.07415032386779785}
diff5_list [0.00798940658569336, 0.007254838943481445, 0.0074007511138916016, 0.007534503936767578, 0.007381439208984375]
time3 0
time4 0.07489562034606934
time5 0.07494044303894043
time7 7.152557373046875e-07
gen_weight_change tensor(-19.6750)
policy weight change tensor(38.2336, grad_fn=<SumBackward0>)
time8 0.0019233226776123047
train_time 0.08693051338195801
eval time 0.14288949966430664
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:19,676 MainThread INFO: EPOCH:567
2024-01-23 01:03:19,677 MainThread INFO: Time Consumed:0.23298335075378418s
2024-01-23 01:03:19,677 MainThread INFO: Total Frames:85950s
  6%|▌         | 568/10000 [04:47<40:25,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12077.74496
Train_Epoch_Reward                6479.86943
Running_Training_Average_Rewards  15229.80348
Explore_Time                      0.00087
Train___Time                      0.08693
Eval____Time                      0.14289
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12341.59110
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.17811     2.76549    91.18023     83.58368
alpha_0                           0.75275      0.00011    0.75290      0.75260
Alpha_loss                        -1.91317     0.00064    -1.91236     -1.91413
Training/policy_loss              -2.87751     0.00269    -2.87278     -2.88086
Training/qf1_loss                 6311.78584   718.35215  7665.69238   5725.02783
Training/qf2_loss                 14630.94473  987.69480  16380.70117  13350.21289
Training/pf_norm                  0.11085      0.03240    0.16984      0.07891
Training/qf1_norm                 503.36779    393.68650  1221.24670   163.49937
Training/qf2_norm                 678.58184    20.88039   701.21399    643.69128
log_std/mean                      -0.13834     0.00006    -0.13823     -0.13840
log_probs/mean                    -2.73843     0.00287    -2.73316     -2.74151
mean/mean                         -0.00378     0.00009    -0.00370     -0.00394
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018631696701049805
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71500
epoch first part time 2.86102294921875e-06
replay_buffer._size: [86250]
collect time 0.000885009765625
inner_dict_sum {'sac_diff0': 0.0002753734588623047, 'sac_diff1': 0.009110212326049805, 'sac_diff2': 0.011066436767578125, 'sac_diff3': 0.013864994049072266, 'sac_diff4': 0.009979963302612305, 'sac_diff5': 0.04325103759765625, 'sac_diff6': 0.0005373954772949219, 'all': 0.08808541297912598}
diff5_list [0.009262800216674805, 0.007897615432739258, 0.008599996566772461, 0.00961446762084961, 0.007876157760620117]
time3 0
time4 0.08897733688354492
time5 0.08903694152832031
time7 9.5367431640625e-07
gen_weight_change tensor(-19.6750)
policy weight change tensor(38.1340, grad_fn=<SumBackward0>)
time8 0.0020761489868164062
train_time 0.10171723365783691
eval time 0.13521099090576172
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:19,939 MainThread INFO: EPOCH:568
2024-01-23 01:03:19,939 MainThread INFO: Time Consumed:0.24012494087219238s
2024-01-23 01:03:19,939 MainThread INFO: Total Frames:86100s
  6%|▌         | 569/10000 [04:47<40:39,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12107.03019
Train_Epoch_Reward                18187.96546
Running_Training_Average_Rewards  15352.57737
Explore_Time                      0.00088
Train___Time                      0.10172
Eval____Time                      0.13521
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12357.71905
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.62914     2.33744     92.90353     87.30233
alpha_0                           0.75238      0.00011     0.75253      0.75223
Alpha_loss                        -1.91462     0.00173     -1.91226     -1.91754
Training/policy_loss              -2.75349     0.00449     -2.74918     -2.76057
Training/qf1_loss                 6821.98896   1252.90630  8870.13574   5288.50488
Training/qf2_loss                 15314.95098  1624.33762  17774.42578  13370.12988
Training/pf_norm                  0.10970      0.01371     0.13196      0.09680
Training/qf1_norm                 1250.63741   437.27812   1845.78638   825.78711
Training/qf2_norm                 600.24927    15.53091    622.10559    584.65436
log_std/mean                      -0.14045     0.00017     -0.14018     -0.14069
log_probs/mean                    -2.73168     0.00636     -2.72576     -2.74196
mean/mean                         -0.00235     0.00014     -0.00213     -0.00253
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01854419708251953
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71500
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [86400]
collect time 0.0009124279022216797
inner_dict_sum {'sac_diff0': 0.0002589225769042969, 'sac_diff1': 0.008202314376831055, 'sac_diff2': 0.01003408432006836, 'sac_diff3': 0.012391090393066406, 'sac_diff4': 0.008331775665283203, 'sac_diff5': 0.041077613830566406, 'sac_diff6': 0.0004971027374267578, 'all': 0.08079290390014648}
diff5_list [0.008046865463256836, 0.007689714431762695, 0.009650945663452148, 0.008076906204223633, 0.007613182067871094]
time3 0
time4 0.08157110214233398
time5 0.08163094520568848
time7 4.76837158203125e-07
gen_weight_change tensor(-19.6750)
policy weight change tensor(38.0575, grad_fn=<SumBackward0>)
time8 0.0019268989562988281
train_time 0.09365463256835938
eval time 0.15680694580078125
epoch last part time 1.3113021850585938e-05
2024-01-23 01:03:20,215 MainThread INFO: EPOCH:569
2024-01-23 01:03:20,215 MainThread INFO: Time Consumed:0.25394392013549805s
2024-01-23 01:03:20,215 MainThread INFO: Total Frames:86250s
  6%|▌         | 570/10000 [04:47<41:30,  3.79it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12145.05626
Train_Epoch_Reward                9290.46297
Running_Training_Average_Rewards  15163.90164
Explore_Time                      0.00091
Train___Time                      0.09365
Eval____Time                      0.15681
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12308.11586
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.52137     2.64663     92.99043     85.37817
alpha_0                           0.75200      0.00011     0.75215      0.75185
Alpha_loss                        -1.91779     0.00235     -1.91360     -1.91959
Training/policy_loss              -2.80821     0.00385     -2.80176     -2.81314
Training/qf1_loss                 6912.71416   1124.78126  8399.58008   5098.58936
Training/qf2_loss                 15633.19570  1548.61576  17329.31445  12841.61523
Training/pf_norm                  0.10507      0.02099     0.13717      0.08016
Training/qf1_norm                 417.24346    401.20434   1193.86597   124.79180
Training/qf2_norm                 643.75911    18.51129    661.26721    607.84265
log_std/mean                      -0.13312     0.00005     -0.13302     -0.13318
log_probs/mean                    -2.73098     0.00560     -2.72099     -2.73720
mean/mean                         -0.00370     0.00003     -0.00368     -0.00375
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019336462020874023
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71500
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [86550]
collect time 0.0009009838104248047
inside mustsac before update, task 0, sumup 71500
inside mustsac after update, task 0, sumup 69954
inner_dict_sum {'sac_diff0': 0.0002620220184326172, 'sac_diff1': 0.008207082748413086, 'sac_diff2': 0.010143518447875977, 'sac_diff3': 0.012615680694580078, 'sac_diff4': 0.00904393196105957, 'sac_diff5': 0.06120657920837402, 'sac_diff6': 0.0005276203155517578, 'all': 0.10200643539428711}
diff5_list [0.013999700546264648, 0.011959552764892578, 0.011755943298339844, 0.011848211288452148, 0.011643171310424805]
time3 0.0011506080627441406
time4 0.10287952423095703
time5 0.10293173789978027
time7 0.00929880142211914
gen_weight_change tensor(-19.4960)
policy weight change tensor(37.9635, grad_fn=<SumBackward0>)
time8 0.002576112747192383
train_time 0.13570737838745117
eval time 0.09381747245788574
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:20,470 MainThread INFO: EPOCH:570
2024-01-23 01:03:20,471 MainThread INFO: Time Consumed:0.2326498031616211s
2024-01-23 01:03:20,471 MainThread INFO: Total Frames:86400s
  6%|▌         | 571/10000 [04:48<41:12,  3.81it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12180.93057
Train_Epoch_Reward                6042.62604
Running_Training_Average_Rewards  14603.20400
Explore_Time                      0.00089
Train___Time                      0.13571
Eval____Time                      0.09382
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12249.87963
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.54971     1.89175    91.76988     86.80153
alpha_0                           0.75162      0.00011    0.75177      0.75147
Alpha_loss                        -1.92184     0.00155    -1.91950     -1.92423
Training/policy_loss              -2.81816     0.03965    -2.74742     -2.86539
Training/qf1_loss                 5969.12871   653.65373  6937.84912   5050.06689
Training/qf2_loss                 14273.27520  916.54784  15487.49023  13010.72363
Training/pf_norm                  0.12049      0.03588    0.15351      0.07667
Training/qf1_norm                 1059.66141   534.08832  1958.15222   315.94672
Training/qf2_norm                 655.84519    38.91424   703.03558    594.72534
log_std/mean                      -0.13792     0.00664    -0.12986     -0.14716
log_probs/mean                    -2.73339     0.00516    -2.72753     -2.74175
mean/mean                         -0.00416     0.00126    -0.00204     -0.00592
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0188596248626709
epoch last part time3 0.0024566650390625
inside rlalgo, task 0, sumup 69954
epoch first part time 2.86102294921875e-06
replay_buffer._size: [86700]
collect time 0.00087738037109375
inner_dict_sum {'sac_diff0': 0.0002493858337402344, 'sac_diff1': 0.0075566768646240234, 'sac_diff2': 0.009102821350097656, 'sac_diff3': 0.011637210845947266, 'sac_diff4': 0.008395671844482422, 'sac_diff5': 0.03736472129821777, 'sac_diff6': 0.0004754066467285156, 'all': 0.07478189468383789}
diff5_list [0.007474422454833984, 0.007570505142211914, 0.007509946823120117, 0.007337808609008789, 0.007472038269042969]
time3 0
time4 0.07556533813476562
time5 0.07561278343200684
time7 7.152557373046875e-07
gen_weight_change tensor(-19.4960)
policy weight change tensor(37.8848, grad_fn=<SumBackward0>)
time8 0.0020296573638916016
train_time 0.08733892440795898
eval time 0.13375639915466309
epoch last part time 1.71661376953125e-05
2024-01-23 01:03:20,719 MainThread INFO: EPOCH:571
2024-01-23 01:03:20,719 MainThread INFO: Time Consumed:0.22431278228759766s
2024-01-23 01:03:20,720 MainThread INFO: Total Frames:86550s
  6%|▌         | 572/10000 [04:48<40:26,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12188.15107
Train_Epoch_Reward                19769.28250
Running_Training_Average_Rewards  14840.90210
Explore_Time                      0.00087
Train___Time                      0.08734
Eval____Time                      0.13376
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12215.30592
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.24523     0.96673    91.87016     89.22777
alpha_0                           0.75125      0.00011    0.75140      0.75110
Alpha_loss                        -1.92511     0.00167    -1.92227     -1.92744
Training/policy_loss              -2.87064     0.00553    -2.86241     -2.87663
Training/qf1_loss                 6744.73213   854.38386  7899.56104   5755.76416
Training/qf2_loss                 15304.63516  910.42094  16586.92383  14183.36914
Training/pf_norm                  0.10071      0.02402    0.12413      0.05880
Training/qf1_norm                 2046.01333   191.96625  2223.49365   1763.40564
Training/qf2_norm                 685.22360    6.89395    697.11469    678.69489
log_std/mean                      -0.12868     0.00010    -0.12853     -0.12881
log_probs/mean                    -2.73307     0.00667    -2.72311     -2.73973
mean/mean                         -0.00395     0.00009    -0.00384     -0.00410
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01849961280822754
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69954
epoch first part time 2.86102294921875e-06
replay_buffer._size: [86850]
collect time 0.0008251667022705078
inner_dict_sum {'sac_diff0': 0.0002460479736328125, 'sac_diff1': 0.007643461227416992, 'sac_diff2': 0.008853912353515625, 'sac_diff3': 0.011812686920166016, 'sac_diff4': 0.00800323486328125, 'sac_diff5': 0.03627610206604004, 'sac_diff6': 0.0004737377166748047, 'all': 0.07330918312072754}
diff5_list [0.007453441619873047, 0.007218599319458008, 0.007325172424316406, 0.007129669189453125, 0.007149219512939453]
time3 0
time4 0.07404804229736328
time5 0.07409119606018066
time7 4.76837158203125e-07
gen_weight_change tensor(-19.4960)
policy weight change tensor(37.8912, grad_fn=<SumBackward0>)
time8 0.0018792152404785156
train_time 0.08579063415527344
eval time 0.14163517951965332
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:20,972 MainThread INFO: EPOCH:572
2024-01-23 01:03:20,972 MainThread INFO: Time Consumed:0.23052000999450684s
2024-01-23 01:03:20,972 MainThread INFO: Total Frames:86700s
  6%|▌         | 573/10000 [04:48<40:12,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12201.68336
Train_Epoch_Reward                14818.92831
Running_Training_Average_Rewards  14936.09358
Explore_Time                      0.00082
Train___Time                      0.08579
Eval____Time                      0.14164
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12193.12998
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.09902     1.41065    92.32318     87.93813
alpha_0                           0.75087      0.00011    0.75102      0.75072
Alpha_loss                        -1.92818     0.00328    -1.92493     -1.93299
Training/policy_loss              -2.78764     0.00823    -2.77737     -2.79777
Training/qf1_loss                 6972.48887   299.01018  7305.45996   6587.51270
Training/qf2_loss                 15636.63340  333.36612  16131.09961  15278.85156
Training/pf_norm                  0.10814      0.02573    0.15520      0.07778
Training/qf1_norm                 473.61423    206.75843  799.90576    160.88791
Training/qf2_norm                 645.68154    10.13033   661.63202    630.19025
log_std/mean                      -0.12692     0.00009    -0.12684     -0.12708
log_probs/mean                    -2.73201     0.01074    -2.71834     -2.74506
mean/mean                         -0.00424     0.00006    -0.00413     -0.00430
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018563032150268555
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69954
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [87000]
collect time 0.000873565673828125
inner_dict_sum {'sac_diff0': 0.0002574920654296875, 'sac_diff1': 0.00806283950805664, 'sac_diff2': 0.009901046752929688, 'sac_diff3': 0.012269020080566406, 'sac_diff4': 0.008545637130737305, 'sac_diff5': 0.041144371032714844, 'sac_diff6': 0.0004951953887939453, 'all': 0.08067560195922852}
diff5_list [0.008036136627197266, 0.00837564468383789, 0.008416175842285156, 0.007883548736572266, 0.008432865142822266]
time3 0
time4 0.08144903182983398
time5 0.0814969539642334
time7 7.152557373046875e-07
gen_weight_change tensor(-19.4960)
policy weight change tensor(37.9046, grad_fn=<SumBackward0>)
time8 0.001968860626220703
train_time 0.09322142601013184
eval time 0.13660383224487305
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:21,227 MainThread INFO: EPOCH:573
2024-01-23 01:03:21,227 MainThread INFO: Time Consumed:0.23311519622802734s
2024-01-23 01:03:21,227 MainThread INFO: Total Frames:86850s
  6%|▌         | 574/10000 [04:48<40:10,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12214.28413
Train_Epoch_Reward                2288.82804
Running_Training_Average_Rewards  14695.49330
Explore_Time                      0.00087
Train___Time                      0.09322
Eval____Time                      0.13660
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12165.85073
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.16800     2.53469     93.45531     85.76912
alpha_0                           0.75050      0.00011     0.75065      0.75035
Alpha_loss                        -1.93276     0.00107     -1.93147     -1.93404
Training/policy_loss              -2.73675     0.00364     -2.73023     -2.74049
Training/qf1_loss                 6545.39932   720.06191   7720.81787   5587.32812
Training/qf2_loss                 15156.18203  1209.91051  17012.03516  13284.15137
Training/pf_norm                  0.09578      0.02232     0.12100      0.06351
Training/qf1_norm                 1667.86029   562.34154   2670.73218   1012.98492
Training/qf2_norm                 599.61141    16.46443    620.99286    571.11298
log_std/mean                      -0.13361     0.00003     -0.13356     -0.13365
log_probs/mean                    -2.73626     0.00387     -2.72940     -2.74064
mean/mean                         -0.00413     0.00003     -0.00409     -0.00416
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01877617835998535
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69954
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [87150]
collect time 0.0009002685546875
inner_dict_sum {'sac_diff0': 0.0002703666687011719, 'sac_diff1': 0.009004831314086914, 'sac_diff2': 0.011733531951904297, 'sac_diff3': 0.016839265823364258, 'sac_diff4': 0.010361194610595703, 'sac_diff5': 0.04205942153930664, 'sac_diff6': 0.0005180835723876953, 'all': 0.09078669548034668}
diff5_list [0.008267402648925781, 0.008472681045532227, 0.009987592697143555, 0.007726192474365234, 0.007605552673339844]
time3 0
time4 0.09165644645690918
time5 0.09172201156616211
time7 7.152557373046875e-07
gen_weight_change tensor(-19.4960)
policy weight change tensor(37.9077, grad_fn=<SumBackward0>)
time8 0.0023577213287353516
train_time 0.10492753982543945
eval time 0.12068343162536621
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:21,479 MainThread INFO: EPOCH:574
2024-01-23 01:03:21,479 MainThread INFO: Time Consumed:0.22922515869140625s
2024-01-23 01:03:21,479 MainThread INFO: Total Frames:87000s
  6%|▌         | 575/10000 [04:49<40:03,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12227.20877
Train_Epoch_Reward                4021.84445
Running_Training_Average_Rewards  14483.06364
Explore_Time                      0.00090
Train___Time                      0.10493
Eval____Time                      0.12068
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12149.49176
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.40316     1.37657     92.63902     89.00376
alpha_0                           0.75012      0.00011     0.75027      0.74997
Alpha_loss                        -1.93492     0.00280     -1.93072     -1.93747
Training/policy_loss              -2.74298     0.00532     -2.73619     -2.74902
Training/qf1_loss                 7255.23740   1154.51448  9019.56543   5881.60156
Training/qf2_loss                 15945.63516  1383.84292  17892.87891  14303.50488
Training/pf_norm                  0.11243      0.00871     0.12270      0.10096
Training/qf1_norm                 909.56283    260.76354   1322.97974   632.59869
Training/qf2_norm                 642.54988    9.34076     657.97125    632.98718
log_std/mean                      -0.13795     0.00003     -0.13793     -0.13800
log_probs/mean                    -2.73203     0.00773     -2.72209     -2.74022
mean/mean                         -0.00486     0.00001     -0.00484     -0.00487
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02051258087158203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69954
epoch first part time 5.7220458984375e-06
replay_buffer._size: [87300]
collect time 0.0010170936584472656
inside mustsac before update, task 0, sumup 69954
inside mustsac after update, task 0, sumup 70782
inner_dict_sum {'sac_diff0': 0.0002677440643310547, 'sac_diff1': 0.008087158203125, 'sac_diff2': 0.009832620620727539, 'sac_diff3': 0.012726783752441406, 'sac_diff4': 0.009329795837402344, 'sac_diff5': 0.06217193603515625, 'sac_diff6': 0.0005161762237548828, 'all': 0.10293221473693848}
diff5_list [0.013558387756347656, 0.012140750885009766, 0.011482954025268555, 0.011661052703857422, 0.013328790664672852]
time3 0.001190185546875
time4 0.10384249687194824
time5 0.10389852523803711
time7 0.009515762329101562
gen_weight_change tensor(-19.3386)
policy weight change tensor(37.8262, grad_fn=<SumBackward0>)
time8 0.0019268989562988281
train_time 0.13655424118041992
eval time 0.0906219482421875
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:21,733 MainThread INFO: EPOCH:575
2024-01-23 01:03:21,734 MainThread INFO: Time Consumed:0.23037934303283691s
2024-01-23 01:03:21,734 MainThread INFO: Total Frames:87150s
  6%|▌         | 576/10000 [04:49<39:56,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12239.11006
Train_Epoch_Reward                15074.95244
Running_Training_Average_Rewards  14725.97545
Explore_Time                      0.00101
Train___Time                      0.13655
Eval____Time                      0.09062
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12163.02809
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.95739     1.62427     91.90673     87.33984
alpha_0                           0.74975      0.00011     0.74990      0.74960
Alpha_loss                        -1.93870     0.00292     -1.93564     -1.94419
Training/policy_loss              -2.80707     0.04710     -2.72328     -2.86389
Training/qf1_loss                 6729.13369   704.39677   7353.79688   5417.77588
Training/qf2_loss                 15316.34746  1008.78125  16275.08398  13525.25586
Training/pf_norm                  0.08642      0.01194     0.10156      0.07156
Training/qf1_norm                 812.34644    655.56228   1960.15930   116.66735
Training/qf2_norm                 637.11387    35.95448    674.25854    569.44379
log_std/mean                      -0.13289     0.00286     -0.12915     -0.13664
log_probs/mean                    -2.73349     0.00912     -2.72285     -2.75020
mean/mean                         -0.00401     0.00073     -0.00309     -0.00511
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018015623092651367
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70782
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [87450]
collect time 0.0008203983306884766
inner_dict_sum {'sac_diff0': 0.00027251243591308594, 'sac_diff1': 0.007929801940917969, 'sac_diff2': 0.009626388549804688, 'sac_diff3': 0.012228250503540039, 'sac_diff4': 0.00861668586730957, 'sac_diff5': 0.0392308235168457, 'sac_diff6': 0.0005133152008056641, 'all': 0.07841777801513672}
diff5_list [0.008131742477416992, 0.0084991455078125, 0.0078887939453125, 0.007358074188232422, 0.007353067398071289]
time3 0
time4 0.0792694091796875
time5 0.07932877540588379
time7 4.76837158203125e-07
gen_weight_change tensor(-19.3386)
policy weight change tensor(37.7150, grad_fn=<SumBackward0>)
time8 0.0019609928131103516
train_time 0.09108424186706543
eval time 0.14382195472717285
epoch last part time 8.58306884765625e-06
2024-01-23 01:03:21,993 MainThread INFO: EPOCH:576
2024-01-23 01:03:21,993 MainThread INFO: Time Consumed:0.23813867568969727s
2024-01-23 01:03:21,994 MainThread INFO: Total Frames:87300s
  6%|▌         | 577/10000 [04:49<40:14,  3.90it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12205.39132
Train_Epoch_Reward                4717.17461
Running_Training_Average_Rewards  14611.57100
Explore_Time                      0.00082
Train___Time                      0.09108
Eval____Time                      0.14382
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11909.80104
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.67652     1.61541    92.68005     87.99416
alpha_0                           0.74937      0.00011    0.74952      0.74922
Alpha_loss                        -1.94185     0.00201    -1.93981     -1.94535
Training/policy_loss              -2.80363     0.00510    -2.79851     -2.81066
Training/qf1_loss                 6674.85674   514.07225  7614.93115   6047.89941
Training/qf2_loss                 15222.36016  760.60510  16709.98047  14613.90918
Training/pf_norm                  0.11274      0.01781    0.13393      0.08939
Training/qf1_norm                 708.11751    313.03190  1287.17578   372.96616
Training/qf2_norm                 665.49071    11.62389   687.12360    653.46887
log_std/mean                      -0.13986     0.00027    -0.13944     -0.14018
log_probs/mean                    -2.73274     0.00646    -2.72567     -2.74252
mean/mean                         -0.00323     0.00012    -0.00306     -0.00339
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019405841827392578
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70782
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [87600]
collect time 0.0010023117065429688
inner_dict_sum {'sac_diff0': 0.0002570152282714844, 'sac_diff1': 0.008041620254516602, 'sac_diff2': 0.009447097778320312, 'sac_diff3': 0.012089252471923828, 'sac_diff4': 0.008198261260986328, 'sac_diff5': 0.03854846954345703, 'sac_diff6': 0.0004832744598388672, 'all': 0.07706499099731445}
diff5_list [0.008003711700439453, 0.007714748382568359, 0.00810551643371582, 0.007391452789306641, 0.007333040237426758]
time3 0
time4 0.07783842086791992
time5 0.07788491249084473
time7 7.152557373046875e-07
gen_weight_change tensor(-19.3386)
policy weight change tensor(37.6365, grad_fn=<SumBackward0>)
time8 0.0020935535430908203
train_time 0.08997941017150879
eval time 0.1382582187652588
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:22,248 MainThread INFO: EPOCH:577
2024-01-23 01:03:22,248 MainThread INFO: Time Consumed:0.23159265518188477s
2024-01-23 01:03:22,248 MainThread INFO: Total Frames:87450s
  6%|▌         | 578/10000 [04:49<40:08,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12163.50355
Train_Epoch_Reward                13372.67014
Running_Training_Average_Rewards  14382.61480
Explore_Time                      0.00100
Train___Time                      0.08998
Eval____Time                      0.13826
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11922.71339
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.67814     2.78651     92.58946     85.46540
alpha_0                           0.74900      0.00011     0.74915      0.74885
Alpha_loss                        -1.94429     0.00162     -1.94164     -1.94639
Training/policy_loss              -2.84857     0.00341     -2.84458     -2.85333
Training/qf1_loss                 7403.56338   743.22469   8410.73340   6486.21924
Training/qf2_loss                 15773.66445  1254.46963  17505.85742  14287.18262
Training/pf_norm                  0.10995      0.01412     0.12275      0.08363
Training/qf1_norm                 606.40067    280.75569   1101.65625   320.19214
Training/qf2_norm                 676.58336    20.81550    705.52795    652.51355
log_std/mean                      -0.12763     0.00010     -0.12753     -0.12781
log_probs/mean                    -2.72952     0.00535     -2.72269     -2.73680
mean/mean                         -0.00439     0.00002     -0.00436     -0.00442
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018526554107666016
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70782
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [87750]
collect time 0.0009553432464599609
inner_dict_sum {'sac_diff0': 0.0002675056457519531, 'sac_diff1': 0.008101224899291992, 'sac_diff2': 0.009854555130004883, 'sac_diff3': 0.012621402740478516, 'sac_diff4': 0.008548974990844727, 'sac_diff5': 0.03976631164550781, 'sac_diff6': 0.0005090236663818359, 'all': 0.07966899871826172}
diff5_list [0.008742809295654297, 0.007729291915893555, 0.008388996124267578, 0.007524728775024414, 0.007380485534667969]
time3 0
time4 0.0804598331451416
time5 0.08051276206970215
time7 4.76837158203125e-07
gen_weight_change tensor(-19.3386)
policy weight change tensor(37.5740, grad_fn=<SumBackward0>)
time8 0.0020148754119873047
train_time 0.09264516830444336
eval time 0.13754558563232422
epoch last part time 8.106231689453125e-06
2024-01-23 01:03:22,504 MainThread INFO: EPOCH:578
2024-01-23 01:03:22,504 MainThread INFO: Time Consumed:0.23364782333374023s
2024-01-23 01:03:22,504 MainThread INFO: Total Frames:87600s
  6%|▌         | 579/10000 [04:50<40:09,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12119.64093
Train_Epoch_Reward                3773.47436
Running_Training_Average_Rewards  14287.36798
Explore_Time                      0.00095
Train___Time                      0.09265
Eval____Time                      0.13755
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11919.09292
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.93300     5.21530     99.59236     83.55785
alpha_0                           0.74862      0.00011     0.74877      0.74847
Alpha_loss                        -1.95049     0.00153     -1.94959     -1.95354
Training/policy_loss              -2.87693     0.00616     -2.87096     -2.88868
Training/qf1_loss                 7196.15068   2031.28093  11051.07031  5144.75098
Training/qf2_loss                 16004.81230  3033.88592  21580.63672  12503.47852
Training/pf_norm                  0.10906      0.02022     0.13424      0.07868
Training/qf1_norm                 1022.85937   597.46324   1995.38403   223.76888
Training/qf2_norm                 711.48517    39.81763    777.37506    654.98566
log_std/mean                      -0.13765     0.00015     -0.13744     -0.13784
log_probs/mean                    -2.73932     0.00800     -2.73156     -2.75454
mean/mean                         -0.00320     0.00009     -0.00305     -0.00329
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01882648468017578
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70782
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [87900]
collect time 0.0009055137634277344
inner_dict_sum {'sac_diff0': 0.00025653839111328125, 'sac_diff1': 0.007786273956298828, 'sac_diff2': 0.01076054573059082, 'sac_diff3': 0.012100696563720703, 'sac_diff4': 0.008185625076293945, 'sac_diff5': 0.03871417045593262, 'sac_diff6': 0.0004935264587402344, 'all': 0.07829737663269043}
diff5_list [0.007987022399902344, 0.007621288299560547, 0.007977724075317383, 0.007521152496337891, 0.007606983184814453]
time3 0
time4 0.07912039756774902
time5 0.07918214797973633
time7 7.152557373046875e-07
gen_weight_change tensor(-19.3386)
policy weight change tensor(37.5339, grad_fn=<SumBackward0>)
time8 0.0020694732666015625
train_time 0.09121084213256836
eval time 0.13622260093688965
epoch last part time 8.58306884765625e-06
2024-01-23 01:03:22,757 MainThread INFO: EPOCH:579
2024-01-23 01:03:22,757 MainThread INFO: Time Consumed:0.23079419136047363s
2024-01-23 01:03:22,758 MainThread INFO: Total Frames:87750s
  6%|▌         | 580/10000 [04:50<40:01,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12081.89351
Train_Epoch_Reward                40485.18593
Running_Training_Average_Rewards  15340.39438
Explore_Time                      0.00090
Train___Time                      0.09121
Eval____Time                      0.13622
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11930.64166
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.14081     2.04692    92.79731     87.33705
alpha_0                           0.74825      0.00011    0.74840      0.74810
Alpha_loss                        -1.95210     0.00072    -1.95081     -1.95302
Training/policy_loss              -2.97383     0.00364    -2.96990     -2.97832
Training/qf1_loss                 6696.24023   766.11185  7685.38184   5828.01172
Training/qf2_loss                 15333.85371  823.28395  16271.38867  13937.72656
Training/pf_norm                  0.11373      0.01931    0.12526      0.07538
Training/qf1_norm                 409.90027    113.31093  615.72943    288.68530
Training/qf2_norm                 781.86309    17.64556   804.11133    757.89410
log_std/mean                      -0.12801     0.00004    -0.12795     -0.12805
log_probs/mean                    -2.73326     0.00437    -2.72878     -2.73875
mean/mean                         -0.00466     0.00014    -0.00445     -0.00484
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01857304573059082
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70782
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [88050]
collect time 0.0008685588836669922
inside mustsac before update, task 0, sumup 70782
inside mustsac after update, task 0, sumup 69634
inner_dict_sum {'sac_diff0': 0.00026416778564453125, 'sac_diff1': 0.008081197738647461, 'sac_diff2': 0.00982975959777832, 'sac_diff3': 0.012341737747192383, 'sac_diff4': 0.008726358413696289, 'sac_diff5': 0.059484004974365234, 'sac_diff6': 0.0005102157592773438, 'all': 0.09923744201660156}
diff5_list [0.012508630752563477, 0.012789487838745117, 0.011454343795776367, 0.011875152587890625, 0.010856389999389648]
time3 0.0011653900146484375
time4 0.10011577606201172
time5 0.10017251968383789
time7 0.00938105583190918
gen_weight_change tensor(-19.2461)
policy weight change tensor(37.4704, grad_fn=<SumBackward0>)
time8 0.0025970935821533203
train_time 0.1325664520263672
eval time 0.1053459644317627
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:23,020 MainThread INFO: EPOCH:580
2024-01-23 01:03:23,021 MainThread INFO: Time Consumed:0.24118590354919434s
2024-01-23 01:03:23,021 MainThread INFO: Total Frames:87900s
  6%|▌         | 581/10000 [04:50<40:34,  3.87it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12057.23390
Train_Epoch_Reward                4629.04743
Running_Training_Average_Rewards  14859.80706
Explore_Time                      0.00086
Train___Time                      0.13257
Eval____Time                      0.10535
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12003.28355
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.69283     1.86108    91.99803     86.87935
alpha_0                           0.74787      0.00011    0.74802      0.74773
Alpha_loss                        -1.95650     0.00234    -1.95236     -1.95936
Training/policy_loss              -2.79026     0.07317    -2.69618     -2.91123
Training/qf1_loss                 7228.04102   390.78686  7523.34229   6468.53857
Training/qf2_loss                 15795.21543  693.32597  16420.95312  14520.07520
Training/pf_norm                  0.10983      0.01686    0.13643      0.08483
Training/qf1_norm                 892.17328    172.80130  1184.36682   698.71686
Training/qf2_norm                 635.02495    40.73669   679.42358    566.40869
log_std/mean                      -0.12981     0.00807    -0.11564     -0.13823
log_probs/mean                    -2.73682     0.00681    -2.72718     -2.74435
mean/mean                         -0.00338     0.00036    -0.00289     -0.00399
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01899266242980957
epoch last part time3 0.0029325485229492188
inside rlalgo, task 0, sumup 69634
epoch first part time 3.337860107421875e-06
replay_buffer._size: [88200]
collect time 0.0009903907775878906
inner_dict_sum {'sac_diff0': 0.0002148151397705078, 'sac_diff1': 0.007232189178466797, 'sac_diff2': 0.00867915153503418, 'sac_diff3': 0.011553049087524414, 'sac_diff4': 0.007664918899536133, 'sac_diff5': 0.034070491790771484, 'sac_diff6': 0.00039577484130859375, 'all': 0.06981039047241211}
diff5_list [0.006738424301147461, 0.006165504455566406, 0.007415294647216797, 0.00712132453918457, 0.00662994384765625]
time3 0
time4 0.07058095932006836
time5 0.07063031196594238
time7 4.76837158203125e-07
gen_weight_change tensor(-19.2461)
policy weight change tensor(37.5004, grad_fn=<SumBackward0>)
time8 0.0019097328186035156
train_time 0.08203482627868652
eval time 0.15334558486938477
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:23,284 MainThread INFO: EPOCH:581
2024-01-23 01:03:23,285 MainThread INFO: Time Consumed:0.23881816864013672s
2024-01-23 01:03:23,285 MainThread INFO: Total Frames:88050s
  6%|▌         | 582/10000 [04:50<40:41,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12025.32146
Train_Epoch_Reward                20729.06200
Running_Training_Average_Rewards  14745.12729
Explore_Time                      0.00099
Train___Time                      0.08203
Eval____Time                      0.15335
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11896.18152
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.54842     2.14619     91.88966     85.95757
alpha_0                           0.74750      0.00011     0.74765      0.74735
Alpha_loss                        -1.95939     0.00235     -1.95561     -1.96290
Training/policy_loss              -2.84384     0.00460     -2.83776     -2.85091
Training/qf1_loss                 7315.59453   1035.09023  8979.97070   6020.96875
Training/qf2_loss                 15848.59629  1400.80573  17882.57617  13888.35645
Training/pf_norm                  0.09441      0.02167     0.12611      0.07182
Training/qf1_norm                 436.75079    199.73148   664.95526    134.09265
Training/qf2_norm                 667.80005    15.66510    685.69629    642.05176
log_std/mean                      -0.11922     0.00011     -0.11911     -0.11939
log_probs/mean                    -2.73518     0.00628     -2.72678     -2.74490
mean/mean                         -0.00455     0.00005     -0.00447     -0.00462
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019032001495361328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69634
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [88350]
collect time 0.0009334087371826172
inner_dict_sum {'sac_diff0': 0.00021266937255859375, 'sac_diff1': 0.00782012939453125, 'sac_diff2': 0.009101629257202148, 'sac_diff3': 0.011729717254638672, 'sac_diff4': 0.008009672164916992, 'sac_diff5': 0.03695344924926758, 'sac_diff6': 0.00042366981506347656, 'all': 0.07425093650817871}
diff5_list [0.007489919662475586, 0.0071218013763427734, 0.007486104965209961, 0.007195472717285156, 0.0076601505279541016]
time3 0
time4 0.07512068748474121
time5 0.07517290115356445
time7 9.5367431640625e-07
gen_weight_change tensor(-19.2461)
policy weight change tensor(37.4967, grad_fn=<SumBackward0>)
time8 0.0019102096557617188
train_time 0.08685088157653809
eval time 0.15918540954589844
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:23,556 MainThread INFO: EPOCH:582
2024-01-23 01:03:23,557 MainThread INFO: Time Consumed:0.24941706657409668s
2024-01-23 01:03:23,557 MainThread INFO: Total Frames:88200s
  6%|▌         | 583/10000 [04:51<41:16,  3.80it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12004.97916
Train_Epoch_Reward                63818.87568
Running_Training_Average_Rewards  16627.18012
Explore_Time                      0.00093
Train___Time                      0.08685
Eval____Time                      0.15919
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11989.70696
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.52760     2.56546     95.56795     89.27069
alpha_0                           0.74713      0.00011     0.74728      0.74698
Alpha_loss                        -1.96197     0.00268     -1.95836     -1.96607
Training/policy_loss              -2.88520     0.00581     -2.87934     -2.89577
Training/qf1_loss                 7720.30986   1350.90563  9784.98242   6014.28467
Training/qf2_loss                 16799.73105  1769.20809  19421.68359  14922.01758
Training/pf_norm                  0.09312      0.01421     0.10492      0.06871
Training/qf1_norm                 798.08095    523.74736   1433.63843   175.66679
Training/qf2_norm                 748.85208    20.17330    772.77905    723.26459
log_std/mean                      -0.14145     0.00010     -0.14128     -0.14156
log_probs/mean                    -2.73245     0.00803     -2.72456     -2.74652
mean/mean                         -0.00469     0.00007     -0.00462     -0.00482
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01864004135131836
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69634
epoch first part time 2.86102294921875e-06
replay_buffer._size: [88500]
collect time 0.0008566379547119141
inner_dict_sum {'sac_diff0': 0.00021696090698242188, 'sac_diff1': 0.007879018783569336, 'sac_diff2': 0.009389162063598633, 'sac_diff3': 0.012161493301391602, 'sac_diff4': 0.008197546005249023, 'sac_diff5': 0.03714132308959961, 'sac_diff6': 0.00042319297790527344, 'all': 0.0754086971282959}
diff5_list [0.006848335266113281, 0.007822990417480469, 0.008070945739746094, 0.007227182388305664, 0.0071718692779541016]
time3 0
time4 0.07627058029174805
time5 0.07632756233215332
time7 7.152557373046875e-07
gen_weight_change tensor(-19.2461)
policy weight change tensor(37.4793, grad_fn=<SumBackward0>)
time8 0.0019621849060058594
train_time 0.08780169486999512
eval time 0.16075634956359863
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:23,830 MainThread INFO: EPOCH:583
2024-01-23 01:03:23,831 MainThread INFO: Time Consumed:0.25188207626342773s
2024-01-23 01:03:23,831 MainThread INFO: Total Frames:88350s
  6%|▌         | 584/10000 [04:51<41:47,  3.75it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11990.12361
Train_Epoch_Reward                13950.61862
Running_Training_Average_Rewards  16700.70601
Explore_Time                      0.00085
Train___Time                      0.08780
Eval____Time                      0.16076
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12017.29521
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.16110     1.36658     91.98645     88.97260
alpha_0                           0.74675      0.00011     0.74690      0.74660
Alpha_loss                        -1.96550     0.00292     -1.96266     -1.97011
Training/policy_loss              -2.82845     0.00540     -2.82183     -2.83754
Training/qf1_loss                 7417.12773   1311.74241  9257.72070   5543.92334
Training/qf2_loss                 16085.52832  1582.48606  18322.98047  13958.69238
Training/pf_norm                  0.11315      0.02814     0.15708      0.08538
Training/qf1_norm                 1084.41683   299.81300   1354.64478   661.59259
Training/qf2_norm                 677.98508    10.46390    692.70416    669.07812
log_std/mean                      -0.12559     0.00001     -0.12558     -0.12561
log_probs/mean                    -2.73300     0.00709     -2.72560     -2.74418
mean/mean                         -0.00591     0.00002     -0.00588     -0.00593
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018776655197143555
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69634
epoch first part time 3.814697265625e-06
replay_buffer._size: [88650]
collect time 0.0008656978607177734
inner_dict_sum {'sac_diff0': 0.0002257823944091797, 'sac_diff1': 0.006712913513183594, 'sac_diff2': 0.007901668548583984, 'sac_diff3': 0.010713577270507812, 'sac_diff4': 0.0071985721588134766, 'sac_diff5': 0.033571481704711914, 'sac_diff6': 0.00043392181396484375, 'all': 0.0667579174041748}
diff5_list [0.006814002990722656, 0.006932973861694336, 0.006292104721069336, 0.0062830448150634766, 0.007249355316162109]
time3 0
time4 0.06764030456542969
time5 0.06769132614135742
time7 9.5367431640625e-07
gen_weight_change tensor(-19.2461)
policy weight change tensor(37.4819, grad_fn=<SumBackward0>)
time8 0.0022749900817871094
train_time 0.07963871955871582
eval time 0.16910791397094727
epoch last part time 7.867813110351562e-06
2024-01-23 01:03:24,105 MainThread INFO: EPOCH:584
2024-01-23 01:03:24,105 MainThread INFO: Time Consumed:0.25206637382507324s
2024-01-23 01:03:24,105 MainThread INFO: Total Frames:88500s
  6%|▌         | 585/10000 [04:51<42:14,  3.71it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11968.32835
Train_Epoch_Reward                16384.85443
Running_Training_Average_Rewards  16517.88323
Explore_Time                      0.00086
Train___Time                      0.07964
Eval____Time                      0.16911
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11931.53916
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.83167     1.76708    90.62575     86.58224
alpha_0                           0.74638      0.00011    0.74653      0.74623
Alpha_loss                        -1.97002     0.00221    -1.96608     -1.97272
Training/policy_loss              -2.82885     0.00560    -2.82231     -2.83653
Training/qf1_loss                 6731.02295   618.22482  7553.16797   5830.12500
Training/qf2_loss                 15144.97129  916.81385  16306.94727  13860.34668
Training/pf_norm                  0.07650      0.01602    0.10541      0.06306
Training/qf1_norm                 392.42278    82.99787   515.57208    270.14053
Training/qf2_norm                 666.61542    13.22791   679.72186    650.44672
log_std/mean                      -0.13195     0.00002    -0.13192     -0.13198
log_probs/mean                    -2.73695     0.00685    -2.72808     -2.74619
mean/mean                         -0.00296     0.00006    -0.00289     -0.00306
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02025628089904785
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69634
epoch first part time 3.337860107421875e-06
replay_buffer._size: [88800]
collect time 0.0008616447448730469
inside mustsac before update, task 0, sumup 69634
inside mustsac after update, task 0, sumup 69871
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.007323026657104492, 'sac_diff2': 0.008767127990722656, 'sac_diff3': 0.011487483978271484, 'sac_diff4': 0.008243083953857422, 'sac_diff5': 0.053583383560180664, 'sac_diff6': 0.000423431396484375, 'all': 0.09004044532775879}
diff5_list [0.011595726013183594, 0.010287761688232422, 0.011866092681884766, 0.010067939758300781, 0.009765863418579102]
time3 0.0009026527404785156
time4 0.09096789360046387
time5 0.09102296829223633
time7 0.009296894073486328
gen_weight_change tensor(-19.1888)
policy weight change tensor(37.4390, grad_fn=<SumBackward0>)
time8 0.0018880367279052734
train_time 0.12140750885009766
eval time 0.12111759185791016
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:24,375 MainThread INFO: EPOCH:585
2024-01-23 01:03:24,375 MainThread INFO: Time Consumed:0.24581575393676758s
2024-01-23 01:03:24,375 MainThread INFO: Total Frames:88650s
  6%|▌         | 586/10000 [04:51<42:13,  3.72it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11938.56419
Train_Epoch_Reward                4082.46627
Running_Training_Average_Rewards  16247.51786
Explore_Time                      0.00086
Train___Time                      0.12141
Eval____Time                      0.12112
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11865.38649
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.80057     1.21794    91.59802     88.05799
alpha_0                           0.74601      0.00011    0.74616      0.74586
Alpha_loss                        -1.97220     0.00140    -1.97013     -1.97380
Training/policy_loss              -2.85157     0.05810    -2.77553     -2.92938
Training/qf1_loss                 6878.14707   662.97820  8104.13037   6172.26660
Training/qf2_loss                 15458.78066  765.05166  16780.98047  14386.30273
Training/pf_norm                  0.10277      0.02237    0.13197      0.07397
Training/qf1_norm                 609.26057    241.36788  1001.13629   289.66095
Training/qf2_norm                 681.07936    42.74022   748.22723    620.20599
log_std/mean                      -0.13388     0.00502    -0.12518     -0.13978
log_probs/mean                    -2.73290     0.00224    -2.73030     -2.73606
mean/mean                         -0.00362     0.00032    -0.00318     -0.00413
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019448041915893555
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69871
epoch first part time 2.86102294921875e-06
replay_buffer._size: [88950]
collect time 0.0008740425109863281
inner_dict_sum {'sac_diff0': 0.00021648406982421875, 'sac_diff1': 0.0075566768646240234, 'sac_diff2': 0.009171485900878906, 'sac_diff3': 0.01172018051147461, 'sac_diff4': 0.007830381393432617, 'sac_diff5': 0.03635764122009277, 'sac_diff6': 0.0004305839538574219, 'all': 0.07328343391418457}
diff5_list [0.007966041564941406, 0.0069463253021240234, 0.007074832916259766, 0.007094621658325195, 0.007275819778442383]
time3 0
time4 0.07414913177490234
time5 0.0742037296295166
time7 7.152557373046875e-07
gen_weight_change tensor(-19.1888)
policy weight change tensor(37.4763, grad_fn=<SumBackward0>)
time8 0.001965045928955078
train_time 0.08606624603271484
eval time 0.15323901176452637
epoch last part time 7.62939453125e-06
2024-01-23 01:03:24,640 MainThread INFO: EPOCH:586
2024-01-23 01:03:24,641 MainThread INFO: Time Consumed:0.24263787269592285s
2024-01-23 01:03:24,641 MainThread INFO: Total Frames:88800s
  6%|▌         | 587/10000 [04:52<42:05,  3.73it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11926.78340
Train_Epoch_Reward                8572.24387
Running_Training_Average_Rewards  16455.05674
Explore_Time                      0.00087
Train___Time                      0.08607
Eval____Time                      0.15324
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11791.99315
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.31046     1.12571    92.80625     89.96476
alpha_0                           0.74563      0.00011    0.74578      0.74549
Alpha_loss                        -1.97626     0.00216    -1.97278     -1.97919
Training/policy_loss              -2.82252     0.00532    -2.81717     -2.83186
Training/qf1_loss                 7145.55176   174.09227  7387.08301   6862.77637
Training/qf2_loss                 15979.50645  340.69470  16455.12305  15600.50293
Training/pf_norm                  0.09828      0.01767    0.12209      0.07207
Training/qf1_norm                 1371.33228   201.54623  1566.93994   1116.83337
Training/qf2_norm                 694.80900    8.36415    705.39142    684.40912
log_std/mean                      -0.12563     0.00009    -0.12551     -0.12576
log_probs/mean                    -2.73525     0.00705    -2.72799     -2.74756
mean/mean                         -0.00421     0.00005    -0.00412     -0.00424
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02019810676574707
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69871
epoch first part time 2.86102294921875e-06
replay_buffer._size: [89100]
collect time 0.0008704662322998047
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.007806062698364258, 'sac_diff2': 0.008905887603759766, 'sac_diff3': 0.011535167694091797, 'sac_diff4': 0.0077555179595947266, 'sac_diff5': 0.035883188247680664, 'sac_diff6': 0.0004258155822753906, 'all': 0.07252931594848633}
diff5_list [0.0075757503509521484, 0.007409811019897461, 0.006946563720703125, 0.007024049758911133, 0.006927013397216797]
time3 0
time4 0.07342529296875
time5 0.07347869873046875
time7 7.152557373046875e-07
gen_weight_change tensor(-19.1888)
policy weight change tensor(37.5032, grad_fn=<SumBackward0>)
time8 0.0019693374633789062
train_time 0.08486127853393555
eval time 0.16712427139282227
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:24,919 MainThread INFO: EPOCH:587
2024-01-23 01:03:24,920 MainThread INFO: Time Consumed:0.255298376083374s
2024-01-23 01:03:24,920 MainThread INFO: Total Frames:88950s
  6%|▌         | 588/10000 [04:52<42:30,  3.69it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11914.74025
Train_Epoch_Reward                6527.65854
Running_Training_Average_Rewards  16380.07978
Explore_Time                      0.00087
Train___Time                      0.08486
Eval____Time                      0.16712
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11802.28190
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.98700     2.37726     93.10693     86.57830
alpha_0                           0.74526      0.00011     0.74541      0.74511
Alpha_loss                        -1.97852     0.00093     -1.97674     -1.97931
Training/policy_loss              -2.84486     0.00191     -2.84258     -2.84834
Training/qf1_loss                 7347.11455   1676.60721  10484.11328  6114.52051
Training/qf2_loss                 15986.32852  2084.58091  19735.07031  14095.96582
Training/pf_norm                  0.09847      0.02099     0.11466      0.05775
Training/qf1_norm                 453.98610    211.47534   731.53394    207.22081
Training/qf2_norm                 682.19543    17.66883    705.44330    656.84882
log_std/mean                      -0.13210     0.00001     -0.13208     -0.13212
log_probs/mean                    -2.73149     0.00203     -2.72914     -2.73505
mean/mean                         -0.00546     0.00012     -0.00532     -0.00564
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018251419067382812
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69871
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [89250]
collect time 0.0008521080017089844
inner_dict_sum {'sac_diff0': 0.0002346038818359375, 'sac_diff1': 0.008774757385253906, 'sac_diff2': 0.010686159133911133, 'sac_diff3': 0.012921333312988281, 'sac_diff4': 0.009270429611206055, 'sac_diff5': 0.04197382926940918, 'sac_diff6': 0.0005340576171875, 'all': 0.08439517021179199}
diff5_list [0.010702133178710938, 0.007816314697265625, 0.008109807968139648, 0.008277416229248047, 0.007068157196044922]
time3 0
time4 0.08556723594665527
time5 0.08564567565917969
time7 9.5367431640625e-07
gen_weight_change tensor(-19.1888)
policy weight change tensor(37.4976, grad_fn=<SumBackward0>)
time8 0.002244234085083008
train_time 0.09807538986206055
eval time 0.14829421043395996
epoch last part time 5.7220458984375e-06
2024-01-23 01:03:25,191 MainThread INFO: EPOCH:588
2024-01-23 01:03:25,192 MainThread INFO: Time Consumed:0.2499678134918213s
2024-01-23 01:03:25,192 MainThread INFO: Total Frames:89100s
  6%|▌         | 589/10000 [04:52<42:41,  3.67it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11904.86378
Train_Epoch_Reward                4471.06468
Running_Training_Average_Rewards  15545.39184
Explore_Time                      0.00085
Train___Time                      0.09808
Eval____Time                      0.14829
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11820.32823
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.02477     2.07300    93.13379     87.64375
alpha_0                           0.74489      0.00011    0.74504      0.74474
Alpha_loss                        -1.98272     0.00180    -1.98010     -1.98503
Training/policy_loss              -2.85794     0.00296    -2.85505     -2.86349
Training/qf1_loss                 7526.72656   393.84654  7983.52100   6860.96484
Training/qf2_loss                 15899.34004  491.70966  16394.83398  14989.73926
Training/pf_norm                  0.08643      0.01820    0.11873      0.06390
Training/qf1_norm                 1411.84700   367.02629  2129.50488   1118.47241
Training/qf2_norm                 667.08832    15.33094   697.40784    656.08844
log_std/mean                      -0.13555     0.00004    -0.13548     -0.13560
log_probs/mean                    -2.73431     0.00419    -2.72999     -2.74218
mean/mean                         -0.00608     0.00002    -0.00606     -0.00610
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.021692991256713867
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69871
epoch first part time 5.9604644775390625e-06
replay_buffer._size: [89400]
collect time 0.0011436939239501953
inner_dict_sum {'sac_diff0': 0.00021648406982421875, 'sac_diff1': 0.0073354244232177734, 'sac_diff2': 0.009208202362060547, 'sac_diff3': 0.011196374893188477, 'sac_diff4': 0.007429838180541992, 'sac_diff5': 0.033659934997558594, 'sac_diff6': 0.00044035911560058594, 'all': 0.06948661804199219}
diff5_list [0.007631063461303711, 0.006512165069580078, 0.006175994873046875, 0.0069580078125, 0.00638270378112793]
time3 0
time4 0.07024979591369629
time5 0.0702981948852539
time7 4.76837158203125e-07
gen_weight_change tensor(-19.1888)
policy weight change tensor(37.4357, grad_fn=<SumBackward0>)
time8 0.0019211769104003906
train_time 0.0822153091430664
eval time 0.16265392303466797
epoch last part time 7.62939453125e-06
2024-01-23 01:03:25,465 MainThread INFO: EPOCH:589
2024-01-23 01:03:25,466 MainThread INFO: Time Consumed:0.2484149932861328s
2024-01-23 01:03:25,466 MainThread INFO: Total Frames:89250s
  6%|▌         | 590/10000 [04:53<42:39,  3.68it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11902.41347
Train_Epoch_Reward                17276.50911
Running_Training_Average_Rewards  16078.26120
Explore_Time                      0.00114
Train___Time                      0.08222
Eval____Time                      0.16265
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11906.13855
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.31749     1.64154    93.19713     88.40002
alpha_0                           0.74452      0.00011    0.74467      0.74437
Alpha_loss                        -1.98694     0.00211    -1.98430     -1.99068
Training/policy_loss              -2.84047     0.00364    -2.83639     -2.84559
Training/qf1_loss                 7184.96494   616.03344  8052.65918   6230.16699
Training/qf2_loss                 15863.40840  899.60883  17299.32422  14519.61523
Training/pf_norm                  0.09021      0.02142    0.12465      0.05913
Training/qf1_norm                 665.87669    303.68348  1035.36121   120.80421
Training/qf2_norm                 696.61564    12.65136   719.27112    682.39160
log_std/mean                      -0.13780     0.00014    -0.13755     -0.13794
log_probs/mean                    -2.73720     0.00544    -2.73053     -2.74530
mean/mean                         -0.00276     0.00001    -0.00275     -0.00278
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018947839736938477
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69871
epoch first part time 3.337860107421875e-06
replay_buffer._size: [89550]
collect time 0.0009610652923583984
inside mustsac before update, task 0, sumup 69871
inside mustsac after update, task 0, sumup 70860
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.007311344146728516, 'sac_diff2': 0.008938312530517578, 'sac_diff3': 0.011045217514038086, 'sac_diff4': 0.007696628570556641, 'sac_diff5': 0.05293893814086914, 'sac_diff6': 0.00041961669921875, 'all': 0.08857202529907227}
diff5_list [0.01126551628112793, 0.009955644607543945, 0.009664535522460938, 0.012014389038085938, 0.01003885269165039]
time3 0.0008761882781982422
time4 0.08946824073791504
time5 0.08952212333679199
time7 0.008935689926147461
gen_weight_change tensor(-18.9208)
policy weight change tensor(37.3977, grad_fn=<SumBackward0>)
time8 0.002623319625854492
train_time 0.11997747421264648
eval time 0.1072702407836914
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:25,719 MainThread INFO: EPOCH:590
2024-01-23 01:03:25,719 MainThread INFO: Time Consumed:0.2306065559387207s
2024-01-23 01:03:25,719 MainThread INFO: Total Frames:89400s
  6%|▌         | 591/10000 [04:53<41:52,  3.74it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11897.08829
Train_Epoch_Reward                8674.91073
Running_Training_Average_Rewards  15377.41553
Explore_Time                      0.00096
Train___Time                      0.11998
Eval____Time                      0.10727
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11950.03179
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.63177     0.50505    92.26001     90.76485
alpha_0                           0.74414      0.00011    0.74429      0.74400
Alpha_loss                        -1.99001     0.00099    -1.98837     -1.99105
Training/policy_loss              -2.82007     0.02699    -2.78450     -2.85375
Training/qf1_loss                 7576.94551   530.20004  8383.27246   6882.76416
Training/qf2_loss                 16482.54980  577.88311  17304.98438  15611.71875
Training/pf_norm                  0.10420      0.03212    0.16522      0.07716
Training/qf1_norm                 724.92071    418.19475  1353.58521   235.21803
Training/qf2_norm                 675.76711    21.73192   705.98053    638.57800
log_std/mean                      -0.13100     0.00328    -0.12639     -0.13404
log_probs/mean                    -2.73621     0.00549    -2.72970     -2.74430
mean/mean                         -0.00384     0.00117    -0.00243     -0.00573
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018231868743896484
epoch last part time3 0.002878427505493164
inside rlalgo, task 0, sumup 70860
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [89700]
collect time 0.0009601116180419922
inner_dict_sum {'sac_diff0': 0.00021314620971679688, 'sac_diff1': 0.006869792938232422, 'sac_diff2': 0.007980823516845703, 'sac_diff3': 0.010311365127563477, 'sac_diff4': 0.007002115249633789, 'sac_diff5': 0.03153228759765625, 'sac_diff6': 0.0004191398620605469, 'all': 0.06432867050170898}
diff5_list [0.0065898895263671875, 0.006424427032470703, 0.006357908248901367, 0.005860805511474609, 0.006299257278442383]
time3 0
time4 0.06511712074279785
time5 0.06516242027282715
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9208)
policy weight change tensor(37.2846, grad_fn=<SumBackward0>)
time8 0.0020024776458740234
train_time 0.07639455795288086
eval time 0.15101861953735352
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:25,974 MainThread INFO: EPOCH:591
2024-01-23 01:03:25,974 MainThread INFO: Time Consumed:0.2307605743408203s
2024-01-23 01:03:25,974 MainThread INFO: Total Frames:89550s
  6%|▌         | 592/10000 [04:53<41:09,  3.81it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11895.96464
Train_Epoch_Reward                12776.42033
Running_Training_Average_Rewards  14702.41192
Explore_Time                      0.00096
Train___Time                      0.07639
Eval____Time                      0.15102
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11884.94498
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.83307     1.42907    89.99995     86.12761
alpha_0                           0.74377      0.00011    0.74392      0.74362
Alpha_loss                        -1.99308     0.00324    -1.98812     -1.99676
Training/policy_loss              -2.81588     0.00611    -2.80709     -2.82400
Training/qf1_loss                 6293.88359   419.21558  7018.73926   5838.99463
Training/qf2_loss                 14515.84375  533.25733  15431.25488  13829.67480
Training/pf_norm                  0.12542      0.01541    0.14051      0.10584
Training/qf1_norm                 366.66022    245.73232  737.20056    92.09513
Training/qf2_norm                 648.00417    10.20680   663.61798    635.71094
log_std/mean                      -0.13271     0.00020    -0.13242     -0.13297
log_probs/mean                    -2.73520     0.00843    -2.72297     -2.74617
mean/mean                         -0.00265     0.00001    -0.00263     -0.00267
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01805853843688965
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70860
epoch first part time 2.86102294921875e-06
replay_buffer._size: [89850]
collect time 0.0010001659393310547
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.007387399673461914, 'sac_diff2': 0.008638381958007812, 'sac_diff3': 0.010420083999633789, 'sac_diff4': 0.007222175598144531, 'sac_diff5': 0.032958269119262695, 'sac_diff6': 0.0003917217254638672, 'all': 0.06723904609680176}
diff5_list [0.007015705108642578, 0.00751948356628418, 0.0064160823822021484, 0.006000041961669922, 0.006006956100463867]
time3 0
time4 0.06807661056518555
time5 0.0681312084197998
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9208)
policy weight change tensor(37.2061, grad_fn=<SumBackward0>)
time8 0.0019423961639404297
train_time 0.07944107055664062
eval time 0.15505266189575195
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:26,234 MainThread INFO: EPOCH:592
2024-01-23 01:03:26,234 MainThread INFO: Time Consumed:0.23797392845153809s
2024-01-23 01:03:26,234 MainThread INFO: Total Frames:89700s
  6%|▌         | 593/10000 [04:53<41:06,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11887.35520
Train_Epoch_Reward                22297.54469
Running_Training_Average_Rewards  14810.20989
Explore_Time                      0.00099
Train___Time                      0.07944
Eval____Time                      0.15505
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11903.61257
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.07040     2.26471     94.12246     87.61858
alpha_0                           0.74340      0.00011     0.74355      0.74325
Alpha_loss                        -1.99647     0.00339     -1.99396     -2.00310
Training/policy_loss              -2.82136     0.00649     -2.81508     -2.83344
Training/qf1_loss                 7393.98848   645.55699   8168.39062   6417.59082
Training/qf2_loss                 16202.84746  1097.68862  17579.45703  14527.66211
Training/pf_norm                  0.11837      0.00601     0.12706      0.10883
Training/qf1_norm                 614.71714    414.13864   1263.54309   124.11116
Training/qf2_norm                 666.27255    16.49022    688.50507    641.05042
log_std/mean                      -0.13219     0.00008     -0.13210     -0.13234
log_probs/mean                    -2.73525     0.00945     -2.72678     -2.75306
mean/mean                         -0.00368     0.00015     -0.00346     -0.00384
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019772768020629883
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70860
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [90000]
collect time 0.0009677410125732422
inner_dict_sum {'sac_diff0': 0.00023603439331054688, 'sac_diff1': 0.006631612777709961, 'sac_diff2': 0.007573843002319336, 'sac_diff3': 0.009556770324707031, 'sac_diff4': 0.006615161895751953, 'sac_diff5': 0.03197884559631348, 'sac_diff6': 0.0003859996795654297, 'all': 0.06297826766967773}
diff5_list [0.006528377532958984, 0.00600123405456543, 0.0063440799713134766, 0.006209850311279297, 0.006895303726196289]
time3 0
time4 0.06373929977416992
time5 0.06378364562988281
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9208)
policy weight change tensor(37.1458, grad_fn=<SumBackward0>)
time8 0.0018985271453857422
train_time 0.0747523307800293
eval time 0.1527557373046875
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:26,488 MainThread INFO: EPOCH:593
2024-01-23 01:03:26,488 MainThread INFO: Time Consumed:0.23086285591125488s
2024-01-23 01:03:26,488 MainThread INFO: Total Frames:89850s
  6%|▌         | 594/10000 [04:54<40:39,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11885.53490
Train_Epoch_Reward                2732.16987
Running_Training_Average_Rewards  13766.41371
Explore_Time                      0.00096
Train___Time                      0.07475
Eval____Time                      0.15276
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11999.09222
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.21728     1.40151     92.98918     89.05500
alpha_0                           0.74303      0.00011     0.74318      0.74288
Alpha_loss                        -1.99978     0.00212     -1.99654     -2.00196
Training/policy_loss              -2.83413     0.00273     -2.83003     -2.83786
Training/qf1_loss                 8049.19687   1034.00394  9505.43848   6564.47363
Training/qf2_loss                 16881.03320  1218.89286  18663.73242  14999.74023
Training/pf_norm                  0.10051      0.01695     0.12822      0.08331
Training/qf1_norm                 1071.09436   255.34661   1397.44324   665.82520
Training/qf2_norm                 677.48374    10.33514    691.01227    661.97937
log_std/mean                      -0.13361     0.00009     -0.13344     -0.13369
log_probs/mean                    -2.73507     0.00431     -2.72870     -2.74007
mean/mean                         -0.00389     0.00012     -0.00375     -0.00405
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0182797908782959
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70860
epoch first part time 2.86102294921875e-06
replay_buffer._size: [90150]
collect time 0.0010519027709960938
inner_dict_sum {'sac_diff0': 0.00021791458129882812, 'sac_diff1': 0.007318019866943359, 'sac_diff2': 0.008710145950317383, 'sac_diff3': 0.011111974716186523, 'sac_diff4': 0.0072977542877197266, 'sac_diff5': 0.03293180465698242, 'sac_diff6': 0.00040435791015625, 'all': 0.06799197196960449}
diff5_list [0.0067441463470458984, 0.007080793380737305, 0.0068511962890625, 0.0062067508697509766, 0.006048917770385742]
time3 0
time4 0.06886577606201172
time5 0.06891942024230957
time7 9.5367431640625e-07
gen_weight_change tensor(-18.9208)
policy weight change tensor(37.1313, grad_fn=<SumBackward0>)
time8 0.001802206039428711
train_time 0.0801095962524414
eval time 0.14917469024658203
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:26,743 MainThread INFO: EPOCH:594
2024-01-23 01:03:26,743 MainThread INFO: Time Consumed:0.2327289581298828s
2024-01-23 01:03:26,743 MainThread INFO: Total Frames:90000s
  6%|▌         | 595/10000 [04:54<40:26,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11900.42298
Train_Epoch_Reward                26057.89533
Running_Training_Average_Rewards  14083.22330
Explore_Time                      0.00105
Train___Time                      0.08011
Eval____Time                      0.14917
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12080.41996
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.38776     1.88455    93.63078     88.15404
alpha_0                           0.74266      0.00011    0.74281      0.74251
Alpha_loss                        -2.00218     0.00262    -1.99824     -2.00544
Training/policy_loss              -2.82025     0.00435    -2.81379     -2.82523
Training/qf1_loss                 6941.79092   606.93942  7848.14355   5985.95752
Training/qf2_loss                 15469.29160  865.49181  16486.28320  14040.83398
Training/pf_norm                  0.10206      0.02833    0.13294      0.06552
Training/qf1_norm                 2669.05798   384.06062  3088.23975   2004.27136
Training/qf2_norm                 671.53519    13.31047   694.20734    655.65314
log_std/mean                      -0.12600     0.00003    -0.12598     -0.12605
log_probs/mean                    -2.73180     0.00568    -2.72309     -2.73825
mean/mean                         -0.00458     0.00004    -0.00451     -0.00463
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01817464828491211
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70860
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [90300]
collect time 0.0008461475372314453
inside mustsac before update, task 0, sumup 70860
inside mustsac after update, task 0, sumup 70952
inner_dict_sum {'sac_diff0': 0.0002162456512451172, 'sac_diff1': 0.0070953369140625, 'sac_diff2': 0.008815526962280273, 'sac_diff3': 0.010972976684570312, 'sac_diff4': 0.007566213607788086, 'sac_diff5': 0.05168318748474121, 'sac_diff6': 0.0004096031188964844, 'all': 0.08675909042358398}
diff5_list [0.0106353759765625, 0.009697437286376953, 0.010019540786743164, 0.011500358581542969, 0.009830474853515625]
time3 0.0008747577667236328
time4 0.08763551712036133
time5 0.08768868446350098
time7 0.009281396865844727
gen_weight_change tensor(-18.7914)
policy weight change tensor(37.1132, grad_fn=<SumBackward0>)
time8 0.0018939971923828125
train_time 0.11723184585571289
eval time 0.11327624320983887
epoch last part time 7.62939453125e-06
2024-01-23 01:03:26,998 MainThread INFO: EPOCH:595
2024-01-23 01:03:26,999 MainThread INFO: Time Consumed:0.23379230499267578s
2024-01-23 01:03:26,999 MainThread INFO: Total Frames:90150s
  6%|▌         | 596/10000 [04:54<40:20,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11926.09748
Train_Epoch_Reward                21761.27122
Running_Training_Average_Rewards  14111.20037
Explore_Time                      0.00084
Train___Time                      0.11723
Eval____Time                      0.11328
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12122.13142
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.05770     2.96649     95.04302     87.27616
alpha_0                           0.74229      0.00010     0.74243      0.74214
Alpha_loss                        -2.00509     0.00200     -2.00267     -2.00756
Training/policy_loss              -2.87344     0.06720     -2.78065     -2.96075
Training/qf1_loss                 7870.23027   1970.45111  11211.63477  5347.68896
Training/qf2_loss                 16612.11914  2486.66909  20753.82422  13363.92383
Training/pf_norm                  0.11383      0.02073     0.13699      0.08282
Training/qf1_norm                 1582.03707   1118.23996  3470.73779   186.74939
Training/qf2_norm                 700.52664    61.70212    775.20569    606.66809
log_std/mean                      -0.13055     0.01208     -0.11059     -0.14317
log_probs/mean                    -2.73027     0.00504     -2.72440     -2.73778
mean/mean                         -0.00503     0.00033     -0.00462     -0.00547
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018362998962402344
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70952
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [90450]
collect time 0.0010988712310791016
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.0074462890625, 'sac_diff2': 0.008700370788574219, 'sac_diff3': 0.011571168899536133, 'sac_diff4': 0.007615089416503906, 'sac_diff5': 0.03469991683959961, 'sac_diff6': 0.00041866302490234375, 'all': 0.07066226005554199}
diff5_list [0.0074939727783203125, 0.00700831413269043, 0.006829261779785156, 0.006432294845581055, 0.006936073303222656]
time3 0
time4 0.07150721549987793
time5 0.07155728340148926
time7 7.152557373046875e-07
gen_weight_change tensor(-18.7914)
policy weight change tensor(37.1402, grad_fn=<SumBackward0>)
time8 0.0019910335540771484
train_time 0.08317685127258301
eval time 0.14965581893920898
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:27,257 MainThread INFO: EPOCH:596
2024-01-23 01:03:27,257 MainThread INFO: Time Consumed:0.23647475242614746s
2024-01-23 01:03:27,257 MainThread INFO: Total Frames:90300s
  6%|▌         | 597/10000 [04:54<40:23,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11952.37282
Train_Epoch_Reward                14824.01890
Running_Training_Average_Rewards  14262.99655
Explore_Time                      0.00109
Train___Time                      0.08318
Eval____Time                      0.14966
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12054.74660
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.48050     1.76505     94.54595     89.55404
alpha_0                           0.74191      0.00010     0.74206      0.74177
Alpha_loss                        -2.00967     0.00277     -2.00601     -2.01312
Training/policy_loss              -2.74022     0.00440     -2.73483     -2.74588
Training/qf1_loss                 7927.13867   1146.10326  9767.73145   6383.30762
Training/qf2_loss                 16834.16738  1324.17320  18562.33594  14917.46387
Training/pf_norm                  0.09254      0.01851     0.11925      0.06805
Training/qf1_norm                 334.34140    272.69219   843.87061    82.95525
Training/qf2_norm                 611.28359    11.50787    631.44409    598.96149
log_std/mean                      -0.13327     0.00007     -0.13320     -0.13338
log_probs/mean                    -2.73434     0.00625     -2.72658     -2.74211
mean/mean                         -0.00295     0.00008     -0.00283     -0.00306
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018159151077270508
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70952
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [90600]
collect time 0.0008823871612548828
inner_dict_sum {'sac_diff0': 0.00022077560424804688, 'sac_diff1': 0.007541179656982422, 'sac_diff2': 0.008376121520996094, 'sac_diff3': 0.01080322265625, 'sac_diff4': 0.007215976715087891, 'sac_diff5': 0.03364825248718262, 'sac_diff6': 0.0004208087921142578, 'all': 0.06822633743286133}
diff5_list [0.006651401519775391, 0.006623983383178711, 0.0077168941497802734, 0.0063800811767578125, 0.00627589225769043]
time3 0
time4 0.0690925121307373
time5 0.0691533088684082
time7 7.152557373046875e-07
gen_weight_change tensor(-18.7914)
policy weight change tensor(37.1097, grad_fn=<SumBackward0>)
time8 0.0019588470458984375
train_time 0.0820322036743164
eval time 0.15008902549743652
epoch last part time 8.344650268554688e-06
2024-01-23 01:03:27,514 MainThread INFO: EPOCH:597
2024-01-23 01:03:27,515 MainThread INFO: Time Consumed:0.23545503616333008s
2024-01-23 01:03:27,515 MainThread INFO: Total Frames:90450s
  6%|▌         | 598/10000 [04:55<40:23,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11986.73972
Train_Epoch_Reward                10538.75198
Running_Training_Average_Rewards  14398.29263
Explore_Time                      0.00088
Train___Time                      0.08203
Eval____Time                      0.15009
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12145.95091
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.32960     0.82515    91.42086     89.33839
alpha_0                           0.74154      0.00010    0.74169      0.74140
Alpha_loss                        -2.01370     0.00238    -2.00997     -2.01728
Training/policy_loss              -2.95245     0.00434    -2.94630     -2.95957
Training/qf1_loss                 6932.29268   722.90885  7991.44629   6069.03418
Training/qf2_loss                 15611.35020  773.21992  16880.24609  14885.84570
Training/pf_norm                  0.10095      0.02128    0.12903      0.06638
Training/qf1_norm                 851.75249    163.43005  1066.05273   624.61591
Training/qf2_norm                 742.90164    6.52788    751.66595    734.89087
log_std/mean                      -0.13593     0.00007    -0.13583     -0.13601
log_probs/mean                    -2.73656     0.00579    -2.72860     -2.74629
mean/mean                         -0.00474     0.00006    -0.00465     -0.00482
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018881559371948242
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70952
epoch first part time 3.814697265625e-06
replay_buffer._size: [90750]
collect time 0.0009849071502685547
inner_dict_sum {'sac_diff0': 0.0002295970916748047, 'sac_diff1': 0.00701451301574707, 'sac_diff2': 0.008008003234863281, 'sac_diff3': 0.010523319244384766, 'sac_diff4': 0.006998777389526367, 'sac_diff5': 0.03216719627380371, 'sac_diff6': 0.00039267539978027344, 'all': 0.06533408164978027}
diff5_list [0.006995439529418945, 0.006400346755981445, 0.0065042972564697266, 0.006083011627197266, 0.006184101104736328]
time3 0
time4 0.06608963012695312
time5 0.06613636016845703
time7 7.152557373046875e-07
gen_weight_change tensor(-18.7914)
policy weight change tensor(37.1175, grad_fn=<SumBackward0>)
time8 0.0017952919006347656
train_time 0.0773622989654541
eval time 0.1514582633972168
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:27,769 MainThread INFO: EPOCH:598
2024-01-23 01:03:27,769 MainThread INFO: Time Consumed:0.23210668563842773s
2024-01-23 01:03:27,770 MainThread INFO: Total Frames:90600s
  6%|▌         | 599/10000 [04:55<40:13,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12028.59554
Train_Epoch_Reward                20483.12790
Running_Training_Average_Rewards  14474.79805
Explore_Time                      0.00097
Train___Time                      0.07736
Eval____Time                      0.15146
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12238.88643
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.27612     1.64198    91.55215     86.81985
alpha_0                           0.74117      0.00010    0.74132      0.74103
Alpha_loss                        -2.01566     0.00244    -2.01264     -2.01892
Training/policy_loss              -2.95593     0.00363    -2.95105     -2.96115
Training/qf1_loss                 6675.47842   494.47235  7275.78271   5788.93115
Training/qf2_loss                 15161.21738  790.25953  16186.24219  13813.37793
Training/pf_norm                  0.11011      0.02082    0.13870      0.08147
Training/qf1_norm                 314.82599    140.19426  511.07584    139.28500
Training/qf2_norm                 736.24602    13.50054   754.48541    715.87726
log_std/mean                      -0.12269     0.00012    -0.12255     -0.12289
log_probs/mean                    -2.73187     0.00575    -2.72402     -2.74050
mean/mean                         -0.00254     0.00006    -0.00248     -0.00264
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018323659896850586
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70952
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [90900]
collect time 0.0008387565612792969
inner_dict_sum {'sac_diff0': 0.00020265579223632812, 'sac_diff1': 0.007047414779663086, 'sac_diff2': 0.0076885223388671875, 'sac_diff3': 0.010423660278320312, 'sac_diff4': 0.0069768428802490234, 'sac_diff5': 0.031934261322021484, 'sac_diff6': 0.0004062652587890625, 'all': 0.06467962265014648}
diff5_list [0.006620883941650391, 0.006385087966918945, 0.0061991214752197266, 0.006293296813964844, 0.006435871124267578]
time3 0
time4 0.06548547744750977
time5 0.06552910804748535
time7 4.76837158203125e-07
gen_weight_change tensor(-18.7914)
policy weight change tensor(37.1289, grad_fn=<SumBackward0>)
time8 0.0019397735595703125
train_time 0.07687139511108398
eval time 0.1524369716644287
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:28,024 MainThread INFO: EPOCH:599
2024-01-23 01:03:28,024 MainThread INFO: Time Consumed:0.23256206512451172s
2024-01-23 01:03:28,024 MainThread INFO: Total Frames:90750s
  6%|▌         | 600/10000 [04:55<40:10,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12071.74467
Train_Epoch_Reward                19298.81657
Running_Training_Average_Rewards  14808.40983
Explore_Time                      0.00083
Train___Time                      0.07687
Eval____Time                      0.15244
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12337.62979
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.72638     3.01584     94.46614     87.60609
alpha_0                           0.74080      0.00010     0.74095      0.74065
Alpha_loss                        -2.02089     0.00238     -2.01857     -2.02510
Training/policy_loss              -2.82982     0.00508     -2.82201     -2.83648
Training/qf1_loss                 6762.00244   1232.47186  8410.35645   5520.37012
Training/qf2_loss                 15514.00137  1780.55102  17844.97266  13702.42480
Training/pf_norm                  0.08483      0.00928     0.10050      0.07493
Training/qf1_norm                 1025.09849   574.51998   1792.24451   434.36218
Training/qf2_norm                 669.71469    21.79450    696.20422    647.34302
log_std/mean                      -0.13499     0.00001     -0.13498     -0.13500
log_probs/mean                    -2.73808     0.00692     -2.72809     -2.74763
mean/mean                         -0.00409     0.00010     -0.00395     -0.00425
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019254207611083984
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70952
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [91050]
collect time 0.0008618831634521484
inside mustsac before update, task 0, sumup 70952
inside mustsac after update, task 0, sumup 70507
inner_dict_sum {'sac_diff0': 0.00021982192993164062, 'sac_diff1': 0.00746464729309082, 'sac_diff2': 0.00933384895324707, 'sac_diff3': 0.01115560531616211, 'sac_diff4': 0.007928609848022461, 'sac_diff5': 0.051581382751464844, 'sac_diff6': 0.00041413307189941406, 'all': 0.08809804916381836}
diff5_list [0.010733366012573242, 0.011787891387939453, 0.00994110107421875, 0.009661197662353516, 0.009457826614379883]
time3 0.000873565673828125
time4 0.08899211883544922
time5 0.08904671669006348
time7 0.009270668029785156
gen_weight_change tensor(-18.9320)
policy weight change tensor(37.1714, grad_fn=<SumBackward0>)
time8 0.002696990966796875
train_time 0.11954641342163086
eval time 0.1094508171081543
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:28,279 MainThread INFO: EPOCH:600
2024-01-23 01:03:28,279 MainThread INFO: Time Consumed:0.2322251796722412s
2024-01-23 01:03:28,279 MainThread INFO: Total Frames:90900s
  6%|▌         | 601/10000 [04:55<40:15,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12119.14047
Train_Epoch_Reward                13464.46395
Running_Training_Average_Rewards  15055.80443
Explore_Time                      0.00086
Train___Time                      0.11955
Eval____Time                      0.10945
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12423.98981
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.92251     0.79995    91.93903     89.95329
alpha_0                           0.74043      0.00010    0.74058      0.74028
Alpha_loss                        -2.02297     0.00055    -2.02247     -2.02393
Training/policy_loss              -2.87929     0.02477    -2.83911     -2.91493
Training/qf1_loss                 7085.29072   408.35195  7765.47656   6622.57324
Training/qf2_loss                 15876.16953  446.33790  16585.61914  15453.06836
Training/pf_norm                  0.09214      0.01831    0.11832      0.06725
Training/qf1_norm                 621.08327    591.75582  1722.57092   126.02551
Training/qf2_norm                 707.47952    24.01089   752.81018    683.37103
log_std/mean                      -0.13151     0.00864    -0.11764     -0.14122
log_probs/mean                    -2.73378     0.00296    -2.73016     -2.73697
mean/mean                         -0.00398     0.00038    -0.00338     -0.00456
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019330263137817383
epoch last part time3 0.003076791763305664
inside rlalgo, task 0, sumup 70507
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [91200]
collect time 0.0009722709655761719
inner_dict_sum {'sac_diff0': 0.0002200603485107422, 'sac_diff1': 0.007464885711669922, 'sac_diff2': 0.008816242218017578, 'sac_diff3': 0.010862112045288086, 'sac_diff4': 0.007250785827636719, 'sac_diff5': 0.0334625244140625, 'sac_diff6': 0.0004494190216064453, 'all': 0.06852602958679199}
diff5_list [0.007281303405761719, 0.0067861080169677734, 0.006452083587646484, 0.006298542022705078, 0.006644487380981445]
time3 0
time4 0.06943941116333008
time5 0.06950068473815918
time7 9.5367431640625e-07
gen_weight_change tensor(-18.9320)
policy weight change tensor(37.1847, grad_fn=<SumBackward0>)
time8 0.0018949508666992188
train_time 0.08077001571655273
eval time 0.1411430835723877
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:28,530 MainThread INFO: EPOCH:601
2024-01-23 01:03:28,530 MainThread INFO: Time Consumed:0.2253110408782959s
2024-01-23 01:03:28,531 MainThread INFO: Total Frames:91050s
  6%|▌         | 602/10000 [04:56<39:47,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12208.91507
Train_Epoch_Reward                8873.86116
Running_Training_Average_Rewards  14692.62372
Explore_Time                      0.00097
Train___Time                      0.08077
Eval____Time                      0.14114
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12782.69101
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.69941     2.09065    93.51607     87.90996
alpha_0                           0.74006      0.00010    0.74021      0.73991
Alpha_loss                        -2.02623     0.00117    -2.02496     -2.02765
Training/policy_loss              -3.03234     0.00398    -3.02701     -3.03864
Training/qf1_loss                 7467.03311   691.14887  8363.08789   6340.02441
Training/qf2_loss                 16187.34883  857.58359  16871.77734  14523.12500
Training/pf_norm                  0.14563      0.03392    0.19771      0.10280
Training/qf1_norm                 395.77196    196.75638  682.42297    128.87311
Training/qf2_norm                 825.64806    19.93299   850.79242    799.26025
log_std/mean                      -0.12159     0.00004    -0.12153     -0.12164
log_probs/mean                    -2.73343     0.00482    -2.72564     -2.74023
mean/mean                         -0.00346     0.00004    -0.00339     -0.00350
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018538475036621094
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70507
epoch first part time 2.86102294921875e-06
replay_buffer._size: [91350]
collect time 0.0009455680847167969
inner_dict_sum {'sac_diff0': 0.00022745132446289062, 'sac_diff1': 0.007309436798095703, 'sac_diff2': 0.008654594421386719, 'sac_diff3': 0.010840415954589844, 'sac_diff4': 0.0071141719818115234, 'sac_diff5': 0.032763004302978516, 'sac_diff6': 0.0004010200500488281, 'all': 0.06731009483337402}
diff5_list [0.0065653324127197266, 0.006218671798706055, 0.007331371307373047, 0.00643467903137207, 0.006212949752807617]
time3 0
time4 0.06815981864929199
time5 0.06821012496948242
time7 9.5367431640625e-07
gen_weight_change tensor(-18.9320)
policy weight change tensor(37.1622, grad_fn=<SumBackward0>)
time8 0.0019440650939941406
train_time 0.07944273948669434
eval time 0.15120530128479004
epoch last part time 7.62939453125e-06
2024-01-23 01:03:28,786 MainThread INFO: EPOCH:602
2024-01-23 01:03:28,787 MainThread INFO: Time Consumed:0.23406720161437988s
2024-01-23 01:03:28,787 MainThread INFO: Total Frames:91200s
  6%|▌         | 603/10000 [04:56<39:57,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12316.29625
Train_Epoch_Reward                10719.11950
Running_Training_Average_Rewards  14555.96342
Explore_Time                      0.00094
Train___Time                      0.07944
Eval____Time                      0.15121
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12977.42434
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.40455     1.89684    92.38867     86.59944
alpha_0                           0.73969      0.00010    0.73984      0.73954
Alpha_loss                        -2.02965     0.00244    -2.02613     -2.03271
Training/policy_loss              -2.95263     0.00643    -2.94474     -2.96391
Training/qf1_loss                 6691.25869   644.41544  7432.41113   5727.01465
Training/qf2_loss                 15174.75957  942.23025  16110.96289  13688.52637
Training/pf_norm                  0.12567      0.03460    0.19242      0.09726
Training/qf1_norm                 417.77106    220.32189  809.05957    168.60654
Training/qf2_norm                 742.46295    15.84378   767.72113    719.59869
log_std/mean                      -0.13701     0.00008    -0.13687     -0.13709
log_probs/mean                    -2.73359     0.00871    -2.72193     -2.74822
mean/mean                         -0.00509     0.00005    -0.00499     -0.00513
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019756555557250977
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70507
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [91500]
collect time 0.0010001659393310547
inner_dict_sum {'sac_diff0': 0.00023365020751953125, 'sac_diff1': 0.0066509246826171875, 'sac_diff2': 0.007930755615234375, 'sac_diff3': 0.010065078735351562, 'sac_diff4': 0.0067424774169921875, 'sac_diff5': 0.03156733512878418, 'sac_diff6': 0.0003886222839355469, 'all': 0.06357884407043457}
diff5_list [0.006592750549316406, 0.00615382194519043, 0.006111621856689453, 0.0065653324127197266, 0.006143808364868164]
time3 0
time4 0.06432795524597168
time5 0.06437420845031738
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9320)
policy weight change tensor(37.1021, grad_fn=<SumBackward0>)
time8 0.0018754005432128906
train_time 0.07547116279602051
eval time 0.16491270065307617
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:29,054 MainThread INFO: EPOCH:603
2024-01-23 01:03:29,054 MainThread INFO: Time Consumed:0.24393343925476074s
2024-01-23 01:03:29,055 MainThread INFO: Total Frames:91350s
  6%|▌         | 604/10000 [04:56<40:35,  3.86it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12436.56699
Train_Epoch_Reward                2826.08557
Running_Training_Average_Rewards  14573.87201
Explore_Time                      0.00099
Train___Time                      0.07547
Eval____Time                      0.16491
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13201.79966
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.93502     1.88440    92.26579     87.89352
alpha_0                           0.73932      0.00010    0.73947      0.73917
Alpha_loss                        -2.03125     0.00345    -2.02840     -2.03802
Training/policy_loss              -2.93087     0.00716    -2.92431     -2.94430
Training/qf1_loss                 7562.32227   448.30298  8258.06836   6943.44043
Training/qf2_loss                 16191.95586  718.61008  17368.41797  15385.10742
Training/pf_norm                  0.09577      0.01286    0.12033      0.08440
Training/qf1_norm                 845.11830    355.80517  1279.15173   366.44449
Training/qf2_norm                 743.91733    14.54710   761.86414    728.98193
log_std/mean                      -0.13322     0.00011    -0.13309     -0.13340
log_probs/mean                    -2.72774     0.00962    -2.71831     -2.74570
mean/mean                         -0.00395     0.00008    -0.00385     -0.00406
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02067089080810547
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70507
epoch first part time 3.337860107421875e-06
replay_buffer._size: [91650]
collect time 0.0009465217590332031
inner_dict_sum {'sac_diff0': 0.00024127960205078125, 'sac_diff1': 0.007571220397949219, 'sac_diff2': 0.008998394012451172, 'sac_diff3': 0.011232852935791016, 'sac_diff4': 0.007199764251708984, 'sac_diff5': 0.03329300880432129, 'sac_diff6': 0.0004189014434814453, 'all': 0.0689554214477539}
diff5_list [0.0065762996673583984, 0.006395816802978516, 0.007321834564208984, 0.00658106803894043, 0.006417989730834961]
time3 0
time4 0.06981587409973145
time5 0.06986689567565918
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9320)
policy weight change tensor(37.1297, grad_fn=<SumBackward0>)
time8 0.0019311904907226562
train_time 0.08118414878845215
eval time 0.1515817642211914
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:29,315 MainThread INFO: EPOCH:604
2024-01-23 01:03:29,315 MainThread INFO: Time Consumed:0.23611068725585938s
2024-01-23 01:03:29,315 MainThread INFO: Total Frames:91500s
  6%|▌         | 605/10000 [04:56<40:34,  3.86it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12560.85262
Train_Epoch_Reward                30302.35876
Running_Training_Average_Rewards  15449.88915
Explore_Time                      0.00094
Train___Time                      0.08118
Eval____Time                      0.15158
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13323.27627
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.33095     1.96852    91.46029     87.00770
alpha_0                           0.73895      0.00010    0.73910      0.73881
Alpha_loss                        -2.03624     0.00203    -2.03328     -2.03817
Training/policy_loss              -2.87071     0.00541    -2.86429     -2.87990
Training/qf1_loss                 6708.43437   648.60410  7319.17725   5560.00293
Training/qf2_loss                 15219.24102  848.97557  16144.09961  13616.57227
Training/pf_norm                  0.10119      0.01732    0.11811      0.07015
Training/qf1_norm                 486.72717    386.19788  978.71259    98.73886
Training/qf2_norm                 686.36702    14.48970   703.05377    669.30322
log_std/mean                      -0.12589     0.00008    -0.12580     -0.12603
log_probs/mean                    -2.73313     0.00662    -2.72556     -2.74398
mean/mean                         -0.00468     0.00012    -0.00451     -0.00484
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019002199172973633
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70507
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [91800]
collect time 0.0010480880737304688
inside mustsac before update, task 0, sumup 70507
inside mustsac after update, task 0, sumup 70429
inner_dict_sum {'sac_diff0': 0.00021600723266601562, 'sac_diff1': 0.006951332092285156, 'sac_diff2': 0.008234500885009766, 'sac_diff3': 0.010472297668457031, 'sac_diff4': 0.007450103759765625, 'sac_diff5': 0.050621986389160156, 'sac_diff6': 0.00040650367736816406, 'all': 0.08435273170471191}
diff5_list [0.010697126388549805, 0.009406089782714844, 0.010004997253417969, 0.009215593338012695, 0.011298179626464844]
time3 0.0008628368377685547
time4 0.08522462844848633
time5 0.08527636528015137
time7 0.009235382080078125
gen_weight_change tensor(-18.9774)
policy weight change tensor(37.1128, grad_fn=<SumBackward0>)
time8 0.001898050308227539
train_time 0.11465334892272949
eval time 0.11710405349731445
epoch last part time 5.9604644775390625e-06
2024-01-23 01:03:29,573 MainThread INFO: EPOCH:605
2024-01-23 01:03:29,573 MainThread INFO: Time Consumed:0.23519515991210938s
2024-01-23 01:03:29,573 MainThread INFO: Total Frames:91650s
  6%|▌         | 606/10000 [04:57<40:30,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12681.77552
Train_Epoch_Reward                22454.08454
Running_Training_Average_Rewards  15695.86022
Explore_Time                      0.00104
Train___Time                      0.11465
Eval____Time                      0.11710
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13331.36039
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.32163     1.93817     94.63591     88.58763
alpha_0                           0.73858      0.00010     0.73873      0.73844
Alpha_loss                        -2.04142     0.00103     -2.03995     -2.04305
Training/policy_loss              -2.89499     0.05271     -2.81154     -2.96241
Training/qf1_loss                 7531.85645   888.06031   8888.11328   6356.47363
Training/qf2_loss                 16385.34512  1058.48874  17660.39062  15288.96289
Training/pf_norm                  0.09188      0.02757     0.13915      0.06311
Training/qf1_norm                 903.12072    388.22093   1211.95581   136.25104
Training/qf2_norm                 718.37091    44.33237    769.55383    646.44897
log_std/mean                      -0.12974     0.00348     -0.12550     -0.13500
log_probs/mean                    -2.73911     0.00203     -2.73705     -2.74251
mean/mean                         -0.00392     0.00161     -0.00176     -0.00639
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018828392028808594
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70429
epoch first part time 3.337860107421875e-06
replay_buffer._size: [91950]
collect time 0.0008881092071533203
inner_dict_sum {'sac_diff0': 0.00024247169494628906, 'sac_diff1': 0.0068819522857666016, 'sac_diff2': 0.008523225784301758, 'sac_diff3': 0.010685920715332031, 'sac_diff4': 0.007133007049560547, 'sac_diff5': 0.0327143669128418, 'sac_diff6': 0.00040841102600097656, 'all': 0.06658935546875}
diff5_list [0.007131338119506836, 0.006223201751708984, 0.0072133541107177734, 0.006189584732055664, 0.005956888198852539]
time3 0
time4 0.06734013557434082
time5 0.06740331649780273
time7 4.76837158203125e-07
gen_weight_change tensor(-18.9774)
policy weight change tensor(37.1212, grad_fn=<SumBackward0>)
time8 0.0019414424896240234
train_time 0.0785830020904541
eval time 0.14759254455566406
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:29,825 MainThread INFO: EPOCH:606
2024-01-23 01:03:29,825 MainThread INFO: Time Consumed:0.22944259643554688s
2024-01-23 01:03:29,825 MainThread INFO: Total Frames:91800s
  6%|▌         | 607/10000 [04:57<40:10,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12793.72496
Train_Epoch_Reward                13136.68728
Running_Training_Average_Rewards  15976.51064
Explore_Time                      0.00088
Train___Time                      0.07858
Eval____Time                      0.14759
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13174.24101
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.63548     2.13013     92.64329     87.35118
alpha_0                           0.73821      0.00010     0.73836      0.73807
Alpha_loss                        -2.04254     0.00150     -2.04066     -2.04444
Training/policy_loss              -2.95318     0.00211     -2.95076     -2.95638
Training/qf1_loss                 6968.50176   921.02339   8105.12256   5506.21777
Training/qf2_loss                 15716.65840  1315.10498  17150.17383  13601.83789
Training/pf_norm                  0.08815      0.02165     0.11713      0.06153
Training/qf1_norm                 427.77052    380.37362   1056.35095   123.53490
Training/qf2_norm                 747.64712    16.83290    763.75024    721.66272
log_std/mean                      -0.14170     0.00006     -0.14162     -0.14177
log_probs/mean                    -2.73170     0.00352     -2.72772     -2.73795
mean/mean                         -0.00140     0.00004     -0.00136     -0.00148
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01874852180480957
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70429
epoch first part time 3.337860107421875e-06
replay_buffer._size: [92100]
collect time 0.0009036064147949219
inner_dict_sum {'sac_diff0': 0.00022649765014648438, 'sac_diff1': 0.007054328918457031, 'sac_diff2': 0.008392810821533203, 'sac_diff3': 0.010951757431030273, 'sac_diff4': 0.006875038146972656, 'sac_diff5': 0.03164958953857422, 'sac_diff6': 0.00038361549377441406, 'all': 0.06553363800048828}
diff5_list [0.006688833236694336, 0.0064239501953125, 0.0062258243560791016, 0.006308555603027344, 0.0060024261474609375]
time3 0
time4 0.06627345085144043
time5 0.06631970405578613
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9774)
policy weight change tensor(37.1429, grad_fn=<SumBackward0>)
time8 0.0018651485443115234
train_time 0.07721900939941406
eval time 0.14975261688232422
epoch last part time 7.3909759521484375e-06
2024-01-23 01:03:30,077 MainThread INFO: EPOCH:607
2024-01-23 01:03:30,078 MainThread INFO: Time Consumed:0.2303910255432129s
2024-01-23 01:03:30,078 MainThread INFO: Total Frames:91950s
  6%|▌         | 608/10000 [04:57<40:00,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12889.92437
Train_Epoch_Reward                38905.47899
Running_Training_Average_Rewards  16827.60427
Explore_Time                      0.00090
Train___Time                      0.07722
Eval____Time                      0.14975
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13107.94494
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.32917     2.48725     95.95326     89.01565
alpha_0                           0.73785      0.00010     0.73799      0.73770
Alpha_loss                        -2.04591     0.00089     -2.04476     -2.04731
Training/policy_loss              -2.88649     0.00133     -2.88544     -2.88907
Training/qf1_loss                 7220.44316   968.08705   9020.15039   6360.42529
Training/qf2_loss                 16086.39395  1393.02934  18746.14453  14844.03613
Training/pf_norm                  0.09960      0.02561     0.13683      0.06582
Training/qf1_norm                 762.55375    456.63853   1604.79480   320.78912
Training/qf2_norm                 739.59562    19.80026    776.54297    721.26550
log_std/mean                      -0.12363     0.00014     -0.12349     -0.12387
log_probs/mean                    -2.73171     0.00176     -2.73014     -2.73496
mean/mean                         -0.00394     0.00006     -0.00386     -0.00403
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019170761108398438
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70429
epoch first part time 2.86102294921875e-06
replay_buffer._size: [92250]
collect time 0.0009431838989257812
inner_dict_sum {'sac_diff0': 0.0002205371856689453, 'sac_diff1': 0.007438182830810547, 'sac_diff2': 0.008651018142700195, 'sac_diff3': 0.010831594467163086, 'sac_diff4': 0.007353067398071289, 'sac_diff5': 0.03362703323364258, 'sac_diff6': 0.0004029273986816406, 'all': 0.06852436065673828}
diff5_list [0.006738185882568359, 0.006646871566772461, 0.007547855377197266, 0.00645756721496582, 0.006236553192138672]
time3 0
time4 0.06933832168579102
time5 0.06939029693603516
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9774)
policy weight change tensor(37.2350, grad_fn=<SumBackward0>)
time8 0.00189971923828125
train_time 0.08068156242370605
eval time 0.15059709548950195
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:30,335 MainThread INFO: EPOCH:608
2024-01-23 01:03:30,335 MainThread INFO: Time Consumed:0.23475980758666992s
2024-01-23 01:03:30,336 MainThread INFO: Total Frames:92100s
  6%|▌         | 609/10000 [04:57<40:04,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12965.47114
Train_Epoch_Reward                6062.01138
Running_Training_Average_Rewards  16903.88884
Explore_Time                      0.00094
Train___Time                      0.08068
Eval____Time                      0.15060
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12994.35414
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.38475     1.41477     92.19960     87.91364
alpha_0                           0.73748      0.00010     0.73762      0.73733
Alpha_loss                        -2.04803     0.00143     -2.04607     -2.05015
Training/policy_loss              -2.77977     0.00193     -2.77754     -2.78224
Training/qf1_loss                 6735.85166   1190.81928  8142.47510   4931.89160
Training/qf2_loss                 15424.06484  1467.97530  16984.19531  13097.22461
Training/pf_norm                  0.11511      0.03554     0.18067      0.08135
Training/qf1_norm                 951.37650    317.29806   1531.18286   591.51410
Training/qf2_norm                 668.75625    10.50675    680.73120    649.92322
log_std/mean                      -0.13149     0.00020     -0.13119     -0.13174
log_probs/mean                    -2.72761     0.00243     -2.72489     -2.73085
mean/mean                         -0.00353     0.00005     -0.00348     -0.00361
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01868462562561035
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70429
epoch first part time 3.337860107421875e-06
replay_buffer._size: [92400]
collect time 0.0011057853698730469
inner_dict_sum {'sac_diff0': 0.00021886825561523438, 'sac_diff1': 0.006977081298828125, 'sac_diff2': 0.00816488265991211, 'sac_diff3': 0.010316848754882812, 'sac_diff4': 0.0067596435546875, 'sac_diff5': 0.03166627883911133, 'sac_diff6': 0.0004012584686279297, 'all': 0.06450486183166504}
diff5_list [0.006688833236694336, 0.006259441375732422, 0.006677150726318359, 0.006153106689453125, 0.005887746810913086]
time3 0
time4 0.06528496742248535
time5 0.06533265113830566
time7 4.76837158203125e-07
gen_weight_change tensor(-18.9774)
policy weight change tensor(37.3256, grad_fn=<SumBackward0>)
time8 0.0019528865814208984
train_time 0.0768885612487793
eval time 0.14656448364257812
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:30,585 MainThread INFO: EPOCH:609
2024-01-23 01:03:30,585 MainThread INFO: Time Consumed:0.2270047664642334s
2024-01-23 01:03:30,585 MainThread INFO: Total Frames:92250s
  6%|▌         | 610/10000 [04:58<39:45,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13028.86501
Train_Epoch_Reward                21171.28486
Running_Training_Average_Rewards  16260.09214
Explore_Time                      0.00110
Train___Time                      0.07689
Eval____Time                      0.14656
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12971.56854
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.45184     1.60151    92.19437     87.66597
alpha_0                           0.73711      0.00010    0.73726      0.73696
Alpha_loss                        -2.05154     0.00228    -2.04707     -2.05319
Training/policy_loss              -3.03868     0.00443    -3.03084     -3.04226
Training/qf1_loss                 7135.90381   662.66585  7930.52490   6402.69141
Training/qf2_loss                 15843.31758  893.65663  16881.00391  14567.52832
Training/pf_norm                  0.13109      0.05181    0.21531      0.08313
Training/qf1_norm                 608.63557    302.36877  1132.83325   289.84619
Training/qf2_norm                 816.75620    13.70514   831.17145    792.96509
log_std/mean                      -0.12862     0.00015    -0.12844     -0.12886
log_probs/mean                    -2.72809     0.00538    -2.71786     -2.73253
mean/mean                         -0.00379     0.00004    -0.00375     -0.00383
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018327713012695312
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70429
epoch first part time 2.86102294921875e-06
replay_buffer._size: [92550]
collect time 0.0010123252868652344
inside mustsac before update, task 0, sumup 70429
inside mustsac after update, task 0, sumup 70056
inner_dict_sum {'sac_diff0': 0.00021982192993164062, 'sac_diff1': 0.007877826690673828, 'sac_diff2': 0.00964045524597168, 'sac_diff3': 0.01229238510131836, 'sac_diff4': 0.008940458297729492, 'sac_diff5': 0.05903482437133789, 'sac_diff6': 0.0004887580871582031, 'all': 0.0984945297241211}
diff5_list [0.010681629180908203, 0.011373758316040039, 0.012535810470581055, 0.012017011642456055, 0.012426614761352539]
time3 0.0009469985961914062
time4 0.09950542449951172
time5 0.09956574440002441
time7 0.009181976318359375
gen_weight_change tensor(-19.0723)
policy weight change tensor(37.3274, grad_fn=<SumBackward0>)
time8 0.0026450157165527344
train_time 0.13128900527954102
eval time 0.10652041435241699
epoch last part time 4.291534423828125e-06
2024-01-23 01:03:30,848 MainThread INFO: EPOCH:610
2024-01-23 01:03:30,848 MainThread INFO: Time Consumed:0.24098539352416992s
2024-01-23 01:03:30,848 MainThread INFO: Total Frames:92400s
  6%|▌         | 611/10000 [04:58<40:18,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13084.57884
Train_Epoch_Reward                5839.64445
Running_Training_Average_Rewards  16300.44537
Explore_Time                      0.00101
Train___Time                      0.13129
Eval____Time                      0.10652
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12981.12805
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.03333     3.08421     94.96924     86.40724
alpha_0                           0.73674      0.00010     0.73689      0.73659
Alpha_loss                        -2.05459     0.00181     -2.05175     -2.05680
Training/policy_loss              -2.91939     0.06200     -2.82975     -3.02375
Training/qf1_loss                 8339.00391   993.71158   10280.71484  7585.48682
Training/qf2_loss                 17093.18672  1441.28733  19553.94727  15471.47363
Training/pf_norm                  0.08912      0.01763     0.10800      0.06435
Training/qf1_norm                 1846.65176   640.26435   2443.31616   735.98157
Training/qf2_norm                 741.30386    29.72254    780.04059    703.67438
log_std/mean                      -0.12822     0.00506     -0.12014     -0.13611
log_probs/mean                    -2.72708     0.00324     -2.72218     -2.73213
mean/mean                         -0.00402     0.00117     -0.00280     -0.00611
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018723726272583008
epoch last part time3 0.0025429725646972656
inside rlalgo, task 0, sumup 70056
epoch first part time 2.86102294921875e-06
replay_buffer._size: [92700]
collect time 0.0009248256683349609
inner_dict_sum {'sac_diff0': 0.00023889541625976562, 'sac_diff1': 0.007071733474731445, 'sac_diff2': 0.00840449333190918, 'sac_diff3': 0.010990619659423828, 'sac_diff4': 0.007344245910644531, 'sac_diff5': 0.03182816505432129, 'sac_diff6': 0.00038123130798339844, 'all': 0.06625938415527344}
diff5_list [0.006657123565673828, 0.006298065185546875, 0.0064504146575927734, 0.006345033645629883, 0.00607752799987793]
time3 0
time4 0.06701898574829102
time5 0.06706619262695312
time7 4.76837158203125e-07
gen_weight_change tensor(-19.0723)
policy weight change tensor(37.3340, grad_fn=<SumBackward0>)
time8 0.0018572807312011719
train_time 0.07800030708312988
eval time 0.1485438346862793
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:31,102 MainThread INFO: EPOCH:611
2024-01-23 01:03:31,102 MainThread INFO: Time Consumed:0.22989511489868164s
2024-01-23 01:03:31,103 MainThread INFO: Total Frames:92550s
  6%|▌         | 612/10000 [04:58<40:01,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13156.40116
Train_Epoch_Reward                9293.47317
Running_Training_Average_Rewards  15919.25908
Explore_Time                      0.00092
Train___Time                      0.07800
Eval____Time                      0.14854
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13500.91420
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.48274     2.67310     93.72921     86.38071
alpha_0                           0.73637      0.00010     0.73652      0.73622
Alpha_loss                        -2.05924     0.00164     -2.05637     -2.06128
Training/policy_loss              -3.00083     0.00406     -2.99577     -3.00804
Training/qf1_loss                 6751.72178   944.14911   8577.69141   5967.40918
Training/qf2_loss                 15297.13496  1419.89413  17923.19141  13921.77637
Training/pf_norm                  0.12183      0.01391     0.13505      0.09561
Training/qf1_norm                 492.98253    242.40843   722.59424    190.33218
Training/qf2_norm                 771.75918    22.24365    805.91211    744.78339
log_std/mean                      -0.13893     0.00001     -0.13891     -0.13894
log_probs/mean                    -2.73129     0.00490     -2.72410     -2.73895
mean/mean                         -0.00275     0.00008     -0.00268     -0.00290
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018678903579711914
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70056
epoch first part time 2.86102294921875e-06
replay_buffer._size: [92850]
collect time 0.0009500980377197266
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.006582021713256836, 'sac_diff2': 0.007840871810913086, 'sac_diff3': 0.010150909423828125, 'sac_diff4': 0.006776571273803711, 'sac_diff5': 0.03149127960205078, 'sac_diff6': 0.000385284423828125, 'all': 0.06344795227050781}
diff5_list [0.006373405456542969, 0.006209373474121094, 0.006173849105834961, 0.006380796432495117, 0.006353855133056641]
time3 0
time4 0.06421017646789551
time5 0.0642540454864502
time7 4.76837158203125e-07
gen_weight_change tensor(-19.0723)
policy weight change tensor(37.3784, grad_fn=<SumBackward0>)
time8 0.0018303394317626953
train_time 0.0752408504486084
eval time 0.14631199836730957
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:31,349 MainThread INFO: EPOCH:612
2024-01-23 01:03:31,349 MainThread INFO: Time Consumed:0.22484111785888672s
2024-01-23 01:03:31,350 MainThread INFO: Total Frames:92700s
  6%|▌         | 613/10000 [04:58<39:36,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13212.50632
Train_Epoch_Reward                6220.21762
Running_Training_Average_Rewards  13999.30381
Explore_Time                      0.00095
Train___Time                      0.07524
Eval____Time                      0.14631
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13538.47599
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.70410     1.68746    93.54073     88.41620
alpha_0                           0.73600      0.00010    0.73615      0.73586
Alpha_loss                        -2.06314     0.00163    -2.06104     -2.06515
Training/policy_loss              -3.00861     0.00268    -3.00571     -3.01267
Training/qf1_loss                 7976.36436   724.08614  9282.53223   7319.22119
Training/qf2_loss                 16655.04570  926.01103  18462.84375  15881.41895
Training/pf_norm                  0.09587      0.03064    0.14897      0.06941
Training/qf1_norm                 1534.52947   315.49433  2098.10181   1148.72766
Training/qf2_norm                 789.87118    14.67747   814.60626    770.19678
log_std/mean                      -0.12504     0.00014    -0.12487     -0.12526
log_probs/mean                    -2.73305     0.00395    -2.72954     -2.73960
mean/mean                         -0.00474     0.00005    -0.00465     -0.00479
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018550634384155273
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70056
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [93000]
collect time 0.0008053779602050781
inner_dict_sum {'sac_diff0': 0.0002231597900390625, 'sac_diff1': 0.00672459602355957, 'sac_diff2': 0.008639097213745117, 'sac_diff3': 0.010339975357055664, 'sac_diff4': 0.007897138595581055, 'sac_diff5': 0.03761172294616699, 'sac_diff6': 0.0004208087921142578, 'all': 0.07185649871826172}
diff5_list [0.01239633560180664, 0.0063626766204833984, 0.006047487258911133, 0.006693840026855469, 0.0061113834381103516]
time3 0
time4 0.07265257835388184
time5 0.07271432876586914
time7 7.152557373046875e-07
gen_weight_change tensor(-19.0723)
policy weight change tensor(37.3581, grad_fn=<SumBackward0>)
time8 0.0023005008697509766
train_time 0.08480167388916016
eval time 0.13331341743469238
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:31,593 MainThread INFO: EPOCH:613
2024-01-23 01:03:31,593 MainThread INFO: Time Consumed:0.2215259075164795s
2024-01-23 01:03:31,593 MainThread INFO: Total Frames:92850s
  6%|▌         | 614/10000 [04:59<39:11,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13245.89134
Train_Epoch_Reward                11614.93882
Running_Training_Average_Rewards  13921.44782
Explore_Time                      0.00080
Train___Time                      0.08480
Eval____Time                      0.13331
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13535.64991
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.65618     2.80287     95.71227     87.26784
alpha_0                           0.73564      0.00010     0.73578      0.73549
Alpha_loss                        -2.06809     0.00125     -2.06684     -2.07018
Training/policy_loss              -2.83941     0.00432     -2.83460     -2.84633
Training/qf1_loss                 7961.60713   1621.11404  10001.78613  5654.10059
Training/qf2_loss                 16898.99609  1989.16764  19260.59375  14360.74707
Training/pf_norm                  0.12245      0.01479     0.13847      0.09807
Training/qf1_norm                 547.56290    342.81708   1061.98022   116.48244
Training/qf2_norm                 694.22827    20.00082    722.71100    662.41693
log_std/mean                      -0.13562     0.00011     -0.13545     -0.13573
log_probs/mean                    -2.73821     0.00538     -2.73195     -2.74721
mean/mean                         -0.00318     0.00009     -0.00304     -0.00328
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01978611946105957
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70056
epoch first part time 5.0067901611328125e-06
replay_buffer._size: [93150]
collect time 0.0009095668792724609
inner_dict_sum {'sac_diff0': 0.00022029876708984375, 'sac_diff1': 0.006560802459716797, 'sac_diff2': 0.007848501205444336, 'sac_diff3': 0.00981760025024414, 'sac_diff4': 0.006846904754638672, 'sac_diff5': 0.03155708312988281, 'sac_diff6': 0.0003998279571533203, 'all': 0.06325101852416992}
diff5_list [0.006514072418212891, 0.006300449371337891, 0.00646519660949707, 0.0062236785888671875, 0.0060536861419677734]
time3 0
time4 0.06398320198059082
time5 0.06402730941772461
time7 4.76837158203125e-07
gen_weight_change tensor(-19.0723)
policy weight change tensor(37.2914, grad_fn=<SumBackward0>)
time8 0.0019381046295166016
train_time 0.07553863525390625
eval time 0.14342379570007324
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:31,838 MainThread INFO: EPOCH:614
2024-01-23 01:03:31,838 MainThread INFO: Time Consumed:0.22211003303527832s
2024-01-23 01:03:31,838 MainThread INFO: Total Frames:93000s
  6%|▌         | 615/10000 [04:59<38:53,  4.02it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13270.42913
Train_Epoch_Reward                17519.57789
Running_Training_Average_Rewards  13959.27193
Explore_Time                      0.00091
Train___Time                      0.07554
Eval____Time                      0.14342
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13568.65417
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.14766     2.51382     93.68291     87.72858
alpha_0                           0.73527      0.00010     0.73542      0.73512
Alpha_loss                        -2.07093     0.00251     -2.06737     -2.07466
Training/policy_loss              -2.92933     0.00397     -2.92466     -2.93480
Training/qf1_loss                 6914.88906   579.80297   7444.81250   5895.62988
Training/qf2_loss                 15704.61406  1085.58275  16761.24805  13963.85742
Training/pf_norm                  0.08845      0.02430     0.12370      0.06002
Training/qf1_norm                 1282.78757   536.26245   2063.03418   686.95471
Training/qf2_norm                 739.77744    20.01303    760.16595    712.50983
log_std/mean                      -0.12954     0.00011     -0.12934     -0.12966
log_probs/mean                    -2.73648     0.00559     -2.72928     -2.74424
mean/mean                         -0.00275     0.00001     -0.00273     -0.00277
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018403291702270508
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70056
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [93300]
collect time 0.0008804798126220703
inside mustsac before update, task 0, sumup 70056
inside mustsac after update, task 0, sumup 71065
inner_dict_sum {'sac_diff0': 0.00020384788513183594, 'sac_diff1': 0.006593942642211914, 'sac_diff2': 0.00796365737915039, 'sac_diff3': 0.010378360748291016, 'sac_diff4': 0.007185220718383789, 'sac_diff5': 0.0500795841217041, 'sac_diff6': 0.00039696693420410156, 'all': 0.08280158042907715}
diff5_list [0.011242389678955078, 0.009832143783569336, 0.009710550308227539, 0.00954890251159668, 0.009745597839355469]
time3 0.0008623600006103516
time4 0.08363127708435059
time5 0.08368110656738281
time7 0.00917196273803711
gen_weight_change tensor(-19.2282)
policy weight change tensor(37.3339, grad_fn=<SumBackward0>)
time8 0.0018737316131591797
train_time 0.11258792877197266
eval time 0.10785460472106934
epoch last part time 4.0531158447265625e-06
2024-01-23 01:03:32,083 MainThread INFO: EPOCH:615
2024-01-23 01:03:32,084 MainThread INFO: Time Consumed:0.22350049018859863s
2024-01-23 01:03:32,084 MainThread INFO: Total Frames:93150s
  6%|▌         | 616/10000 [04:59<38:44,  4.04it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13300.54949
Train_Epoch_Reward                12391.93844
Running_Training_Average_Rewards  14236.25434
Explore_Time                      0.00088
Train___Time                      0.11259
Eval____Time                      0.10785
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13632.56397
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.80575     2.83721    93.28457     85.84487
alpha_0                           0.73490      0.00010    0.73505      0.73475
Alpha_loss                        -2.07240     0.00221    -2.06963     -2.07559
Training/policy_loss              -3.00570     0.04980    -2.94516     -3.08428
Training/qf1_loss                 6822.60645   467.00705  7253.95166   5945.07910
Training/qf2_loss                 15580.08496  920.34449  16299.72070  13778.76270
Training/pf_norm                  0.08732      0.02610    0.13603      0.06256
Training/qf1_norm                 803.36182    445.79608  1435.55286   111.01880
Training/qf2_norm                 790.56410    47.83224   836.95776    707.12225
log_std/mean                      -0.13089     0.00428    -0.12325     -0.13618
log_probs/mean                    -2.73032     0.00681    -2.71915     -2.73631
mean/mean                         -0.00364     0.00062    -0.00278     -0.00469
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018178701400756836
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71065
epoch first part time 2.86102294921875e-06
replay_buffer._size: [93450]
collect time 0.0008409023284912109
inner_dict_sum {'sac_diff0': 0.0002353191375732422, 'sac_diff1': 0.006975412368774414, 'sac_diff2': 0.007988929748535156, 'sac_diff3': 0.010419130325317383, 'sac_diff4': 0.006966590881347656, 'sac_diff5': 0.0321197509765625, 'sac_diff6': 0.0003941059112548828, 'all': 0.06509923934936523}
diff5_list [0.0073375701904296875, 0.006270170211791992, 0.0061376094818115234, 0.006178617477416992, 0.006195783615112305]
time3 0
time4 0.06585574150085449
time5 0.06590032577514648
time7 7.152557373046875e-07
gen_weight_change tensor(-19.2282)
policy weight change tensor(37.2864, grad_fn=<SumBackward0>)
time8 0.0019381046295166016
train_time 0.0769650936126709
eval time 0.1511688232421875
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:32,337 MainThread INFO: EPOCH:616
2024-01-23 01:03:32,337 MainThread INFO: Time Consumed:0.23140764236450195s
2024-01-23 01:03:32,337 MainThread INFO: Total Frames:93300s
  6%|▌         | 617/10000 [04:59<38:59,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13299.02471
Train_Epoch_Reward                19215.61051
Running_Training_Average_Rewards  14591.03323
Explore_Time                      0.00084
Train___Time                      0.07697
Eval____Time                      0.15117
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13158.99320
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.41244     3.31254     94.48840     84.88222
alpha_0                           0.73453      0.00010     0.73468      0.73439
Alpha_loss                        -2.07641     0.00300     -2.07149     -2.08038
Training/policy_loss              -3.01037     0.00778     -2.99998     -3.02330
Training/qf1_loss                 7212.43018   1536.35911  9267.36523   5135.48730
Training/qf2_loss                 15728.34258  2131.06092  18735.55273  13166.51367
Training/pf_norm                  0.10724      0.01809     0.13911      0.08277
Training/qf1_norm                 622.92097    318.92562   1205.60120   321.21408
Training/qf2_norm                 784.43844    29.03909    828.91296    745.37354
log_std/mean                      -0.12960     0.00003     -0.12957     -0.12965
log_probs/mean                    -2.73240     0.01051     -2.71863     -2.74967
mean/mean                         -0.00353     0.00014     -0.00338     -0.00376
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01828908920288086
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71065
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [93600]
collect time 0.0008313655853271484
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.0070705413818359375, 'sac_diff2': 0.008177518844604492, 'sac_diff3': 0.010855674743652344, 'sac_diff4': 0.006855964660644531, 'sac_diff5': 0.032288551330566406, 'sac_diff6': 0.0003902912139892578, 'all': 0.06584358215332031}
diff5_list [0.006722450256347656, 0.006847381591796875, 0.006443977355957031, 0.0060825347900390625, 0.006192207336425781]
time3 0
time4 0.06668233871459961
time5 0.06672835350036621
time7 7.152557373046875e-07
gen_weight_change tensor(-19.2282)
policy weight change tensor(37.3056, grad_fn=<SumBackward0>)
time8 0.0018253326416015625
train_time 0.07766222953796387
eval time 0.14040827751159668
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:32,579 MainThread INFO: EPOCH:617
2024-01-23 01:03:32,580 MainThread INFO: Time Consumed:0.22113537788391113s
2024-01-23 01:03:32,580 MainThread INFO: Total Frames:93450s
  6%|▌         | 618/10000 [05:00<38:40,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13309.62155
Train_Epoch_Reward                8690.51283
Running_Training_Average_Rewards  14663.12837
Explore_Time                      0.00083
Train___Time                      0.07766
Eval____Time                      0.14041
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13213.91336
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.09160     3.09642     96.43916     87.37976
alpha_0                           0.73417      0.00010     0.73431      0.73402
Alpha_loss                        -2.07944     0.00222     -2.07705     -2.08312
Training/policy_loss              -2.99323     0.00517     -2.98607     -3.00164
Training/qf1_loss                 6814.17734   757.97447   8093.86475   5996.64746
Training/qf2_loss                 15505.99902  1403.26951  17950.50586  14085.98633
Training/pf_norm                  0.09986      0.02217     0.12246      0.07141
Training/qf1_norm                 2492.67334   714.54649   3335.27246   1274.75977
Training/qf2_norm                 771.13774    25.88792    815.57880    740.05957
log_std/mean                      -0.11936     0.00004     -0.11932     -0.11943
log_probs/mean                    -2.73132     0.00718     -2.72140     -2.74325
mean/mean                         -0.00383     0.00008     -0.00371     -0.00395
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018245458602905273
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71065
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [93750]
collect time 0.0007967948913574219
inner_dict_sum {'sac_diff0': 0.0002110004425048828, 'sac_diff1': 0.006728649139404297, 'sac_diff2': 0.007786750793457031, 'sac_diff3': 0.009750127792358398, 'sac_diff4': 0.006930112838745117, 'sac_diff5': 0.031203746795654297, 'sac_diff6': 0.0004093647003173828, 'all': 0.0630197525024414}
diff5_list [0.006333827972412109, 0.006153106689453125, 0.0062944889068603516, 0.006263017654418945, 0.006159305572509766]
time3 0
time4 0.06378364562988281
time5 0.0638265609741211
time7 4.76837158203125e-07
gen_weight_change tensor(-19.2282)
policy weight change tensor(37.3235, grad_fn=<SumBackward0>)
time8 0.0018734931945800781
train_time 0.0746912956237793
eval time 0.14364337921142578
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:32,822 MainThread INFO: EPOCH:618
2024-01-23 01:03:32,823 MainThread INFO: Time Consumed:0.22133636474609375s
2024-01-23 01:03:32,823 MainThread INFO: Total Frames:93600s
  6%|▌         | 619/10000 [05:00<38:28,  4.06it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13332.50057
Train_Epoch_Reward                29552.51942
Running_Training_Average_Rewards  15499.17686
Explore_Time                      0.00079
Train___Time                      0.07469
Eval____Time                      0.14364
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13223.14434
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.75857     1.85830     93.50027     88.26375
alpha_0                           0.73380      0.00010     0.73395      0.73365
Alpha_loss                        -2.08367     0.00271     -2.08039     -2.08772
Training/policy_loss              -3.04576     0.00488     -3.03987     -3.05348
Training/qf1_loss                 6907.60947   803.62535   8050.72021   6026.00488
Training/qf2_loss                 15671.31289  1149.76162  17344.08203  14297.70898
Training/pf_norm                  0.09384      0.00698     0.10059      0.08127
Training/qf1_norm                 396.69880    204.24849   739.28912    162.00980
Training/qf2_norm                 820.13344    16.12849    844.38837    799.39746
log_std/mean                      -0.13848     0.00003     -0.13842     -0.13852
log_probs/mean                    -2.73410     0.00692     -2.72559     -2.74502
mean/mean                         -0.00284     0.00009     -0.00271     -0.00297
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01829671859741211
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71065
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [93900]
collect time 0.0009026527404785156
inner_dict_sum {'sac_diff0': 0.00023436546325683594, 'sac_diff1': 0.006267547607421875, 'sac_diff2': 0.007390022277832031, 'sac_diff3': 0.009569168090820312, 'sac_diff4': 0.00646662712097168, 'sac_diff5': 0.030747175216674805, 'sac_diff6': 0.0003743171691894531, 'all': 0.06104922294616699}
diff5_list [0.006394147872924805, 0.006032228469848633, 0.006175518035888672, 0.006136894226074219, 0.0060083866119384766]
time3 0
time4 0.06177186965942383
time5 0.061815738677978516
time7 4.76837158203125e-07
gen_weight_change tensor(-19.2282)
policy weight change tensor(37.3514, grad_fn=<SumBackward0>)
time8 0.0019485950469970703
train_time 0.07255339622497559
eval time 0.14847373962402344
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:33,068 MainThread INFO: EPOCH:619
2024-01-23 01:03:33,069 MainThread INFO: Time Consumed:0.22420978546142578s
2024-01-23 01:03:33,069 MainThread INFO: Total Frames:93750s
  6%|▌         | 620/10000 [05:00<38:27,  4.06it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13353.28039
Train_Epoch_Reward                13918.11057
Running_Training_Average_Rewards  15387.23024
Explore_Time                      0.00090
Train___Time                      0.07255
Eval____Time                      0.14847
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13179.36671
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.72851     2.21833    94.96394     88.98061
alpha_0                           0.73343      0.00010    0.73358      0.73329
Alpha_loss                        -2.08537     0.00307    -2.08139     -2.08932
Training/policy_loss              -2.98639     0.00469    -2.98087     -2.99326
Training/qf1_loss                 7325.89766   546.82165  8185.20312   6690.29785
Training/qf2_loss                 16279.07949  977.84603  17773.01953  15203.84668
Training/pf_norm                  0.09211      0.04124    0.14473      0.04477
Training/qf1_norm                 458.66167    274.34360  819.96667    103.48676
Training/qf2_norm                 808.41346    19.06635   835.82910    784.22900
log_std/mean                      -0.13252     0.00012    -0.13234     -0.13268
log_probs/mean                    -2.72873     0.00710    -2.72022     -2.73931
mean/mean                         -0.00407     0.00007    -0.00396     -0.00413
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018125534057617188
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71065
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [94050]
collect time 0.0008358955383300781
inside mustsac before update, task 0, sumup 71065
inside mustsac after update, task 0, sumup 69914
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.006939411163330078, 'sac_diff2': 0.008370399475097656, 'sac_diff3': 0.01117086410522461, 'sac_diff4': 0.007670164108276367, 'sac_diff5': 0.05149221420288086, 'sac_diff6': 0.0004055500030517578, 'all': 0.0862584114074707}
diff5_list [0.01055455207824707, 0.010818243026733398, 0.010609149932861328, 0.009963274002075195, 0.009546995162963867]
time3 0.0008726119995117188
time4 0.08709979057312012
time5 0.08715057373046875
time7 0.009233713150024414
gen_weight_change tensor(-19.3095)
policy weight change tensor(37.3798, grad_fn=<SumBackward0>)
time8 0.0027472972869873047
train_time 0.11761975288391113
eval time 0.10612702369689941
epoch last part time 4.0531158447265625e-06
2024-01-23 01:03:33,317 MainThread INFO: EPOCH:620
2024-01-23 01:03:33,317 MainThread INFO: Time Consumed:0.22682476043701172s
2024-01-23 01:03:33,317 MainThread INFO: Total Frames:93900s
  6%|▌         | 621/10000 [05:00<38:43,  4.04it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13364.97357
Train_Epoch_Reward                43245.30406
Running_Training_Average_Rewards  16539.57669
Explore_Time                      0.00083
Train___Time                      0.11762
Eval____Time                      0.10613
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13098.05984
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.67354     1.17276    91.71835     88.48547
alpha_0                           0.73307      0.00010    0.73321      0.73292
Alpha_loss                        -2.09182     0.00133    -2.09029     -2.09376
Training/policy_loss              -2.99703     0.03856    -2.94080     -3.03678
Training/qf1_loss                 7176.23594   577.13726  8011.04541   6241.84277
Training/qf2_loss                 15700.54570  546.77832  16684.07227  15188.17480
Training/pf_norm                  0.09591      0.01534    0.11590      0.07519
Training/qf1_norm                 931.57037    657.34183  2188.11938   362.89819
Training/qf2_norm                 783.33020    24.00304   816.24829    755.41034
log_std/mean                      -0.13034     0.00308    -0.12485     -0.13443
log_probs/mean                    -2.73870     0.00453    -2.73375     -2.74711
mean/mean                         -0.00470     0.00074    -0.00378     -0.00583
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01859116554260254
epoch last part time3 0.002552509307861328
inside rlalgo, task 0, sumup 69914
epoch first part time 2.86102294921875e-06
replay_buffer._size: [94200]
collect time 0.0009036064147949219
inner_dict_sum {'sac_diff0': 0.0002307891845703125, 'sac_diff1': 0.00681614875793457, 'sac_diff2': 0.008054018020629883, 'sac_diff3': 0.010025978088378906, 'sac_diff4': 0.0071375370025634766, 'sac_diff5': 0.03190445899963379, 'sac_diff6': 0.00037860870361328125, 'all': 0.06454753875732422}
diff5_list [0.0065500736236572266, 0.006527423858642578, 0.006192922592163086, 0.006505489349365234, 0.006128549575805664]
time3 0
time4 0.06528162956237793
time5 0.06532406806945801
time7 7.152557373046875e-07
gen_weight_change tensor(-19.3095)
policy weight change tensor(37.3314, grad_fn=<SumBackward0>)
time8 0.001836538314819336
train_time 0.07629680633544922
eval time 0.14191126823425293
epoch last part time 4.291534423828125e-06
2024-01-23 01:03:33,563 MainThread INFO: EPOCH:621
2024-01-23 01:03:33,563 MainThread INFO: Time Consumed:0.22133159637451172s
2024-01-23 01:03:33,563 MainThread INFO: Total Frames:94050s
  6%|▌         | 622/10000 [05:01<38:28,  4.06it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13407.98812
Train_Epoch_Reward                5450.11107
Running_Training_Average_Rewards  16295.36638
Explore_Time                      0.00090
Train___Time                      0.07630
Eval____Time                      0.14191
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13931.05971
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.84485     1.14331    93.03455     89.73897
alpha_0                           0.73270      0.00010    0.73285      0.73255
Alpha_loss                        -2.09313     0.00212    -2.09040     -2.09625
Training/policy_loss              -3.03910     0.00589    -3.03295     -3.04756
Training/qf1_loss                 6796.25635   428.71135  7250.19824   6055.20605
Training/qf2_loss                 15549.79746  567.19593  16346.78613  14718.21875
Training/pf_norm                  0.10321      0.02947    0.13670      0.05130
Training/qf1_norm                 994.25869    202.89227  1374.23438   764.88312
Training/qf2_norm                 813.64573    10.88718   834.65070    802.98328
log_std/mean                      -0.13117     0.00014    -0.13097     -0.13135
log_probs/mean                    -2.73207     0.00726    -2.72544     -2.74185
mean/mean                         -0.00305     0.00003    -0.00301     -0.00308
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018176555633544922
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69914
epoch first part time 2.86102294921875e-06
replay_buffer._size: [94350]
collect time 0.0009031295776367188
inner_dict_sum {'sac_diff0': 0.00024247169494628906, 'sac_diff1': 0.0065424442291259766, 'sac_diff2': 0.0076062679290771484, 'sac_diff3': 0.009401798248291016, 'sac_diff4': 0.00662994384765625, 'sac_diff5': 0.03062129020690918, 'sac_diff6': 0.0003757476806640625, 'all': 0.06141996383666992}
diff5_list [0.006285429000854492, 0.005999565124511719, 0.006011009216308594, 0.006490230560302734, 0.005835056304931641]
time3 0
time4 0.062164306640625
time5 0.06220722198486328
time7 7.152557373046875e-07
gen_weight_change tensor(-19.3095)
policy weight change tensor(37.3035, grad_fn=<SumBackward0>)
time8 0.0017902851104736328
train_time 0.07288146018981934
eval time 0.14948129653930664
epoch last part time 4.291534423828125e-06
2024-01-23 01:03:33,810 MainThread INFO: EPOCH:622
2024-01-23 01:03:33,810 MainThread INFO: Time Consumed:0.22548484802246094s
2024-01-23 01:03:33,810 MainThread INFO: Total Frames:94200s
  6%|▌         | 623/10000 [05:01<38:31,  4.06it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13446.76040
Train_Epoch_Reward                8876.36159
Running_Training_Average_Rewards  15847.99361
Explore_Time                      0.00090
Train___Time                      0.07288
Eval____Time                      0.14948
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13926.19884
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.39391     1.49439     93.60233     89.57630
alpha_0                           0.73233      0.00010     0.73248      0.73219
Alpha_loss                        -2.09729     0.00126     -2.09491     -2.09850
Training/policy_loss              -2.93451     0.00383     -2.92948     -2.93923
Training/qf1_loss                 7504.21992   815.60446   8566.89648   6338.07275
Training/qf2_loss                 16599.54023  1063.14123  17887.14453  14901.20117
Training/pf_norm                  0.08934      0.01948     0.11865      0.06689
Training/qf1_norm                 1082.57379   316.58855   1333.50769   508.16922
Training/qf2_norm                 777.39658    12.60292    787.18182    753.27625
log_std/mean                      -0.12677     0.00006     -0.12670     -0.12686
log_probs/mean                    -2.73461     0.00527     -2.72696     -2.74065
mean/mean                         -0.00339     0.00013     -0.00322     -0.00357
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018260955810546875
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69914
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [94500]
collect time 0.0009026527404785156
inner_dict_sum {'sac_diff0': 0.00023818016052246094, 'sac_diff1': 0.006394624710083008, 'sac_diff2': 0.007647275924682617, 'sac_diff3': 0.01002812385559082, 'sac_diff4': 0.006780147552490234, 'sac_diff5': 0.030885934829711914, 'sac_diff6': 0.00038123130798339844, 'all': 0.06235551834106445}
diff5_list [0.0064656734466552734, 0.006190061569213867, 0.00602412223815918, 0.006326198577880859, 0.005879878997802734]
time3 0
time4 0.06308913230895996
time5 0.06313204765319824
time7 9.5367431640625e-07
gen_weight_change tensor(-19.3095)
policy weight change tensor(37.2794, grad_fn=<SumBackward0>)
time8 0.0018846988677978516
train_time 0.07401847839355469
eval time 0.15626978874206543
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:34,065 MainThread INFO: EPOCH:623
2024-01-23 01:03:34,065 MainThread INFO: Time Consumed:0.2335205078125s
2024-01-23 01:03:34,066 MainThread INFO: Total Frames:94350s
  6%|▌         | 624/10000 [05:01<38:56,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13499.92363
Train_Epoch_Reward                8078.52005
Running_Training_Average_Rewards  16026.20528
Explore_Time                      0.00090
Train___Time                      0.07402
Eval____Time                      0.15627
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14067.28215
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.97292     3.64710     95.83221     84.61080
alpha_0                           0.73197      0.00010     0.73211      0.73182
Alpha_loss                        -2.10043     0.00217     -2.09833     -2.10389
Training/policy_loss              -3.04309     0.00345     -3.03748     -3.04785
Training/qf1_loss                 7162.53047   1091.08290  8430.02930   5472.63574
Training/qf2_loss                 15946.77949  1789.24709  18175.77734  12995.24805
Training/pf_norm                  0.11689      0.01441     0.13169      0.09773
Training/qf1_norm                 806.45012    610.09403   1988.16772   354.43524
Training/qf2_norm                 806.63562    31.19044    848.59729    752.31372
log_std/mean                      -0.13428     0.00008     -0.13415     -0.13436
log_probs/mean                    -2.73389     0.00457     -2.72714     -2.74064
mean/mean                         -0.00447     0.00003     -0.00443     -0.00451
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01841568946838379
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69914
epoch first part time 2.384185791015625e-06
replay_buffer._size: [94650]
collect time 0.0009529590606689453
inner_dict_sum {'sac_diff0': 0.000217437744140625, 'sac_diff1': 0.006948947906494141, 'sac_diff2': 0.007935285568237305, 'sac_diff3': 0.010880231857299805, 'sac_diff4': 0.007459163665771484, 'sac_diff5': 0.03399395942687988, 'sac_diff6': 0.0003943443298339844, 'all': 0.06782937049865723}
diff5_list [0.0070879459381103516, 0.006252288818359375, 0.006276130676269531, 0.006825447082519531, 0.007552146911621094]
time3 0
time4 0.06860756874084473
time5 0.06865596771240234
time7 7.152557373046875e-07
gen_weight_change tensor(-19.3095)
policy weight change tensor(37.2678, grad_fn=<SumBackward0>)
time8 0.002103567123413086
train_time 0.08004307746887207
eval time 0.14928483963012695
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:34,320 MainThread INFO: EPOCH:624
2024-01-23 01:03:34,320 MainThread INFO: Time Consumed:0.2326793670654297s
2024-01-23 01:03:34,320 MainThread INFO: Total Frames:94500s
  6%|▋         | 625/10000 [05:01<39:11,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13566.05280
Train_Epoch_Reward                4575.05678
Running_Training_Average_Rewards  15310.11066
Explore_Time                      0.00095
Train___Time                      0.08004
Eval____Time                      0.14928
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14229.94590
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.94435     2.83943     93.90228     85.57771
alpha_0                           0.73160      0.00010     0.73175      0.73145
Alpha_loss                        -2.10318     0.00196     -2.10006     -2.10546
Training/policy_loss              -2.94909     0.00451     -2.94048     -2.95360
Training/qf1_loss                 7028.72422   1198.43099  9063.53320   5677.95947
Training/qf2_loss                 15427.66582  1687.21938  18355.76953  13485.86719
Training/pf_norm                  0.09522      0.03434     0.15009      0.05987
Training/qf1_norm                 907.43783    521.04161   1816.76196   317.34674
Training/qf2_norm                 737.17339    22.64990    776.39868    710.00964
log_std/mean                      -0.12930     0.00005     -0.12922     -0.12934
log_probs/mean                    -2.73190     0.00518     -2.72192     -2.73703
mean/mean                         -0.00454     0.00013     -0.00434     -0.00471
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01856827735900879
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69914
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [94800]
collect time 0.0009768009185791016
inside mustsac before update, task 0, sumup 69914
inside mustsac after update, task 0, sumup 70591
inner_dict_sum {'sac_diff0': 0.00022268295288085938, 'sac_diff1': 0.00716710090637207, 'sac_diff2': 0.008594036102294922, 'sac_diff3': 0.01071929931640625, 'sac_diff4': 0.007781505584716797, 'sac_diff5': 0.05025625228881836, 'sac_diff6': 0.00042319297790527344, 'all': 0.08516407012939453}
diff5_list [0.010329246520996094, 0.010433673858642578, 0.010139942169189453, 0.009919881820678711, 0.009433507919311523]
time3 0.0008649826049804688
time4 0.0860605239868164
time5 0.08611631393432617
time7 0.00946187973022461
gen_weight_change tensor(-19.4973)
policy weight change tensor(37.2976, grad_fn=<SumBackward0>)
time8 0.0019016265869140625
train_time 0.11590290069580078
eval time 0.10290336608886719
epoch last part time 5.9604644775390625e-06
2024-01-23 01:03:34,564 MainThread INFO: EPOCH:625
2024-01-23 01:03:34,564 MainThread INFO: Time Consumed:0.22205066680908203s
2024-01-23 01:03:34,564 MainThread INFO: Total Frames:94650s
  6%|▋         | 626/10000 [05:02<38:50,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13635.03225
Train_Epoch_Reward                37615.20302
Running_Training_Average_Rewards  15838.57505
Explore_Time                      0.00097
Train___Time                      0.11590
Eval____Time                      0.10290
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14322.35843
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.52323     0.66616    91.84121     90.06738
alpha_0                           0.73124      0.00010    0.73138      0.73109
Alpha_loss                        -2.10721     0.00217    -2.10440     -2.10966
Training/policy_loss              -2.97574     0.05981    -2.90487     -3.06046
Training/qf1_loss                 6688.41943   579.85429  7554.84375   5912.06299
Training/qf2_loss                 15367.19336  724.64518  16513.88672  14422.88086
Training/pf_norm                  0.10354      0.03487    0.13665      0.05157
Training/qf1_norm                 1065.20007   488.02682  2012.07104   615.57739
Training/qf2_norm                 776.56260    46.79422   844.22589    725.27643
log_std/mean                      -0.13270     0.00582    -0.12359     -0.13851
log_probs/mean                    -2.73402     0.00494    -2.72850     -2.74186
mean/mean                         -0.00482     0.00061    -0.00374     -0.00559
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.017925024032592773
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [94950]
collect time 0.0008800029754638672
inner_dict_sum {'sac_diff0': 0.0002110004425048828, 'sac_diff1': 0.006651878356933594, 'sac_diff2': 0.007996797561645508, 'sac_diff3': 0.009909629821777344, 'sac_diff4': 0.0069544315338134766, 'sac_diff5': 0.03193092346191406, 'sac_diff6': 0.0003814697265625, 'all': 0.06403613090515137}
diff5_list [0.006558418273925781, 0.0065021514892578125, 0.006232023239135742, 0.006471157073974609, 0.006167173385620117]
time3 0
time4 0.06478309631347656
time5 0.06482863426208496
time7 4.76837158203125e-07
gen_weight_change tensor(-19.4973)
policy weight change tensor(37.3469, grad_fn=<SumBackward0>)
time8 0.0019767284393310547
train_time 0.0757439136505127
eval time 0.1457831859588623
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:34,810 MainThread INFO: EPOCH:626
2024-01-23 01:03:34,810 MainThread INFO: Time Consumed:0.22467803955078125s
2024-01-23 01:03:34,810 MainThread INFO: Total Frames:94800s
  6%|▋         | 627/10000 [05:02<38:44,  4.03it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13650.67419
Train_Epoch_Reward                12591.18329
Running_Training_Average_Rewards  15764.14720
Explore_Time                      0.00088
Train___Time                      0.07574
Eval____Time                      0.14578
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13315.41261
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.24181     0.30374    89.65483     88.90675
alpha_0                           0.73087      0.00010    0.73102      0.73072
Alpha_loss                        -2.11150     0.00216    -2.10840     -2.11404
Training/policy_loss              -3.02957     0.00265    -3.02518     -3.03350
Training/qf1_loss                 6436.24287   297.45903  6697.05762   6040.66650
Training/qf2_loss                 14896.87637  318.25239  15240.08008  14425.52051
Training/pf_norm                  0.09260      0.02643    0.13354      0.05296
Training/qf1_norm                 385.44593    85.02154   534.20227    273.84714
Training/qf2_norm                 798.53960    2.67574    801.74713    795.40833
log_std/mean                      -0.11805     0.00013    -0.11790     -0.11824
log_probs/mean                    -2.73696     0.00461    -2.72923     -2.74231
mean/mean                         -0.00319     0.00014    -0.00298     -0.00337
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018730878829956055
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 2.86102294921875e-06
replay_buffer._size: [95100]
collect time 0.0008983612060546875
inner_dict_sum {'sac_diff0': 0.00023674964904785156, 'sac_diff1': 0.0065686702728271484, 'sac_diff2': 0.007810354232788086, 'sac_diff3': 0.010013103485107422, 'sac_diff4': 0.0068094730377197266, 'sac_diff5': 0.032018184661865234, 'sac_diff6': 0.0003809928894042969, 'all': 0.06383752822875977}
diff5_list [0.0064389705657958984, 0.006353139877319336, 0.006381511688232422, 0.006592273712158203, 0.006252288818359375]
time3 0
time4 0.06457161903381348
time5 0.06463909149169922
time7 4.76837158203125e-07
gen_weight_change tensor(-19.4973)
policy weight change tensor(37.4282, grad_fn=<SumBackward0>)
time8 0.0018253326416015625
train_time 0.07531309127807617
eval time 0.1484231948852539
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:35,059 MainThread INFO: EPOCH:627
2024-01-23 01:03:35,059 MainThread INFO: Time Consumed:0.2269444465637207s
2024-01-23 01:03:35,059 MainThread INFO: Total Frames:94950s
  6%|▋         | 628/10000 [05:02<38:47,  4.03it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13664.89844
Train_Epoch_Reward                19164.27563
Running_Training_Average_Rewards  16051.66466
Explore_Time                      0.00089
Train___Time                      0.07531
Eval____Time                      0.14842
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13356.15592
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.53542     2.21589     94.98999     88.71888
alpha_0                           0.73050      0.00010     0.73065      0.73036
Alpha_loss                        -2.11225     0.00151     -2.10944     -2.11401
Training/policy_loss              -3.06069     0.00386     -3.05384     -3.06463
Training/qf1_loss                 7112.77510   1294.20030  9142.18457   5770.87598
Training/qf2_loss                 15942.20859  1600.44933  18252.21680  14496.85742
Training/pf_norm                  0.10869      0.02525     0.13598      0.07723
Training/qf1_norm                 1602.99199   430.93407   2290.96631   1039.02466
Training/qf2_norm                 853.79376    20.51356    885.94592    828.38593
log_std/mean                      -0.13725     0.00018     -0.13694     -0.13746
log_probs/mean                    -2.72862     0.00520     -2.71967     -2.73397
mean/mean                         -0.00342     0.00004     -0.00338     -0.00349
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018500089645385742
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 2.86102294921875e-06
replay_buffer._size: [95250]
collect time 0.0009620189666748047
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.007191896438598633, 'sac_diff2': 0.00835418701171875, 'sac_diff3': 0.011230230331420898, 'sac_diff4': 0.007334232330322266, 'sac_diff5': 0.032022953033447266, 'sac_diff6': 0.00038695335388183594, 'all': 0.0667414665222168}
diff5_list [0.006884098052978516, 0.006297588348388672, 0.006478548049926758, 0.006149768829345703, 0.006212949752807617]
time3 0
time4 0.06750154495239258
time5 0.06755495071411133
time7 7.152557373046875e-07
gen_weight_change tensor(-19.4973)
policy weight change tensor(37.4250, grad_fn=<SumBackward0>)
time8 0.0019936561584472656
train_time 0.07886838912963867
eval time 0.1471714973449707
epoch last part time 5.9604644775390625e-06
2024-01-23 01:03:35,310 MainThread INFO: EPOCH:628
2024-01-23 01:03:35,311 MainThread INFO: Time Consumed:0.22952914237976074s
2024-01-23 01:03:35,311 MainThread INFO: Total Frames:95100s
  6%|▋         | 629/10000 [05:02<38:57,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13677.63507
Train_Epoch_Reward                11378.29296
Running_Training_Average_Rewards  15748.17016
Explore_Time                      0.00096
Train___Time                      0.07887
Eval____Time                      0.14717
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13350.51064
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.14757     3.47713     95.17154     85.69683
alpha_0                           0.73014      0.00010     0.73029      0.72999
Alpha_loss                        -2.11748     0.00221     -2.11430     -2.12123
Training/policy_loss              -3.06335     0.00392     -3.05755     -3.06890
Training/qf1_loss                 7399.26250   1039.88326  9171.75098   6170.25586
Training/qf2_loss                 16261.77949  1552.73532  18804.18555  14729.41602
Training/pf_norm                  0.12100      0.02795     0.16204      0.07443
Training/qf1_norm                 725.23799    419.67955   1329.61340   137.09219
Training/qf2_norm                 834.74747    31.12183    870.80444    785.06097
log_std/mean                      -0.14097     0.00002     -0.14094     -0.14098
log_probs/mean                    -2.73456     0.00551     -2.72657     -2.74219
mean/mean                         -0.00269     0.00003     -0.00263     -0.00273
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018829345703125
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [95400]
collect time 0.000957489013671875
inner_dict_sum {'sac_diff0': 0.00023317337036132812, 'sac_diff1': 0.006526947021484375, 'sac_diff2': 0.007942438125610352, 'sac_diff3': 0.011676549911499023, 'sac_diff4': 0.006992340087890625, 'sac_diff5': 0.0319063663482666, 'sac_diff6': 0.0003936290740966797, 'all': 0.06567144393920898}
diff5_list [0.006638765335083008, 0.006135463714599609, 0.0062787532806396484, 0.0063402652740478516, 0.006513118743896484]
time3 0
time4 0.0664527416229248
time5 0.06650519371032715
time7 4.76837158203125e-07
gen_weight_change tensor(-19.4973)
policy weight change tensor(37.3937, grad_fn=<SumBackward0>)
time8 0.0019249916076660156
train_time 0.07758545875549316
eval time 0.15128326416015625
epoch last part time 1.0251998901367188e-05
2024-01-23 01:03:35,565 MainThread INFO: EPOCH:629
2024-01-23 01:03:35,565 MainThread INFO: Time Consumed:0.23216629028320312s
2024-01-23 01:03:35,566 MainThread INFO: Total Frames:95250s
  6%|▋         | 630/10000 [05:03<39:11,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13693.60899
Train_Epoch_Reward                8092.15249
Running_Training_Average_Rewards  15374.61469
Explore_Time                      0.00095
Train___Time                      0.07759
Eval____Time                      0.15128
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13339.10582
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.63670     1.51708    94.02690     89.86841
alpha_0                           0.72977      0.00010    0.72992      0.72963
Alpha_loss                        -2.12134     0.00096    -2.11983     -2.12220
Training/policy_loss              -2.87307     0.00381    -2.86734     -2.87916
Training/qf1_loss                 7726.00205   666.12566  8765.25684   6905.56494
Training/qf2_loss                 16607.22207  921.07490  18078.00391  15463.16211
Training/pf_norm                  0.09283      0.03199    0.13192      0.03747
Training/qf1_norm                 1551.82832   268.01989  1980.06079   1251.27026
Training/qf2_norm                 719.45527    11.38448   737.64124    706.97235
log_std/mean                      -0.13468     0.00007    -0.13461     -0.13481
log_probs/mean                    -2.73613     0.00454    -2.72918     -2.74314
mean/mean                         -0.00175     0.00005    -0.00168     -0.00184
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018823623657226562
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [95550]
collect time 0.0010833740234375
inside mustsac before update, task 0, sumup 70591
inside mustsac after update, task 0, sumup 69438
inner_dict_sum {'sac_diff0': 0.00021338462829589844, 'sac_diff1': 0.007161855697631836, 'sac_diff2': 0.008483171463012695, 'sac_diff3': 0.010831117630004883, 'sac_diff4': 0.007693767547607422, 'sac_diff5': 0.05066490173339844, 'sac_diff6': 0.0004124641418457031, 'all': 0.08546066284179688}
diff5_list [0.010802745819091797, 0.010507822036743164, 0.010059595108032227, 0.009551286697387695, 0.009743452072143555]
time3 0.0008749961853027344
time4 0.08629417419433594
time5 0.08634591102600098
time7 0.009431123733520508
gen_weight_change tensor(-19.6967)
policy weight change tensor(37.4019, grad_fn=<SumBackward0>)
time8 0.0025794506072998047
train_time 0.1170344352722168
eval time 0.13389253616333008
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:35,842 MainThread INFO: EPOCH:630
2024-01-23 01:03:35,842 MainThread INFO: Time Consumed:0.25418829917907715s
2024-01-23 01:03:35,842 MainThread INFO: Total Frames:95400s
  6%|▋         | 631/10000 [05:03<40:30,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13709.67753
Train_Epoch_Reward                15722.06966
Running_Training_Average_Rewards  15449.86821
Explore_Time                      0.00107
Train___Time                      0.11703
Eval____Time                      0.13389
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13258.74532
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.64572     2.37686     94.08522     87.11714
alpha_0                           0.72941      0.00010     0.72956      0.72926
Alpha_loss                        -2.12412     0.00166     -2.12236     -2.12700
Training/policy_loss              -3.03242     0.06176     -2.96347     -3.13369
Training/qf1_loss                 6992.12500   902.01268   8360.33691   6047.56885
Training/qf2_loss                 15540.86465  1261.09408  17166.07422  14135.87500
Training/pf_norm                  0.08754      0.01380     0.11328      0.07357
Training/qf1_norm                 753.67742    496.94770   1382.48804   118.86514
Training/qf2_norm                 808.36053    43.43146    891.13306    770.90668
log_std/mean                      -0.13471     0.00300     -0.13049     -0.13912
log_probs/mean                    -2.73425     0.00458     -2.73005     -2.74054
mean/mean                         -0.00358     0.00070     -0.00279     -0.00461
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01870560646057129
epoch last part time3 0.0026650428771972656
inside rlalgo, task 0, sumup 69438
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [95700]
collect time 0.0009281635284423828
inner_dict_sum {'sac_diff0': 0.00022983551025390625, 'sac_diff1': 0.006560325622558594, 'sac_diff2': 0.007995367050170898, 'sac_diff3': 0.010267257690429688, 'sac_diff4': 0.006826639175415039, 'sac_diff5': 0.03149247169494629, 'sac_diff6': 0.00038051605224609375, 'all': 0.06375241279602051}
diff5_list [0.0065462589263916016, 0.006270408630371094, 0.0062062740325927734, 0.006263256072998047, 0.0062062740325927734]
time3 0
time4 0.06449699401855469
time5 0.06454277038574219
time7 7.152557373046875e-07
gen_weight_change tensor(-19.6967)
policy weight change tensor(37.3387, grad_fn=<SumBackward0>)
time8 0.0018291473388671875
train_time 0.07542777061462402
eval time 0.16757488250732422
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:36,113 MainThread INFO: EPOCH:631
2024-01-23 01:03:36,113 MainThread INFO: Time Consumed:0.2462902069091797s
2024-01-23 01:03:36,113 MainThread INFO: Total Frames:95550s
  6%|▋         | 632/10000 [05:03<40:54,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13624.62295
Train_Epoch_Reward                12870.46927
Running_Training_Average_Rewards  15583.08848
Explore_Time                      0.00092
Train___Time                      0.07543
Eval____Time                      0.16757
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13080.51389
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.97634     2.38385     94.01308     86.61774
alpha_0                           0.72904      0.00010     0.72919      0.72890
Alpha_loss                        -2.12658     0.00095     -2.12501     -2.12762
Training/policy_loss              -3.06653     0.00120     -3.06434     -3.06782
Training/qf1_loss                 7252.75918   801.30141   8217.74609   5837.53418
Training/qf2_loss                 15844.99824  1062.20153  16656.23047  13806.66113
Training/pf_norm                  0.11031      0.01838     0.12509      0.07647
Training/qf1_norm                 704.54219    414.69669   1395.04028   118.14950
Training/qf2_norm                 850.30818    21.94287    887.62445    819.69769
log_std/mean                      -0.14063     0.00017     -0.14036     -0.14081
log_probs/mean                    -2.73138     0.00076     -2.73041     -2.73233
mean/mean                         -0.00103     0.00009     -0.00094     -0.00117
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018451690673828125
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69438
epoch first part time 2.384185791015625e-06
replay_buffer._size: [95850]
collect time 0.0008213520050048828
inner_dict_sum {'sac_diff0': 0.0002071857452392578, 'sac_diff1': 0.0069577693939208984, 'sac_diff2': 0.008215665817260742, 'sac_diff3': 0.010885477066040039, 'sac_diff4': 0.007224321365356445, 'sac_diff5': 0.03398442268371582, 'sac_diff6': 0.0004284381866455078, 'all': 0.06790328025817871}
diff5_list [0.0063855648040771484, 0.006144046783447266, 0.0062563419342041016, 0.0076754093170166016, 0.007523059844970703]
time3 0
time4 0.06876444816589355
time5 0.06882500648498535
time7 7.152557373046875e-07
gen_weight_change tensor(-19.6967)
policy weight change tensor(37.2328, grad_fn=<SumBackward0>)
time8 0.0019211769104003906
train_time 0.08007073402404785
eval time 0.14982390403747559
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:36,368 MainThread INFO: EPOCH:632
2024-01-23 01:03:36,368 MainThread INFO: Time Consumed:0.23323392868041992s
2024-01-23 01:03:36,368 MainThread INFO: Total Frames:95700s
  6%|▋         | 633/10000 [05:03<40:35,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13528.60890
Train_Epoch_Reward                7733.87677
Running_Training_Average_Rewards  15483.58039
Explore_Time                      0.00082
Train___Time                      0.08007
Eval____Time                      0.14982
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12966.05829
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.13443     3.15334     98.57539     89.17180
alpha_0                           0.72868      0.00010     0.72883      0.72853
Alpha_loss                        -2.13059     0.00215     -2.12700     -2.13288
Training/policy_loss              -3.02903     0.00337     -3.02255     -3.03159
Training/qf1_loss                 8086.15195   650.51980   8765.91699   6907.92578
Training/qf2_loss                 17309.84609  1081.79869  18869.29883  15728.23242
Training/pf_norm                  0.10363      0.03041     0.14280      0.05169
Training/qf1_norm                 542.30265    380.81424   1031.49060   103.85181
Training/qf2_norm                 857.75000    28.61595    907.00470    821.53192
log_std/mean                      -0.13630     0.00022     -0.13594     -0.13654
log_probs/mean                    -2.73343     0.00505     -2.72420     -2.73802
mean/mean                         -0.00138     0.00004     -0.00130     -0.00142
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018588542938232422
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69438
epoch first part time 2.86102294921875e-06
replay_buffer._size: [96000]
collect time 0.0009686946868896484
inner_dict_sum {'sac_diff0': 0.00021147727966308594, 'sac_diff1': 0.007038116455078125, 'sac_diff2': 0.008339643478393555, 'sac_diff3': 0.010698556900024414, 'sac_diff4': 0.007017850875854492, 'sac_diff5': 0.033187150955200195, 'sac_diff6': 0.00039649009704589844, 'all': 0.06688928604125977}
diff5_list [0.0077877044677734375, 0.0063517093658447266, 0.00654149055480957, 0.0064122676849365234, 0.0060939788818359375]
time3 0
time4 0.0677022933959961
time5 0.06775355339050293
time7 4.76837158203125e-07
gen_weight_change tensor(-19.6967)
policy weight change tensor(37.1842, grad_fn=<SumBackward0>)
time8 0.002000570297241211
train_time 0.07916712760925293
eval time 0.13968753814697266
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:36,612 MainThread INFO: EPOCH:633
2024-01-23 01:03:36,612 MainThread INFO: Time Consumed:0.22211837768554688s
2024-01-23 01:03:36,613 MainThread INFO: Total Frames:95850s
  6%|▋         | 634/10000 [05:04<39:51,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13403.45138
Train_Epoch_Reward                21861.43845
Running_Training_Average_Rewards  16118.09215
Explore_Time                      0.00096
Train___Time                      0.07917
Eval____Time                      0.13969
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12815.70701
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.29371     2.23458     92.34837     86.45158
alpha_0                           0.72832      0.00010     0.72846      0.72817
Alpha_loss                        -2.13358     0.00130     -2.13240     -2.13570
Training/policy_loss              -3.08656     0.00272     -3.08288     -3.08991
Training/qf1_loss                 7017.30723   1185.73632  9311.79785   6053.69336
Training/qf2_loss                 15393.15586  1573.73344  18341.15234  14012.81250
Training/pf_norm                  0.10294      0.02481     0.14265      0.07134
Training/qf1_norm                 2617.90410   489.23656   3284.88794   1988.17554
Training/qf2_norm                 829.76288    20.01892    858.08130    805.01923
log_std/mean                      -0.12418     0.00007     -0.12413     -0.12430
log_probs/mean                    -2.73223     0.00331     -2.72639     -2.73496
mean/mean                         -0.00529     0.00015     -0.00515     -0.00555
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018642425537109375
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69438
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [96150]
collect time 0.0008668899536132812
inner_dict_sum {'sac_diff0': 0.0002090930938720703, 'sac_diff1': 0.006737470626831055, 'sac_diff2': 0.007856607437133789, 'sac_diff3': 0.009743452072143555, 'sac_diff4': 0.006693601608276367, 'sac_diff5': 0.031149625778198242, 'sac_diff6': 0.00038242340087890625, 'all': 0.06277227401733398}
diff5_list [0.006433725357055664, 0.006228208541870117, 0.006299495697021484, 0.00627589225769043, 0.005912303924560547]
time3 0
time4 0.06352782249450684
time5 0.06357884407043457
time7 7.152557373046875e-07
gen_weight_change tensor(-19.6967)
policy weight change tensor(37.1554, grad_fn=<SumBackward0>)
time8 0.0018906593322753906
train_time 0.07456207275390625
eval time 0.1475672721862793
epoch last part time 4.291534423828125e-06
2024-01-23 01:03:36,860 MainThread INFO: EPOCH:634
2024-01-23 01:03:36,860 MainThread INFO: Time Consumed:0.2252044677734375s
2024-01-23 01:03:36,860 MainThread INFO: Total Frames:96000s
  6%|▋         | 635/10000 [05:04<39:28,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13250.54517
Train_Epoch_Reward                11578.77478
Running_Training_Average_Rewards  15493.97269
Explore_Time                      0.00086
Train___Time                      0.07456
Eval____Time                      0.14757
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12700.88382
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.00579     2.65025     93.96778     87.84386
alpha_0                           0.72795      0.00010     0.72810      0.72781
Alpha_loss                        -2.13698     0.00216     -2.13464     -2.13990
Training/policy_loss              -2.93020     0.00738     -2.92069     -2.94063
Training/qf1_loss                 7180.28184   1104.04556  8682.68164   5735.74170
Training/qf2_loss                 16009.44258  1586.93417  18101.99414  13931.35938
Training/pf_norm                  0.08918      0.02366     0.13134      0.06332
Training/qf1_norm                 734.93613    479.70998   1346.07202   208.07907
Training/qf2_norm                 752.03015    21.50282    776.33319    726.21552
log_std/mean                      -0.13560     0.00009     -0.13547     -0.13572
log_probs/mean                    -2.73235     0.00941     -2.72073     -2.74579
mean/mean                         -0.00513     0.00013     -0.00494     -0.00532
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018531084060668945
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69438
epoch first part time 2.86102294921875e-06
replay_buffer._size: [96300]
collect time 0.0009438991546630859
inside mustsac before update, task 0, sumup 69438
inside mustsac after update, task 0, sumup 70989
inner_dict_sum {'sac_diff0': 0.00019693374633789062, 'sac_diff1': 0.0063817501068115234, 'sac_diff2': 0.0077817440032958984, 'sac_diff3': 0.00962519645690918, 'sac_diff4': 0.00675654411315918, 'sac_diff5': 0.04763293266296387, 'sac_diff6': 0.0003895759582519531, 'all': 0.07876467704772949}
diff5_list [0.010004758834838867, 0.009482383728027344, 0.009662866592407227, 0.009241342544555664, 0.009241580963134766]
time3 0.0008511543273925781
time4 0.07955813407897949
time5 0.0796041488647461
time7 0.008959293365478516
gen_weight_change tensor(-19.8782)
policy weight change tensor(37.1469, grad_fn=<SumBackward0>)
time8 0.0017764568328857422
train_time 0.10821366310119629
eval time 0.11215424537658691
epoch last part time 4.0531158447265625e-06
2024-01-23 01:03:37,105 MainThread INFO: EPOCH:635
2024-01-23 01:03:37,105 MainThread INFO: Time Consumed:0.22351336479187012s
2024-01-23 01:03:37,106 MainThread INFO: Total Frames:96150s
  6%|▋         | 636/10000 [05:04<39:06,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13079.34983
Train_Epoch_Reward                7984.16834
Running_Training_Average_Rewards  15011.64215
Explore_Time                      0.00094
Train___Time                      0.10821
Eval____Time                      0.11215
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12610.40498
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.60299     2.72227     93.16135     85.68660
alpha_0                           0.72759      0.00010     0.72773      0.72744
Alpha_loss                        -2.13915     0.00238     -2.13471     -2.14106
Training/policy_loss              -3.04625     0.04940     -2.98032     -3.12157
Training/qf1_loss                 6796.55605   765.26424   8272.33594   6117.85791
Training/qf2_loss                 15348.04492  1150.12417  17447.33984  14350.65137
Training/pf_norm                  0.09246      0.02432     0.14067      0.07659
Training/qf1_norm                 667.22586    551.97117   1441.17151   169.38786
Training/qf2_norm                 813.33414    37.29074    861.79358    746.41809
log_std/mean                      -0.13298     0.00219     -0.12871     -0.13478
log_probs/mean                    -2.72857     0.00525     -2.71884     -2.73385
mean/mean                         -0.00452     0.00108     -0.00325     -0.00624
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01818251609802246
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70989
epoch first part time 2.384185791015625e-06
replay_buffer._size: [96450]
collect time 0.0009329319000244141
inner_dict_sum {'sac_diff0': 0.00023555755615234375, 'sac_diff1': 0.006312847137451172, 'sac_diff2': 0.00760197639465332, 'sac_diff3': 0.009578704833984375, 'sac_diff4': 0.0066912174224853516, 'sac_diff5': 0.03062295913696289, 'sac_diff6': 0.00037097930908203125, 'all': 0.061414241790771484}
diff5_list [0.006148099899291992, 0.005837440490722656, 0.0061185359954833984, 0.0064585208892822266, 0.006060361862182617]
time3 0
time4 0.06213116645812988
time5 0.06217360496520996
time7 4.76837158203125e-07
gen_weight_change tensor(-19.8782)
policy weight change tensor(37.0852, grad_fn=<SumBackward0>)
time8 0.0019195079803466797
train_time 0.07285881042480469
eval time 0.1528916358947754
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:37,356 MainThread INFO: EPOCH:636
2024-01-23 01:03:37,356 MainThread INFO: Time Consumed:0.22901201248168945s
2024-01-23 01:03:37,356 MainThread INFO: Total Frames:96300s
  6%|▋         | 637/10000 [05:04<39:08,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13014.12973
Train_Epoch_Reward                7708.70962
Running_Training_Average_Rewards  14830.70956
Explore_Time                      0.00093
Train___Time                      0.07286
Eval____Time                      0.15289
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12663.21165
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.82458     1.19079    90.39877     87.37090
alpha_0                           0.72722      0.00010    0.72737      0.72708
Alpha_loss                        -2.14380     0.00211    -2.14115     -2.14697
Training/policy_loss              -2.97316     0.00607    -2.96328     -2.98026
Training/qf1_loss                 6248.51377   380.57077  6664.54980   5585.59033
Training/qf2_loss                 14646.10879  402.73766  15257.60645  14110.62012
Training/pf_norm                  0.08357      0.01627    0.09867      0.05280
Training/qf1_norm                 343.15892    193.48887  627.30072    117.87742
Training/qf2_norm                 778.49581    9.82157    791.44818    766.55957
log_std/mean                      -0.13319     0.00018    -0.13294     -0.13343
log_probs/mean                    -2.73263     0.00803    -2.72008     -2.74215
mean/mean                         -0.00471     0.00015    -0.00449     -0.00493
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01899123191833496
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70989
epoch first part time 2.86102294921875e-06
replay_buffer._size: [96600]
collect time 0.0008282661437988281
inner_dict_sum {'sac_diff0': 0.0002231597900390625, 'sac_diff1': 0.006891489028930664, 'sac_diff2': 0.007754325866699219, 'sac_diff3': 0.010095834732055664, 'sac_diff4': 0.006726264953613281, 'sac_diff5': 0.03168487548828125, 'sac_diff6': 0.00038242340087890625, 'all': 0.06375837326049805}
diff5_list [0.006830930709838867, 0.006466388702392578, 0.006118059158325195, 0.006312131881713867, 0.005957365036010742]
time3 0
time4 0.0645148754119873
time5 0.0645592212677002
time7 4.76837158203125e-07
gen_weight_change tensor(-19.8782)
policy weight change tensor(37.1134, grad_fn=<SumBackward0>)
time8 0.0018610954284667969
train_time 0.07575225830078125
eval time 0.1415390968322754
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:37,599 MainThread INFO: EPOCH:637
2024-01-23 01:03:37,599 MainThread INFO: Time Consumed:0.22031044960021973s
2024-01-23 01:03:37,599 MainThread INFO: Total Frames:96450s
  6%|▋         | 638/10000 [05:05<38:44,  4.03it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12930.06003
Train_Epoch_Reward                13837.81348
Running_Training_Average_Rewards  13995.12071
Explore_Time                      0.00082
Train___Time                      0.07575
Eval____Time                      0.14154
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12515.45889
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.90656     2.36525     95.04170     88.78176
alpha_0                           0.72686      0.00010     0.72701      0.72672
Alpha_loss                        -2.14692     0.00118     -2.14539     -2.14818
Training/policy_loss              -3.19740     0.00087     -3.19601     -3.19858
Training/qf1_loss                 7455.46562   868.90870   8870.16504   6488.65527
Training/qf2_loss                 16245.31523  1262.86944  18448.30859  14897.81445
Training/pf_norm                  0.09678      0.01019     0.11125      0.08282
Training/qf1_norm                 430.01715    223.49085   852.79749    236.44035
Training/qf2_norm                 917.50198    23.82545    958.49536    896.53217
log_std/mean                      -0.13043     0.00006     -0.13035     -0.13052
log_probs/mean                    -2.73186     0.00159     -2.72984     -2.73376
mean/mean                         -0.00447     0.00005     -0.00439     -0.00452
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018332719802856445
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70989
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [96750]
collect time 0.0008182525634765625
inner_dict_sum {'sac_diff0': 0.00023031234741210938, 'sac_diff1': 0.006470680236816406, 'sac_diff2': 0.00783681869506836, 'sac_diff3': 0.009704351425170898, 'sac_diff4': 0.006566762924194336, 'sac_diff5': 0.03148460388183594, 'sac_diff6': 0.0003790855407714844, 'all': 0.06267261505126953}
diff5_list [0.006644487380981445, 0.0064449310302734375, 0.006025552749633789, 0.006258726119995117, 0.0061109066009521484]
time3 0
time4 0.06340599060058594
time5 0.06345009803771973
time7 4.76837158203125e-07
gen_weight_change tensor(-19.8782)
policy weight change tensor(37.1662, grad_fn=<SumBackward0>)
time8 0.0017895698547363281
train_time 0.07400059700012207
eval time 0.14733505249023438
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:37,845 MainThread INFO: EPOCH:638
2024-01-23 01:03:37,845 MainThread INFO: Time Consumed:0.22440409660339355s
2024-01-23 01:03:37,846 MainThread INFO: Total Frames:96600s
  6%|▋         | 639/10000 [05:05<38:38,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12836.07872
Train_Epoch_Reward                13431.80818
Running_Training_Average_Rewards  14240.78060
Explore_Time                      0.00081
Train___Time                      0.07400
Eval____Time                      0.14734
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12410.69751
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.32111     2.21632     94.36113     88.70972
alpha_0                           0.72650      0.00010     0.72664      0.72635
Alpha_loss                        -2.15183     0.00152     -2.15005     -2.15428
Training/policy_loss              -2.90280     0.00335     -2.89772     -2.90721
Training/qf1_loss                 7090.03760   662.04416   8309.00391   6288.64551
Training/qf2_loss                 15996.45273  1030.89135  17811.38867  14696.28320
Training/pf_norm                  0.07288      0.01464     0.09150      0.04787
Training/qf1_norm                 441.15238    226.07154   777.24603    128.28386
Training/qf2_norm                 720.43523    17.56986    743.68115    699.45880
log_std/mean                      -0.12341     0.00006     -0.12336     -0.12352
log_probs/mean                    -2.73668     0.00446     -2.73006     -2.74220
mean/mean                         -0.00546     0.00002     -0.00543     -0.00549
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018234968185424805
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70989
epoch first part time 2.86102294921875e-06
replay_buffer._size: [96900]
collect time 0.0008935928344726562
inner_dict_sum {'sac_diff0': 0.0002377033233642578, 'sac_diff1': 0.006624698638916016, 'sac_diff2': 0.007842779159545898, 'sac_diff3': 0.01007843017578125, 'sac_diff4': 0.006739616394042969, 'sac_diff5': 0.03158164024353027, 'sac_diff6': 0.00039076805114746094, 'all': 0.06349563598632812}
diff5_list [0.006659269332885742, 0.006375789642333984, 0.006182193756103516, 0.006490468978881836, 0.005873918533325195]
time3 0
time4 0.06423091888427734
time5 0.06427407264709473
time7 7.152557373046875e-07
gen_weight_change tensor(-19.8782)
policy weight change tensor(37.2062, grad_fn=<SumBackward0>)
time8 0.0017771720886230469
train_time 0.07512927055358887
eval time 0.14652466773986816
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:38,092 MainThread INFO: EPOCH:639
2024-01-23 01:03:38,092 MainThread INFO: Time Consumed:0.22474956512451172s
2024-01-23 01:03:38,092 MainThread INFO: Total Frames:96750s
  6%|▋         | 640/10000 [05:05<38:35,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12739.65733
Train_Epoch_Reward                2831.60236
Running_Training_Average_Rewards  13629.45785
Explore_Time                      0.00089
Train___Time                      0.07513
Eval____Time                      0.14652
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12374.89192
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.24269     1.63554     93.25933     88.94737
alpha_0                           0.72613      0.00010     0.72628      0.72599
Alpha_loss                        -2.15373     0.00155     -2.15140     -2.15585
Training/policy_loss              -2.96911     0.00403     -2.96369     -2.97412
Training/qf1_loss                 7440.78838   1017.93388  8883.22266   5856.00928
Training/qf2_loss                 16308.82988  1260.08063  18172.68555  14261.96094
Training/pf_norm                  0.10328      0.01958     0.13865      0.08615
Training/qf1_norm                 418.93882    248.53930   784.40509    175.98680
Training/qf2_norm                 796.11842    14.52428    814.79694    775.77435
log_std/mean                      -0.13475     0.00004     -0.13469     -0.13480
log_probs/mean                    -2.73209     0.00557     -2.72461     -2.73872
mean/mean                         -0.00371     0.00005     -0.00362     -0.00377
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018541812896728516
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70989
epoch first part time 2.384185791015625e-06
replay_buffer._size: [97050]
collect time 0.0009264945983886719
inside mustsac before update, task 0, sumup 70989
inside mustsac after update, task 0, sumup 70985
inner_dict_sum {'sac_diff0': 0.00020170211791992188, 'sac_diff1': 0.006518840789794922, 'sac_diff2': 0.007782936096191406, 'sac_diff3': 0.009936809539794922, 'sac_diff4': 0.007039546966552734, 'sac_diff5': 0.0499265193939209, 'sac_diff6': 0.00041675567626953125, 'all': 0.08182311058044434}
diff5_list [0.010355710983276367, 0.009601831436157227, 0.01007080078125, 0.0094757080078125, 0.010422468185424805]
time3 0.00084686279296875
time4 0.08265018463134766
time5 0.08270072937011719
time7 0.009356260299682617
gen_weight_change tensor(-20.0139)
policy weight change tensor(37.1946, grad_fn=<SumBackward0>)
time8 0.0028133392333984375
train_time 0.11276984214782715
eval time 0.11633443832397461
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:38,346 MainThread INFO: EPOCH:640
2024-01-23 01:03:38,347 MainThread INFO: Time Consumed:0.2322854995727539s
2024-01-23 01:03:38,347 MainThread INFO: Total Frames:96900s
  6%|▋         | 641/10000 [05:05<39:02,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12647.81487
Train_Epoch_Reward                2786.69283
Running_Training_Average_Rewards  13527.69280
Explore_Time                      0.00092
Train___Time                      0.11277
Eval____Time                      0.11633
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12340.32079
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.90826     2.23880     91.15691     85.07121
alpha_0                           0.72577      0.00010     0.72592      0.72563
Alpha_loss                        -2.15749     0.00206     -2.15509     -2.15996
Training/policy_loss              -3.06672     0.05213     -2.98781     -3.13947
Training/qf1_loss                 6595.55586   1168.82363  7866.92969   5224.72168
Training/qf2_loss                 15002.19375  1583.69642  16724.21094  12904.84082
Training/pf_norm                  0.10620      0.03064     0.15459      0.07143
Training/qf1_norm                 528.18618    425.25910   1182.21240   93.57092
Training/qf2_norm                 803.03389    40.06383    858.75061    745.01538
log_std/mean                      -0.13617     0.00591     -0.13015     -0.14669
log_probs/mean                    -2.73333     0.00421     -2.72822     -2.73947
mean/mean                         -0.00572     0.00119     -0.00391     -0.00730
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01813220977783203
epoch last part time3 0.002584695816040039
inside rlalgo, task 0, sumup 70985
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [97200]
collect time 0.0009143352508544922
inner_dict_sum {'sac_diff0': 0.00020837783813476562, 'sac_diff1': 0.006824493408203125, 'sac_diff2': 0.00788736343383789, 'sac_diff3': 0.009681940078735352, 'sac_diff4': 0.006754398345947266, 'sac_diff5': 0.03142356872558594, 'sac_diff6': 0.00037789344787597656, 'all': 0.06315803527832031}
diff5_list [0.006520748138427734, 0.006448507308959961, 0.0063669681549072266, 0.0062122344970703125, 0.005875110626220703]
time3 0
time4 0.06391644477844238
time5 0.06395983695983887
time7 7.152557373046875e-07
gen_weight_change tensor(-20.0139)
policy weight change tensor(37.1188, grad_fn=<SumBackward0>)
time8 0.0018994808197021484
train_time 0.07497715950012207
eval time 0.13957524299621582
epoch last part time 1.2159347534179688e-05
2024-01-23 01:03:38,588 MainThread INFO: EPOCH:641
2024-01-23 01:03:38,588 MainThread INFO: Time Consumed:0.2177426815032959s
2024-01-23 01:03:38,589 MainThread INFO: Total Frames:97050s
  6%|▋         | 642/10000 [05:06<38:31,  4.05it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12554.63181
Train_Epoch_Reward                11145.62229
Running_Training_Average_Rewards  13589.43110
Explore_Time                      0.00091
Train___Time                      0.07498
Eval____Time                      0.13958
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12148.68329
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.86018     2.46261     94.83163     87.70966
alpha_0                           0.72541      0.00010     0.72555      0.72526
Alpha_loss                        -2.16107     0.00197     -2.15910     -2.16475
Training/policy_loss              -3.19685     0.00336     -3.19138     -3.20163
Training/qf1_loss                 7607.44209   904.63597   9220.18457   6747.00000
Training/qf2_loss                 16341.33809  1273.27738  18792.19531  15242.17188
Training/pf_norm                  0.10232      0.03008     0.13558      0.06004
Training/qf1_norm                 1575.81573   505.83315   2174.03174   769.55536
Training/qf2_norm                 922.99655    24.95936    963.53296    891.13641
log_std/mean                      -0.13823     0.00020     -0.13795     -0.13851
log_probs/mean                    -2.73399     0.00407     -2.72901     -2.74127
mean/mean                         -0.00499     0.00010     -0.00485     -0.00513
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018561840057373047
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70985
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [97350]
collect time 0.0008227825164794922
inner_dict_sum {'sac_diff0': 0.0002269744873046875, 'sac_diff1': 0.006500720977783203, 'sac_diff2': 0.007551670074462891, 'sac_diff3': 0.00941157341003418, 'sac_diff4': 0.00630497932434082, 'sac_diff5': 0.03093266487121582, 'sac_diff6': 0.0003771781921386719, 'all': 0.06130576133728027}
diff5_list [0.0063550472259521484, 0.006150007247924805, 0.0060236454010009766, 0.006337881088256836, 0.006066083908081055]
time3 0
time4 0.062044382095336914
time5 0.0620882511138916
time7 4.76837158203125e-07
gen_weight_change tensor(-20.0139)
policy weight change tensor(37.1336, grad_fn=<SumBackward0>)
time8 0.001867532730102539
train_time 0.07294654846191406
eval time 0.14410185813903809
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:38,830 MainThread INFO: EPOCH:642
2024-01-23 01:03:38,830 MainThread INFO: Time Consumed:0.22011065483093262s
2024-01-23 01:03:38,831 MainThread INFO: Total Frames:97200s
  6%|▋         | 643/10000 [05:06<38:18,  4.07it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12468.36245
Train_Epoch_Reward                3230.94203
Running_Training_Average_Rewards  13489.78858
Explore_Time                      0.00082
Train___Time                      0.07295
Eval____Time                      0.14410
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12103.36468
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.87492     1.26951     91.67598     88.02457
alpha_0                           0.72505      0.00010     0.72519      0.72490
Alpha_loss                        -2.16487     0.00120     -2.16336     -2.16698
Training/policy_loss              -3.12128     0.00453     -3.11638     -3.12779
Training/qf1_loss                 6899.17529   844.32746   8355.59863   6007.63770
Training/qf2_loss                 15482.59531  1030.31447  17254.94336  14494.96094
Training/pf_norm                  0.10647      0.01554     0.12262      0.08071
Training/qf1_norm                 1312.69424   251.11683   1671.57031   966.38635
Training/qf2_norm                 856.30723    12.25483    873.93341    838.85162
log_std/mean                      -0.11844     0.00009     -0.11835     -0.11861
log_probs/mean                    -2.73534     0.00587     -2.72854     -2.74401
mean/mean                         -0.00453     0.00004     -0.00450     -0.00460
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0189208984375
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70985
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [97500]
collect time 0.0009293556213378906
inner_dict_sum {'sac_diff0': 0.00022649765014648438, 'sac_diff1': 0.0064623355865478516, 'sac_diff2': 0.007918596267700195, 'sac_diff3': 0.009995698928833008, 'sac_diff4': 0.0065882205963134766, 'sac_diff5': 0.03119373321533203, 'sac_diff6': 0.00037670135498046875, 'all': 0.06276178359985352}
diff5_list [0.006871700286865234, 0.006079912185668945, 0.006158590316772461, 0.006077766418457031, 0.006005764007568359]
time3 0
time4 0.06349301338195801
time5 0.06353640556335449
time7 4.76837158203125e-07
gen_weight_change tensor(-20.0139)
policy weight change tensor(37.1294, grad_fn=<SumBackward0>)
time8 0.0018792152404785156
train_time 0.07455968856811523
eval time 0.1543891429901123
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:39,085 MainThread INFO: EPOCH:643
2024-01-23 01:03:39,085 MainThread INFO: Time Consumed:0.23218727111816406s
2024-01-23 01:03:39,085 MainThread INFO: Total Frames:97350s
  6%|▋         | 644/10000 [05:06<38:43,  4.03it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12387.22685
Train_Epoch_Reward                4514.28536
Running_Training_Average_Rewards  13253.10013
Explore_Time                      0.00092
Train___Time                      0.07456
Eval____Time                      0.15439
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12004.35093
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.89057     0.99179     89.79648     87.33415
alpha_0                           0.72468      0.00010     0.72483      0.72454
Alpha_loss                        -2.16830     0.00232     -2.16566     -2.17156
Training/policy_loss              -3.07343     0.00415     -3.06881     -3.07844
Training/qf1_loss                 6991.78408   986.72774   8428.34180   6038.16113
Training/qf2_loss                 15424.06699  1026.49804  16720.46680  14160.22949
Training/pf_norm                  0.10540      0.03333     0.15048      0.06483
Training/qf1_norm                 355.54761    207.62589   690.17511    152.73744
Training/qf2_norm                 831.16755    9.12917     839.55511    816.81226
log_std/mean                      -0.14225     0.00010     -0.14205     -0.14234
log_probs/mean                    -2.73553     0.00694     -2.72900     -2.74436
mean/mean                         -0.00475     0.00001     -0.00474     -0.00477
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018795490264892578
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70985
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [97650]
collect time 0.0009784698486328125
inner_dict_sum {'sac_diff0': 0.00022935867309570312, 'sac_diff1': 0.0066585540771484375, 'sac_diff2': 0.008048534393310547, 'sac_diff3': 0.0104522705078125, 'sac_diff4': 0.006731510162353516, 'sac_diff5': 0.03217816352844238, 'sac_diff6': 0.0004086494445800781, 'all': 0.06470704078674316}
diff5_list [0.006417512893676758, 0.006700992584228516, 0.00635981559753418, 0.006361961364746094, 0.006337881088256836]
time3 0
time4 0.06547665596008301
time5 0.065521240234375
time7 4.76837158203125e-07
gen_weight_change tensor(-20.0139)
policy weight change tensor(37.0887, grad_fn=<SumBackward0>)
time8 0.0020148754119873047
train_time 0.07671451568603516
eval time 0.1516425609588623
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:39,339 MainThread INFO: EPOCH:644
2024-01-23 01:03:39,339 MainThread INFO: Time Consumed:0.23166465759277344s
2024-01-23 01:03:39,339 MainThread INFO: Total Frames:97500s
  6%|▋         | 645/10000 [05:06<38:58,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12305.55768
Train_Epoch_Reward                10781.07159
Running_Training_Average_Rewards  13028.48326
Explore_Time                      0.00097
Train___Time                      0.07671
Eval____Time                      0.15164
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11884.19217
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.13760     1.63587     95.06590     90.83740
alpha_0                           0.72432      0.00010     0.72447      0.72418
Alpha_loss                        -2.17256     0.00379     -2.16615     -2.17799
Training/policy_loss              -3.01062     0.00796     -2.99555     -3.01902
Training/qf1_loss                 8017.29971   1314.30805  10076.37500  6608.02490
Training/qf2_loss                 17047.81387  1610.02885  19657.57422  15391.83789
Training/pf_norm                  0.08583      0.02271     0.12614      0.05695
Training/qf1_norm                 1030.55964   338.74876   1628.29150   735.22375
Training/qf2_norm                 806.34905    13.96136    831.84583    795.51843
log_std/mean                      -0.12649     0.00003     -0.12647     -0.12654
log_probs/mean                    -2.73828     0.01072     -2.71843     -2.75096
mean/mean                         -0.00462     0.00011     -0.00446     -0.00474
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018392086029052734
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70985
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [97800]
collect time 0.0008220672607421875
inside mustsac before update, task 0, sumup 70985
inside mustsac after update, task 0, sumup 70501
inner_dict_sum {'sac_diff0': 0.00021004676818847656, 'sac_diff1': 0.006767749786376953, 'sac_diff2': 0.00794076919555664, 'sac_diff3': 0.010334253311157227, 'sac_diff4': 0.007224559783935547, 'sac_diff5': 0.048409461975097656, 'sac_diff6': 0.00039458274841308594, 'all': 0.08128142356872559}
diff5_list [0.010145187377929688, 0.009874820709228516, 0.009762287139892578, 0.009547948837280273, 0.009079217910766602]
time3 0.0008404254913330078
time4 0.08208036422729492
time5 0.08212924003601074
time7 0.009022712707519531
gen_weight_change tensor(-20.1394)
policy weight change tensor(37.0592, grad_fn=<SumBackward0>)
time8 0.0017859935760498047
train_time 0.1111764907836914
eval time 0.11079216003417969
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:39,586 MainThread INFO: EPOCH:645
2024-01-23 01:03:39,586 MainThread INFO: Time Consumed:0.22498035430908203s
2024-01-23 01:03:39,586 MainThread INFO: Total Frames:97650s
  6%|▋         | 646/10000 [05:07<38:48,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12228.19317
Train_Epoch_Reward                7383.48452
Running_Training_Average_Rewards  12861.53479
Explore_Time                      0.00082
Train___Time                      0.11118
Eval____Time                      0.11079
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11836.75988
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.24398     2.10199    93.79668     87.48354
alpha_0                           0.72396      0.00010    0.72410      0.72381
Alpha_loss                        -2.17501     0.00213    -2.17204     -2.17777
Training/policy_loss              -3.07819     0.11871    -2.93762     -3.25000
Training/qf1_loss                 7217.66064   495.30667  8053.90625   6776.58740
Training/qf2_loss                 16014.31094  774.51700  16927.49414  14880.30469
Training/pf_norm                  0.09043      0.01998    0.12400      0.06439
Training/qf1_norm                 1402.32319   695.89437  2436.92358   318.41467
Training/qf2_norm                 854.71008    64.09833   926.22156    768.76343
log_std/mean                      -0.13270     0.00310    -0.12780     -0.13671
log_probs/mean                    -2.73546     0.00610    -2.72769     -2.74092
mean/mean                         -0.00419     0.00123    -0.00235     -0.00552
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01808953285217285
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70501
epoch first part time 2.86102294921875e-06
replay_buffer._size: [97950]
collect time 0.0009086132049560547
inner_dict_sum {'sac_diff0': 0.00024390220642089844, 'sac_diff1': 0.0064051151275634766, 'sac_diff2': 0.007626056671142578, 'sac_diff3': 0.00984501838684082, 'sac_diff4': 0.00654149055480957, 'sac_diff5': 0.030894994735717773, 'sac_diff6': 0.0003752708435058594, 'all': 0.06193184852600098}
diff5_list [0.006210803985595703, 0.0061187744140625, 0.005917787551879883, 0.006577730178833008, 0.00606989860534668]
time3 0
time4 0.0626685619354248
time5 0.06271219253540039
time7 7.152557373046875e-07
gen_weight_change tensor(-20.1394)
policy weight change tensor(36.9423, grad_fn=<SumBackward0>)
time8 0.0017828941345214844
train_time 0.0734553337097168
eval time 0.15146398544311523
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:39,835 MainThread INFO: EPOCH:646
2024-01-23 01:03:39,836 MainThread INFO: Time Consumed:0.22803831100463867s
2024-01-23 01:03:39,836 MainThread INFO: Total Frames:97800s
  6%|▋         | 647/10000 [05:07<38:50,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12188.40369
Train_Epoch_Reward                5666.04424
Running_Training_Average_Rewards  12409.88258
Explore_Time                      0.00090
Train___Time                      0.07346
Eval____Time                      0.15146
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12265.31690
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.88859     2.72426     92.89317     84.85046
alpha_0                           0.72360      0.00010     0.72374      0.72345
Alpha_loss                        -2.17907     0.00189     -2.17571     -2.18153
Training/policy_loss              -3.09537     0.00410     -3.08803     -3.10054
Training/qf1_loss                 7016.08779   1451.82302  9478.02930   5630.13721
Training/qf2_loss                 15318.45664  1925.70171  18636.87305  13099.66211
Training/pf_norm                  0.12491      0.01918     0.13922      0.08690
Training/qf1_norm                 2085.85447   585.52605   2931.91553   1183.31213
Training/qf2_norm                 814.98899    24.58512    850.06891    777.19287
log_std/mean                      -0.13244     0.00029     -0.13202     -0.13281
log_probs/mean                    -2.73758     0.00590     -2.72721     -2.74417
mean/mean                         -0.00357     0.00007     -0.00350     -0.00369
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018149852752685547
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70501
epoch first part time 2.86102294921875e-06
replay_buffer._size: [98100]
collect time 0.0008966922760009766
inner_dict_sum {'sac_diff0': 0.00024175643920898438, 'sac_diff1': 0.006621122360229492, 'sac_diff2': 0.007727146148681641, 'sac_diff3': 0.009739875793457031, 'sac_diff4': 0.006756305694580078, 'sac_diff5': 0.031111717224121094, 'sac_diff6': 0.00037169456481933594, 'all': 0.06256961822509766}
diff5_list [0.006412506103515625, 0.006112575531005859, 0.006235599517822266, 0.006205558776855469, 0.006145477294921875]
time3 0
time4 0.0632939338684082
time5 0.06333637237548828
time7 7.152557373046875e-07
gen_weight_change tensor(-20.1394)
policy weight change tensor(36.8314, grad_fn=<SumBackward0>)
time8 0.0019457340240478516
train_time 0.07423019409179688
eval time 0.14924097061157227
epoch last part time 4.291534423828125e-06
2024-01-23 01:03:40,084 MainThread INFO: EPOCH:647
2024-01-23 01:03:40,084 MainThread INFO: Time Consumed:0.22661924362182617s
2024-01-23 01:03:40,084 MainThread INFO: Total Frames:97950s
  6%|▋         | 648/10000 [05:07<38:49,  4.02it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12159.85800
Train_Epoch_Reward                2414.44604
Running_Training_Average_Rewards  12200.68036
Explore_Time                      0.00089
Train___Time                      0.07423
Eval____Time                      0.14924
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12230.00197
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.86258     2.07731     92.08653     86.05832
alpha_0                           0.72324      0.00010     0.72338      0.72309
Alpha_loss                        -2.18181     0.00115     -2.18009     -2.18336
Training/policy_loss              -2.92945     0.00335     -2.92447     -2.93495
Training/qf1_loss                 7308.46455   934.72124   8541.41406   6117.68164
Training/qf2_loss                 15676.29824  1284.63598  17563.08008  14235.43750
Training/pf_norm                  0.08924      0.02815     0.13488      0.05341
Training/qf1_norm                 1239.40391   437.11631   1830.87952   580.66858
Training/qf2_norm                 751.10853    17.12719    778.35889    728.08728
log_std/mean                      -0.13250     0.00017     -0.13228     -0.13276
log_probs/mean                    -2.73563     0.00447     -2.72826     -2.74233
mean/mean                         -0.00376     0.00008     -0.00362     -0.00384
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018651485443115234
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70501
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [98250]
collect time 0.0008289813995361328
inner_dict_sum {'sac_diff0': 0.00022125244140625, 'sac_diff1': 0.0064656734466552734, 'sac_diff2': 0.007827281951904297, 'sac_diff3': 0.009933948516845703, 'sac_diff4': 0.006559610366821289, 'sac_diff5': 0.03157353401184082, 'sac_diff6': 0.00037789344787597656, 'all': 0.06295919418334961}
diff5_list [0.006315946578979492, 0.0064313411712646484, 0.0061113834381103516, 0.0065386295318603516, 0.0061762332916259766]
time3 0
time4 0.0637056827545166
time5 0.06374764442443848
time7 4.76837158203125e-07
gen_weight_change tensor(-20.1394)
policy weight change tensor(36.6985, grad_fn=<SumBackward0>)
time8 0.0018537044525146484
train_time 0.07448077201843262
eval time 0.15651416778564453
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:40,340 MainThread INFO: EPOCH:648
2024-01-23 01:03:40,340 MainThread INFO: Time Consumed:0.23408222198486328s
2024-01-23 01:03:40,341 MainThread INFO: Total Frames:98100s
  6%|▋         | 649/10000 [05:07<39:07,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12139.50585
Train_Epoch_Reward                20866.48856
Running_Training_Average_Rewards  11911.14600
Explore_Time                      0.00082
Train___Time                      0.07448
Eval____Time                      0.15651
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12207.17599
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.38323     1.64158    92.67548     88.12460
alpha_0                           0.72287      0.00010    0.72302      0.72273
Alpha_loss                        -2.18477     0.00240    -2.18137     -2.18823
Training/policy_loss              -3.06148     0.00476    -3.05272     -3.06626
Training/qf1_loss                 7145.19531   563.45534  7858.11816   6527.52637
Training/qf2_loss                 15837.38633  854.98211  16863.26953  14831.75293
Training/pf_norm                  0.11044      0.02743    0.13246      0.05844
Training/qf1_norm                 648.11455    309.41204  1053.02820   220.78699
Training/qf2_norm                 815.43737    14.45598   835.69379    795.48181
log_std/mean                      -0.13136     0.00022    -0.13107     -0.13170
log_probs/mean                    -2.73437     0.00634    -2.72391     -2.74089
mean/mean                         -0.00556     0.00012    -0.00543     -0.00577
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.017969846725463867
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70501
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [98400]
collect time 0.0009198188781738281
inner_dict_sum {'sac_diff0': 0.0002493858337402344, 'sac_diff1': 0.006742000579833984, 'sac_diff2': 0.007967948913574219, 'sac_diff3': 0.010680437088012695, 'sac_diff4': 0.0070438385009765625, 'sac_diff5': 0.0314939022064209, 'sac_diff6': 0.00037670135498046875, 'all': 0.06455421447753906}
diff5_list [0.006422996520996094, 0.00627446174621582, 0.006532192230224609, 0.0062084197998046875, 0.0060558319091796875]
time3 0
time4 0.06528878211975098
time5 0.06533026695251465
time7 7.152557373046875e-07
gen_weight_change tensor(-20.1394)
policy weight change tensor(36.6156, grad_fn=<SumBackward0>)
time8 0.0019042491912841797
train_time 0.07619762420654297
eval time 0.14786338806152344
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:40,589 MainThread INFO: EPOCH:649
2024-01-23 01:03:40,589 MainThread INFO: Time Consumed:0.22724246978759766s
2024-01-23 01:03:40,590 MainThread INFO: Total Frames:98250s
  6%|▋         | 650/10000 [05:08<39:01,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12125.11493
Train_Epoch_Reward                10978.04712
Running_Training_Average_Rewards  11813.14388
Explore_Time                      0.00092
Train___Time                      0.07620
Eval____Time                      0.14786
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12230.98268
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.97085     1.79836     93.17412     88.69367
alpha_0                           0.72251      0.00010     0.72266      0.72237
Alpha_loss                        -2.18796     0.00127     -2.18611     -2.18941
Training/policy_loss              -3.15423     0.00087     -3.15324     -3.15567
Training/qf1_loss                 7500.36924   1073.33566  9492.95508   6260.92041
Training/qf2_loss                 16304.35898  1162.13025  18273.93555  14740.75977
Training/pf_norm                  0.09108      0.02654     0.13746      0.05658
Training/qf1_norm                 434.82547    302.68648   845.55499    111.68363
Training/qf2_norm                 876.87454    17.03044    897.59796    855.43774
log_std/mean                      -0.12524     0.00003     -0.12520     -0.12528
log_probs/mean                    -2.73382     0.00135     -2.73229     -2.73622
mean/mean                         -0.00416     0.00017     -0.00393     -0.00440
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018178939819335938
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70501
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [98550]
collect time 0.0009200572967529297
inside mustsac before update, task 0, sumup 70501
inside mustsac after update, task 0, sumup 70572
inner_dict_sum {'sac_diff0': 0.00019979476928710938, 'sac_diff1': 0.006561994552612305, 'sac_diff2': 0.008086681365966797, 'sac_diff3': 0.010214805603027344, 'sac_diff4': 0.007029533386230469, 'sac_diff5': 0.05022764205932617, 'sac_diff6': 0.0003979206085205078, 'all': 0.0827183723449707}
diff5_list [0.009862899780273438, 0.009598016738891602, 0.010311365127563477, 0.01057887077331543, 0.009876489639282227]
time3 0.0008327960968017578
time4 0.08354711532592773
time5 0.08359527587890625
time7 0.008875846862792969
gen_weight_change tensor(-20.2961)
policy weight change tensor(36.5969, grad_fn=<SumBackward0>)
time8 0.0026586055755615234
train_time 0.1129453182220459
eval time 0.11106061935424805
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:40,838 MainThread INFO: EPOCH:650
2024-01-23 01:03:40,838 MainThread INFO: Time Consumed:0.22713971138000488s
2024-01-23 01:03:40,839 MainThread INFO: Total Frames:98400s
  7%|▋         | 651/10000 [05:08<39:05,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12115.89691
Train_Epoch_Reward                7212.56532
Running_Training_Average_Rewards  10612.05259
Explore_Time                      0.00091
Train___Time                      0.11295
Eval____Time                      0.11106
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12248.14062
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.23140     1.73863     91.18029     86.88811
alpha_0                           0.72215      0.00010     0.72230      0.72201
Alpha_loss                        -2.19304     0.00277     -2.19063     -2.19725
Training/policy_loss              -3.12801     0.09042     -2.96505     -3.23172
Training/qf1_loss                 7288.94629   379.78456   7731.21582   6688.10010
Training/qf2_loss                 15708.55566  603.43388   16484.19141  14625.57520
Training/pf_norm                  0.10185      0.02826     0.14762      0.07071
Training/qf1_norm                 1215.14191   1131.89954  3270.55078   240.32808
Training/qf2_norm                 867.60446    40.45153    907.03247    794.13318
log_std/mean                      -0.12891     0.00230     -0.12673     -0.13174
log_probs/mean                    -2.73910     0.01076     -2.72756     -2.75617
mean/mean                         -0.00570     0.00066     -0.00444     -0.00630
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018203020095825195
epoch last part time3 0.002641916275024414
inside rlalgo, task 0, sumup 70572
epoch first part time 2.86102294921875e-06
replay_buffer._size: [98700]
collect time 0.0009136199951171875
inner_dict_sum {'sac_diff0': 0.00023102760314941406, 'sac_diff1': 0.006398677825927734, 'sac_diff2': 0.007691860198974609, 'sac_diff3': 0.009917020797729492, 'sac_diff4': 0.006750583648681641, 'sac_diff5': 0.03199625015258789, 'sac_diff6': 0.00037670135498046875, 'all': 0.06336212158203125}
diff5_list [0.006497621536254883, 0.006266355514526367, 0.006277799606323242, 0.00655817985534668, 0.006396293640136719]
time3 0
time4 0.06410670280456543
time5 0.0641477108001709
time7 4.76837158203125e-07
gen_weight_change tensor(-20.2961)
policy weight change tensor(36.5401, grad_fn=<SumBackward0>)
time8 0.0018398761749267578
train_time 0.07491683959960938
eval time 0.14308714866638184
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:41,084 MainThread INFO: EPOCH:651
2024-01-23 01:03:41,084 MainThread INFO: Time Consumed:0.2211751937866211s
2024-01-23 01:03:41,084 MainThread INFO: Total Frames:98550s
  7%|▋         | 652/10000 [05:08<38:43,  4.02it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12078.79927
Train_Epoch_Reward                4253.09440
Running_Training_Average_Rewards  10572.15203
Explore_Time                      0.00091
Train___Time                      0.07492
Eval____Time                      0.14309
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11777.70693
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.60610     3.04178     93.82047     85.52602
alpha_0                           0.72179      0.00010     0.72193      0.72165
Alpha_loss                        -2.19447     0.00154     -2.19252     -2.19651
Training/policy_loss              -3.00010     0.00343     -2.99562     -3.00560
Training/qf1_loss                 7260.45596   1320.93059  8696.26367   5345.12500
Training/qf2_loss                 15789.21406  1713.80935  17572.72266  13067.83008
Training/pf_norm                  0.09310      0.01069     0.10355      0.07341
Training/qf1_norm                 843.00547    560.16013   1631.05286   132.53339
Training/qf2_norm                 804.78228    27.05077    841.79272    769.72937
log_std/mean                      -0.12699     0.00006     -0.12694     -0.12710
log_probs/mean                    -2.73314     0.00404     -2.72851     -2.73940
mean/mean                         -0.00692     0.00005     -0.00683     -0.00698
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018580198287963867
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70572
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [98850]
collect time 0.0008444786071777344
inner_dict_sum {'sac_diff0': 0.0002281665802001953, 'sac_diff1': 0.00644230842590332, 'sac_diff2': 0.007739543914794922, 'sac_diff3': 0.00967097282409668, 'sac_diff4': 0.006735563278198242, 'sac_diff5': 0.030953645706176758, 'sac_diff6': 0.0003886222839355469, 'all': 0.062158823013305664}
diff5_list [0.0062713623046875, 0.006093502044677734, 0.006012916564941406, 0.006372928619384766, 0.0062029361724853516]
time3 0
time4 0.0628962516784668
time5 0.0629415512084961
time7 9.5367431640625e-07
gen_weight_change tensor(-20.2961)
policy weight change tensor(36.5861, grad_fn=<SumBackward0>)
time8 0.0018124580383300781
train_time 0.07380223274230957
eval time 0.1568160057067871
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:41,340 MainThread INFO: EPOCH:652
2024-01-23 01:03:41,340 MainThread INFO: Time Consumed:0.23382830619812012s
2024-01-23 01:03:41,340 MainThread INFO: Total Frames:98700s
  7%|▋         | 653/10000 [05:08<39:05,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12053.04647
Train_Epoch_Reward                40602.98272
Running_Training_Average_Rewards  11629.70607
Explore_Time                      0.00084
Train___Time                      0.07380
Eval____Time                      0.15682
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11845.83668
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.15298     2.21951    92.87584     86.66319
alpha_0                           0.72143      0.00010    0.72157      0.72128
Alpha_loss                        -2.19774     0.00074    -2.19653     -2.19839
Training/policy_loss              -3.04107     0.00394    -3.03551     -3.04631
Training/qf1_loss                 6545.83193   622.83144  7657.80713   5883.67188
Training/qf2_loss                 15006.57090  962.01800  16845.27344  14117.67480
Training/pf_norm                  0.10324      0.04177    0.15929      0.04216
Training/qf1_norm                 658.28578    318.82799  1052.63892   168.63286
Training/qf2_norm                 794.88538    18.84240   826.37213    773.07465
log_std/mean                      -0.11735     0.00013    -0.11721     -0.11758
log_probs/mean                    -2.73284     0.00441    -2.72714     -2.73869
mean/mean                         -0.00623     0.00003    -0.00618     -0.00626
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018885374069213867
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70572
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [99000]
collect time 0.001020193099975586
inner_dict_sum {'sac_diff0': 0.00020837783813476562, 'sac_diff1': 0.007259845733642578, 'sac_diff2': 0.008841753005981445, 'sac_diff3': 0.010575056076049805, 'sac_diff4': 0.007040977478027344, 'sac_diff5': 0.03247261047363281, 'sac_diff6': 0.00039577484130859375, 'all': 0.06679439544677734}
diff5_list [0.007022380828857422, 0.0065000057220458984, 0.006535530090332031, 0.006240367889404297, 0.006174325942993164]
time3 0
time4 0.06757569313049316
time5 0.06762123107910156
time7 4.76837158203125e-07
gen_weight_change tensor(-20.2961)
policy weight change tensor(36.7327, grad_fn=<SumBackward0>)
time8 0.0018115043640136719
train_time 0.07897233963012695
eval time 0.17815899848937988
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:41,623 MainThread INFO: EPOCH:653
2024-01-23 01:03:41,623 MainThread INFO: Time Consumed:0.2605409622192383s
2024-01-23 01:03:41,624 MainThread INFO: Total Frames:98850s
  7%|▋         | 654/10000 [05:09<40:35,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12044.11227
Train_Epoch_Reward                10140.87761
Running_Training_Average_Rewards  11698.45132
Explore_Time                      0.00102
Train___Time                      0.07897
Eval____Time                      0.17816
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11915.00890
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.70468     2.32699    91.98481     85.49635
alpha_0                           0.72107      0.00010    0.72121      0.72092
Alpha_loss                        -2.20281     0.00272    -2.19901     -2.20693
Training/policy_loss              -3.14309     0.00401    -3.13770     -3.14900
Training/qf1_loss                 6833.41152   453.64751  7350.17969   6209.27051
Training/qf2_loss                 15176.96211  594.84829  16126.16211  14371.51562
Training/pf_norm                  0.10535      0.00964    0.11740      0.09072
Training/qf1_norm                 1448.79580   440.20909  2006.07520   763.75842
Training/qf2_norm                 852.01527    22.03814   883.21356    821.65088
log_std/mean                      -0.12576     0.00029    -0.12542     -0.12621
log_probs/mean                    -2.73806     0.00573    -2.73055     -2.74652
mean/mean                         -0.00552     0.00008    -0.00543     -0.00564
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018523216247558594
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70572
epoch first part time 3.337860107421875e-06
replay_buffer._size: [99150]
collect time 0.0008990764617919922
inner_dict_sum {'sac_diff0': 0.00022673606872558594, 'sac_diff1': 0.007431983947753906, 'sac_diff2': 0.0086517333984375, 'sac_diff3': 0.011564970016479492, 'sac_diff4': 0.0077991485595703125, 'sac_diff5': 0.03456687927246094, 'sac_diff6': 0.0004050731658935547, 'all': 0.07064652442932129}
diff5_list [0.007486820220947266, 0.006945371627807617, 0.006689310073852539, 0.00696253776550293, 0.006482839584350586]
time3 0
time4 0.07142972946166992
time5 0.07147860527038574
time7 7.152557373046875e-07
gen_weight_change tensor(-20.2961)
policy weight change tensor(36.9085, grad_fn=<SumBackward0>)
time8 0.0020079612731933594
train_time 0.08284115791320801
eval time 0.16213107109069824
epoch last part time 5.7220458984375e-06
2024-01-23 01:03:41,894 MainThread INFO: EPOCH:654
2024-01-23 01:03:41,894 MainThread INFO: Time Consumed:0.24817228317260742s
2024-01-23 01:03:41,894 MainThread INFO: Total Frames:99000s
  7%|▋         | 655/10000 [05:09<41:03,  3.79it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12044.47689
Train_Epoch_Reward                2002.30320
Running_Training_Average_Rewards  11612.69287
Explore_Time                      0.00089
Train___Time                      0.08284
Eval____Time                      0.16213
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11887.83839
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.79098     2.29594     92.77440     86.35567
alpha_0                           0.72071      0.00010     0.72085      0.72056
Alpha_loss                        -2.20516     0.00146     -2.20398     -2.20799
Training/policy_loss              -3.17857     0.00266     -3.17354     -3.18140
Training/qf1_loss                 7159.29443   1457.79452  9296.20312   5217.54395
Training/qf2_loss                 15730.85059  1852.54969  18247.85742  13160.31445
Training/pf_norm                  0.12009      0.02165     0.13945      0.07777
Training/qf1_norm                 856.66858    432.12454   1397.54797   207.60475
Training/qf2_norm                 883.01082    21.73401    910.83838    850.64880
log_std/mean                      -0.12757     0.00029     -0.12720     -0.12799
log_probs/mean                    -2.73493     0.00351     -2.72929     -2.73944
mean/mean                         -0.00695     0.00010     -0.00677     -0.00705
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01874685287475586
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70572
epoch first part time 2.86102294921875e-06
replay_buffer._size: [99300]
collect time 0.0009391307830810547
inside mustsac before update, task 0, sumup 70572
inside mustsac after update, task 0, sumup 71241
inner_dict_sum {'sac_diff0': 0.000217437744140625, 'sac_diff1': 0.007449626922607422, 'sac_diff2': 0.009493589401245117, 'sac_diff3': 0.01089620590209961, 'sac_diff4': 0.008663654327392578, 'sac_diff5': 0.05553293228149414, 'sac_diff6': 0.00044846534729003906, 'all': 0.09270191192626953}
diff5_list [0.010205984115600586, 0.009964942932128906, 0.015408754348754883, 0.010406494140625, 0.009546756744384766]
time3 0.0008842945098876953
time4 0.09370732307434082
time5 0.09377002716064453
time7 0.010442018508911133
gen_weight_change tensor(-20.4409)
policy weight change tensor(36.9250, grad_fn=<SumBackward0>)
time8 0.0019724369049072266
train_time 0.12482309341430664
eval time 0.09995293617248535
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:42,144 MainThread INFO: EPOCH:655
2024-01-23 01:03:42,145 MainThread INFO: Time Consumed:0.22794198989868164s
2024-01-23 01:03:42,145 MainThread INFO: Total Frames:99150s
  7%|▋         | 656/10000 [05:09<40:27,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12043.67326
Train_Epoch_Reward                15810.35871
Running_Training_Average_Rewards  10885.86473
Explore_Time                      0.00093
Train___Time                      0.12482
Eval____Time                      0.09995
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11828.72356
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.25082     3.43610     96.81046     86.72073
alpha_0                           0.72035      0.00010     0.72049      0.72020
Alpha_loss                        -2.20557     0.00191     -2.20277     -2.20737
Training/policy_loss              -3.15486     0.07794     -3.06617     -3.28762
Training/qf1_loss                 7060.57119   1444.51330  9765.87793   5574.63281
Training/qf2_loss                 15725.97734  2087.21971  19651.88281  13561.74219
Training/pf_norm                  0.08840      0.02752     0.13496      0.05847
Training/qf1_norm                 869.75156    615.88387   1927.46899   211.61060
Training/qf2_norm                 885.54380    84.01057    1040.07568   814.52216
log_std/mean                      -0.13011     0.00592     -0.12466     -0.14094
log_probs/mean                    -2.72590     0.00753     -2.71531     -2.73509
mean/mean                         -0.00500     0.00098     -0.00415     -0.00630
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019040346145629883
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71241
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [99450]
collect time 0.0008664131164550781
inner_dict_sum {'sac_diff0': 0.0002257823944091797, 'sac_diff1': 0.006682157516479492, 'sac_diff2': 0.0077397823333740234, 'sac_diff3': 0.010239839553833008, 'sac_diff4': 0.007054567337036133, 'sac_diff5': 0.03238272666931152, 'sac_diff6': 0.00038242340087890625, 'all': 0.06470727920532227}
diff5_list [0.006511211395263672, 0.006461620330810547, 0.007039785385131836, 0.006228446960449219, 0.00614166259765625]
time3 0
time4 0.06545519828796387
time5 0.06550097465515137
time7 4.76837158203125e-07
gen_weight_change tensor(-20.4409)
policy weight change tensor(37.0410, grad_fn=<SumBackward0>)
time8 0.0017740726470947266
train_time 0.07630729675292969
eval time 0.14957404136657715
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:42,396 MainThread INFO: EPOCH:656
2024-01-23 01:03:42,396 MainThread INFO: Time Consumed:0.22901105880737305s
2024-01-23 01:03:42,396 MainThread INFO: Total Frames:99300s
  7%|▋         | 657/10000 [05:10<40:08,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12027.03665
Train_Epoch_Reward                34281.18952
Running_Training_Average_Rewards  11608.86494
Explore_Time                      0.00086
Train___Time                      0.07631
Eval____Time                      0.14957
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12098.95077
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.71739     2.22608     92.49292     86.06447
alpha_0                           0.71999      0.00010     0.72013      0.71984
Alpha_loss                        -2.21221     0.00236     -2.20841     -2.21567
Training/policy_loss              -3.25750     0.00380     -3.25241     -3.26403
Training/qf1_loss                 7078.38027   1144.38028  9141.53516   5637.00146
Training/qf2_loss                 15425.00820  1528.28943  18156.70898  13506.38477
Training/pf_norm                  0.11221      0.01119     0.13198      0.10211
Training/qf1_norm                 1062.69108   443.37345   1809.85815   509.80664
Training/qf2_norm                 924.88115    22.72410    963.04938    897.81281
log_std/mean                      -0.13243     0.00015     -0.13219     -0.13262
log_probs/mean                    -2.73589     0.00524     -2.72842     -2.74438
mean/mean                         -0.00626     0.00008     -0.00618     -0.00638
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020160675048828125
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71241
epoch first part time 2.384185791015625e-06
replay_buffer._size: [99600]
collect time 0.0009837150573730469
inner_dict_sum {'sac_diff0': 0.0002014636993408203, 'sac_diff1': 0.007013559341430664, 'sac_diff2': 0.00821065902709961, 'sac_diff3': 0.011136531829833984, 'sac_diff4': 0.007345438003540039, 'sac_diff5': 0.0331878662109375, 'sac_diff6': 0.000377655029296875, 'all': 0.06747317314147949}
diff5_list [0.006876945495605469, 0.006407022476196289, 0.006814002990722656, 0.006615400314331055, 0.006474494934082031]
time3 0
time4 0.06825542449951172
time5 0.06830024719238281
time7 7.152557373046875e-07
gen_weight_change tensor(-20.4409)
policy weight change tensor(37.0217, grad_fn=<SumBackward0>)
time8 0.0019588470458984375
train_time 0.0797724723815918
eval time 0.14206528663635254
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:42,645 MainThread INFO: EPOCH:657
2024-01-23 01:03:42,645 MainThread INFO: Time Consumed:0.22507572174072266s
2024-01-23 01:03:42,646 MainThread INFO: Total Frames:99450s
  7%|▋         | 658/10000 [05:10<39:38,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12010.94868
Train_Epoch_Reward                16402.30567
Running_Training_Average_Rewards  11516.79927
Explore_Time                      0.00098
Train___Time                      0.07977
Eval____Time                      0.14207
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12069.12227
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.31013     2.96814     95.10190     87.03437
alpha_0                           0.71963      0.00010     0.71977      0.71948
Alpha_loss                        -2.21525     0.00243     -2.21221     -2.21759
Training/policy_loss              -3.21986     0.00576     -3.21140     -3.22710
Training/qf1_loss                 7859.59414   1657.21042  10868.24023  5777.05225
Training/qf2_loss                 16565.84805  2177.15825  20494.46484  13851.63477
Training/pf_norm                  0.10898      0.01912     0.13236      0.07633
Training/qf1_norm                 612.59291    535.95595   1473.43823   88.19055
Training/qf2_norm                 940.76124    30.81544    990.42487    906.48529
log_std/mean                      -0.13714     0.00013     -0.13693     -0.13727
log_probs/mean                    -2.73490     0.00728     -2.72415     -2.74341
mean/mean                         -0.00512     0.00002     -0.00510     -0.00514
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018471479415893555
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71241
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [99750]
collect time 0.0009150505065917969
inner_dict_sum {'sac_diff0': 0.00023245811462402344, 'sac_diff1': 0.00690770149230957, 'sac_diff2': 0.008131027221679688, 'sac_diff3': 0.010955572128295898, 'sac_diff4': 0.006907224655151367, 'sac_diff5': 0.03267240524291992, 'sac_diff6': 0.0003921985626220703, 'all': 0.06619858741760254}
diff5_list [0.0066645145416259766, 0.006690263748168945, 0.0061528682708740234, 0.006426572799682617, 0.006738185882568359]
time3 0
time4 0.06693220138549805
time5 0.06697583198547363
time7 4.76837158203125e-07
gen_weight_change tensor(-20.4409)
policy weight change tensor(36.9600, grad_fn=<SumBackward0>)
time8 0.002077341079711914
train_time 0.07807660102844238
eval time 0.14745378494262695
epoch last part time 7.867813110351562e-06
2024-01-23 01:03:42,896 MainThread INFO: EPOCH:658
2024-01-23 01:03:42,896 MainThread INFO: Time Consumed:0.22890138626098633s
2024-01-23 01:03:42,897 MainThread INFO: Total Frames:99600s
  7%|▋         | 659/10000 [05:10<39:29,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11992.50041
Train_Epoch_Reward                9152.56075
Running_Training_Average_Rewards  11442.60820
Explore_Time                      0.00091
Train___Time                      0.07808
Eval____Time                      0.14745
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12022.69336
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.17204     0.77797    92.09792     89.82788
alpha_0                           0.71927      0.00010    0.71941      0.71912
Alpha_loss                        -2.21824     0.00053    -2.21754     -2.21911
Training/policy_loss              -3.15774     0.00215    -3.15475     -3.16104
Training/qf1_loss                 7237.82959   512.12923  7825.73828   6487.20459
Training/qf2_loss                 16090.63613  624.75571  16859.36328  15076.75195
Training/pf_norm                  0.08198      0.01367    0.10631      0.06496
Training/qf1_norm                 359.34178    161.55497  600.74518    94.04293
Training/qf2_norm                 911.72379    7.74171    921.10400    898.11621
log_std/mean                      -0.13414     0.00011    -0.13402     -0.13430
log_probs/mean                    -2.73375     0.00285    -2.72957     -2.73824
mean/mean                         -0.00612     0.00004    -0.00608     -0.00620
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01875758171081543
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71241
epoch first part time 2.1457672119140625e-06
replay_buffer._size: [99900]
collect time 0.0009670257568359375
inner_dict_sum {'sac_diff0': 0.00022649765014648438, 'sac_diff1': 0.006949424743652344, 'sac_diff2': 0.008201837539672852, 'sac_diff3': 0.010625600814819336, 'sac_diff4': 0.007308483123779297, 'sac_diff5': 0.03288149833679199, 'sac_diff6': 0.0003943443298339844, 'all': 0.06658768653869629}
diff5_list [0.0065190792083740234, 0.006522655487060547, 0.006129264831542969, 0.006610393524169922, 0.007100105285644531]
time3 0
time4 0.06740760803222656
time5 0.06745696067810059
time7 7.152557373046875e-07
gen_weight_change tensor(-20.4409)
policy weight change tensor(36.9130, grad_fn=<SumBackward0>)
time8 0.0020418167114257812
train_time 0.07886314392089844
eval time 0.14422893524169922
epoch last part time 7.867813110351562e-06
2024-01-23 01:03:43,145 MainThread INFO: EPOCH:659
2024-01-23 01:03:43,145 MainThread INFO: Time Consumed:0.22646212577819824s
2024-01-23 01:03:43,146 MainThread INFO: Total Frames:99750s
  7%|▋         | 660/10000 [05:10<39:16,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11972.84638
Train_Epoch_Reward                4833.43982
Running_Training_Average_Rewards  11333.98444
Explore_Time                      0.00096
Train___Time                      0.07886
Eval____Time                      0.14423
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12034.44232
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.54956     1.05340    90.32668     87.42946
alpha_0                           0.71891      0.00010    0.71905      0.71876
Alpha_loss                        -2.22035     0.00059    -2.21927     -2.22104
Training/policy_loss              -3.27868     0.00165    -3.27639     -3.28137
Training/qf1_loss                 6497.60967   757.19881  7915.10303   5739.14941
Training/qf2_loss                 14846.13340  944.93405  16593.21094  13915.09961
Training/pf_norm                  0.11144      0.01906    0.13769      0.08647
Training/qf1_norm                 888.97333    222.42418  1251.94543   669.28040
Training/qf2_norm                 941.78932    10.88077   960.50610    929.56573
log_std/mean                      -0.13496     0.00008    -0.13486     -0.13509
log_probs/mean                    -2.72995     0.00188    -2.72775     -2.73274
mean/mean                         -0.00614     0.00008    -0.00604     -0.00625
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01893758773803711
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71241
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [100050]
collect time 0.0009756088256835938
inside mustsac before update, task 0, sumup 71241
inside mustsac after update, task 0, sumup 70807
inner_dict_sum {'sac_diff0': 0.00020837783813476562, 'sac_diff1': 0.007189273834228516, 'sac_diff2': 0.008491992950439453, 'sac_diff3': 0.010994195938110352, 'sac_diff4': 0.00751495361328125, 'sac_diff5': 0.05117321014404297, 'sac_diff6': 0.0004184246063232422, 'all': 0.08599042892456055}
diff5_list [0.010547876358032227, 0.0110321044921875, 0.010095834732055664, 0.009595394134521484, 0.009902000427246094]
time3 0.0008840560913085938
time4 0.08685970306396484
time5 0.08691143989562988
time7 0.009496688842773438
gen_weight_change tensor(-20.5904)
policy weight change tensor(36.9475, grad_fn=<SumBackward0>)
time8 0.0029463768005371094
train_time 0.11791419982910156
eval time 0.10773611068725586
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:43,397 MainThread INFO: EPOCH:660
2024-01-23 01:03:43,397 MainThread INFO: Time Consumed:0.22891497611999512s
2024-01-23 01:03:43,397 MainThread INFO: Total Frames:99900s
  7%|▋         | 661/10000 [05:11<39:24,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11949.08273
Train_Epoch_Reward                7891.66249
Running_Training_Average_Rewards  11072.97087
Explore_Time                      0.00097
Train___Time                      0.11791
Eval____Time                      0.10774
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12010.50408
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.45677     3.19588     96.30894     87.39332
alpha_0                           0.71855      0.00010     0.71869      0.71841
Alpha_loss                        -2.22499     0.00240     -2.22070     -2.22789
Training/policy_loss              -3.18293     0.10835     -3.08022     -3.34259
Training/qf1_loss                 7393.73125   1113.77662  8684.36230   5755.21240
Training/qf2_loss                 16237.08477  1610.12363  18383.62891  14320.47168
Training/pf_norm                  0.11310      0.02555     0.16166      0.09077
Training/qf1_norm                 1439.57934   692.80560   2042.72253   176.22360
Training/qf2_norm                 899.21696    89.47806    1059.49463   795.78656
log_std/mean                      -0.13113     0.00503     -0.12521     -0.14032
log_probs/mean                    -2.73379     0.00515     -2.72489     -2.73920
mean/mean                         -0.00541     0.00067     -0.00465     -0.00661
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01979684829711914
epoch last part time3 0.002907991409301758
inside rlalgo, task 0, sumup 70807
epoch first part time 2.86102294921875e-06
replay_buffer._size: [100200]
collect time 0.0009331703186035156
inner_dict_sum {'sac_diff0': 0.00022411346435546875, 'sac_diff1': 0.006870746612548828, 'sac_diff2': 0.008215188980102539, 'sac_diff3': 0.010511636734008789, 'sac_diff4': 0.006857872009277344, 'sac_diff5': 0.03262805938720703, 'sac_diff6': 0.00040030479431152344, 'all': 0.06570792198181152}
diff5_list [0.00714421272277832, 0.0063135623931884766, 0.006133317947387695, 0.006753444671630859, 0.00628352165222168]
time3 0
time4 0.06647539138793945
time5 0.06652021408081055
time7 7.152557373046875e-07
gen_weight_change tensor(-20.5904)
policy weight change tensor(36.9196, grad_fn=<SumBackward0>)
time8 0.0018856525421142578
train_time 0.07768583297729492
eval time 0.14862060546875
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:43,652 MainThread INFO: EPOCH:661
2024-01-23 01:03:43,653 MainThread INFO: Time Consumed:0.22964954376220703s
2024-01-23 01:03:43,653 MainThread INFO: Total Frames:100050s
  7%|▋         | 662/10000 [05:11<39:18,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11959.47522
Train_Epoch_Reward                3426.74272
Running_Training_Average_Rewards  10758.17998
Explore_Time                      0.00093
Train___Time                      0.07769
Eval____Time                      0.14862
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11881.63187
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.67877     3.91223     94.40309     84.71935
alpha_0                           0.71819      0.00010     0.71833      0.71805
Alpha_loss                        -2.22940     0.00278     -2.22574     -2.23319
Training/policy_loss              -3.14471     0.00499     -3.13926     -3.15232
Training/qf1_loss                 7123.76055   1194.02099  9301.56445   5689.88818
Training/qf2_loss                 15585.46270  1851.87886  18764.31250  13271.56543
Training/pf_norm                  0.10857      0.02327     0.13668      0.07411
Training/qf1_norm                 2032.46880   844.32072   3096.60034   1035.05322
Training/qf2_norm                 881.41364    37.00761    925.98328    835.87317
log_std/mean                      -0.13149     0.00002     -0.13146     -0.13151
log_probs/mean                    -2.73696     0.00640     -2.72996     -2.74638
mean/mean                         -0.00479     0.00009     -0.00465     -0.00491
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018505573272705078
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70807
epoch first part time 2.384185791015625e-06
replay_buffer._size: [100350]
collect time 0.0009944438934326172
inner_dict_sum {'sac_diff0': 0.00023937225341796875, 'sac_diff1': 0.007259845733642578, 'sac_diff2': 0.008535146713256836, 'sac_diff3': 0.010935306549072266, 'sac_diff4': 0.007330179214477539, 'sac_diff5': 0.03375077247619629, 'sac_diff6': 0.0003876686096191406, 'all': 0.06843829154968262}
diff5_list [0.0070760250091552734, 0.006713151931762695, 0.00628352165222168, 0.006760120391845703, 0.0069179534912109375]
time3 0
time4 0.0692145824432373
time5 0.0692586898803711
time7 7.152557373046875e-07
gen_weight_change tensor(-20.5904)
policy weight change tensor(36.8731, grad_fn=<SumBackward0>)
time8 0.0019352436065673828
train_time 0.0803532600402832
eval time 0.14223027229309082
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:43,900 MainThread INFO: EPOCH:662
2024-01-23 01:03:43,901 MainThread INFO: Time Consumed:0.22601103782653809s
2024-01-23 01:03:43,901 MainThread INFO: Total Frames:100200s
  7%|▋         | 663/10000 [05:11<39:08,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           11967.67661
Train_Epoch_Reward                19388.63146
Running_Training_Average_Rewards  11146.67181
Explore_Time                      0.00099
Train___Time                      0.08035
Eval____Time                      0.14223
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11927.85057
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.30060     2.24610     94.56789     88.29591
alpha_0                           0.71783      0.00010     0.71797      0.71769
Alpha_loss                        -2.23065     0.00133     -2.22875     -2.23224
Training/policy_loss              -3.21449     0.00330     -3.21039     -3.21840
Training/qf1_loss                 6960.96133   745.37202   8017.18506   5951.31934
Training/qf2_loss                 15637.61934  1153.66132  17524.89453  14236.61719
Training/pf_norm                  0.07712      0.02095     0.11354      0.05980
Training/qf1_norm                 517.41244    139.18100   638.04449    258.53165
Training/qf2_norm                 913.22662    22.43803    955.84113    893.40479
log_std/mean                      -0.13918     0.00008     -0.13906     -0.13925
log_probs/mean                    -2.73055     0.00396     -2.72517     -2.73530
mean/mean                         -0.00559     0.00009     -0.00545     -0.00572
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019229650497436523
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70807
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [100500]
collect time 0.001081228256225586
inner_dict_sum {'sac_diff0': 0.00020956993103027344, 'sac_diff1': 0.00782012939453125, 'sac_diff2': 0.009252786636352539, 'sac_diff3': 0.011771440505981445, 'sac_diff4': 0.0077288150787353516, 'sac_diff5': 0.03510928153991699, 'sac_diff6': 0.00042748451232910156, 'all': 0.07231950759887695}
diff5_list [0.006766080856323242, 0.007703542709350586, 0.007653474807739258, 0.006672382354736328, 0.006313800811767578]
time3 0
time4 0.07313966751098633
time5 0.07318854331970215
time7 9.5367431640625e-07
gen_weight_change tensor(-20.5904)
policy weight change tensor(36.8724, grad_fn=<SumBackward0>)
time8 0.0018358230590820312
train_time 0.08457303047180176
eval time 0.14934158325195312
epoch last part time 7.62939453125e-06
2024-01-23 01:03:44,161 MainThread INFO: EPOCH:663
2024-01-23 01:03:44,162 MainThread INFO: Time Consumed:0.23747754096984863s
2024-01-23 01:03:44,162 MainThread INFO: Total Frames:100350s
  7%|▋         | 664/10000 [05:11<39:33,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11972.28526
Train_Epoch_Reward                2268.08245
Running_Training_Average_Rewards  10493.55994
Explore_Time                      0.00108
Train___Time                      0.08457
Eval____Time                      0.14934
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11961.09542
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.34916     1.94079    94.17832     89.21111
alpha_0                           0.71747      0.00010    0.71762      0.71733
Alpha_loss                        -2.23583     0.00188    -2.23362     -2.23842
Training/policy_loss              -3.19485     0.00390    -3.18825     -3.19828
Training/qf1_loss                 7318.00459   327.08687  7729.40723   6900.20703
Training/qf2_loss                 16205.89766  396.31034  16917.39453  15784.02344
Training/pf_norm                  0.09876      0.01641    0.12748      0.08121
Training/qf1_norm                 390.40564    230.22740  776.22552    190.38719
Training/qf2_norm                 921.75375    19.46706   948.70465    898.67078
log_std/mean                      -0.12713     0.00004    -0.12708     -0.12719
log_probs/mean                    -2.73602     0.00606    -2.72531     -2.74179
mean/mean                         -0.00600     0.00006    -0.00592     -0.00609
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018951892852783203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70807
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [100650]
collect time 0.0009996891021728516
inner_dict_sum {'sac_diff0': 0.00021147727966308594, 'sac_diff1': 0.0069963932037353516, 'sac_diff2': 0.008208751678466797, 'sac_diff3': 0.01061558723449707, 'sac_diff4': 0.0068209171295166016, 'sac_diff5': 0.0325927734375, 'sac_diff6': 0.0004010200500488281, 'all': 0.06584692001342773}
diff5_list [0.006650209426879883, 0.007245540618896484, 0.006190061569213867, 0.0061414241790771484, 0.006365537643432617]
time3 0
time4 0.06660151481628418
time5 0.06664562225341797
time7 9.5367431640625e-07
gen_weight_change tensor(-20.5904)
policy weight change tensor(36.8534, grad_fn=<SumBackward0>)
time8 0.0019876956939697266
train_time 0.07798647880554199
eval time 0.14631032943725586
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:44,412 MainThread INFO: EPOCH:664
2024-01-23 01:03:44,412 MainThread INFO: Time Consumed:0.22771739959716797s
2024-01-23 01:03:44,413 MainThread INFO: Total Frames:100500s
  7%|▋         | 665/10000 [05:12<39:22,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11981.59945
Train_Epoch_Reward                8267.13007
Running_Training_Average_Rewards  10383.17178
Explore_Time                      0.00099
Train___Time                      0.07799
Eval____Time                      0.14631
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11980.98027
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.81385     1.50475    91.59497     87.84940
alpha_0                           0.71711      0.00010    0.71726      0.71697
Alpha_loss                        -2.23644     0.00196    -2.23422     -2.23908
Training/policy_loss              -3.28263     0.00426    -3.27695     -3.28911
Training/qf1_loss                 6786.82324   420.49732  7510.13916   6321.04834
Training/qf2_loss                 15398.11973  689.40483  16463.36133  14645.58789
Training/pf_norm                  0.08421      0.03747    0.14968      0.03930
Training/qf1_norm                 316.92686    94.77234   457.86737    184.02028
Training/qf2_norm                 954.14194    15.51516   972.31116    933.74255
log_std/mean                      -0.13619     0.00009    -0.13610     -0.13633
log_probs/mean                    -2.72773     0.00610    -2.71977     -2.73598
mean/mean                         -0.00326     0.00003    -0.00320     -0.00329
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018838167190551758
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70807
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [100800]
collect time 0.0009555816650390625
inside mustsac before update, task 0, sumup 70807
inside mustsac after update, task 0, sumup 70412
inner_dict_sum {'sac_diff0': 0.0002231597900390625, 'sac_diff1': 0.007288455963134766, 'sac_diff2': 0.00892186164855957, 'sac_diff3': 0.010946989059448242, 'sac_diff4': 0.007439374923706055, 'sac_diff5': 0.05112266540527344, 'sac_diff6': 0.0004181861877441406, 'all': 0.08636069297790527}
diff5_list [0.010551214218139648, 0.009796619415283203, 0.00973367691040039, 0.010158777236938477, 0.010882377624511719]
time3 0.0008769035339355469
time4 0.08728432655334473
time5 0.08733844757080078
time7 0.009192705154418945
gen_weight_change tensor(-20.3563)
policy weight change tensor(36.8370, grad_fn=<SumBackward0>)
time8 0.0018768310546875
train_time 0.11706972122192383
eval time 0.10480570793151855
epoch last part time 5.9604644775390625e-06
2024-01-23 01:03:44,660 MainThread INFO: EPOCH:665
2024-01-23 01:03:44,660 MainThread INFO: Time Consumed:0.2252368927001953s
2024-01-23 01:03:44,660 MainThread INFO: Total Frames:100650s
  7%|▋         | 666/10000 [05:12<39:07,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           11998.52850
Train_Epoch_Reward                11340.61698
Running_Training_Average_Rewards  10495.05340
Explore_Time                      0.00095
Train___Time                      0.11707
Eval____Time                      0.10481
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             11998.01406
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.12175     1.61472    91.10075     86.65358
alpha_0                           0.71676      0.00010    0.71690      0.71661
Alpha_loss                        -2.24231     0.00314    -2.23840     -2.24693
Training/policy_loss              -3.20027     0.07913    -3.08048     -3.30274
Training/qf1_loss                 7121.54395   733.49427  8043.16260   6344.82861
Training/qf2_loss                 15543.28789  809.37159  16694.53516  14497.55176
Training/pf_norm                  0.11313      0.02565    0.14066      0.06710
Training/qf1_norm                 1525.43799   532.56952  2484.62280   872.55188
Training/qf2_norm                 886.86383    53.07440   962.18488    795.64410
log_std/mean                      -0.13070     0.00857    -0.11883     -0.14415
log_probs/mean                    -2.73526     0.00659    -2.72757     -2.74508
mean/mean                         -0.00571     0.00132    -0.00337     -0.00716
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018792390823364258
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70412
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [100950]
collect time 0.0009021759033203125
inner_dict_sum {'sac_diff0': 0.00023317337036132812, 'sac_diff1': 0.006823301315307617, 'sac_diff2': 0.008044958114624023, 'sac_diff3': 0.010160684585571289, 'sac_diff4': 0.0069844722747802734, 'sac_diff5': 0.03165888786315918, 'sac_diff6': 0.0004057884216308594, 'all': 0.06431126594543457}
diff5_list [0.006726980209350586, 0.006232500076293945, 0.0063931941986083984, 0.00615692138671875, 0.0061492919921875]
time3 0
time4 0.06507754325866699
time5 0.06512284278869629
time7 7.152557373046875e-07
gen_weight_change tensor(-20.3563)
policy weight change tensor(36.8591, grad_fn=<SumBackward0>)
time8 0.0018830299377441406
train_time 0.076324462890625
eval time 0.14869236946105957
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:44,910 MainThread INFO: EPOCH:666
2024-01-23 01:03:44,911 MainThread INFO: Time Consumed:0.22833561897277832s
2024-01-23 01:03:44,911 MainThread INFO: Total Frames:100800s
  7%|▋         | 667/10000 [05:12<39:04,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12016.47215
Train_Epoch_Reward                11431.34541
Running_Training_Average_Rewards  10619.14126
Explore_Time                      0.00090
Train___Time                      0.07632
Eval____Time                      0.14869
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12278.38727
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.75138     2.04185     93.61829     87.98582
alpha_0                           0.71640      0.00010     0.71654      0.71625
Alpha_loss                        -2.24582     0.00192     -2.24264     -2.24795
Training/policy_loss              -3.20826     0.00311     -3.20290     -3.21193
Training/qf1_loss                 7197.87158   1245.24719  9603.07520   6080.09424
Training/qf2_loss                 15964.10938  1589.97554  18919.33008  14318.32812
Training/pf_norm                  0.11887      0.02555     0.16706      0.09564
Training/qf1_norm                 437.67948    236.12977   851.59735    143.05321
Training/qf2_norm                 931.25912    21.01613    959.95404    902.91443
log_std/mean                      -0.12003     0.00005     -0.11997     -0.12011
log_probs/mean                    -2.73568     0.00405     -2.72816     -2.74008
mean/mean                         -0.00593     0.00006     -0.00586     -0.00603
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018756866455078125
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70412
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [101100]
collect time 0.0009458065032958984
inner_dict_sum {'sac_diff0': 0.00022792816162109375, 'sac_diff1': 0.006906747817993164, 'sac_diff2': 0.008432388305664062, 'sac_diff3': 0.010309696197509766, 'sac_diff4': 0.007489681243896484, 'sac_diff5': 0.033188819885253906, 'sac_diff6': 0.0004019737243652344, 'all': 0.06695723533630371}
diff5_list [0.006610870361328125, 0.006218671798706055, 0.0063283443450927734, 0.006585597991943359, 0.007445335388183594]
time3 0
time4 0.06777334213256836
time5 0.06782245635986328
time7 9.5367431640625e-07
gen_weight_change tensor(-20.3563)
policy weight change tensor(36.8749, grad_fn=<SumBackward0>)
time8 0.0019490718841552734
train_time 0.07904338836669922
eval time 0.14732980728149414
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:45,163 MainThread INFO: EPOCH:667
2024-01-23 01:03:45,163 MainThread INFO: Time Consumed:0.22975969314575195s
2024-01-23 01:03:45,163 MainThread INFO: Total Frames:100950s
  7%|▋         | 668/10000 [05:12<39:06,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12045.47665
Train_Epoch_Reward                1394.16077
Running_Training_Average_Rewards  10204.35284
Explore_Time                      0.00094
Train___Time                      0.07904
Eval____Time                      0.14733
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12359.16726
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.97854     1.24205    91.84583     88.16117
alpha_0                           0.71604      0.00010    0.71618      0.71590
Alpha_loss                        -2.24763     0.00156    -2.24556     -2.24992
Training/policy_loss              -3.09360     0.00378    -3.09048     -3.10080
Training/qf1_loss                 7191.80977   701.95518  8577.94141   6754.32959
Training/qf2_loss                 15799.86211  571.98009  16890.95312  15230.40625
Training/pf_norm                  0.09424      0.01974    0.13100      0.07238
Training/qf1_norm                 889.96969    221.68563  1218.00476   554.63861
Training/qf2_norm                 868.35278    11.45979   886.01733    851.81519
log_std/mean                      -0.13129     0.00002    -0.13128     -0.13132
log_probs/mean                    -2.73103     0.00564    -2.72684     -2.74192
mean/mean                         -0.00611     0.00007    -0.00600     -0.00618
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0185854434967041
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70412
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [101250]
collect time 0.0009365081787109375
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.007241725921630859, 'sac_diff2': 0.008947134017944336, 'sac_diff3': 0.011103153228759766, 'sac_diff4': 0.0074214935302734375, 'sac_diff5': 0.03389716148376465, 'sac_diff6': 0.0003991127014160156, 'all': 0.06922268867492676}
diff5_list [0.0065042972564697266, 0.0071947574615478516, 0.0063343048095703125, 0.007495403289794922, 0.006368398666381836]
time3 0
time4 0.07004332542419434
time5 0.07009482383728027
time7 7.152557373046875e-07
gen_weight_change tensor(-20.3563)
policy weight change tensor(36.9039, grad_fn=<SumBackward0>)
time8 0.0018856525421142578
train_time 0.08127379417419434
eval time 0.14754629135131836
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:45,417 MainThread INFO: EPOCH:668
2024-01-23 01:03:45,417 MainThread INFO: Time Consumed:0.2321925163269043s
2024-01-23 01:03:45,418 MainThread INFO: Total Frames:101100s
  7%|▋         | 669/10000 [05:13<39:14,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12091.00922
Train_Epoch_Reward                33609.66419
Running_Training_Average_Rewards  10876.94804
Explore_Time                      0.00093
Train___Time                      0.08127
Eval____Time                      0.14755
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12478.01907
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.90247     1.38462     91.28078     87.72292
alpha_0                           0.71568      0.00010     0.71582      0.71554
Alpha_loss                        -2.25194     0.00098     -2.25049     -2.25350
Training/policy_loss              -3.20136     0.00305     -3.19688     -3.20598
Training/qf1_loss                 6467.52725   810.27864   7591.36084   5538.24902
Training/qf2_loss                 14830.27383  1046.84756  16376.70312  13687.92480
Training/pf_norm                  0.09191      0.02128     0.12777      0.06078
Training/qf1_norm                 1238.00110   272.55735   1673.63293   962.07501
Training/qf2_norm                 888.75924    14.16689    913.81763    876.17181
log_std/mean                      -0.13563     0.00006     -0.13556     -0.13570
log_probs/mean                    -2.73385     0.00325     -2.72987     -2.73867
mean/mean                         -0.00400     0.00002     -0.00398     -0.00403
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018599510192871094
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70412
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [101400]
collect time 0.0009548664093017578
inner_dict_sum {'sac_diff0': 0.00020956993103027344, 'sac_diff1': 0.007129669189453125, 'sac_diff2': 0.00862741470336914, 'sac_diff3': 0.011571645736694336, 'sac_diff4': 0.007230997085571289, 'sac_diff5': 0.033211708068847656, 'sac_diff6': 0.0004124641418457031, 'all': 0.06839346885681152}
diff5_list [0.007036924362182617, 0.006637096405029297, 0.0063915252685546875, 0.006551265716552734, 0.00659489631652832]
time3 0
time4 0.06920552253723145
time5 0.0692601203918457
time7 7.152557373046875e-07
gen_weight_change tensor(-20.3563)
policy weight change tensor(36.9212, grad_fn=<SumBackward0>)
time8 0.0019290447235107422
train_time 0.08054614067077637
eval time 0.14624476432800293
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:45,669 MainThread INFO: EPOCH:669
2024-01-23 01:03:45,670 MainThread INFO: Time Consumed:0.23015499114990234s
2024-01-23 01:03:45,670 MainThread INFO: Total Frames:101250s
  7%|▋         | 670/10000 [05:13<39:17,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12145.20859
Train_Epoch_Reward                2933.41675
Running_Training_Average_Rewards  10880.34185
Explore_Time                      0.00095
Train___Time                      0.08055
Eval____Time                      0.14624
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12576.43602
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.16272     1.02656     90.32837     87.55819
alpha_0                           0.71532      0.00010     0.71547      0.71518
Alpha_loss                        -2.25605     0.00305     -2.25122     -2.26023
Training/policy_loss              -3.15754     0.00428     -3.15121     -3.16380
Training/qf1_loss                 6955.74092   884.54583   7773.48584   5333.78760
Training/qf2_loss                 15403.60898  1054.17162  16439.46680  13494.33887
Training/pf_norm                  0.11031      0.01552     0.13523      0.09074
Training/qf1_norm                 883.92444    177.53497   1117.65857   655.91217
Training/qf2_norm                 908.24165    10.58922    920.70056    891.78552
log_std/mean                      -0.12888     0.00008     -0.12876     -0.12897
log_probs/mean                    -2.73606     0.00641     -2.72568     -2.74453
mean/mean                         -0.00641     0.00012     -0.00629     -0.00661
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019441843032836914
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70412
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [101550]
collect time 0.0008718967437744141
inside mustsac before update, task 0, sumup 70412
inside mustsac after update, task 0, sumup 70137
inner_dict_sum {'sac_diff0': 0.00020837783813476562, 'sac_diff1': 0.007188558578491211, 'sac_diff2': 0.008434534072875977, 'sac_diff3': 0.010451793670654297, 'sac_diff4': 0.007345676422119141, 'sac_diff5': 0.050232648849487305, 'sac_diff6': 0.0004029273986816406, 'all': 0.08426451683044434}
diff5_list [0.010712623596191406, 0.009854316711425781, 0.010033845901489258, 0.00987100601196289, 0.009760856628417969]
time3 0.0008540153503417969
time4 0.08511853218078613
time5 0.08516907691955566
time7 0.009505033493041992
gen_weight_change tensor(-19.9569)
policy weight change tensor(36.8769, grad_fn=<SumBackward0>)
time8 0.0029647350311279297
train_time 0.11617398262023926
eval time 0.10642647743225098
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:45,919 MainThread INFO: EPOCH:670
2024-01-23 01:03:45,919 MainThread INFO: Time Consumed:0.2258470058441162s
2024-01-23 01:03:45,919 MainThread INFO: Total Frames:101400s
  7%|▋         | 671/10000 [05:13<39:14,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12200.71157
Train_Epoch_Reward                27186.71705
Running_Training_Average_Rewards  11693.67599
Explore_Time                      0.00087
Train___Time                      0.11617
Eval____Time                      0.10643
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12565.53390
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.83403     1.12548    90.39442     87.34250
alpha_0                           0.71497      0.00010    0.71511      0.71482
Alpha_loss                        -2.25957     0.00176    -2.25685     -2.26240
Training/policy_loss              -3.19090     0.04539    -3.11544     -3.24088
Training/qf1_loss                 6463.67100   633.90709  7191.59619   5682.60449
Training/qf2_loss                 14824.16895  609.73452  15615.89062  14014.60449
Training/pf_norm                  0.10432      0.03509    0.17215      0.07559
Training/qf1_norm                 1276.54202   708.41124  2333.64600   311.82639
Training/qf2_norm                 880.42690    13.82907   895.88525    857.94946
log_std/mean                      -0.12555     0.00429    -0.11979     -0.13078
log_probs/mean                    -2.73652     0.00796    -2.72439     -2.74900
mean/mean                         -0.00698     0.00095    -0.00557     -0.00816
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018680334091186523
epoch last part time3 0.0030252933502197266
inside rlalgo, task 0, sumup 70137
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [101700]
collect time 0.0009806156158447266
inner_dict_sum {'sac_diff0': 0.00023674964904785156, 'sac_diff1': 0.006723880767822266, 'sac_diff2': 0.00783848762512207, 'sac_diff3': 0.010486841201782227, 'sac_diff4': 0.007066011428833008, 'sac_diff5': 0.03208351135253906, 'sac_diff6': 0.0003910064697265625, 'all': 0.06482648849487305}
diff5_list [0.006822109222412109, 0.00626826286315918, 0.00616145133972168, 0.0062029361724853516, 0.006628751754760742]
time3 0
time4 0.06561803817749023
time5 0.06566524505615234
time7 7.152557373046875e-07
gen_weight_change tensor(-19.9569)
policy weight change tensor(36.9035, grad_fn=<SumBackward0>)
time8 0.002084016799926758
train_time 0.07698392868041992
eval time 0.1447596549987793
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:46,169 MainThread INFO: EPOCH:671
2024-01-23 01:03:46,169 MainThread INFO: Time Consumed:0.22515535354614258s
2024-01-23 01:03:46,170 MainThread INFO: Total Frames:101550s
  7%|▋         | 672/10000 [05:13<38:59,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12266.05033
Train_Epoch_Reward                13720.91515
Running_Training_Average_Rewards  11779.51909
Explore_Time                      0.00098
Train___Time                      0.07698
Eval____Time                      0.14476
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12535.01943
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.71719     1.52784    90.12476     85.76670
alpha_0                           0.71461      0.00010    0.71475      0.71447
Alpha_loss                        -2.26359     0.00325    -2.25947     -2.26941
Training/policy_loss              -3.24991     0.00565    -3.24435     -3.26030
Training/qf1_loss                 6434.27061   489.87856  7181.46631   5708.22754
Training/qf2_loss                 14794.01270  644.73137  15814.37793  14018.46680
Training/pf_norm                  0.09550      0.01588    0.11397      0.06639
Training/qf1_norm                 800.02269    297.68688  1362.09863   482.07446
Training/qf2_norm                 919.66187    15.49539   934.72150    889.75598
log_std/mean                      -0.12090     0.00009    -0.12078     -0.12104
log_probs/mean                    -2.73845     0.00750    -2.73021     -2.75178
mean/mean                         -0.00713     0.00009    -0.00702     -0.00727
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018767356872558594
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70137
epoch first part time 2.86102294921875e-06
replay_buffer._size: [101850]
collect time 0.0008563995361328125
inner_dict_sum {'sac_diff0': 0.00021791458129882812, 'sac_diff1': 0.006986379623413086, 'sac_diff2': 0.008429765701293945, 'sac_diff3': 0.010435104370117188, 'sac_diff4': 0.007179975509643555, 'sac_diff5': 0.033502817153930664, 'sac_diff6': 0.0004017353057861328, 'all': 0.0671536922454834}
diff5_list [0.006972312927246094, 0.006441831588745117, 0.006185293197631836, 0.0062983036041259766, 0.007605075836181641]
time3 0
time4 0.06794524192810059
time5 0.06799435615539551
time7 1.1920928955078125e-06
gen_weight_change tensor(-19.9569)
policy weight change tensor(36.9200, grad_fn=<SumBackward0>)
time8 0.001953125
train_time 0.07941198348999023
eval time 0.15193653106689453
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:46,426 MainThread INFO: EPOCH:672
2024-01-23 01:03:46,426 MainThread INFO: Time Consumed:0.2345905303955078s
2024-01-23 01:03:46,426 MainThread INFO: Total Frames:101700s
  7%|▋         | 673/10000 [05:14<39:17,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12328.01858
Train_Epoch_Reward                17048.70855
Running_Training_Average_Rewards  12240.11131
Explore_Time                      0.00085
Train___Time                      0.07941
Eval____Time                      0.15194
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12547.53311
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.46022     1.49799    90.79756     86.89587
alpha_0                           0.71425      0.00010    0.71439      0.71411
Alpha_loss                        -2.26547     0.00207    -2.26153     -2.26740
Training/policy_loss              -3.29089     0.00364    -3.28601     -3.29516
Training/qf1_loss                 7567.22627   832.99818  8445.12891   6057.55176
Training/qf2_loss                 15837.26172  814.03561  16922.39062  14540.35449
Training/pf_norm                  0.12947      0.02317    0.17155      0.10227
Training/qf1_norm                 1751.67588   299.39674  2095.99048   1252.22400
Training/qf2_norm                 935.74731    16.64313   961.23218    916.76520
log_std/mean                      -0.12699     0.00003    -0.12694     -0.12702
log_probs/mean                    -2.73403     0.00448    -2.72632     -2.73978
mean/mean                         -0.00521     0.00011    -0.00506     -0.00539
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019016027450561523
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70137
epoch first part time 2.86102294921875e-06
replay_buffer._size: [102000]
collect time 0.0008578300476074219
inner_dict_sum {'sac_diff0': 0.00022459030151367188, 'sac_diff1': 0.007306575775146484, 'sac_diff2': 0.009040355682373047, 'sac_diff3': 0.011079072952270508, 'sac_diff4': 0.007251262664794922, 'sac_diff5': 0.03316760063171387, 'sac_diff6': 0.0004093647003173828, 'all': 0.06847882270812988}
diff5_list [0.006606101989746094, 0.007495880126953125, 0.006342649459838867, 0.006540060043334961, 0.00618290901184082]
time3 0
time4 0.0692911148071289
time5 0.06934499740600586
time7 7.152557373046875e-07
gen_weight_change tensor(-19.9569)
policy weight change tensor(36.9642, grad_fn=<SumBackward0>)
time8 0.0019197463989257812
train_time 0.08059382438659668
eval time 0.14575505256652832
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:46,678 MainThread INFO: EPOCH:673
2024-01-23 01:03:46,679 MainThread INFO: Time Consumed:0.22954058647155762s
2024-01-23 01:03:46,679 MainThread INFO: Total Frames:101850s
  7%|▋         | 674/10000 [05:14<39:14,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12389.52445
Train_Epoch_Reward                9140.10148
Running_Training_Average_Rewards  12394.30518
Explore_Time                      0.00085
Train___Time                      0.08059
Eval____Time                      0.14576
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12576.15412
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.25360     1.72437     91.33916     87.31569
alpha_0                           0.71389      0.00010     0.71404      0.71375
Alpha_loss                        -2.26713     0.00212     -2.26501     -2.27075
Training/policy_loss              -3.37284     0.00588     -3.36557     -3.38278
Training/qf1_loss                 6413.59160   805.37646   7623.38428   5379.43652
Training/qf2_loss                 14898.39629  1003.71130  16515.47852  13482.13867
Training/pf_norm                  0.08566      0.01909     0.11568      0.06705
Training/qf1_norm                 334.56465    109.89670   471.43283    155.31300
Training/qf2_norm                 990.65353    18.72152    1014.10382   969.58813
log_std/mean                      -0.12923     0.00012     -0.12904     -0.12938
log_probs/mean                    -2.72896     0.00853     -2.71867     -2.74370
mean/mean                         -0.00700     0.00021     -0.00671     -0.00728
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018523454666137695
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70137
epoch first part time 3.337860107421875e-06
replay_buffer._size: [102150]
collect time 0.0010673999786376953
inner_dict_sum {'sac_diff0': 0.00021958351135253906, 'sac_diff1': 0.0070569515228271484, 'sac_diff2': 0.008681058883666992, 'sac_diff3': 0.010641098022460938, 'sac_diff4': 0.006971120834350586, 'sac_diff5': 0.03218698501586914, 'sac_diff6': 0.0004010200500488281, 'all': 0.06615781784057617}
diff5_list [0.0065915584564208984, 0.0065555572509765625, 0.006834745407104492, 0.006028175354003906, 0.006176948547363281]
time3 0
time4 0.06691598892211914
time5 0.06696486473083496
time7 7.152557373046875e-07
gen_weight_change tensor(-19.9569)
policy weight change tensor(36.9628, grad_fn=<SumBackward0>)
time8 0.0020112991333007812
train_time 0.07852458953857422
eval time 0.1481616497039795
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:46,931 MainThread INFO: EPOCH:674
2024-01-23 01:03:46,931 MainThread INFO: Time Consumed:0.230238676071167s
2024-01-23 01:03:46,931 MainThread INFO: Total Frames:102000s
  7%|▋         | 675/10000 [05:14<39:14,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12453.33038
Train_Epoch_Reward                5665.02061
Running_Training_Average_Rewards  12223.77014
Explore_Time                      0.00106
Train___Time                      0.07852
Eval____Time                      0.14816
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12619.03958
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.35082     2.77327     93.83948     86.53626
alpha_0                           0.71354      0.00010     0.71368      0.71339
Alpha_loss                        -2.27205     0.00267     -2.26759     -2.27424
Training/policy_loss              -3.32322     0.00451     -3.31444     -3.32697
Training/qf1_loss                 7451.58223   1807.18809  10693.72070  5218.46729
Training/qf2_loss                 16159.15703  2232.44505  20090.32422  13199.00293
Training/pf_norm                  0.10577      0.01824     0.13455      0.08124
Training/qf1_norm                 514.66100    236.40411   833.79480    150.46991
Training/qf2_norm                 1009.09458   30.05693    1046.31189   967.14703
log_std/mean                      -0.13074     0.00002     -0.13070     -0.13076
log_probs/mean                    -2.73355     0.00608     -2.72234     -2.73958
mean/mean                         -0.00866     0.00027     -0.00831     -0.00907
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018578052520751953
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70137
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [102300]
collect time 0.0009622573852539062
inside mustsac before update, task 0, sumup 70137
inside mustsac after update, task 0, sumup 70991
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.007379770278930664, 'sac_diff2': 0.008219718933105469, 'sac_diff3': 0.0106658935546875, 'sac_diff4': 0.007004499435424805, 'sac_diff5': 0.05133247375488281, 'sac_diff6': 0.00042366981506347656, 'all': 0.08524012565612793}
diff5_list [0.010622024536132812, 0.00955510139465332, 0.010071516036987305, 0.011432647705078125, 0.00965118408203125]
time3 0.000919342041015625
time4 0.0860908031463623
time5 0.08614230155944824
time7 0.009403705596923828
gen_weight_change tensor(-19.7919)
policy weight change tensor(36.9774, grad_fn=<SumBackward0>)
time8 0.001966714859008789
train_time 0.11646056175231934
eval time 0.10858273506164551
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:47,182 MainThread INFO: EPOCH:675
2024-01-23 01:03:47,182 MainThread INFO: Time Consumed:0.22842049598693848s
2024-01-23 01:03:47,182 MainThread INFO: Total Frames:102150s
  7%|▋         | 676/10000 [05:14<39:10,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12515.65855
Train_Epoch_Reward                9048.44057
Running_Training_Average_Rewards  12279.26868
Explore_Time                      0.00096
Train___Time                      0.11646
Eval____Time                      0.10858
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12621.29575
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.55305     1.98615    92.40357     86.42138
alpha_0                           0.71318      0.00010    0.71332      0.71304
Alpha_loss                        -2.27602     0.00223    -2.27332     -2.27988
Training/policy_loss              -3.27304     0.04792    -3.19575     -3.33762
Training/qf1_loss                 7098.58457   522.73857  8006.02832   6539.61719
Training/qf2_loss                 15598.06035  695.89080  16375.39258  14577.96875
Training/pf_norm                  0.10230      0.02530    0.13524      0.06860
Training/qf1_norm                 1171.07034   566.65105  1794.85095   168.31776
Training/qf2_norm                 943.64336    19.12499   968.10938    910.91077
log_std/mean                      -0.13130     0.00376    -0.12543     -0.13629
log_probs/mean                    -2.73536     0.00499    -2.72862     -2.74279
mean/mean                         -0.00752     0.00059    -0.00660     -0.00831
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018820762634277344
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70991
epoch first part time 3.337860107421875e-06
replay_buffer._size: [102450]
collect time 0.0010166168212890625
inner_dict_sum {'sac_diff0': 0.00021600723266601562, 'sac_diff1': 0.007372617721557617, 'sac_diff2': 0.008009910583496094, 'sac_diff3': 0.010567903518676758, 'sac_diff4': 0.006974458694458008, 'sac_diff5': 0.03374195098876953, 'sac_diff6': 0.0003993511199951172, 'all': 0.06728219985961914}
diff5_list [0.007209300994873047, 0.006592273712158203, 0.007270097732543945, 0.0064258575439453125, 0.0062444210052490234]
time3 0
time4 0.06808686256408691
time5 0.06814002990722656
time7 4.76837158203125e-07
gen_weight_change tensor(-19.7919)
policy weight change tensor(36.9367, grad_fn=<SumBackward0>)
time8 0.0019137859344482422
train_time 0.07973194122314453
eval time 0.15023183822631836
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:47,438 MainThread INFO: EPOCH:676
2024-01-23 01:03:47,438 MainThread INFO: Time Consumed:0.23337578773498535s
2024-01-23 01:03:47,438 MainThread INFO: Total Frames:102300s
  7%|▋         | 677/10000 [05:15<39:21,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12612.56203
Train_Epoch_Reward                9081.15383
Running_Training_Average_Rewards  12393.10567
Explore_Time                      0.00101
Train___Time                      0.07973
Eval____Time                      0.15023
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13247.42201
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.76598     1.68867    92.52905     87.89912
alpha_0                           0.71282      0.00010    0.71297      0.71268
Alpha_loss                        -2.27800     0.00105    -2.27697     -2.27998
Training/policy_loss              -3.17753     0.00377    -3.17350     -3.18411
Training/qf1_loss                 7133.17754   435.98256  7550.40625   6303.11035
Training/qf2_loss                 15910.95273  755.76373  16671.19336  14522.48438
Training/pf_norm                  0.09971      0.01880    0.11839      0.07325
Training/qf1_norm                 320.04880    184.37208  668.63654    155.25677
Training/qf2_norm                 909.40369    16.50428   926.69556    881.39777
log_std/mean                      -0.13266     0.00008    -0.13252     -0.13272
log_probs/mean                    -2.73126     0.00533    -2.72623     -2.74109
mean/mean                         -0.00818     0.00018    -0.00787     -0.00840
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018847227096557617
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70991
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [102600]
collect time 0.0009479522705078125
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.0074079036712646484, 'sac_diff2': 0.008066415786743164, 'sac_diff3': 0.010956525802612305, 'sac_diff4': 0.007046699523925781, 'sac_diff5': 0.03406500816345215, 'sac_diff6': 0.0003993511199951172, 'all': 0.06816387176513672}
diff5_list [0.007768154144287109, 0.00661778450012207, 0.007083415985107422, 0.006525516510009766, 0.006070137023925781]
time3 0
time4 0.06893157958984375
time5 0.06897878646850586
time7 4.76837158203125e-07
gen_weight_change tensor(-19.7919)
policy weight change tensor(36.9065, grad_fn=<SumBackward0>)
time8 0.0018894672393798828
train_time 0.08046412467956543
eval time 0.14205312728881836
epoch last part time 5.4836273193359375e-06
2024-01-23 01:03:47,686 MainThread INFO: EPOCH:677
2024-01-23 01:03:47,686 MainThread INFO: Time Consumed:0.2258286476135254s
2024-01-23 01:03:47,687 MainThread INFO: Total Frames:102450s
  7%|▋         | 678/10000 [05:15<39:07,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12704.96648
Train_Epoch_Reward                9518.68395
Running_Training_Average_Rewards  12629.91360
Explore_Time                      0.00094
Train___Time                      0.08046
Eval____Time                      0.14205
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13283.21185
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.10474     1.78412    92.20425     87.27286
alpha_0                           0.71247      0.00010    0.71261      0.71232
Alpha_loss                        -2.28191     0.00247    -2.27918     -2.28604
Training/policy_loss              -3.34308     0.00389    -3.33768     -3.34743
Training/qf1_loss                 7105.80850   644.63546  7888.27979   6144.05176
Training/qf2_loss                 15529.53848  881.12600  16898.75586  14240.67383
Training/pf_norm                  0.11787      0.02184    0.15945      0.09670
Training/qf1_norm                 358.47640    175.20904  570.15228    134.12091
Training/qf2_norm                 962.80313    18.66302   995.45465    942.98877
log_std/mean                      -0.12629     0.00006    -0.12623     -0.12639
log_probs/mean                    -2.73284     0.00601    -2.72588     -2.74107
mean/mean                         -0.00721     0.00006    -0.00710     -0.00726
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018550634384155273
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70991
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [102750]
collect time 0.00092315673828125
inner_dict_sum {'sac_diff0': 0.00023055076599121094, 'sac_diff1': 0.007044076919555664, 'sac_diff2': 0.008584976196289062, 'sac_diff3': 0.010996341705322266, 'sac_diff4': 0.007468700408935547, 'sac_diff5': 0.03229713439941406, 'sac_diff6': 0.0003807544708251953, 'all': 0.06700253486633301}
diff5_list [0.006905794143676758, 0.006221294403076172, 0.006495952606201172, 0.006539583206176758, 0.006134510040283203]
time3 0
time4 0.06777095794677734
time5 0.06781673431396484
time7 4.76837158203125e-07
gen_weight_change tensor(-19.7919)
policy weight change tensor(36.8970, grad_fn=<SumBackward0>)
time8 0.0018627643585205078
train_time 0.07930803298950195
eval time 0.15018630027770996
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:47,942 MainThread INFO: EPOCH:678
2024-01-23 01:03:47,942 MainThread INFO: Time Consumed:0.23288965225219727s
2024-01-23 01:03:47,942 MainThread INFO: Total Frames:102600s
  7%|▋         | 679/10000 [05:15<39:17,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12785.95804
Train_Epoch_Reward                7972.74493
Running_Training_Average_Rewards  12200.12214
Explore_Time                      0.00092
Train___Time                      0.07931
Eval____Time                      0.15019
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13287.93462
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.92741     1.75548    90.84648     86.03719
alpha_0                           0.71211      0.00010    0.71225      0.71197
Alpha_loss                        -2.28721     0.00070    -2.28610     -2.28806
Training/policy_loss              -3.29042     0.00180    -3.28712     -3.29204
Training/qf1_loss                 7148.98164   469.40857  7577.66895   6256.46045
Training/qf2_loss                 15493.96523  680.58662  16179.08008  14359.90918
Training/pf_norm                  0.10505      0.01061    0.12432      0.09174
Training/qf1_norm                 1682.93499   347.44513  2163.32788   1246.98071
Training/qf2_norm                 964.29904    18.72978   983.82574    933.33087
log_std/mean                      -0.12966     0.00002    -0.12964     -0.12969
log_probs/mean                    -2.73857     0.00221    -2.73426     -2.74048
mean/mean                         -0.01025     0.00004    -0.01017     -0.01029
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018766164779663086
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70991
epoch first part time 2.86102294921875e-06
replay_buffer._size: [102900]
collect time 0.0009646415710449219
inner_dict_sum {'sac_diff0': 0.00021266937255859375, 'sac_diff1': 0.0069158077239990234, 'sac_diff2': 0.008296728134155273, 'sac_diff3': 0.010689258575439453, 'sac_diff4': 0.007200956344604492, 'sac_diff5': 0.03158259391784668, 'sac_diff6': 0.0003902912139892578, 'all': 0.06528830528259277}
diff5_list [0.006576061248779297, 0.006278038024902344, 0.006245136260986328, 0.006200313568115234, 0.0062830448150634766]
time3 0
time4 0.06605386734008789
time5 0.06609821319580078
time7 7.152557373046875e-07
gen_weight_change tensor(-19.7919)
policy weight change tensor(36.8739, grad_fn=<SumBackward0>)
time8 0.0018613338470458984
train_time 0.07713747024536133
eval time 0.14899373054504395
epoch last part time 5.9604644775390625e-06
2024-01-23 01:03:48,194 MainThread INFO: EPOCH:679
2024-01-23 01:03:48,194 MainThread INFO: Time Consumed:0.22955846786499023s
2024-01-23 01:03:48,194 MainThread INFO: Total Frames:102750s
  7%|▋         | 680/10000 [05:15<39:15,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12860.99347
Train_Epoch_Reward                6850.21424
Running_Training_Average_Rewards  12062.52771
Explore_Time                      0.00096
Train___Time                      0.07714
Eval____Time                      0.14899
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13326.79032
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.11122     2.53354     91.29704     84.16594
alpha_0                           0.71176      0.00010     0.71190      0.71161
Alpha_loss                        -2.28809     0.00250     -2.28589     -2.29181
Training/policy_loss              -3.18602     0.00356     -3.18147     -3.19104
Training/qf1_loss                 7099.03623   1114.96996  7877.07178   4953.79541
Training/qf2_loss                 15373.72793  1561.49777  16720.99805  12469.96094
Training/pf_norm                  0.09204      0.01769     0.11461      0.06437
Training/qf1_norm                 447.39586    231.29053   831.18298    162.77452
Training/qf2_norm                 894.96295    25.13594    925.92114    855.40070
log_std/mean                      -0.13582     0.00004     -0.13576     -0.13586
log_probs/mean                    -2.73123     0.00514     -2.72474     -2.73822
mean/mean                         -0.01070     0.00010     -0.01056     -0.01085
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01867842674255371
epoch last part time3 1.430511474609375e-06
inside rlalgo, task 0, sumup 70991
epoch first part time 3.337860107421875e-06
replay_buffer._size: [103050]
collect time 0.0009882450103759766
inside mustsac before update, task 0, sumup 70991
inside mustsac after update, task 0, sumup 70857
inner_dict_sum {'sac_diff0': 0.0002231597900390625, 'sac_diff1': 0.008666753768920898, 'sac_diff2': 0.009064912796020508, 'sac_diff3': 0.010807275772094727, 'sac_diff4': 0.007650852203369141, 'sac_diff5': 0.05186295509338379, 'sac_diff6': 0.0004265308380126953, 'all': 0.08870244026184082}
diff5_list [0.010484695434570312, 0.010047435760498047, 0.011029243469238281, 0.010461091995239258, 0.00984048843383789]
time3 0.0009098052978515625
time4 0.08964014053344727
time5 0.08970022201538086
time7 0.00913548469543457
gen_weight_change tensor(-19.7079)
policy weight change tensor(36.9163, grad_fn=<SumBackward0>)
time8 0.002773761749267578
train_time 0.12039899826049805
eval time 0.10728693008422852
epoch last part time 7.62939453125e-06
2024-01-23 01:03:48,448 MainThread INFO: EPOCH:680
2024-01-23 01:03:48,448 MainThread INFO: Time Consumed:0.23109173774719238s
2024-01-23 01:03:48,448 MainThread INFO: Total Frames:102900s
  7%|▋         | 681/10000 [05:16<39:25,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12947.07189
Train_Epoch_Reward                2642.83349
Running_Training_Average_Rewards  11910.20332
Explore_Time                      0.00098
Train___Time                      0.12040
Eval____Time                      0.10729
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13426.31810
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.10121     1.75160    91.46532     86.70419
alpha_0                           0.71140      0.00010    0.71154      0.71126
Alpha_loss                        -2.29303     0.00289    -2.28800     -2.29587
Training/policy_loss              -3.23767     0.08113    -3.14286     -3.34084
Training/qf1_loss                 6303.34531   315.81339  6579.41260   5797.42676
Training/qf2_loss                 14725.19707  557.75018  15379.20801  13772.43457
Training/pf_norm                  0.08978      0.01666    0.10865      0.06280
Training/qf1_norm                 878.28743    455.61496  1505.12976   160.42361
Training/qf2_norm                 913.38524    48.01722   979.50317    854.70361
log_std/mean                      -0.12821     0.00471    -0.12253     -0.13684
log_probs/mean                    -2.73585     0.00705    -2.72503     -2.74618
mean/mean                         -0.00730     0.00165    -0.00531     -0.00995
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018589496612548828
epoch last part time3 0.0028786659240722656
inside rlalgo, task 0, sumup 70857
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [103200]
collect time 0.0009641647338867188
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.00722956657409668, 'sac_diff2': 0.00894474983215332, 'sac_diff3': 0.011202096939086914, 'sac_diff4': 0.0073451995849609375, 'sac_diff5': 0.03393721580505371, 'sac_diff6': 0.00041294097900390625, 'all': 0.06927919387817383}
diff5_list [0.006993532180786133, 0.0069332122802734375, 0.0063440799713134766, 0.006849050521850586, 0.006817340850830078]
time3 0
time4 0.07012653350830078
time5 0.07017898559570312
time7 9.5367431640625e-07
gen_weight_change tensor(-19.7079)
policy weight change tensor(36.8279, grad_fn=<SumBackward0>)
time8 0.0019254684448242188
train_time 0.08146953582763672
eval time 0.13831138610839844
epoch last part time 7.62939453125e-06
2024-01-23 01:03:48,696 MainThread INFO: EPOCH:681
2024-01-23 01:03:48,696 MainThread INFO: Time Consumed:0.22315478324890137s
2024-01-23 01:03:48,696 MainThread INFO: Total Frames:103050s
  7%|▋         | 682/10000 [05:16<39:00,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           12967.02274
Train_Epoch_Reward                15552.33326
Running_Training_Average_Rewards  12286.84461
Explore_Time                      0.00096
Train___Time                      0.08147
Eval____Time                      0.13831
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12734.52796
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.06190     3.05683     92.15362     83.52939
alpha_0                           0.71104      0.00010     0.71119      0.71090
Alpha_loss                        -2.29494     0.00280     -2.29102     -2.29832
Training/policy_loss              -3.31408     0.00368     -3.30886     -3.31913
Training/qf1_loss                 7334.77539   1161.16185  8974.30664   5757.52637
Training/qf2_loss                 15579.94629  1750.41179  18013.38281  13121.27832
Training/pf_norm                  0.09921      0.01536     0.12073      0.07948
Training/qf1_norm                 657.31635    445.00833   1421.05835   184.87349
Training/qf2_norm                 959.12073    33.47609    1003.77228   910.09064
log_std/mean                      -0.12953     0.00012     -0.12932     -0.12966
log_probs/mean                    -2.73158     0.00604     -2.72403     -2.73953
mean/mean                         -0.00621     0.00017     -0.00596     -0.00642
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018436670303344727
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70857
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [103350]
collect time 0.0009391307830810547
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.007488250732421875, 'sac_diff2': 0.00885629653930664, 'sac_diff3': 0.011380910873413086, 'sac_diff4': 0.007868289947509766, 'sac_diff5': 0.03371310234069824, 'sac_diff6': 0.0004088878631591797, 'all': 0.0699303150177002}
diff5_list [0.006625652313232422, 0.007225513458251953, 0.007369518280029297, 0.006314277648925781, 0.006178140640258789]
time3 0
time4 0.07079648971557617
time5 0.07085132598876953
time7 7.152557373046875e-07
gen_weight_change tensor(-19.7079)
policy weight change tensor(36.7540, grad_fn=<SumBackward0>)
time8 0.0018520355224609375
train_time 0.08205556869506836
eval time 0.14115500450134277
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:48,944 MainThread INFO: EPOCH:682
2024-01-23 01:03:48,945 MainThread INFO: Time Consumed:0.22655463218688965s
2024-01-23 01:03:48,945 MainThread INFO: Total Frames:103200s
  7%|▋         | 683/10000 [05:16<38:54,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           12997.80565
Train_Epoch_Reward                31034.10556
Running_Training_Average_Rewards  11967.88204
Explore_Time                      0.00093
Train___Time                      0.08206
Eval____Time                      0.14116
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12855.36220
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.88333     1.39516    91.96734     87.69859
alpha_0                           0.71069      0.00010    0.71083      0.71055
Alpha_loss                        -2.30026     0.00154    -2.29813     -2.30223
Training/policy_loss              -3.09589     0.00358    -3.09212     -3.10231
Training/qf1_loss                 7659.31836   755.93103  8537.54102   6443.20605
Training/qf2_loss                 16161.90273  890.31609  17048.62109  14572.88574
Training/pf_norm                  0.07872      0.01535    0.09701      0.05187
Training/qf1_norm                 1778.92314   242.40802  2159.77563   1406.49182
Training/qf2_norm                 877.67019    13.54561   897.71570    856.33936
log_std/mean                      -0.13257     0.00012    -0.13244     -0.13276
log_probs/mean                    -2.73730     0.00541    -2.73106     -2.74562
mean/mean                         -0.00652     0.00010    -0.00637     -0.00665
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018791913986206055
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70857
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [103500]
collect time 0.0009799003601074219
inner_dict_sum {'sac_diff0': 0.0002200603485107422, 'sac_diff1': 0.00773167610168457, 'sac_diff2': 0.008913516998291016, 'sac_diff3': 0.011129617691040039, 'sac_diff4': 0.007120370864868164, 'sac_diff5': 0.03309226036071777, 'sac_diff6': 0.000431060791015625, 'all': 0.06863856315612793}
diff5_list [0.0077021121978759766, 0.0068340301513671875, 0.0062181949615478516, 0.0062046051025390625, 0.006133317947387695]
time3 0
time4 0.06948351860046387
time5 0.06954002380371094
time7 4.76837158203125e-07
gen_weight_change tensor(-19.7079)
policy weight change tensor(36.7960, grad_fn=<SumBackward0>)
time8 0.0018715858459472656
train_time 0.08113741874694824
eval time 0.1543893814086914
epoch last part time 5.9604644775390625e-06
2024-01-23 01:03:49,206 MainThread INFO: EPOCH:683
2024-01-23 01:03:49,207 MainThread INFO: Time Consumed:0.23891973495483398s
2024-01-23 01:03:49,207 MainThread INFO: Total Frames:103350s
  7%|▋         | 684/10000 [05:16<39:26,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13035.66018
Train_Epoch_Reward                17126.91883
Running_Training_Average_Rewards  12200.75008
Explore_Time                      0.00098
Train___Time                      0.08114
Eval____Time                      0.15439
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             12954.69937
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.17865     2.83269     90.99372     83.57865
alpha_0                           0.71033      0.00010     0.71047      0.71019
Alpha_loss                        -2.30083     0.00155     -2.29950     -2.30377
Training/policy_loss              -3.28074     0.00339     -3.27509     -3.28491
Training/qf1_loss                 6800.69648   1085.25417  7689.14551   4740.54980
Training/qf2_loss                 15269.41094  1635.34233  16514.51758  12091.27051
Training/pf_norm                  0.11458      0.03005     0.16954      0.08540
Training/qf1_norm                 931.44309    574.04396   2064.52710   564.43640
Training/qf2_norm                 964.00796    29.96463    983.42291    904.90070
log_std/mean                      -0.13358     0.00014     -0.13342     -0.13383
log_probs/mean                    -2.72910     0.00553     -2.72158     -2.73771
mean/mean                         -0.00781     0.00014     -0.00764     -0.00802
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019088029861450195
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70857
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [103650]
collect time 0.0009357929229736328
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.0077190399169921875, 'sac_diff2': 0.009225130081176758, 'sac_diff3': 0.012029886245727539, 'sac_diff4': 0.007557392120361328, 'sac_diff5': 0.0338289737701416, 'sac_diff6': 0.0004329681396484375, 'all': 0.07101106643676758}
diff5_list [0.007310628890991211, 0.00657200813293457, 0.006823062896728516, 0.006486177444458008, 0.006637096405029297]
time3 0
time4 0.07188653945922852
time5 0.07194256782531738
time7 9.5367431640625e-07
gen_weight_change tensor(-19.7079)
policy weight change tensor(36.8866, grad_fn=<SumBackward0>)
time8 0.0018951892852783203
train_time 0.08329558372497559
eval time 0.14692401885986328
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:49,463 MainThread INFO: EPOCH:684
2024-01-23 01:03:49,463 MainThread INFO: Time Consumed:0.23363065719604492s
2024-01-23 01:03:49,463 MainThread INFO: Total Frames:103500s
  7%|▋         | 685/10000 [05:17<39:30,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13089.69179
Train_Epoch_Reward                4627.19395
Running_Training_Average_Rewards  12288.24644
Explore_Time                      0.00093
Train___Time                      0.08330
Eval____Time                      0.14692
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13159.35574
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.13467     1.40176    91.74504     87.92296
alpha_0                           0.70998      0.00010    0.71012      0.70984
Alpha_loss                        -2.30643     0.00221    -2.30322     -2.30886
Training/policy_loss              -3.26663     0.00361    -3.25979     -3.26999
Training/qf1_loss                 6699.81504   562.05520  7708.32715   6052.27881
Training/qf2_loss                 15321.72402  543.92061  16266.25293  14857.49707
Training/pf_norm                  0.09903      0.01121    0.11343      0.08511
Training/qf1_norm                 778.11734    295.85155  1260.84644   431.27829
Training/qf2_norm                 941.91538    14.22432   958.02002    919.56750
log_std/mean                      -0.13452     0.00020    -0.13425     -0.13482
log_probs/mean                    -2.73563     0.00509    -2.72628     -2.74078
mean/mean                         -0.00641     0.00012    -0.00625     -0.00659
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018284082412719727
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70857
epoch first part time 2.86102294921875e-06
replay_buffer._size: [103800]
collect time 0.0009698867797851562
inside mustsac before update, task 0, sumup 70857
inside mustsac after update, task 0, sumup 71273
inner_dict_sum {'sac_diff0': 0.00022482872009277344, 'sac_diff1': 0.007225751876831055, 'sac_diff2': 0.008629798889160156, 'sac_diff3': 0.010733604431152344, 'sac_diff4': 0.007244110107421875, 'sac_diff5': 0.05163264274597168, 'sac_diff6': 0.0004169940948486328, 'all': 0.08610773086547852}
diff5_list [0.010797977447509766, 0.009921073913574219, 0.010772466659545898, 0.01029062271118164, 0.009850502014160156]
time3 0.0008919239044189453
time4 0.08708715438842773
time5 0.0871431827545166
time7 0.008962631225585938
gen_weight_change tensor(-19.7528)
policy weight change tensor(36.9417, grad_fn=<SumBackward0>)
time8 0.0018150806427001953
train_time 0.11672711372375488
eval time 0.10779666900634766
epoch last part time 6.67572021484375e-06
2024-01-23 01:03:49,713 MainThread INFO: EPOCH:685
2024-01-23 01:03:49,713 MainThread INFO: Time Consumed:0.22788500785827637s
2024-01-23 01:03:49,713 MainThread INFO: Total Frames:103650s
  7%|▋         | 686/10000 [05:17<39:17,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13159.91921
Train_Epoch_Reward                7878.57139
Running_Training_Average_Rewards  12023.85353
Explore_Time                      0.00096
Train___Time                      0.11673
Eval____Time                      0.10780
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13323.56991
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.09973     1.97015    89.20128     83.56493
alpha_0                           0.70962      0.00010    0.70976      0.70948
Alpha_loss                        -2.30964     0.00165    -2.30738     -2.31165
Training/policy_loss              -3.29390     0.06966    -3.21702     -3.40029
Training/qf1_loss                 5808.82471   563.67636  6739.88037   5117.78662
Training/qf2_loss                 13882.80879  830.33158  15200.77344  12957.41602
Training/pf_norm                  0.08739      0.01698    0.11157      0.06454
Training/qf1_norm                 513.27743    545.08157  1558.77051   116.14914
Training/qf2_norm                 929.95624    45.34599   995.21637    858.05927
log_std/mean                      -0.12795     0.00560    -0.11765     -0.13300
log_probs/mean                    -2.73518     0.00604    -2.72771     -2.74498
mean/mean                         -0.00685     0.00095    -0.00509     -0.00769
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018380165100097656
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71273
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [103950]
collect time 0.0009882450103759766
inner_dict_sum {'sac_diff0': 0.0002186298370361328, 'sac_diff1': 0.0069086551666259766, 'sac_diff2': 0.008407115936279297, 'sac_diff3': 0.010622024536132812, 'sac_diff4': 0.007133960723876953, 'sac_diff5': 0.03180813789367676, 'sac_diff6': 0.0003876686096191406, 'all': 0.06548619270324707}
diff5_list [0.007205963134765625, 0.006279945373535156, 0.0063436031341552734, 0.006026268005371094, 0.005952358245849609]
time3 0
time4 0.06625747680664062
time5 0.06630945205688477
time7 9.5367431640625e-07
gen_weight_change tensor(-19.7528)
policy weight change tensor(37.0915, grad_fn=<SumBackward0>)
time8 0.001837015151977539
train_time 0.07750821113586426
eval time 0.14510822296142578
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:49,961 MainThread INFO: EPOCH:686
2024-01-23 01:03:49,961 MainThread INFO: Time Consumed:0.22597956657409668s
2024-01-23 01:03:49,961 MainThread INFO: Total Frames:103800s
  7%|▋         | 687/10000 [05:17<39:04,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13166.31846
Train_Epoch_Reward                8137.77913
Running_Training_Average_Rewards  11152.40652
Explore_Time                      0.00098
Train___Time                      0.07751
Eval____Time                      0.14511
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13311.41454
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.14768     1.02622    90.83424     87.78953
alpha_0                           0.70927      0.00010    0.70941      0.70913
Alpha_loss                        -2.31176     0.00198    -2.30922     -2.31500
Training/policy_loss              -3.33189     0.00575    -3.32389     -3.34153
Training/qf1_loss                 7039.44600   545.73675  7594.45410   6114.15674
Training/qf2_loss                 15478.90000  467.11953  15968.28711  14621.84863
Training/pf_norm                  0.10892      0.02073    0.13166      0.07562
Training/qf1_norm                 1223.20947   163.34447  1380.14795   951.78162
Training/qf2_norm                 967.37994    10.68613   985.19299    952.81787
log_std/mean                      -0.12354     0.00024    -0.12326     -0.12393
log_probs/mean                    -2.73155     0.00760    -2.72024     -2.74295
mean/mean                         -0.00755     0.00016    -0.00731     -0.00774
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018632173538208008
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71273
epoch first part time 3.337860107421875e-06
replay_buffer._size: [104100]
collect time 0.0009121894836425781
inner_dict_sum {'sac_diff0': 0.00022339820861816406, 'sac_diff1': 0.007382631301879883, 'sac_diff2': 0.008717775344848633, 'sac_diff3': 0.010694742202758789, 'sac_diff4': 0.00726008415222168, 'sac_diff5': 0.033365726470947266, 'sac_diff6': 0.0003960132598876953, 'all': 0.06804037094116211}
diff5_list [0.008083105087280273, 0.006603240966796875, 0.006337165832519531, 0.006269931793212891, 0.006072282791137695]
time3 0
time4 0.0688319206237793
time5 0.06888008117675781
time7 7.152557373046875e-07
gen_weight_change tensor(-19.7528)
policy weight change tensor(37.2099, grad_fn=<SumBackward0>)
time8 0.0019044876098632812
train_time 0.08025050163269043
eval time 0.14450502395629883
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:50,211 MainThread INFO: EPOCH:687
2024-01-23 01:03:50,212 MainThread INFO: Time Consumed:0.22818875312805176s
2024-01-23 01:03:50,212 MainThread INFO: Total Frames:103950s
  7%|▋         | 688/10000 [05:17<39:01,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13185.03524
Train_Epoch_Reward                4662.32455
Running_Training_Average_Rewards  10761.07381
Explore_Time                      0.00091
Train___Time                      0.08025
Eval____Time                      0.14451
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13470.37968
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.24522     3.21145     92.57591     84.02396
alpha_0                           0.70891      0.00010     0.70906      0.70877
Alpha_loss                        -2.31552     0.00240     -2.31192     -2.31912
Training/policy_loss              -3.12421     0.00377     -3.11882     -3.13033
Training/qf1_loss                 7363.16582   1140.73693  8939.92285   5735.08838
Training/qf2_loss                 15713.33496  1657.03672  17837.04102  13230.49805
Training/pf_norm                  0.12305      0.01266     0.14083      0.10460
Training/qf1_norm                 1976.23733   565.22612   2555.56079   1053.63330
Training/qf2_norm                 859.90848    30.41364    891.46454    810.63684
log_std/mean                      -0.13914     0.00017     -0.13888     -0.13938
log_probs/mean                    -2.73270     0.00606     -2.72419     -2.74315
mean/mean                         -0.00744     0.00014     -0.00728     -0.00766
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018940448760986328
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 71273
epoch first part time 2.86102294921875e-06
replay_buffer._size: [104250]
collect time 0.0009462833404541016
inner_dict_sum {'sac_diff0': 0.0002372264862060547, 'sac_diff1': 0.0068433284759521484, 'sac_diff2': 0.008151769638061523, 'sac_diff3': 0.010550498962402344, 'sac_diff4': 0.0069580078125, 'sac_diff5': 0.031740665435791016, 'sac_diff6': 0.00038170814514160156, 'all': 0.06486320495605469}
diff5_list [0.006529808044433594, 0.00616908073425293, 0.006078481674194336, 0.006520986557006836, 0.00644230842590332]
time3 0
time4 0.06563448905944824
time5 0.06567811965942383
time7 4.76837158203125e-07
gen_weight_change tensor(-19.7528)
policy weight change tensor(37.2633, grad_fn=<SumBackward0>)
time8 0.0019812583923339844
train_time 0.07727360725402832
eval time 0.1494748592376709
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:50,464 MainThread INFO: EPOCH:688
2024-01-23 01:03:50,464 MainThread INFO: Time Consumed:0.2301650047302246s
2024-01-23 01:03:50,465 MainThread INFO: Total Frames:104100s
  7%|▋         | 689/10000 [05:18<39:06,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13212.02900
Train_Epoch_Reward                6074.51868
Running_Training_Average_Rewards  10658.47241
Explore_Time                      0.00094
Train___Time                      0.07727
Eval____Time                      0.14947
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13557.87214
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.94199     1.65652    93.40922     88.96284
alpha_0                           0.70856      0.00010    0.70870      0.70842
Alpha_loss                        -2.31670     0.00174    -2.31409     -2.31947
Training/policy_loss              -3.30082     0.00196    -3.29808     -3.30369
Training/qf1_loss                 7305.68906   660.07700  8170.49707   6165.61719
Training/qf2_loss                 16102.28125  893.74956  16976.30078  14569.99023
Training/pf_norm                  0.13145      0.02733    0.16528      0.10526
Training/qf1_norm                 381.14938    189.88089  654.44232    134.09932
Training/qf2_norm                 975.34763    18.09477   1002.69763   953.30237
log_std/mean                      -0.12757     0.00014    -0.12740     -0.12776
log_probs/mean                    -2.72634     0.00252    -2.72266     -2.73049
mean/mean                         -0.00512     0.00005    -0.00505     -0.00518
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019329071044921875
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71273
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [104400]
collect time 0.001027822494506836
inner_dict_sum {'sac_diff0': 0.00021219253540039062, 'sac_diff1': 0.007737636566162109, 'sac_diff2': 0.00885462760925293, 'sac_diff3': 0.011156558990478516, 'sac_diff4': 0.007495880126953125, 'sac_diff5': 0.03424692153930664, 'sac_diff6': 0.00041556358337402344, 'all': 0.07011938095092773}
diff5_list [0.00687718391418457, 0.006832599639892578, 0.0065157413482666016, 0.007707834243774414, 0.0063135623931884766]
time3 0
time4 0.07094645500183105
time5 0.07100033760070801
time7 7.152557373046875e-07
gen_weight_change tensor(-19.7528)
policy weight change tensor(37.1699, grad_fn=<SumBackward0>)
time8 0.0018944740295410156
train_time 0.08249068260192871
eval time 0.1490492820739746
epoch last part time 7.62939453125e-06
2024-01-23 01:03:50,723 MainThread INFO: EPOCH:689
2024-01-23 01:03:50,723 MainThread INFO: Time Consumed:0.2349855899810791s
2024-01-23 01:03:50,723 MainThread INFO: Total Frames:104250s
  7%|▋         | 690/10000 [05:18<39:21,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13238.95341
Train_Epoch_Reward                11150.96453
Running_Training_Average_Rewards  10869.05657
Explore_Time                      0.00102
Train___Time                      0.08249
Eval____Time                      0.14905
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13596.03447
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.45486     1.48626    91.57520     87.16747
alpha_0                           0.70821      0.00010    0.70835      0.70806
Alpha_loss                        -2.32165     0.00293    -2.31638     -2.32461
Training/policy_loss              -3.39652     0.00664    -3.38518     -3.40369
Training/qf1_loss                 7406.18125   451.86604  8229.60352   6856.57910
Training/qf2_loss                 15920.93770  648.06209  16887.46680  14964.19043
Training/pf_norm                  0.11452      0.01631    0.13662      0.08681
Training/qf1_norm                 547.21974    279.97808  970.88702    129.46193
Training/qf2_norm                 1012.28121   15.47535   1035.15894   988.64874
log_std/mean                      -0.14602     0.00030    -0.14556     -0.14638
log_probs/mean                    -2.73096     0.00879    -2.71566     -2.74019
mean/mean                         -0.00468     0.00003    -0.00463     -0.00472
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01831674575805664
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71273
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [104550]
collect time 0.0009605884552001953
inside mustsac before update, task 0, sumup 71273
inside mustsac after update, task 0, sumup 70968
inner_dict_sum {'sac_diff0': 0.00021529197692871094, 'sac_diff1': 0.0071909427642822266, 'sac_diff2': 0.008718490600585938, 'sac_diff3': 0.010764360427856445, 'sac_diff4': 0.00766444206237793, 'sac_diff5': 0.05332779884338379, 'sac_diff6': 0.00044465065002441406, 'all': 0.08832597732543945}
diff5_list [0.011540651321411133, 0.01099538803100586, 0.010674238204956055, 0.010114669799804688, 0.010002851486206055]
time3 0.0008983612060546875
time4 0.08925819396972656
time5 0.08931350708007812
time7 0.009192705154418945
gen_weight_change tensor(-19.7172)
policy weight change tensor(37.1647, grad_fn=<SumBackward0>)
time8 0.0025835037231445312
train_time 0.11988377571105957
eval time 0.11075377464294434
epoch last part time 6.198883056640625e-06
2024-01-23 01:03:50,979 MainThread INFO: EPOCH:690
2024-01-23 01:03:50,979 MainThread INFO: Time Consumed:0.23401403427124023s
2024-01-23 01:03:50,979 MainThread INFO: Total Frames:104400s
  7%|▋         | 691/10000 [05:18<47:31,  3.26it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13263.06062
Train_Epoch_Reward                24200.05363
Running_Training_Average_Rewards  11412.66961
Explore_Time                      0.00096
Train___Time                      0.11988
Eval____Time                      0.11075
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13667.39019
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.02138     2.44909    92.17630     85.90312
alpha_0                           0.70785      0.00010    0.70799      0.70771
Alpha_loss                        -2.32681     0.00113    -2.32551     -2.32880
Training/policy_loss              -3.34487     0.05597    -3.27797     -3.42664
Training/qf1_loss                 6459.21387   439.09262  7134.39746   5766.20117
Training/qf2_loss                 14879.63652  828.14132  15956.76562  13641.36719
Training/pf_norm                  0.11118      0.02743    0.14669      0.06543
Training/qf1_norm                 666.07762    554.94044  1431.97241   155.75414
Training/qf2_norm                 970.31929    25.70017   1010.17126   938.44745
log_std/mean                      -0.13239     0.00489    -0.12412     -0.13782
log_probs/mean                    -2.73616     0.00242    -2.73198     -2.73898
mean/mean                         -0.00518     0.00111    -0.00412     -0.00663
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.18841171264648438
epoch last part time3 0.003137826919555664
inside rlalgo, task 0, sumup 70968
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [104700]
collect time 0.0009720325469970703
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.007232189178466797, 'sac_diff2': 0.008148908615112305, 'sac_diff3': 0.011202573776245117, 'sac_diff4': 0.007893562316894531, 'sac_diff5': 0.03267502784729004, 'sac_diff6': 0.00042319297790527344, 'all': 0.06778526306152344}
diff5_list [0.006584882736206055, 0.006471395492553711, 0.007224082946777344, 0.0063207149505615234, 0.006073951721191406]
time3 0
time4 0.06864285469055176
time5 0.06869626045227051
time7 9.5367431640625e-07
gen_weight_change tensor(-19.7172)
policy weight change tensor(37.1215, grad_fn=<SumBackward0>)
time8 0.0018503665924072266
train_time 0.08010172843933105
eval time 0.0005486011505126953
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:51,257 MainThread INFO: EPOCH:691
2024-01-23 01:03:51,257 MainThread INFO: Time Consumed:0.08382630348205566s
2024-01-23 01:03:51,258 MainThread INFO: Total Frames:104550s
  7%|▋         | 692/10000 [05:18<38:10,  4.06it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13440.22189
Train_Epoch_Reward                24355.10009
Running_Training_Average_Rewards  12110.28152
Explore_Time                      0.00097
Train___Time                      0.08010
Eval____Time                      0.00055
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14506.14063
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.28845     2.00581     93.47282     87.24564
alpha_0                           0.70750      0.00010     0.70764      0.70736
Alpha_loss                        -2.32713     0.00214     -2.32406     -2.33041
Training/policy_loss              -3.20857     0.00456     -3.20311     -3.21394
Training/qf1_loss                 7033.76475   884.80605   7978.44531   5770.48877
Training/qf2_loss                 15681.03203  1119.11759  16675.07422  13861.04492
Training/pf_norm                  0.11291      0.02587     0.15005      0.08446
Training/qf1_norm                 1083.88822   395.24904   1691.49817   477.14014
Training/qf2_norm                 899.81003    20.01440    931.17041    868.74457
log_std/mean                      -0.12555     0.00003     -0.12552     -0.12559
log_probs/mean                    -2.72735     0.00564     -2.72041     -2.73431
mean/mean                         -0.00561     0.00012     -0.00545     -0.00576
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018524646759033203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70968
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [104831]
collect time 0.0009937286376953125
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.0071451663970947266, 'sac_diff2': 0.008392572402954102, 'sac_diff3': 0.011384248733520508, 'sac_diff4': 0.007292747497558594, 'sac_diff5': 0.03301501274108887, 'sac_diff6': 0.0004112720489501953, 'all': 0.06785178184509277}
diff5_list [0.00626373291015625, 0.006193637847900391, 0.006975650787353516, 0.007376432418823242, 0.006205558776855469]
time3 0
time4 0.06866979598999023
time5 0.06872344017028809
time7 4.76837158203125e-07
gen_weight_change tensor(-19.7172)
policy weight change tensor(37.0681, grad_fn=<SumBackward0>)
time8 0.001975536346435547
train_time 0.07993197441101074
eval time 0.11881446838378906
epoch last part time 6.9141387939453125e-06
2024-01-23 01:03:51,481 MainThread INFO: EPOCH:692
2024-01-23 01:03:51,482 MainThread INFO: Time Consumed:0.20220732688903809s
2024-01-23 01:03:51,482 MainThread INFO: Total Frames:104700s
  7%|▋         | 693/10000 [05:19<37:09,  4.17it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13605.29973
Train_Epoch_Reward                13753.14306
Running_Training_Average_Rewards  11922.43190
Explore_Time                      0.00099
Train___Time                      0.07993
Eval____Time                      0.11881
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14506.14063
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.90172     1.38228    92.01442     88.00594
alpha_0                           0.70714      0.00010    0.70729      0.70700
Alpha_loss                        -2.33274     0.00209    -2.33009     -2.33612
Training/policy_loss              -3.24920     0.00342    -3.24603     -3.25583
Training/qf1_loss                 6956.67656   622.92100  7853.60645   6003.71094
Training/qf2_loss                 15507.85820  704.63103  16292.11426  14205.77051
Training/pf_norm                  0.08286      0.01816    0.10776      0.05994
Training/qf1_norm                 1441.94622   257.20675  1779.05359   1071.72180
Training/qf2_norm                 969.90348    14.64779   992.89758    949.10938
log_std/mean                      -0.12795     0.00010    -0.12781     -0.12811
log_probs/mean                    -2.73382     0.00495    -2.73006     -2.74357
mean/mean                         -0.00337     0.00001    -0.00335     -0.00339
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01871967315673828
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70968
epoch first part time 2.384185791015625e-06
replay_buffer._size: [105000]
collect time 0.0008020401000976562
inner_dict_sum {'sac_diff0': 0.00022172927856445312, 'sac_diff1': 0.0075833797454833984, 'sac_diff2': 0.008785247802734375, 'sac_diff3': 0.011255502700805664, 'sac_diff4': 0.007743358612060547, 'sac_diff5': 0.033547401428222656, 'sac_diff6': 0.00040650367736816406, 'all': 0.06954312324523926}
diff5_list [0.00693964958190918, 0.006682395935058594, 0.0072481632232666016, 0.006333589553833008, 0.0063436031341552734]
time3 0
time4 0.07036542892456055
time5 0.0704188346862793
time7 7.152557373046875e-07
gen_weight_change tensor(-19.7172)
policy weight change tensor(37.0296, grad_fn=<SumBackward0>)
time8 0.0018761157989501953
train_time 0.08191514015197754
eval time 0.14500761032104492
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:51,734 MainThread INFO: EPOCH:693
2024-01-23 01:03:51,734 MainThread INFO: Time Consumed:0.23013830184936523s
2024-01-23 01:03:51,734 MainThread INFO: Total Frames:104850s
  7%|▋         | 694/10000 [05:19<37:45,  4.11it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13778.03006
Train_Epoch_Reward                8199.00610
Running_Training_Average_Rewards  12120.12936
Explore_Time                      0.00080
Train___Time                      0.08192
Eval____Time                      0.14501
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14682.00270
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.63690     2.06265     91.02919     85.90191
alpha_0                           0.70679      0.00010     0.70693      0.70665
Alpha_loss                        -2.33753     0.00257     -2.33390     -2.34147
Training/policy_loss              -3.36699     0.00413     -3.36259     -3.37354
Training/qf1_loss                 7084.72666   667.22178   7907.73047   6289.27148
Training/qf2_loss                 15459.02637  1020.90647  16727.63867  14481.06543
Training/pf_norm                  0.11369      0.02172     0.14758      0.08552
Training/qf1_norm                 423.58340    170.17032   725.94489    281.67294
Training/qf2_norm                 975.17097    22.56137    1001.29248   945.41003
log_std/mean                      -0.12733     0.00004     -0.12728     -0.12740
log_probs/mean                    -2.73794     0.00516     -2.73135     -2.74541
mean/mean                         -0.00325     0.00017     -0.00309     -0.00353
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018764972686767578
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70968
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [105150]
collect time 0.0010769367218017578
inner_dict_sum {'sac_diff0': 0.00023031234741210938, 'sac_diff1': 0.0069921016693115234, 'sac_diff2': 0.008328914642333984, 'sac_diff3': 0.010454416275024414, 'sac_diff4': 0.00693964958190918, 'sac_diff5': 0.031969547271728516, 'sac_diff6': 0.0003845691680908203, 'all': 0.06529951095581055}
diff5_list [0.006841421127319336, 0.006335258483886719, 0.006258249282836914, 0.00629734992980957, 0.0062372684478759766]
time3 0
time4 0.06607174873352051
time5 0.06611800193786621
time7 9.5367431640625e-07
gen_weight_change tensor(-19.7172)
policy weight change tensor(37.0473, grad_fn=<SumBackward0>)
time8 0.0019159317016601562
train_time 0.07735991477966309
eval time 0.15262794494628906
epoch last part time 7.152557373046875e-06
2024-01-23 01:03:51,990 MainThread INFO: EPOCH:694
2024-01-23 01:03:51,991 MainThread INFO: Time Consumed:0.23359131813049316s
2024-01-23 01:03:51,991 MainThread INFO: Total Frames:105000s
  7%|▋         | 695/10000 [05:19<38:23,  4.04it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13925.28833
Train_Epoch_Reward                10088.90737
Running_Training_Average_Rewards  12180.85527
Explore_Time                      0.00107
Train___Time                      0.07736
Eval____Time                      0.15263
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14631.93844
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.50854     2.80001    91.89349     84.00489
alpha_0                           0.70644      0.00010    0.70658      0.70630
Alpha_loss                        -2.33882     0.00228    -2.33687     -2.34326
Training/policy_loss              -3.25278     0.00480    -3.24859     -3.26115
Training/qf1_loss                 6698.23438   366.47733  7181.81641   6086.28369
Training/qf2_loss                 15037.80332  871.67721  16163.65918  13560.54199
Training/pf_norm                  0.15532      0.01610    0.16924      0.13517
Training/qf1_norm                 567.92622    449.34663  1362.19543   132.21803
Training/qf2_norm                 957.47517    28.22075   992.21417    912.10419
log_std/mean                      -0.12398     0.00008    -0.12391     -0.12413
log_probs/mean                    -2.73194     0.00717    -2.72527     -2.74474
mean/mean                         -0.00768     0.00018    -0.00742     -0.00792
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01956796646118164
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70968
epoch first part time 2.86102294921875e-06
replay_buffer._size: [105300]
collect time 0.0008649826049804688
inside mustsac before update, task 0, sumup 70968
inside mustsac after update, task 0, sumup 69953
inner_dict_sum {'sac_diff0': 0.0002181529998779297, 'sac_diff1': 0.007863521575927734, 'sac_diff2': 0.008973836898803711, 'sac_diff3': 0.011656522750854492, 'sac_diff4': 0.0076487064361572266, 'sac_diff5': 0.05483102798461914, 'sac_diff6': 0.00042438507080078125, 'all': 0.09161615371704102}
diff5_list [0.01127171516418457, 0.010385274887084961, 0.010605812072753906, 0.012341976165771484, 0.010226249694824219]
time3 0.0009176731109619141
time4 0.09250259399414062
time5 0.09255528450012207
time7 0.009146928787231445
gen_weight_change tensor(-19.6226)
policy weight change tensor(36.9987, grad_fn=<SumBackward0>)
time8 0.0019042491912841797
train_time 0.12263941764831543
eval time 0.10423922538757324
epoch last part time 6.4373016357421875e-06
2024-01-23 01:03:52,244 MainThread INFO: EPOCH:695
2024-01-23 01:03:52,244 MainThread INFO: Time Consumed:0.2301337718963623s
2024-01-23 01:03:52,244 MainThread INFO: Total Frames:105150s
  7%|▋         | 696/10000 [05:19<38:36,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14045.27699
Train_Epoch_Reward                3774.19590
Running_Training_Average_Rewards  11928.64123
Explore_Time                      0.00086
Train___Time                      0.12264
Eval____Time                      0.10424
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14523.45652
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.12961     2.25245    91.76356     85.33607
alpha_0                           0.70608      0.00010    0.70623      0.70594
Alpha_loss                        -2.34465     0.00252    -2.34197     -2.34895
Training/policy_loss              -3.39533     0.08078    -3.29574     -3.50156
Training/qf1_loss                 6749.79482   450.14892  7344.63672   6011.34131
Training/qf2_loss                 15190.05762  842.54118  16268.60449  13758.27637
Training/pf_norm                  0.08512      0.01670    0.11157      0.06213
Training/qf1_norm                 965.49965    634.49186  1840.67603   110.33941
Training/qf2_norm                 1006.75032   43.53496   1059.13586   944.84564
log_std/mean                      -0.13070     0.00689    -0.12118     -0.14196
log_probs/mean                    -2.73904     0.00784    -2.72745     -2.74944
mean/mean                         -0.00522     0.00144    -0.00399     -0.00764
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018275022506713867
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69953
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [105450]
collect time 0.0009455680847167969
inner_dict_sum {'sac_diff0': 0.0002090930938720703, 'sac_diff1': 0.007456302642822266, 'sac_diff2': 0.008570194244384766, 'sac_diff3': 0.011055707931518555, 'sac_diff4': 0.007036924362182617, 'sac_diff5': 0.033270835876464844, 'sac_diff6': 0.00041937828063964844, 'all': 0.06801843643188477}
diff5_list [0.006647348403930664, 0.006306648254394531, 0.006754636764526367, 0.006262540817260742, 0.007299661636352539]
time3 0
time4 0.06891441345214844
time5 0.06896591186523438
time7 9.5367431640625e-07
gen_weight_change tensor(-19.6226)
policy weight change tensor(37.1113, grad_fn=<SumBackward0>)
time8 0.0019783973693847656
train_time 0.08018159866333008
eval time 0.15294981002807617
epoch last part time 5.7220458984375e-06
2024-01-23 01:03:52,502 MainThread INFO: EPOCH:696
2024-01-23 01:03:52,502 MainThread INFO: Time Consumed:0.23642253875732422s
2024-01-23 01:03:52,502 MainThread INFO: Total Frames:105300s
  7%|▋         | 697/10000 [05:20<39:05,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14128.74033
Train_Epoch_Reward                18567.47540
Running_Training_Average_Rewards  12166.51223
Explore_Time                      0.00094
Train___Time                      0.08018
Eval____Time                      0.15295
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14146.04788
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.31197     2.27977     93.82866     87.75430
alpha_0                           0.70573      0.00010     0.70587      0.70559
Alpha_loss                        -2.34389     0.00271     -2.34044     -2.34844
Training/policy_loss              -3.43232     0.00592     -3.42110     -3.43857
Training/qf1_loss                 6676.92451   771.31030   7921.73340   5611.79932
Training/qf2_loss                 15187.74102  1138.12237  17242.65820  13835.37793
Training/pf_norm                  0.13327      0.01928     0.16113      0.11317
Training/qf1_norm                 737.29257    444.74161   1615.45142   428.31775
Training/qf2_norm                 1048.58901   25.17878    1097.90515   1031.07568
log_std/mean                      -0.12452     0.00022     -0.12423     -0.12485
log_probs/mean                    -2.72718     0.00694     -2.71533     -2.73637
mean/mean                         -0.00495     0.00018     -0.00468     -0.00521
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019562244415283203
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69953
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [105600]
collect time 0.0008847713470458984
inner_dict_sum {'sac_diff0': 0.00021028518676757812, 'sac_diff1': 0.007104635238647461, 'sac_diff2': 0.008554220199584961, 'sac_diff3': 0.010742902755737305, 'sac_diff4': 0.007050752639770508, 'sac_diff5': 0.03280472755432129, 'sac_diff6': 0.00038886070251464844, 'all': 0.06685638427734375}
diff5_list [0.006945371627807617, 0.006820201873779297, 0.006361961364746094, 0.006376504898071289, 0.006300687789916992]
time3 0
time4 0.06761288642883301
time5 0.06765913963317871
time7 9.5367431640625e-07
gen_weight_change tensor(-19.6226)
policy weight change tensor(37.2376, grad_fn=<SumBackward0>)
time8 0.00185394287109375
train_time 0.078765869140625
eval time 0.15506792068481445
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:52,762 MainThread INFO: EPOCH:697
2024-01-23 01:03:52,763 MainThread INFO: Time Consumed:0.23694825172424316s
2024-01-23 01:03:52,763 MainThread INFO: Total Frames:105450s
  7%|▋         | 698/10000 [05:20<39:24,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14197.57769
Train_Epoch_Reward                4043.13102
Running_Training_Average_Rewards  12254.81124
Explore_Time                      0.00088
Train___Time                      0.07877
Eval____Time                      0.15507
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14158.75333
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.38732     1.17243    91.43978     88.43813
alpha_0                           0.70538      0.00010    0.70552      0.70524
Alpha_loss                        -2.35123     0.00188    -2.34805     -2.35288
Training/policy_loss              -3.28280     0.00450    -3.27754     -3.28839
Training/qf1_loss                 7340.49746   500.69554  7729.12891   6358.92090
Training/qf2_loss                 16036.38906  634.19383  16618.54688  14895.28906
Training/pf_norm                  0.07971      0.02953    0.12832      0.04567
Training/qf1_norm                 617.39738    211.58018  877.98560    306.98218
Training/qf2_norm                 992.21718    13.04135   1005.11310   969.88336
log_std/mean                      -0.12674     0.00022    -0.12641     -0.12703
log_probs/mean                    -2.73858     0.00589    -2.73139     -2.74520
mean/mean                         -0.00570     0.00015    -0.00549     -0.00593
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018219470977783203
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69953
epoch first part time 2.86102294921875e-06
replay_buffer._size: [105750]
collect time 0.0009152889251708984
inner_dict_sum {'sac_diff0': 0.00021195411682128906, 'sac_diff1': 0.0070476531982421875, 'sac_diff2': 0.008231878280639648, 'sac_diff3': 0.01043248176574707, 'sac_diff4': 0.006999492645263672, 'sac_diff5': 0.03202629089355469, 'sac_diff6': 0.00038313865661621094, 'all': 0.06533288955688477}
diff5_list [0.006912946701049805, 0.006382942199707031, 0.006513833999633789, 0.006253242492675781, 0.005963325500488281]
time3 0
time4 0.06607651710510254
time5 0.06612110137939453
time7 4.76837158203125e-07
gen_weight_change tensor(-19.6226)
policy weight change tensor(37.2368, grad_fn=<SumBackward0>)
time8 0.0019164085388183594
train_time 0.07729148864746094
eval time 0.1499795913696289
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:53,015 MainThread INFO: EPOCH:698
2024-01-23 01:03:53,015 MainThread INFO: Time Consumed:0.23046231269836426s
2024-01-23 01:03:53,015 MainThread INFO: Total Frames:105600s
  7%|▋         | 699/10000 [05:20<39:20,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14262.39077
Train_Epoch_Reward                12691.51048
Running_Training_Average_Rewards  11557.53945
Explore_Time                      0.00091
Train___Time                      0.07729
Eval____Time                      0.14998
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14206.00295
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.87858     1.90849    93.44910     88.01521
alpha_0                           0.70503      0.00010    0.70517      0.70488
Alpha_loss                        -2.35354     0.00315    -2.34888     -2.35730
Training/policy_loss              -3.22568     0.00478    -3.21830     -3.23233
Training/qf1_loss                 7309.15879   858.97554  8294.69922   5970.25342
Training/qf2_loss                 15936.56309  929.65243  16880.94141  14215.03418
Training/pf_norm                  0.10121      0.01028    0.11471      0.08605
Training/qf1_norm                 483.71396    138.96028  730.11682    357.19745
Training/qf2_norm                 911.45037    19.18319   947.17328    893.37262
log_std/mean                      -0.13771     0.00006    -0.13760     -0.13775
log_probs/mean                    -2.73552     0.00703    -2.72413     -2.74436
mean/mean                         -0.00435     0.00010    -0.00419     -0.00446
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01805877685546875
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69953
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [105900]
collect time 0.0009059906005859375
inner_dict_sum {'sac_diff0': 0.00023508071899414062, 'sac_diff1': 0.006395816802978516, 'sac_diff2': 0.0075299739837646484, 'sac_diff3': 0.009458780288696289, 'sac_diff4': 0.006516933441162109, 'sac_diff5': 0.03074955940246582, 'sac_diff6': 0.0003802776336669922, 'all': 0.061266422271728516}
diff5_list [0.0062408447265625, 0.006064653396606445, 0.006017446517944336, 0.0063250064849853516, 0.0061016082763671875]
time3 0
time4 0.0619962215423584
time5 0.062038421630859375
time7 7.152557373046875e-07
gen_weight_change tensor(-19.6226)
policy weight change tensor(37.2205, grad_fn=<SumBackward0>)
time8 0.0017864704132080078
train_time 0.07275271415710449
eval time 0.1496877670288086
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:53,263 MainThread INFO: EPOCH:699
2024-01-23 01:03:53,263 MainThread INFO: Time Consumed:0.22566008567810059s
2024-01-23 01:03:53,263 MainThread INFO: Total Frames:105750s
  7%|▋         | 700/10000 [05:20<39:03,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14323.77572
Train_Epoch_Reward                18503.62111
Running_Training_Average_Rewards  12076.54626
Explore_Time                      0.00090
Train___Time                      0.07275
Eval____Time                      0.14969
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14209.88394
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.47348     2.59362     92.26380     85.53794
alpha_0                           0.70467      0.00010     0.70481      0.70453
Alpha_loss                        -2.35547     0.00264     -2.35179     -2.35873
Training/policy_loss              -3.34252     0.00531     -3.33412     -3.35028
Training/qf1_loss                 6576.35625   930.98541   8436.75977   6079.72559
Training/qf2_loss                 14895.67852  1336.22487  17495.28516  13904.67480
Training/pf_norm                  0.09297      0.02854     0.12466      0.04850
Training/qf1_norm                 668.60612    390.68684   1141.70911   241.31619
Training/qf2_norm                 998.54198    28.58295    1040.63831   967.98877
log_std/mean                      -0.12614     0.00003     -0.12610     -0.12619
log_probs/mean                    -2.73143     0.00816     -2.71899     -2.74314
mean/mean                         -0.00557     0.00009     -0.00544     -0.00570
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01842212677001953
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69953
epoch first part time 2.86102294921875e-06
replay_buffer._size: [106050]
collect time 0.0008175373077392578
inside mustsac before update, task 0, sumup 69953
inside mustsac after update, task 0, sumup 71042
inner_dict_sum {'sac_diff0': 0.00020647048950195312, 'sac_diff1': 0.0066792964935302734, 'sac_diff2': 0.008184432983398438, 'sac_diff3': 0.010925054550170898, 'sac_diff4': 0.007376194000244141, 'sac_diff5': 0.051592111587524414, 'sac_diff6': 0.0003924369812011719, 'all': 0.08535599708557129}
diff5_list [0.010510683059692383, 0.010950565338134766, 0.010581254959106445, 0.009820222854614258, 0.009729385375976562]
time3 0.0008392333984375
time4 0.08617901802062988
time5 0.0862274169921875
time7 0.008882522583007812
gen_weight_change tensor(-19.5467)
policy weight change tensor(37.1826, grad_fn=<SumBackward0>)
time8 0.002498626708984375
train_time 0.11572504043579102
eval time 0.10694241523742676
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:53,510 MainThread INFO: EPOCH:700
2024-01-23 01:03:53,511 MainThread INFO: Time Consumed:0.22577667236328125s
2024-01-23 01:03:53,511 MainThread INFO: Total Frames:105900s
  7%|▋         | 701/10000 [05:21<39:01,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14375.99595
Train_Epoch_Reward                11714.73192
Running_Training_Average_Rewards  11560.81343
Explore_Time                      0.00081
Train___Time                      0.11573
Eval____Time                      0.10694
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14189.59251
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.55459     1.70434    92.67718     87.72526
alpha_0                           0.70432      0.00010    0.70446      0.70418
Alpha_loss                        -2.35829     0.00256    -2.35412     -2.36114
Training/policy_loss              -3.32829     0.11215    -3.19633     -3.47893
Training/qf1_loss                 6373.73691   556.73509  7357.01318   5809.59814
Training/qf2_loss                 14890.04316  885.79127  16505.09766  13964.15137
Training/pf_norm                  0.10442      0.03337    0.14210      0.05027
Training/qf1_norm                 918.65305    278.32554  1322.30359   544.24622
Training/qf2_norm                 972.12905    72.22514   1061.03357   895.29987
log_std/mean                      -0.13381     0.00326    -0.13021     -0.13919
log_probs/mean                    -2.72988     0.00686    -2.71989     -2.73801
mean/mean                         -0.00589     0.00093    -0.00418     -0.00700
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019367218017578125
epoch last part time3 0.0028247833251953125
inside rlalgo, task 0, sumup 71042
epoch first part time 2.86102294921875e-06
replay_buffer._size: [106200]
collect time 0.0009648799896240234
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.006613969802856445, 'sac_diff2': 0.0079345703125, 'sac_diff3': 0.010000944137573242, 'sac_diff4': 0.006793498992919922, 'sac_diff5': 0.03165292739868164, 'sac_diff6': 0.00038552284240722656, 'all': 0.0635991096496582}
diff5_list [0.006488800048828125, 0.0062334537506103516, 0.006120920181274414, 0.006424427032470703, 0.006385326385498047]
time3 0
time4 0.06435227394104004
time5 0.06439471244812012
time7 7.152557373046875e-07
gen_weight_change tensor(-19.5467)
policy weight change tensor(37.1444, grad_fn=<SumBackward0>)
time8 0.0018322467803955078
train_time 0.07543420791625977
eval time 0.14387845993041992
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:53,758 MainThread INFO: EPOCH:701
2024-01-23 01:03:53,759 MainThread INFO: Time Consumed:0.22255253791809082s
2024-01-23 01:03:53,759 MainThread INFO: Total Frames:106050s
  7%|▋         | 702/10000 [05:21<38:41,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14324.37158
Train_Epoch_Reward                12672.48051
Running_Training_Average_Rewards  11525.86560
Explore_Time                      0.00096
Train___Time                      0.07543
Eval____Time                      0.14388
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13989.89690
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.32329     2.29033     92.51211     86.20044
alpha_0                           0.70397      0.00010     0.70411      0.70383
Alpha_loss                        -2.36202     0.00105     -2.36040     -2.36297
Training/policy_loss              -3.38979     0.00266     -3.38465     -3.39212
Training/qf1_loss                 7027.90234   828.81186   8615.77734   6362.75586
Training/qf2_loss                 15276.41289  1286.30094  17716.04297  14188.48145
Training/pf_norm                  0.08442      0.02616     0.11790      0.05347
Training/qf1_norm                 1587.91927   417.91421   1933.87878   805.37067
Training/qf2_norm                 1031.05125   26.46410    1079.16113   1006.29718
log_std/mean                      -0.13438     0.00007     -0.13428     -0.13446
log_probs/mean                    -2.73093     0.00353     -2.72462     -2.73484
mean/mean                         -0.00546     0.00002     -0.00543     -0.00549
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018764734268188477
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71042
epoch first part time 2.86102294921875e-06
replay_buffer._size: [106350]
collect time 0.0009388923645019531
inner_dict_sum {'sac_diff0': 0.00023436546325683594, 'sac_diff1': 0.00665736198425293, 'sac_diff2': 0.007845401763916016, 'sac_diff3': 0.00976872444152832, 'sac_diff4': 0.006612300872802734, 'sac_diff5': 0.031081199645996094, 'sac_diff6': 0.0003750324249267578, 'all': 0.06257438659667969}
diff5_list [0.006365060806274414, 0.0063593387603759766, 0.006132364273071289, 0.006138324737548828, 0.006086111068725586]
time3 0
time4 0.06333231925964355
time5 0.06337547302246094
time7 4.76837158203125e-07
gen_weight_change tensor(-19.5467)
policy weight change tensor(37.0536, grad_fn=<SumBackward0>)
time8 0.0019085407257080078
train_time 0.07428336143493652
eval time 0.15424323081970215
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:54,013 MainThread INFO: EPOCH:702
2024-01-23 01:03:54,013 MainThread INFO: Time Consumed:0.23180198669433594s
2024-01-23 01:03:54,013 MainThread INFO: Total Frames:106200s
  7%|▋         | 703/10000 [05:21<38:55,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14259.27052
Train_Epoch_Reward                11880.08980
Running_Training_Average_Rewards  11353.57831
Explore_Time                      0.00093
Train___Time                      0.07428
Eval____Time                      0.15424
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13855.13003
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.71087     2.85346     95.36577     86.42612
alpha_0                           0.70362      0.00010     0.70376      0.70348
Alpha_loss                        -2.36669     0.00282     -2.36145     -2.36975
Training/policy_loss              -3.42438     0.00456     -3.41644     -3.43060
Training/qf1_loss                 7548.71709   964.88836   9028.45898   6451.06885
Training/qf2_loss                 16284.07520  1266.93481  17649.15039  14321.12207
Training/pf_norm                  0.08755      0.02499     0.13411      0.06280
Training/qf1_norm                 1021.67324   571.59440   1921.85291   122.26797
Training/qf2_norm                 1045.11851   32.53387    1097.91931   995.83691
log_std/mean                      -0.13242     0.00019     -0.13215     -0.13265
log_probs/mean                    -2.73463     0.00636     -2.72354     -2.74333
mean/mean                         -0.00710     0.00002     -0.00708     -0.00713
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018911123275756836
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71042
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [106500]
collect time 0.0008337497711181641
inner_dict_sum {'sac_diff0': 0.0002372264862060547, 'sac_diff1': 0.006711721420288086, 'sac_diff2': 0.007874011993408203, 'sac_diff3': 0.010183572769165039, 'sac_diff4': 0.00670933723449707, 'sac_diff5': 0.031542301177978516, 'sac_diff6': 0.00039649009704589844, 'all': 0.06365466117858887}
diff5_list [0.006432533264160156, 0.006197690963745117, 0.00628352165222168, 0.006269931793212891, 0.006358623504638672]
time3 0
time4 0.06443309783935547
time5 0.06447720527648926
time7 7.152557373046875e-07
gen_weight_change tensor(-19.5467)
policy weight change tensor(36.9569, grad_fn=<SumBackward0>)
time8 0.001886129379272461
train_time 0.0753641128540039
eval time 0.14715790748596191
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:54,261 MainThread INFO: EPOCH:703
2024-01-23 01:03:54,261 MainThread INFO: Time Consumed:0.22563409805297852s
2024-01-23 01:03:54,262 MainThread INFO: Total Frames:106350s
  7%|▋         | 704/10000 [05:21<38:44,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14162.05161
Train_Epoch_Reward                14981.41568
Running_Training_Average_Rewards  11548.28879
Explore_Time                      0.00083
Train___Time                      0.07536
Eval____Time                      0.14716
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13709.81359
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.16341     3.25349     93.74288     85.13731
alpha_0                           0.70327      0.00010     0.70341      0.70312
Alpha_loss                        -2.36896     0.00195     -2.36576     -2.37114
Training/policy_loss              -3.43363     0.00544     -3.42640     -3.44125
Training/qf1_loss                 7160.38965   1840.37994  10631.75684  5363.71582
Training/qf2_loss                 15649.00801  2411.03772  20009.41797  13073.73535
Training/pf_norm                  0.09760      0.02418     0.12607      0.05375
Training/qf1_norm                 602.21603    265.78376   957.73950    190.39273
Training/qf2_norm                 1037.50312   37.14659    1089.48816   991.23285
log_std/mean                      -0.14027     0.00015     -0.14002     -0.14045
log_probs/mean                    -2.73150     0.00693     -2.72243     -2.74155
mean/mean                         -0.00545     0.00006     -0.00536     -0.00552
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01814579963684082
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71042
epoch first part time 2.384185791015625e-06
replay_buffer._size: [106650]
collect time 0.000804901123046875
inner_dict_sum {'sac_diff0': 0.0002090930938720703, 'sac_diff1': 0.00670313835144043, 'sac_diff2': 0.007818937301635742, 'sac_diff3': 0.010238170623779297, 'sac_diff4': 0.006942272186279297, 'sac_diff5': 0.031929731369018555, 'sac_diff6': 0.00042319297790527344, 'all': 0.06426453590393066}
diff5_list [0.006524562835693359, 0.006173849105834961, 0.0062274932861328125, 0.0068433284759521484, 0.0061604976654052734]
time3 0
time4 0.06501507759094238
time5 0.06506681442260742
time7 7.152557373046875e-07
gen_weight_change tensor(-19.5467)
policy weight change tensor(36.9280, grad_fn=<SumBackward0>)
time8 0.001779794692993164
train_time 0.07579231262207031
eval time 0.14568781852722168
epoch last part time 5.245208740234375e-06
2024-01-23 01:03:54,507 MainThread INFO: EPOCH:704
2024-01-23 01:03:54,507 MainThread INFO: Time Consumed:0.22450876235961914s
2024-01-23 01:03:54,508 MainThread INFO: Total Frames:106500s
  7%|▋         | 705/10000 [05:22<38:36,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14063.75685
Train_Epoch_Reward                15703.78948
Running_Training_Average_Rewards  11882.91442
Explore_Time                      0.00080
Train___Time                      0.07579
Eval____Time                      0.14569
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13648.99085
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.37945     2.05038     91.03935     85.68309
alpha_0                           0.70291      0.00010     0.70305      0.70277
Alpha_loss                        -2.37396     0.00286     -2.36932     -2.37771
Training/policy_loss              -3.36811     0.00467     -3.36183     -3.37410
Training/qf1_loss                 6660.68291   878.24474   8090.33496   5420.18018
Training/qf2_loss                 15133.90645  1178.40160  16849.87109  13238.66699
Training/pf_norm                  0.10454      0.01471     0.12736      0.08268
Training/qf1_norm                 1038.09822   377.18941   1345.16565   348.75159
Training/qf2_norm                 989.56748    22.60219    1008.62335   949.07343
log_std/mean                      -0.12293     0.00002     -0.12290     -0.12296
log_probs/mean                    -2.73614     0.00658     -2.72681     -2.74377
mean/mean                         -0.00530     0.00006     -0.00523     -0.00539
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01931285858154297
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71042
epoch first part time 3.337860107421875e-06
replay_buffer._size: [106800]
collect time 0.0009162425994873047
inside mustsac before update, task 0, sumup 71042
inside mustsac after update, task 0, sumup 70130
inner_dict_sum {'sac_diff0': 0.00021147727966308594, 'sac_diff1': 0.006752490997314453, 'sac_diff2': 0.007963418960571289, 'sac_diff3': 0.009868621826171875, 'sac_diff4': 0.006857633590698242, 'sac_diff5': 0.0496366024017334, 'sac_diff6': 0.0003857612609863281, 'all': 0.08167600631713867}
diff5_list [0.010387182235717773, 0.009816646575927734, 0.010065793991088867, 0.009683847427368164, 0.00968313217163086]
time3 0.0008523464202880859
time4 0.08248376846313477
time5 0.08253145217895508
time7 0.008850812911987305
gen_weight_change tensor(-19.4676)
policy weight change tensor(36.8919, grad_fn=<SumBackward0>)
time8 0.0019407272338867188
train_time 0.11128401756286621
eval time 0.1123507022857666
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:54,757 MainThread INFO: EPOCH:705
2024-01-23 01:03:54,757 MainThread INFO: Time Consumed:0.22679495811462402s
2024-01-23 01:03:54,758 MainThread INFO: Total Frames:106650s
  7%|▋         | 706/10000 [05:22<38:33,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13972.71783
Train_Epoch_Reward                6723.55969
Running_Training_Average_Rewards  11805.41839
Explore_Time                      0.00091
Train___Time                      0.11128
Eval____Time                      0.11235
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13613.06630
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.75336     1.17971    91.88023     88.28811
alpha_0                           0.70256      0.00010    0.70270      0.70242
Alpha_loss                        -2.37596     0.00273    -2.37269     -2.38025
Training/policy_loss              -3.32906     0.16208    -3.07243     -3.49115
Training/qf1_loss                 7085.73809   460.16487  7803.74902   6613.32715
Training/qf2_loss                 15671.15469  320.06516  16132.59375  15249.03125
Training/pf_norm                  0.09851      0.01813    0.12502      0.06810
Training/qf1_norm                 535.52256    202.22195  897.39642    302.13727
Training/qf2_norm                 989.41488    97.66895   1085.45325   829.27673
log_std/mean                      -0.13224     0.00299    -0.12927     -0.13663
log_probs/mean                    -2.73228     0.00565    -2.72490     -2.74061
mean/mean                         -0.00638     0.00072    -0.00515     -0.00700
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01778101921081543
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70130
epoch first part time 2.384185791015625e-06
replay_buffer._size: [106950]
collect time 0.0008370876312255859
inner_dict_sum {'sac_diff0': 0.00022840499877929688, 'sac_diff1': 0.006351947784423828, 'sac_diff2': 0.007550954818725586, 'sac_diff3': 0.009694099426269531, 'sac_diff4': 0.006459474563598633, 'sac_diff5': 0.030712604522705078, 'sac_diff6': 0.00037598609924316406, 'all': 0.06137347221374512}
diff5_list [0.006437778472900391, 0.006033182144165039, 0.0060007572174072266, 0.0062656402587890625, 0.005975246429443359]
time3 0
time4 0.06211733818054199
time5 0.06216001510620117
time7 4.76837158203125e-07
gen_weight_change tensor(-19.4676)
policy weight change tensor(36.9460, grad_fn=<SumBackward0>)
time8 0.0019085407257080078
train_time 0.07314133644104004
eval time 0.14890408515930176
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:55,003 MainThread INFO: EPOCH:706
2024-01-23 01:03:55,004 MainThread INFO: Time Consumed:0.22514128684997559s
2024-01-23 01:03:55,004 MainThread INFO: Total Frames:106800s
  7%|▋         | 707/10000 [05:22<38:26,  4.03it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13915.56480
Train_Epoch_Reward                12620.86058
Running_Training_Average_Rewards  11923.40861
Explore_Time                      0.00083
Train___Time                      0.07314
Eval____Time                      0.14890
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13574.51759
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.22460     1.83051     91.71579     87.19794
alpha_0                           0.70221      0.00010     0.70235      0.70207
Alpha_loss                        -2.37972     0.00351     -2.37397     -2.38264
Training/policy_loss              -3.36564     0.00489     -3.35656     -3.37030
Training/qf1_loss                 7011.45781   816.45479   8121.48193   5822.53174
Training/qf2_loss                 15428.22266  1075.60115  16975.18164  14008.96387
Training/pf_norm                  0.10450      0.01576     0.12752      0.08205
Training/qf1_norm                 1568.39109   299.61402   1972.07520   1207.81628
Training/qf2_norm                 966.72734    19.20622    992.35242    946.92017
log_std/mean                      -0.12951     0.00013     -0.12933     -0.12968
log_probs/mean                    -2.73337     0.00810     -2.71902     -2.74166
mean/mean                         -0.00623     0.00003     -0.00619     -0.00629
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018143177032470703
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70130
epoch first part time 2.86102294921875e-06
replay_buffer._size: [107100]
collect time 0.0008089542388916016
inner_dict_sum {'sac_diff0': 0.00023126602172851562, 'sac_diff1': 0.006476879119873047, 'sac_diff2': 0.007789134979248047, 'sac_diff3': 0.00967264175415039, 'sac_diff4': 0.006412029266357422, 'sac_diff5': 0.03062152862548828, 'sac_diff6': 0.0003809928894042969, 'all': 0.06158447265625}
diff5_list [0.006318807601928711, 0.005831241607666016, 0.005994319915771484, 0.006450653076171875, 0.006026506423950195]
time3 0
time4 0.06229996681213379
time5 0.062341928482055664
time7 4.76837158203125e-07
gen_weight_change tensor(-19.4676)
policy weight change tensor(36.9520, grad_fn=<SumBackward0>)
time8 0.001917123794555664
train_time 0.07329893112182617
eval time 0.14808106422424316
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:55,249 MainThread INFO: EPOCH:707
2024-01-23 01:03:55,250 MainThread INFO: Time Consumed:0.22448158264160156s
2024-01-23 01:03:55,250 MainThread INFO: Total Frames:106950s
  7%|▋         | 708/10000 [05:22<38:21,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13854.74042
Train_Epoch_Reward                6927.31755
Running_Training_Average_Rewards  11837.02973
Explore_Time                      0.00080
Train___Time                      0.07330
Eval____Time                      0.14808
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13550.50953
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.93236     1.80439     93.03376     87.51183
alpha_0                           0.70186      0.00010     0.70200      0.70172
Alpha_loss                        -2.38316     0.00138     -2.38147     -2.38446
Training/policy_loss              -3.50640     0.00390     -3.50222     -3.51373
Training/qf1_loss                 6613.31416   803.88247   8177.23193   5888.26611
Training/qf2_loss                 15241.16094  1103.97063  17394.08008  14386.84082
Training/pf_norm                  0.10766      0.02154     0.13694      0.07473
Training/qf1_norm                 335.71019    243.82680   805.54816    141.08028
Training/qf2_norm                 1081.69988   20.88215    1117.65186   1054.23853
log_std/mean                      -0.13901     0.00010     -0.13887     -0.13912
log_probs/mean                    -2.73360     0.00407     -2.72883     -2.74078
mean/mean                         -0.00649     0.00011     -0.00637     -0.00668
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01848888397216797
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70130
epoch first part time 2.384185791015625e-06
replay_buffer._size: [107250]
collect time 0.0008025169372558594
inner_dict_sum {'sac_diff0': 0.00022912025451660156, 'sac_diff1': 0.006520271301269531, 'sac_diff2': 0.007760286331176758, 'sac_diff3': 0.009517431259155273, 'sac_diff4': 0.006157875061035156, 'sac_diff5': 0.030974149703979492, 'sac_diff6': 0.00039649009704589844, 'all': 0.06155562400817871}
diff5_list [0.006356954574584961, 0.006018638610839844, 0.006236076354980469, 0.006377458572387695, 0.0059850215911865234]
time3 0
time4 0.06227898597717285
time5 0.06231999397277832
time7 4.76837158203125e-07
gen_weight_change tensor(-19.4676)
policy weight change tensor(36.9120, grad_fn=<SumBackward0>)
time8 0.001852273941040039
train_time 0.0731956958770752
eval time 0.14347219467163086
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:55,491 MainThread INFO: EPOCH:708
2024-01-23 01:03:55,491 MainThread INFO: Time Consumed:0.21967411041259766s
2024-01-23 01:03:55,491 MainThread INFO: Total Frames:107100s
  7%|▋         | 709/10000 [05:23<38:03,  4.07it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13789.86713
Train_Epoch_Reward                17193.55683
Running_Training_Average_Rewards  12144.39013
Explore_Time                      0.00080
Train___Time                      0.07320
Eval____Time                      0.14347
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13557.27006
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.32514     1.58783     93.85679     88.96144
alpha_0                           0.70151      0.00010     0.70165      0.70137
Alpha_loss                        -2.38645     0.00278     -2.38167     -2.38938
Training/policy_loss              -3.47966     0.00599     -3.47032     -3.48515
Training/qf1_loss                 7349.13105   824.92174   8331.76562   6001.90234
Training/qf2_loss                 16239.57031  1094.37976  17387.52734  14440.93164
Training/pf_norm                  0.12107      0.03055     0.17549      0.08695
Training/qf1_norm                 296.33936    170.45949   583.07910    96.51917
Training/qf2_norm                 1077.55813   18.83825    1107.52026   1049.31006
log_std/mean                      -0.13786     0.00009     -0.13774     -0.13799
log_probs/mean                    -2.73337     0.00760     -2.72179     -2.74035
mean/mean                         -0.00805     0.00011     -0.00786     -0.00816
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018270492553710938
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70130
epoch first part time 2.384185791015625e-06
replay_buffer._size: [107400]
collect time 0.0008203983306884766
inner_dict_sum {'sac_diff0': 0.0002295970916748047, 'sac_diff1': 0.007314205169677734, 'sac_diff2': 0.008501768112182617, 'sac_diff3': 0.010655641555786133, 'sac_diff4': 0.00725555419921875, 'sac_diff5': 0.03423309326171875, 'sac_diff6': 0.0004181861877441406, 'all': 0.06860804557800293}
diff5_list [0.007007122039794922, 0.007180452346801758, 0.00749659538269043, 0.006420135498046875, 0.006128787994384766]
time3 0
time4 0.06938314437866211
time5 0.0694272518157959
time7 7.152557373046875e-07
gen_weight_change tensor(-19.4676)
policy weight change tensor(36.8561, grad_fn=<SumBackward0>)
time8 0.0019161701202392578
train_time 0.08037638664245605
eval time 0.14852213859558105
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:55,744 MainThread INFO: EPOCH:709
2024-01-23 01:03:55,745 MainThread INFO: Time Consumed:0.23195838928222656s
2024-01-23 01:03:55,745 MainThread INFO: Total Frames:107250s
  7%|▋         | 710/10000 [05:23<38:25,  4.03it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13722.60488
Train_Epoch_Reward                7149.21390
Running_Training_Average_Rewards  12154.35678
Explore_Time                      0.00082
Train___Time                      0.08038
Eval____Time                      0.14852
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13537.26144
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.14998     1.46304     93.02307     89.13662
alpha_0                           0.70116      0.00010     0.70130      0.70102
Alpha_loss                        -2.39006     0.00062     -2.38926     -2.39101
Training/policy_loss              -3.50351     0.00206     -3.50142     -3.50740
Training/qf1_loss                 7465.67422   934.42473   8939.48633   6357.92090
Training/qf2_loss                 16074.10469  1213.97017  18132.42773  14734.09180
Training/pf_norm                  0.08617      0.00807     0.10084      0.07642
Training/qf1_norm                 1677.49773   284.65116   1907.83728   1130.57971
Training/qf2_norm                 1070.41887   17.11292    1103.95227   1058.45764
log_std/mean                      -0.13656     0.00010     -0.13639     -0.13667
log_probs/mean                    -2.73407     0.00102     -2.73294     -2.73559
mean/mean                         -0.00756     0.00005     -0.00751     -0.00763
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018335819244384766
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70130
epoch first part time 2.86102294921875e-06
replay_buffer._size: [107550]
collect time 0.0008273124694824219
inside mustsac before update, task 0, sumup 70130
inside mustsac after update, task 0, sumup 71110
inner_dict_sum {'sac_diff0': 0.00020122528076171875, 'sac_diff1': 0.006762504577636719, 'sac_diff2': 0.007850885391235352, 'sac_diff3': 0.010317087173461914, 'sac_diff4': 0.0068759918212890625, 'sac_diff5': 0.050298213958740234, 'sac_diff6': 0.0003840923309326172, 'all': 0.08269000053405762}
diff5_list [0.010775566101074219, 0.009966850280761719, 0.010005712509155273, 0.009562969207763672, 0.009987115859985352]
time3 0.0008447170257568359
time4 0.08347749710083008
time5 0.0835270881652832
time7 0.008795499801635742
gen_weight_change tensor(-19.3795)
policy weight change tensor(36.8368, grad_fn=<SumBackward0>)
time8 0.0025930404663085938
train_time 0.11303019523620605
eval time 0.10660958290100098
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:55,989 MainThread INFO: EPOCH:710
2024-01-23 01:03:55,989 MainThread INFO: Time Consumed:0.2226414680480957s
2024-01-23 01:03:55,989 MainThread INFO: Total Frames:107400s
  7%|▋         | 711/10000 [05:23<38:21,  4.04it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13649.37073
Train_Epoch_Reward                18627.57041
Running_Training_Average_Rewards  12687.18135
Explore_Time                      0.00082
Train___Time                      0.11303
Eval____Time                      0.10661
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13457.25101
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.89206     1.90672    89.69344     84.73190
alpha_0                           0.70081      0.00010    0.70095      0.70067
Alpha_loss                        -2.39448     0.00252    -2.39156     -2.39910
Training/policy_loss              -3.52159     0.09805    -3.32830     -3.59462
Training/qf1_loss                 6458.49775   390.24566  6927.23193   5846.22803
Training/qf2_loss                 14649.61250  355.94242  15235.80762  14256.26562
Training/pf_norm                  0.08797      0.03487    0.13548      0.03494
Training/qf1_norm                 1238.57702   536.32396  1734.77026   324.14902
Training/qf2_norm                 1063.69604   64.62610   1117.87854   941.93579
log_std/mean                      -0.13330     0.00517    -0.12687     -0.14238
log_probs/mean                    -2.73703     0.00707    -2.72943     -2.75001
mean/mean                         -0.00722     0.00186    -0.00408     -0.00946
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018253803253173828
epoch last part time3 0.002479076385498047
inside rlalgo, task 0, sumup 71110
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [107700]
collect time 0.0008172988891601562
inner_dict_sum {'sac_diff0': 0.00023889541625976562, 'sac_diff1': 0.006586313247680664, 'sac_diff2': 0.007712125778198242, 'sac_diff3': 0.010113239288330078, 'sac_diff4': 0.006600379943847656, 'sac_diff5': 0.031491994857788086, 'sac_diff6': 0.0003819465637207031, 'all': 0.0631248950958252}
diff5_list [0.006448030471801758, 0.00624847412109375, 0.006242513656616211, 0.006453752517700195, 0.006099224090576172]
time3 0
time4 0.06386208534240723
time5 0.06390738487243652
time7 7.152557373046875e-07
gen_weight_change tensor(-19.3795)
policy weight change tensor(36.8019, grad_fn=<SumBackward0>)
time8 0.0017552375793457031
train_time 0.07452225685119629
eval time 0.13948774337768555
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:56,230 MainThread INFO: EPOCH:711
2024-01-23 01:03:56,231 MainThread INFO: Time Consumed:0.21723389625549316s
2024-01-23 01:03:56,231 MainThread INFO: Total Frames:107550s
  7%|▋         | 712/10000 [05:23<37:56,  4.08it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13567.51392
Train_Epoch_Reward                14549.32779
Running_Training_Average_Rewards  12653.74783
Explore_Time                      0.00081
Train___Time                      0.07452
Eval____Time                      0.13949
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13171.32880
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.03966     2.19221    91.26649     85.56095
alpha_0                           0.70046      0.00010    0.70060      0.70032
Alpha_loss                        -2.39630     0.00347    -2.39218     -2.40177
Training/policy_loss              -3.68067     0.00385    -3.67593     -3.68646
Training/qf1_loss                 6564.33584   490.43327  7143.98975   5768.71191
Training/qf2_loss                 14990.82285  802.96057  15894.74414  13881.92773
Training/pf_norm                  0.09101      0.02380    0.13458      0.07048
Training/qf1_norm                 432.68105    109.72537  609.62018    295.71774
Training/qf2_norm                 1166.37539   28.31765   1195.07800   1121.50330
log_std/mean                      -0.12693     0.00002    -0.12691     -0.12696
log_probs/mean                    -2.73268     0.00718    -2.72487     -2.74426
mean/mean                         -0.00910     0.00001    -0.00908     -0.00912
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018461942672729492
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71110
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [107850]
collect time 0.0008089542388916016
inner_dict_sum {'sac_diff0': 0.00023365020751953125, 'sac_diff1': 0.00622248649597168, 'sac_diff2': 0.007055521011352539, 'sac_diff3': 0.009659767150878906, 'sac_diff4': 0.006447553634643555, 'sac_diff5': 0.0306246280670166, 'sac_diff6': 0.0003917217254638672, 'all': 0.06063532829284668}
diff5_list [0.00613856315612793, 0.005991935729980469, 0.005941867828369141, 0.006205081939697266, 0.006347179412841797]
time3 0
time4 0.061370849609375
time5 0.06141376495361328
time7 4.76837158203125e-07
gen_weight_change tensor(-19.3795)
policy weight change tensor(36.8012, grad_fn=<SumBackward0>)
time8 0.0018780231475830078
train_time 0.07233095169067383
eval time 0.14603233337402344
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:56,474 MainThread INFO: EPOCH:712
2024-01-23 01:03:56,474 MainThread INFO: Time Consumed:0.22144365310668945s
2024-01-23 01:03:56,474 MainThread INFO: Total Frames:107700s
  7%|▋         | 713/10000 [05:24<37:53,  4.09it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13492.58231
Train_Epoch_Reward                8754.92464
Running_Training_Average_Rewards  11911.10847
Explore_Time                      0.00080
Train___Time                      0.07233
Eval____Time                      0.14603
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13105.81391
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.10733     2.59058    91.37268     84.90630
alpha_0                           0.70011      0.00010    0.70025      0.69997
Alpha_loss                        -2.40101     0.00323    -2.39523     -2.40469
Training/policy_loss              -3.50310     0.00718    -3.49178     -3.51255
Training/qf1_loss                 6323.95010   359.10462  6600.94629   5662.43311
Training/qf2_loss                 14588.43691  589.43613  15482.69336  13933.97363
Training/pf_norm                  0.10415      0.00734    0.11538      0.09573
Training/qf1_norm                 607.38256    428.88108  1156.74292   195.39342
Training/qf2_norm                 1031.76139   30.86240   1070.74573   993.87445
log_std/mean                      -0.13020     0.00003    -0.13017     -0.13024
log_probs/mean                    -2.73644     0.01003    -2.72024     -2.75056
mean/mean                         -0.00525     0.00008    -0.00513     -0.00534
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019131898880004883
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71110
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [108000]
collect time 0.0008370876312255859
inner_dict_sum {'sac_diff0': 0.0002357959747314453, 'sac_diff1': 0.0066606998443603516, 'sac_diff2': 0.007782936096191406, 'sac_diff3': 0.010152101516723633, 'sac_diff4': 0.0067250728607177734, 'sac_diff5': 0.03190970420837402, 'sac_diff6': 0.0003876686096191406, 'all': 0.06385397911071777}
diff5_list [0.006226778030395508, 0.006223201751708984, 0.00635218620300293, 0.006203889846801758, 0.006903648376464844]
time3 0
time4 0.06459569931030273
time5 0.06463909149169922
time7 7.152557373046875e-07
gen_weight_change tensor(-19.3795)
policy weight change tensor(36.7627, grad_fn=<SumBackward0>)
time8 0.001981973648071289
train_time 0.0757150650024414
eval time 0.15209078788757324
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:56,727 MainThread INFO: EPOCH:713
2024-01-23 01:03:56,727 MainThread INFO: Time Consumed:0.23088455200195312s
2024-01-23 01:03:56,727 MainThread INFO: Total Frames:107850s
  7%|▋         | 714/10000 [05:24<38:15,  4.05it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13433.04346
Train_Epoch_Reward                18580.57537
Running_Training_Average_Rewards  11959.56369
Explore_Time                      0.00083
Train___Time                      0.07572
Eval____Time                      0.15209
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13114.42511
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.61043     2.12079     92.03984     86.41723
alpha_0                           0.69976      0.00010     0.69990      0.69962
Alpha_loss                        -2.40374     0.00172     -2.40112     -2.40633
Training/policy_loss              -3.44048     0.00386     -3.43434     -3.44544
Training/qf1_loss                 7772.81230   742.91888   8598.99316   6418.70068
Training/qf2_loss                 16361.53203  1176.76771  17389.42578  14094.05469
Training/pf_norm                  0.10808      0.02680     0.14280      0.06137
Training/qf1_norm                 2848.68906   423.10081   3680.90039   2503.42676
Training/qf2_norm                 1019.66746   23.18988    1033.40991   973.50854
log_std/mean                      -0.13352     0.00011     -0.13333     -0.13364
log_probs/mean                    -2.73466     0.00534     -2.72919     -2.74191
mean/mean                         -0.00667     0.00009     -0.00653     -0.00676
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018436908721923828
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71110
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [108150]
collect time 0.0008809566497802734
inner_dict_sum {'sac_diff0': 0.00024318695068359375, 'sac_diff1': 0.006346702575683594, 'sac_diff2': 0.007368326187133789, 'sac_diff3': 0.009393692016601562, 'sac_diff4': 0.006351947784423828, 'sac_diff5': 0.03035426139831543, 'sac_diff6': 0.00037407875061035156, 'all': 0.06043219566345215}
diff5_list [0.006463527679443359, 0.005921363830566406, 0.005810976028442383, 0.006094932556152344, 0.0060634613037109375]
time3 0
time4 0.0611419677734375
time5 0.06118321418762207
time7 4.76837158203125e-07
gen_weight_change tensor(-19.3795)
policy weight change tensor(36.6339, grad_fn=<SumBackward0>)
time8 0.0017709732055664062
train_time 0.07186388969421387
eval time 0.15004205703735352
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:56,974 MainThread INFO: EPOCH:714
2024-01-23 01:03:56,974 MainThread INFO: Time Consumed:0.2250359058380127s
2024-01-23 01:03:56,975 MainThread INFO: Total Frames:108000s
  7%|▋         | 715/10000 [05:24<38:15,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13386.88859
Train_Epoch_Reward                23853.80198
Running_Training_Average_Rewards  12600.45062
Explore_Time                      0.00088
Train___Time                      0.07186
Eval____Time                      0.15004
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13187.44215
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.92320     3.45283     96.75635     87.28346
alpha_0                           0.69941      0.00010     0.69955      0.69927
Alpha_loss                        -2.40790     0.00115     -2.40608     -2.40968
Training/policy_loss              -3.47630     0.00393     -3.47121     -3.48167
Training/qf1_loss                 7133.31953   1031.57920  8997.46387   5887.22510
Training/qf2_loss                 16067.55234  1607.69286  18839.60352  14260.48438
Training/pf_norm                  0.09545      0.02079     0.11963      0.06467
Training/qf1_norm                 1278.54954   677.62339   2259.80078   361.33469
Training/qf2_norm                 1103.08535   40.05456    1158.48303   1049.70618
log_std/mean                      -0.14039     0.00028     -0.13995     -0.14074
log_probs/mean                    -2.73687     0.00452     -2.73177     -2.74374
mean/mean                         -0.00563     0.00009     -0.00550     -0.00574
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01861882209777832
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71110
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [108300]
collect time 0.0009222030639648438
inside mustsac before update, task 0, sumup 71110
inside mustsac after update, task 0, sumup 70077
inner_dict_sum {'sac_diff0': 0.00021982192993164062, 'sac_diff1': 0.006768465042114258, 'sac_diff2': 0.007813215255737305, 'sac_diff3': 0.009904861450195312, 'sac_diff4': 0.007008552551269531, 'sac_diff5': 0.051241397857666016, 'sac_diff6': 0.00039267539978027344, 'all': 0.08334898948669434}
diff5_list [0.010735273361206055, 0.009888410568237305, 0.010404348373413086, 0.00991368293762207, 0.0102996826171875]
time3 0.0008256435394287109
time4 0.08417177200317383
time5 0.08422255516052246
time7 0.008980751037597656
gen_weight_change tensor(-19.2205)
policy weight change tensor(36.5890, grad_fn=<SumBackward0>)
time8 0.0018455982208251953
train_time 0.11271238327026367
eval time 0.11144089698791504
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:57,224 MainThread INFO: EPOCH:715
2024-01-23 01:03:57,224 MainThread INFO: Time Consumed:0.2272660732269287s
2024-01-23 01:03:57,224 MainThread INFO: Total Frames:108150s
  7%|▋         | 716/10000 [05:24<38:23,  4.03it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13352.46805
Train_Epoch_Reward                16310.98508
Running_Training_Average_Rewards  12881.53108
Explore_Time                      0.00092
Train___Time                      0.11271
Eval____Time                      0.11144
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13268.86085
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.87702     1.76478    93.70032     88.14854
alpha_0                           0.69906      0.00010    0.69920      0.69892
Alpha_loss                        -2.41068     0.00194    -2.40767     -2.41375
Training/policy_loss              -3.38166     0.09204    -3.20921     -3.45265
Training/qf1_loss                 6956.71006   380.29519  7478.17090   6489.54541
Training/qf2_loss                 15734.01934  480.21638  16282.94531  14854.77344
Training/pf_norm                  0.10833      0.02525    0.14833      0.08313
Training/qf1_norm                 649.21422    361.99930  1176.82996   123.60437
Training/qf2_norm                 1026.10294   51.56502   1060.19153   924.96088
log_std/mean                      -0.13447     0.00215    -0.13214     -0.13820
log_probs/mean                    -2.73522     0.00629    -2.72681     -2.74569
mean/mean                         -0.00654     0.00118    -0.00541     -0.00869
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01899886131286621
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70077
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [108450]
collect time 0.0010213851928710938
inner_dict_sum {'sac_diff0': 0.00022673606872558594, 'sac_diff1': 0.007615566253662109, 'sac_diff2': 0.009467601776123047, 'sac_diff3': 0.011815071105957031, 'sac_diff4': 0.00789332389831543, 'sac_diff5': 0.03540849685668945, 'sac_diff6': 0.0004074573516845703, 'all': 0.07283425331115723}
diff5_list [0.007046699523925781, 0.006818056106567383, 0.007135629653930664, 0.007236003875732422, 0.007172107696533203]
time3 0
time4 0.07368087768554688
time5 0.07373619079589844
time7 4.76837158203125e-07
gen_weight_change tensor(-19.2205)
policy weight change tensor(36.5256, grad_fn=<SumBackward0>)
time8 0.001967906951904297
train_time 0.08588886260986328
eval time 0.15416288375854492
epoch last part time 5.9604644775390625e-06
2024-01-23 01:03:57,490 MainThread INFO: EPOCH:716
2024-01-23 01:03:57,491 MainThread INFO: Time Consumed:0.24339771270751953s
2024-01-23 01:03:57,491 MainThread INFO: Total Frames:108300s
  7%|▋         | 717/10000 [05:25<39:23,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13320.81310
Train_Epoch_Reward                17171.20950
Running_Training_Average_Rewards  13182.64542
Explore_Time                      0.00101
Train___Time                      0.08589
Eval____Time                      0.15416
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13257.96813
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.73747     1.78385    91.03898     85.82579
alpha_0                           0.69871      0.00010    0.69885      0.69857
Alpha_loss                        -2.41490     0.00221    -2.41155     -2.41781
Training/policy_loss              -3.41975     0.00192    -3.41638     -3.42169
Training/qf1_loss                 6682.77383   446.76106  7052.47900   5904.76172
Training/qf2_loss                 15055.84844  733.12452  15822.19727  13769.10449
Training/pf_norm                  0.09639      0.03308    0.14187      0.05374
Training/qf1_norm                 1010.94125   340.83007  1446.81189   447.75552
Training/qf2_norm                 1014.77465   19.82164   1040.23438   983.47333
log_std/mean                      -0.12370     0.00006    -0.12363     -0.12380
log_probs/mean                    -2.73758     0.00393    -2.73201     -2.74196
mean/mean                         -0.00751     0.00013    -0.00732     -0.00770
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02244710922241211
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70077
epoch first part time 2.86102294921875e-06
replay_buffer._size: [108600]
collect time 0.0009622573852539062
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.007827997207641602, 'sac_diff2': 0.009376049041748047, 'sac_diff3': 0.0119171142578125, 'sac_diff4': 0.00828695297241211, 'sac_diff5': 0.0374600887298584, 'sac_diff6': 0.00042939186096191406, 'all': 0.07550263404846191}
diff5_list [0.007469654083251953, 0.007158041000366211, 0.007016658782958984, 0.007735729217529297, 0.008080005645751953]
time3 0
time4 0.07635188102722168
time5 0.07640957832336426
time7 9.5367431640625e-07
gen_weight_change tensor(-19.2205)
policy weight change tensor(36.4850, grad_fn=<SumBackward0>)
time8 0.0020897388458251953
train_time 0.08899402618408203
eval time 0.1499040126800537
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:57,758 MainThread INFO: EPOCH:717
2024-01-23 01:03:57,759 MainThread INFO: Time Consumed:0.2421886920928955s
2024-01-23 01:03:57,759 MainThread INFO: Total Frames:108450s
  7%|▋         | 718/10000 [05:25<40:02,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13301.20764
Train_Epoch_Reward                19110.59140
Running_Training_Average_Rewards  13664.25432
Explore_Time                      0.00096
Train___Time                      0.08899
Eval____Time                      0.14990
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13354.45491
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.81582     3.03149     95.26454     86.01482
alpha_0                           0.69836      0.00010     0.69850      0.69822
Alpha_loss                        -2.41678     0.00227     -2.41297     -2.41918
Training/policy_loss              -3.30832     0.00325     -3.30476     -3.31306
Training/qf1_loss                 6627.68418   679.97590   7549.54053   5555.09619
Training/qf2_loss                 15102.70996  1151.15432  16946.42578  13383.00586
Training/pf_norm                  0.09796      0.02514     0.13117      0.05589
Training/qf1_norm                 1560.89340   545.11106   2541.11841   904.22833
Training/qf2_norm                 989.70507    32.87090    1048.69543   948.04541
log_std/mean                      -0.13446     0.00002     -0.13445     -0.13451
log_probs/mean                    -2.73343     0.00496     -2.72658     -2.73969
mean/mean                         -0.00612     0.00020     -0.00583     -0.00640
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.023011445999145508
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70077
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [108750]
collect time 0.0009548664093017578
inner_dict_sum {'sac_diff0': 0.00020074844360351562, 'sac_diff1': 0.006844282150268555, 'sac_diff2': 0.008137702941894531, 'sac_diff3': 0.010193109512329102, 'sac_diff4': 0.007006406784057617, 'sac_diff5': 0.031713247299194336, 'sac_diff6': 0.0003829002380371094, 'all': 0.06447839736938477}
diff5_list [0.006707906723022461, 0.006183147430419922, 0.0063436031341552734, 0.0062558650970458984, 0.006222724914550781]
time3 0
time4 0.06525945663452148
time5 0.0653078556060791
time7 7.152557373046875e-07
gen_weight_change tensor(-19.2205)
policy weight change tensor(36.4469, grad_fn=<SumBackward0>)
time8 0.0018596649169921875
train_time 0.07707834243774414
eval time 0.14806556701660156
epoch last part time 5.0067901611328125e-06
2024-01-23 01:03:58,013 MainThread INFO: EPOCH:718
2024-01-23 01:03:58,013 MainThread INFO: Time Consumed:0.22834563255310059s
2024-01-23 01:03:58,014 MainThread INFO: Total Frames:108600s
  7%|▋         | 719/10000 [05:25<39:49,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13286.74043
Train_Epoch_Reward                16024.39167
Running_Training_Average_Rewards  13995.91675
Explore_Time                      0.00095
Train___Time                      0.07708
Eval____Time                      0.14807
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13412.59801
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.23609     1.42760    92.50171     88.65821
alpha_0                           0.69801      0.00010    0.69815      0.69787
Alpha_loss                        -2.42082     0.00232    -2.41803     -2.42495
Training/policy_loss              -3.47669     0.00300    -3.47373     -3.48182
Training/qf1_loss                 7131.48467   549.96244  7816.09229   6244.06641
Training/qf2_loss                 15796.46191  679.99206  16439.61914  14615.31543
Training/pf_norm                  0.10869      0.01105    0.12566      0.09649
Training/qf1_norm                 862.29111    263.19098  1255.45654   568.07471
Training/qf2_norm                 1042.66418   16.48549   1068.91504   1024.32764
log_std/mean                      -0.13150     0.00004    -0.13145     -0.13156
log_probs/mean                    -2.73533     0.00487    -2.72942     -2.74304
mean/mean                         -0.00429     0.00023    -0.00394     -0.00460
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.022665977478027344
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70077
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [108900]
collect time 0.0008978843688964844
inner_dict_sum {'sac_diff0': 0.00019812583923339844, 'sac_diff1': 0.006736278533935547, 'sac_diff2': 0.007977962493896484, 'sac_diff3': 0.010007381439208984, 'sac_diff4': 0.006764650344848633, 'sac_diff5': 0.030568838119506836, 'sac_diff6': 0.0003724098205566406, 'all': 0.06262564659118652}
diff5_list [0.006560802459716797, 0.00604248046875, 0.0060613155364990234, 0.0059278011322021484, 0.005976438522338867]
time3 0
time4 0.06337523460388184
time5 0.06342267990112305
time7 9.5367431640625e-07
gen_weight_change tensor(-19.2205)
policy weight change tensor(36.4285, grad_fn=<SumBackward0>)
time8 0.0068171024322509766
train_time 0.08004331588745117
eval time 0.1443803310394287
epoch last part time 1.1205673217773438e-05
2024-01-23 01:03:58,267 MainThread INFO: EPOCH:719
2024-01-23 01:03:58,267 MainThread INFO: Time Consumed:0.2279653549194336s
2024-01-23 01:03:58,268 MainThread INFO: Total Frames:108750s
  7%|▋         | 720/10000 [05:25<39:29,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13275.82441
Train_Epoch_Reward                24120.66282
Running_Training_Average_Rewards  14428.24003
Explore_Time                      0.00089
Train___Time                      0.08004
Eval____Time                      0.14438
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13428.10128
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.77481     2.56898     93.95760     85.90761
alpha_0                           0.69766      0.00010     0.69780      0.69752
Alpha_loss                        -2.42302     0.00203     -2.42048     -2.42648
Training/policy_loss              -3.52723     0.00251     -3.52482     -3.53194
Training/qf1_loss                 6437.41084   780.67448   7727.20215   5424.89746
Training/qf2_loss                 15007.68281  1234.80835  17061.29883  13288.52148
Training/pf_norm                  0.09199      0.02588     0.13333      0.06251
Training/qf1_norm                 803.25749    435.99724   1511.82312   149.97490
Training/qf2_norm                 1088.44966   30.26825    1137.41760   1042.82019
log_std/mean                      -0.13582     0.00001     -0.13580     -0.13583
log_probs/mean                    -2.73207     0.00353     -2.72689     -2.73794
mean/mean                         -0.00402     0.00021     -0.00370     -0.00430
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01932048797607422
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70077
epoch first part time 5.7220458984375e-06
replay_buffer._size: [109050]
collect time 0.0010223388671875
inside mustsac before update, task 0, sumup 70077
inside mustsac after update, task 0, sumup 71360
inner_dict_sum {'sac_diff0': 0.0002162456512451172, 'sac_diff1': 0.008015632629394531, 'sac_diff2': 0.010042428970336914, 'sac_diff3': 0.01125335693359375, 'sac_diff4': 0.008856773376464844, 'sac_diff5': 0.05235862731933594, 'sac_diff6': 0.00040721893310546875, 'all': 0.09115028381347656}
diff5_list [0.012789487838745117, 0.010401248931884766, 0.009752511978149414, 0.009782552719116211, 0.00963282585144043]
time3 0.0008957386016845703
time4 0.09196877479553223
time5 0.09203767776489258
time7 0.009497404098510742
gen_weight_change tensor(-19.0464)
policy weight change tensor(36.4023, grad_fn=<SumBackward0>)
time8 0.0025959014892578125
train_time 0.12390518188476562
eval time 0.10037684440612793
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:58,518 MainThread INFO: EPOCH:720
2024-01-23 01:03:58,518 MainThread INFO: Time Consumed:0.22748279571533203s
2024-01-23 01:03:58,518 MainThread INFO: Total Frames:108900s
  7%|▋         | 721/10000 [05:26<39:25,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13276.42930
Train_Epoch_Reward                3459.16410
Running_Training_Average_Rewards  13736.87704
Explore_Time                      0.00102
Train___Time                      0.12391
Eval____Time                      0.10038
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13463.29985
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.27051     1.99817    90.39632     84.83826
alpha_0                           0.69731      0.00010    0.69745      0.69717
Alpha_loss                        -2.42910     0.00195    -2.42639     -2.43131
Training/policy_loss              -3.47721     0.07524    -3.35712     -3.59281
Training/qf1_loss                 6342.78916   539.88539  6834.55957   5300.01074
Training/qf2_loss                 14612.81758  914.39810  15462.36426  12890.27441
Training/pf_norm                  0.11524      0.02166    0.15211      0.08497
Training/qf1_norm                 851.44036    593.36576  1792.09082   144.18954
Training/qf2_norm                 1031.82531   56.19612   1125.92554   955.40631
log_std/mean                      -0.12636     0.00575    -0.12093     -0.13743
log_probs/mean                    -2.73960     0.00319    -2.73582     -2.74401
mean/mean                         -0.00477     0.00095    -0.00366     -0.00647
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01973414421081543
epoch last part time3 0.002538919448852539
inside rlalgo, task 0, sumup 71360
epoch first part time 6.67572021484375e-06
replay_buffer._size: [109200]
collect time 0.0008997917175292969
inner_dict_sum {'sac_diff0': 0.00020313262939453125, 'sac_diff1': 0.0066678524017333984, 'sac_diff2': 0.007541656494140625, 'sac_diff3': 0.010465621948242188, 'sac_diff4': 0.006742715835571289, 'sac_diff5': 0.03295135498046875, 'sac_diff6': 0.0003991127014160156, 'all': 0.0649714469909668}
diff5_list [0.006504535675048828, 0.006110191345214844, 0.006039619445800781, 0.006987333297729492, 0.007309675216674805]
time3 0
time4 0.0657045841217041
time5 0.0657496452331543
time7 4.76837158203125e-07
gen_weight_change tensor(-19.0464)
policy weight change tensor(36.4238, grad_fn=<SumBackward0>)
time8 0.0019073486328125
train_time 0.07735610008239746
eval time 0.15133047103881836
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:58,775 MainThread INFO: EPOCH:721
2024-01-23 01:03:58,775 MainThread INFO: Time Consumed:0.23185014724731445s
2024-01-23 01:03:58,775 MainThread INFO: Total Frames:109050s
  7%|▋         | 722/10000 [05:26<39:20,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13316.69677
Train_Epoch_Reward                7652.80619
Running_Training_Average_Rewards  13180.13391
Explore_Time                      0.00088
Train___Time                      0.07736
Eval____Time                      0.15133
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13574.00352
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.26449     1.63938    91.27739     86.25954
alpha_0                           0.69696      0.00010    0.69710      0.69682
Alpha_loss                        -2.42932     0.00241    -2.42683     -2.43321
Training/policy_loss              -3.56079     0.00365    -3.55555     -3.56697
Training/qf1_loss                 6612.08379   580.91056  7366.95166   6009.29688
Training/qf2_loss                 15071.00840  823.50735  16197.16406  13910.50293
Training/pf_norm                  0.10898      0.01045    0.12344      0.09164
Training/qf1_norm                 396.69708    147.78607  683.29205    260.59875
Training/qf2_norm                 1115.95391   20.59802   1140.82568   1077.97681
log_std/mean                      -0.12893     0.00007    -0.12889     -0.12907
log_probs/mean                    -2.73087     0.00623    -2.72267     -2.74166
mean/mean                         -0.00369     0.00006    -0.00362     -0.00379
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018579483032226562
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71360
epoch first part time 2.86102294921875e-06
replay_buffer._size: [109350]
collect time 0.0008947849273681641
inner_dict_sum {'sac_diff0': 0.0001995563507080078, 'sac_diff1': 0.006451129913330078, 'sac_diff2': 0.007835865020751953, 'sac_diff3': 0.009892702102661133, 'sac_diff4': 0.006779193878173828, 'sac_diff5': 0.031758785247802734, 'sac_diff6': 0.00037860870361328125, 'all': 0.06329584121704102}
diff5_list [0.006703853607177734, 0.006469249725341797, 0.006250619888305664, 0.0060079097747802734, 0.006327152252197266]
time3 0
time4 0.06404352188110352
time5 0.0640861988067627
time7 9.5367431640625e-07
gen_weight_change tensor(-19.0464)
policy weight change tensor(36.4502, grad_fn=<SumBackward0>)
time8 0.001848459243774414
train_time 0.07526016235351562
eval time 0.15439915657043457
epoch last part time 4.76837158203125e-06
2024-01-23 01:03:59,030 MainThread INFO: EPOCH:722
2024-01-23 01:03:59,030 MainThread INFO: Time Consumed:0.2327866554260254s
2024-01-23 01:03:59,030 MainThread INFO: Total Frames:109200s
  7%|▋         | 723/10000 [05:26<39:20,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13366.25925
Train_Epoch_Reward                6609.41161
Running_Training_Average_Rewards  12942.00953
Explore_Time                      0.00089
Train___Time                      0.07526
Eval____Time                      0.15440
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13601.43874
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.88554     1.47179     92.43716     88.35404
alpha_0                           0.69662      0.00010     0.69675      0.69648
Alpha_loss                        -2.43502     0.00251     -2.43202     -2.43936
Training/policy_loss              -3.48686     0.00495     -3.48103     -3.49469
Training/qf1_loss                 7501.29385   1004.36164  8896.15332   6176.59033
Training/qf2_loss                 16276.77832  1157.60927  17875.18750  14472.91113
Training/pf_norm                  0.09806      0.02629     0.14643      0.07426
Training/qf1_norm                 467.74603    208.04223   749.45123    138.31636
Training/qf2_norm                 1089.09033   18.52319    1109.16943   1057.25330
log_std/mean                      -0.12487     0.00002     -0.12484     -0.12489
log_probs/mean                    -2.73732     0.00906     -2.72717     -2.75308
mean/mean                         -0.00312     0.00007     -0.00302     -0.00324
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01806354522705078
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71360
epoch first part time 2.384185791015625e-06
replay_buffer._size: [109500]
collect time 0.000904083251953125
inner_dict_sum {'sac_diff0': 0.00019860267639160156, 'sac_diff1': 0.006540536880493164, 'sac_diff2': 0.007807493209838867, 'sac_diff3': 0.009976625442504883, 'sac_diff4': 0.006590604782104492, 'sac_diff5': 0.031133651733398438, 'sac_diff6': 0.00037789344787597656, 'all': 0.06262540817260742}
diff5_list [0.006119728088378906, 0.006184577941894531, 0.006430387496948242, 0.0063478946685791016, 0.006051063537597656]
time3 0
time4 0.06336140632629395
time5 0.06340479850769043
time7 7.152557373046875e-07
gen_weight_change tensor(-19.0464)
policy weight change tensor(36.5112, grad_fn=<SumBackward0>)
time8 0.0018019676208496094
train_time 0.07449722290039062
eval time 0.15410733222961426
epoch last part time 5.7220458984375e-06
2024-01-23 01:03:59,283 MainThread INFO: EPOCH:723
2024-01-23 01:03:59,283 MainThread INFO: Time Consumed:0.23183774948120117s
2024-01-23 01:03:59,284 MainThread INFO: Total Frames:109350s
  7%|▋         | 724/10000 [05:26<39:17,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13414.21327
Train_Epoch_Reward                17068.79292
Running_Training_Average_Rewards  13237.66909
Explore_Time                      0.00090
Train___Time                      0.07450
Eval____Time                      0.15411
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13593.96526
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.95585     1.75848     89.69933     84.85655
alpha_0                           0.69627      0.00010     0.69641      0.69613
Alpha_loss                        -2.43585     0.00297     -2.43116     -2.43902
Training/policy_loss              -3.45336     0.00606     -3.44235     -3.45909
Training/qf1_loss                 6388.70361   992.42629   7842.18848   5043.53906
Training/qf2_loss                 14633.28359  1325.21495  16466.41602  12667.27051
Training/pf_norm                  0.09584      0.02254     0.12938      0.06204
Training/qf1_norm                 1144.53544   360.94010   1764.75403   729.84418
Training/qf2_norm                 1018.44647   20.15190    1039.81458   983.27148
log_std/mean                      -0.12726     0.00011     -0.12710     -0.12743
log_probs/mean                    -2.73030     0.00895     -2.71364     -2.73870
mean/mean                         -0.00263     0.00015     -0.00247     -0.00287
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018340110778808594
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71360
epoch first part time 2.86102294921875e-06
replay_buffer._size: [109650]
collect time 0.000835418701171875
inner_dict_sum {'sac_diff0': 0.0002014636993408203, 'sac_diff1': 0.006848335266113281, 'sac_diff2': 0.008014917373657227, 'sac_diff3': 0.009998083114624023, 'sac_diff4': 0.006636857986450195, 'sac_diff5': 0.03096771240234375, 'sac_diff6': 0.00037932395935058594, 'all': 0.06304669380187988}
diff5_list [0.006194114685058594, 0.006251096725463867, 0.006390810012817383, 0.0061435699462890625, 0.005988121032714844]
time3 0
time4 0.06378984451293945
time5 0.06383252143859863
time7 7.152557373046875e-07
gen_weight_change tensor(-19.0464)
policy weight change tensor(36.5851, grad_fn=<SumBackward0>)
time8 0.0018680095672607422
train_time 0.0749959945678711
eval time 0.14777684211730957
epoch last part time 4.5299530029296875e-06
2024-01-23 01:03:59,531 MainThread INFO: EPOCH:724
2024-01-23 01:03:59,531 MainThread INFO: Time Consumed:0.22583532333374023s
2024-01-23 01:03:59,531 MainThread INFO: Total Frames:109500s
  7%|▋         | 725/10000 [05:27<38:59,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13447.18931
Train_Epoch_Reward                13738.88755
Running_Training_Average_Rewards  13359.33510
Explore_Time                      0.00083
Train___Time                      0.07500
Eval____Time                      0.14778
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13517.20252
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.53382     2.67035     94.74403     86.47020
alpha_0                           0.69592      0.00010     0.69606      0.69578
Alpha_loss                        -2.44117     0.00143     -2.43939     -2.44302
Training/policy_loss              -3.26419     0.00144     -3.26136     -3.26536
Training/qf1_loss                 7405.25947   832.35574   8570.64648   6249.40723
Training/qf2_loss                 16087.76738  1283.75513  17984.61133  14217.16406
Training/pf_norm                  0.09169      0.01827     0.11616      0.06320
Training/qf1_norm                 1471.64442   502.57997   2264.06226   707.37445
Training/qf2_norm                 935.09794    26.65217    977.11011    894.61896
log_std/mean                      -0.13232     0.00017     -0.13207     -0.13252
log_probs/mean                    -2.73570     0.00268     -2.73080     -2.73894
mean/mean                         -0.00363     0.00013     -0.00344     -0.00381
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01838374137878418
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71360
epoch first part time 2.86102294921875e-06
replay_buffer._size: [109800]
collect time 0.0007998943328857422
inside mustsac before update, task 0, sumup 71360
inside mustsac after update, task 0, sumup 70550
inner_dict_sum {'sac_diff0': 0.00020885467529296875, 'sac_diff1': 0.006737470626831055, 'sac_diff2': 0.00825810432434082, 'sac_diff3': 0.010506868362426758, 'sac_diff4': 0.0071413516998291016, 'sac_diff5': 0.05250358581542969, 'sac_diff6': 0.0004382133483886719, 'all': 0.08579444885253906}
diff5_list [0.010564088821411133, 0.00977778434753418, 0.010545492172241211, 0.011689901351928711, 0.009926319122314453]
time3 0.0008518695831298828
time4 0.08665680885314941
time5 0.08670735359191895
time7 0.009227275848388672
gen_weight_change tensor(-18.9356)
policy weight change tensor(36.6188, grad_fn=<SumBackward0>)
time8 0.0019047260284423828
train_time 0.11624598503112793
eval time 0.11130094528198242
epoch last part time 3.814697265625e-06
2024-01-23 01:03:59,783 MainThread INFO: EPOCH:725
2024-01-23 01:03:59,783 MainThread INFO: Time Consumed:0.23055267333984375s
2024-01-23 01:03:59,783 MainThread INFO: Total Frames:109650s
  7%|▋         | 726/10000 [05:27<38:59,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13461.65246
Train_Epoch_Reward                11102.81189
Running_Training_Average_Rewards  13603.62230
Explore_Time                      0.00080
Train___Time                      0.11625
Eval____Time                      0.11130
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13413.49244
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.11165     2.18141     92.12502     86.12575
alpha_0                           0.69557      0.00010     0.69571      0.69543
Alpha_loss                        -2.44283     0.00193     -2.44022     -2.44608
Training/policy_loss              -3.53292     0.06739     -3.44376     -3.62259
Training/qf1_loss                 6972.96895   840.80050   8567.97266   6079.96826
Training/qf2_loss                 15611.70176  1059.33652  17509.91016  14591.60449
Training/pf_norm                  0.09979      0.02794     0.13529      0.05921
Training/qf1_norm                 455.78130    423.02367   1275.20215   134.12869
Training/qf2_norm                 1096.96250   30.02312    1145.09570   1067.58020
log_std/mean                      -0.12807     0.00545     -0.12182     -0.13799
log_probs/mean                    -2.73099     0.00536     -2.72196     -2.73649
mean/mean                         -0.00384     0.00090     -0.00307     -0.00498
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018271446228027344
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70550
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [109950]
collect time 0.0008115768432617188
inner_dict_sum {'sac_diff0': 0.00021529197692871094, 'sac_diff1': 0.006629228591918945, 'sac_diff2': 0.007711648941040039, 'sac_diff3': 0.010230064392089844, 'sac_diff4': 0.006689548492431641, 'sac_diff5': 0.030634641647338867, 'sac_diff6': 0.00037360191345214844, 'all': 0.062484025955200195}
diff5_list [0.006406307220458984, 0.006096601486206055, 0.006151914596557617, 0.006135225296020508, 0.005844593048095703]
time3 0
time4 0.06322813034057617
time5 0.06327152252197266
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9356)
policy weight change tensor(36.6841, grad_fn=<SumBackward0>)
time8 0.005697488784790039
train_time 0.07831001281738281
eval time 0.1449728012084961
epoch last part time 7.867813110351562e-06
2024-01-23 01:04:00,031 MainThread INFO: EPOCH:726
2024-01-23 01:04:00,032 MainThread INFO: Time Consumed:0.2266695499420166s
2024-01-23 01:04:00,032 MainThread INFO: Total Frames:109800s
  7%|▋         | 727/10000 [05:27<38:50,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13514.26708
Train_Epoch_Reward                6434.71943
Running_Training_Average_Rewards  13199.19710
Explore_Time                      0.00081
Train___Time                      0.07831
Eval____Time                      0.14497
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13784.11431
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       86.04436     1.96668     87.82627     83.63107
alpha_0                           0.69522      0.00010     0.69536      0.69508
Alpha_loss                        -2.44542     0.00140     -2.44328     -2.44676
Training/policy_loss              -3.58869     0.00206     -3.58558     -3.59138
Training/qf1_loss                 5912.44092   676.78604   6928.37109   4933.89600
Training/qf2_loss                 13659.28105  1039.92358  15029.72168  12180.93262
Training/pf_norm                  0.11008      0.02331     0.14266      0.07481
Training/qf1_norm                 2402.82727   422.50168   2920.28076   2018.31519
Training/qf2_norm                 1086.72429   24.96689    1109.20190   1056.14709
log_std/mean                      -0.12359     0.00014     -0.12343     -0.12381
log_probs/mean                    -2.72888     0.00207     -2.72669     -2.73197
mean/mean                         -0.00513     0.00005     -0.00503     -0.00518
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019353866577148438
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70550
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [110100]
collect time 0.0008924007415771484
inner_dict_sum {'sac_diff0': 0.00019884109497070312, 'sac_diff1': 0.0072438716888427734, 'sac_diff2': 0.009289741516113281, 'sac_diff3': 0.010339975357055664, 'sac_diff4': 0.0074269771575927734, 'sac_diff5': 0.031389713287353516, 'sac_diff6': 0.000370025634765625, 'all': 0.06625914573669434}
diff5_list [0.00697016716003418, 0.0061795711517333984, 0.006183147430419922, 0.005930423736572266, 0.00612640380859375]
time3 0
time4 0.06698417663574219
time5 0.06702828407287598
time7 9.5367431640625e-07
gen_weight_change tensor(-18.9356)
policy weight change tensor(36.7738, grad_fn=<SumBackward0>)
time8 0.0018830299377441406
train_time 0.078704833984375
eval time 0.14667439460754395
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:00,283 MainThread INFO: EPOCH:727
2024-01-23 01:04:00,283 MainThread INFO: Time Consumed:0.22847747802734375s
2024-01-23 01:04:00,283 MainThread INFO: Total Frames:109950s
  7%|▋         | 728/10000 [05:27<38:48,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13560.26012
Train_Epoch_Reward                7880.24699
Running_Training_Average_Rewards  13327.10096
Explore_Time                      0.00089
Train___Time                      0.07870
Eval____Time                      0.14667
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13814.38527
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.49083     2.51500     93.51564     87.02140
alpha_0                           0.69488      0.00010     0.69501      0.69474
Alpha_loss                        -2.45048     0.00320     -2.44678     -2.45637
Training/policy_loss              -3.38278     0.00538     -3.37880     -3.39311
Training/qf1_loss                 6285.64473   870.66995   7901.17969   5327.74268
Training/qf2_loss                 14624.26973  1298.19261  17147.12305  13458.84863
Training/pf_norm                  0.07737      0.01948     0.10514      0.04643
Training/qf1_norm                 751.71749    473.06110   1697.03223   494.35263
Training/qf2_norm                 1017.39626   28.46252    1074.18982   999.53296
log_std/mean                      -0.12714     0.00017     -0.12690     -0.12735
log_probs/mean                    -2.73352     0.00753     -2.72704     -2.74787
mean/mean                         -0.00685     0.00004     -0.00677     -0.00689
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01828742027282715
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70550
epoch first part time 3.337860107421875e-06
replay_buffer._size: [110250]
collect time 0.0008134841918945312
inner_dict_sum {'sac_diff0': 0.00020766258239746094, 'sac_diff1': 0.006461381912231445, 'sac_diff2': 0.007703065872192383, 'sac_diff3': 0.010022401809692383, 'sac_diff4': 0.007048845291137695, 'sac_diff5': 0.03238725662231445, 'sac_diff6': 0.0003876686096191406, 'all': 0.06421828269958496}
diff5_list [0.006697654724121094, 0.006348371505737305, 0.007172822952270508, 0.006234407424926758, 0.005934000015258789]
time3 0
time4 0.06494903564453125
time5 0.06499075889587402
time7 4.76837158203125e-07
gen_weight_change tensor(-18.9356)
policy weight change tensor(36.8100, grad_fn=<SumBackward0>)
time8 0.0017936229705810547
train_time 0.07592606544494629
eval time 0.14752507209777832
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:00,531 MainThread INFO: EPOCH:728
2024-01-23 01:04:00,531 MainThread INFO: Time Consumed:0.226485013961792s
2024-01-23 01:04:00,532 MainThread INFO: Total Frames:110100s
  7%|▋         | 729/10000 [05:28<38:39,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13612.23043
Train_Epoch_Reward                13964.81596
Running_Training_Average_Rewards  13369.54448
Explore_Time                      0.00081
Train___Time                      0.07593
Eval____Time                      0.14753
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13932.30115
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       86.78750     1.62446     88.69423     84.26299
alpha_0                           0.69453      0.00010     0.69467      0.69439
Alpha_loss                        -2.45545     0.00119     -2.45387     -2.45708
Training/policy_loss              -3.64296     0.00261     -3.63913     -3.64622
Training/qf1_loss                 5739.73711   707.23291   6695.08057   4852.69824
Training/qf2_loss                 13754.04785  1021.04202  15088.31250  12369.02930
Training/pf_norm                  0.11709      0.02903     0.15458      0.08487
Training/qf1_norm                 816.02847    347.71521   1354.41003   421.49347
Training/qf2_norm                 1117.93662   21.06739    1141.62781   1086.35449
log_std/mean                      -0.13401     0.00003     -0.13398     -0.13406
log_probs/mean                    -2.73792     0.00242     -2.73364     -2.74081
mean/mean                         -0.00742     0.00002     -0.00739     -0.00745
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01802992820739746
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70550
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [110400]
collect time 0.0008182525634765625
inner_dict_sum {'sac_diff0': 0.0002040863037109375, 'sac_diff1': 0.006224870681762695, 'sac_diff2': 0.007352113723754883, 'sac_diff3': 0.009852409362792969, 'sac_diff4': 0.006655693054199219, 'sac_diff5': 0.03126049041748047, 'sac_diff6': 0.0003821849822998047, 'all': 0.06193184852600098}
diff5_list [0.006339550018310547, 0.0061969757080078125, 0.006094932556152344, 0.006481647491455078, 0.0061473846435546875]
time3 0
time4 0.06267547607421875
time5 0.06271743774414062
time7 9.5367431640625e-07
gen_weight_change tensor(-18.9356)
policy weight change tensor(36.8102, grad_fn=<SumBackward0>)
time8 0.0018787384033203125
train_time 0.0735323429107666
eval time 0.15254735946655273
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:00,782 MainThread INFO: EPOCH:729
2024-01-23 01:04:00,782 MainThread INFO: Time Consumed:0.22919821739196777s
2024-01-23 01:04:00,782 MainThread INFO: Total Frames:110250s
  7%|▋         | 730/10000 [05:28<38:40,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13678.37137
Train_Epoch_Reward                10815.18118
Running_Training_Average_Rewards  13113.26315
Explore_Time                      0.00081
Train___Time                      0.07353
Eval____Time                      0.15255
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14089.51062
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.02216     1.14518    89.95169     86.84208
alpha_0                           0.69418      0.00010    0.69432      0.69404
Alpha_loss                        -2.45654     0.00158    -2.45404     -2.45894
Training/policy_loss              -3.73148     0.00463    -3.72489     -3.73831
Training/qf1_loss                 6531.80420   671.87318  7802.59668   5850.42334
Training/qf2_loss                 14787.49434  876.51578  16440.38281  13865.37500
Training/pf_norm                  0.11856      0.03384    0.16411      0.08428
Training/qf1_norm                 712.02435    217.05560  1017.76068   367.26605
Training/qf2_norm                 1174.56936   14.94644   1201.09949   1158.70129
log_std/mean                      -0.12904     0.00004    -0.12899     -0.12910
log_probs/mean                    -2.73169     0.00438    -2.72648     -2.73641
mean/mean                         -0.00110     0.00008    -0.00103     -0.00124
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018026113510131836
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70550
epoch first part time 2.86102294921875e-06
replay_buffer._size: [110550]
collect time 0.0007991790771484375
inside mustsac before update, task 0, sumup 70550
inside mustsac after update, task 0, sumup 70831
inner_dict_sum {'sac_diff0': 0.0002162456512451172, 'sac_diff1': 0.006682634353637695, 'sac_diff2': 0.008107900619506836, 'sac_diff3': 0.010290145874023438, 'sac_diff4': 0.007363080978393555, 'sac_diff5': 0.052088260650634766, 'sac_diff6': 0.00041103363037109375, 'all': 0.0851593017578125}
diff5_list [0.011838197708129883, 0.010042667388916016, 0.01025390625, 0.009692668914794922, 0.010260820388793945]
time3 0.0008146762847900391
time4 0.08597493171691895
time5 0.08602142333984375
time7 0.008881568908691406
gen_weight_change tensor(-18.8004)
policy weight change tensor(36.8173, grad_fn=<SumBackward0>)
time8 0.0024957656860351562
train_time 0.11520743370056152
eval time 0.10384368896484375
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:01,025 MainThread INFO: EPOCH:730
2024-01-23 01:04:01,026 MainThread INFO: Time Consumed:0.22200751304626465s
2024-01-23 01:04:01,026 MainThread INFO: Total Frames:110400s
  7%|▋         | 731/10000 [05:28<38:27,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13755.39573
Train_Epoch_Reward                11715.82747
Running_Training_Average_Rewards  13113.29967
Explore_Time                      0.00079
Train___Time                      0.11521
Eval____Time                      0.10384
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14233.54343
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.16543     1.80100    89.26286     84.49699
alpha_0                           0.69383      0.00010    0.69397      0.69370
Alpha_loss                        -2.46129     0.00318    -2.45669     -2.46533
Training/policy_loss              -3.57571     0.10027    -3.41541     -3.67745
Training/qf1_loss                 6109.60371   433.59900  6667.72412   5399.69824
Training/qf2_loss                 14159.51211  670.40847  14807.80957  12948.89355
Training/pf_norm                  0.12616      0.03509    0.18915      0.09379
Training/qf1_norm                 1369.83802   504.48541  1876.80066   435.29922
Training/qf2_norm                 1073.69215   50.97048   1128.66016   987.72919
log_std/mean                      -0.13286     0.00424    -0.12509     -0.13690
log_probs/mean                    -2.73548     0.00990    -2.72103     -2.75021
mean/mean                         -0.00467     0.00178    -0.00214     -0.00699
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.017841100692749023
epoch last part time3 0.0023789405822753906
inside rlalgo, task 0, sumup 70831
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [110700]
collect time 0.0007884502410888672
inner_dict_sum {'sac_diff0': 0.0002067089080810547, 'sac_diff1': 0.006547689437866211, 'sac_diff2': 0.0076444149017333984, 'sac_diff3': 0.009690523147583008, 'sac_diff4': 0.006548404693603516, 'sac_diff5': 0.031164884567260742, 'sac_diff6': 0.0003707408905029297, 'all': 0.06217336654663086}
diff5_list [0.006449222564697266, 0.00596308708190918, 0.006185054779052734, 0.006523847579956055, 0.006043672561645508]
time3 0
time4 0.0628972053527832
time5 0.06293916702270508
time7 4.76837158203125e-07
gen_weight_change tensor(-18.8004)
policy weight change tensor(36.9038, grad_fn=<SumBackward0>)
time8 0.001745462417602539
train_time 0.07357573509216309
eval time 0.14497089385986328
epoch last part time 5.4836273193359375e-06
2024-01-23 01:04:01,270 MainThread INFO: EPOCH:731
2024-01-23 01:04:01,270 MainThread INFO: Time Consumed:0.22163009643554688s
2024-01-23 01:04:01,271 MainThread INFO: Total Frames:110550s
  7%|▋         | 732/10000 [05:28<38:09,  4.05it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13795.98028
Train_Epoch_Reward                19069.84406
Running_Training_Average_Rewards  13326.54512
Explore_Time                      0.00078
Train___Time                      0.07358
Eval____Time                      0.14497
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             13979.84910
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.63701     3.39175     94.16191     83.86393
alpha_0                           0.69349      0.00010     0.69363      0.69335
Alpha_loss                        -2.46458     0.00125     -2.46279     -2.46581
Training/policy_loss              -3.66766     0.00132     -3.66564     -3.66923
Training/qf1_loss                 6740.98184   674.96492   7790.65723   5782.34326
Training/qf2_loss                 14971.00117  1368.24516  17189.98242  13023.29492
Training/pf_norm                  0.09792      0.01006     0.11790      0.09077
Training/qf1_norm                 2543.26152   670.60396   3429.77197   1381.40479
Training/qf2_norm                 1144.99734   43.47607    1215.44873   1083.54272
log_std/mean                      -0.12565     0.00016     -0.12546     -0.12589
log_probs/mean                    -2.73525     0.00176     -2.73220     -2.73707
mean/mean                         -0.00562     0.00015     -0.00542     -0.00585
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018078327178955078
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70831
epoch first part time 2.86102294921875e-06
replay_buffer._size: [110850]
collect time 0.0008571147918701172
inner_dict_sum {'sac_diff0': 0.00020265579223632812, 'sac_diff1': 0.006825923919677734, 'sac_diff2': 0.008092164993286133, 'sac_diff3': 0.01031041145324707, 'sac_diff4': 0.0068662166595458984, 'sac_diff5': 0.03221535682678223, 'sac_diff6': 0.00038695335388183594, 'all': 0.06489968299865723}
diff5_list [0.006479024887084961, 0.006270647048950195, 0.006453990936279297, 0.0069429874420166016, 0.006068706512451172]
time3 0
time4 0.06565237045288086
time5 0.06569695472717285
time7 4.76837158203125e-07
gen_weight_change tensor(-18.8004)
policy weight change tensor(36.9377, grad_fn=<SumBackward0>)
time8 0.0018002986907958984
train_time 0.07675790786743164
eval time 0.14377355575561523
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:01,515 MainThread INFO: EPOCH:732
2024-01-23 01:04:01,516 MainThread INFO: Time Consumed:0.22364115715026855s
2024-01-23 01:04:01,516 MainThread INFO: Total Frames:110700s
  7%|▋         | 733/10000 [05:29<38:05,  4.05it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           13854.12117
Train_Epoch_Reward                26818.85149
Running_Training_Average_Rewards  13824.50384
Explore_Time                      0.00085
Train___Time                      0.07676
Eval____Time                      0.14377
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14182.84762
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.24740     1.87649    93.24393     88.14708
alpha_0                           0.69314      0.00010    0.69328      0.69300
Alpha_loss                        -2.46858     0.00280    -2.46496     -2.47271
Training/policy_loss              -3.44459     0.00420    -3.43816     -3.44869
Training/qf1_loss                 7084.40820   523.39918  7964.61719   6493.77637
Training/qf2_loss                 15733.62207  795.32460  17223.62500  14901.45898
Training/pf_norm                  0.09960      0.01701    0.12267      0.07483
Training/qf1_norm                 1079.89653   374.66413  1535.49561   541.15259
Training/qf2_norm                 1045.01947   20.83057   1078.83997   1021.99078
log_std/mean                      -0.13331     0.00001    -0.13329     -0.13333
log_probs/mean                    -2.73696     0.00642    -2.72801     -2.74455
mean/mean                         -0.00505     0.00021    -0.00475     -0.00533
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018445968627929688
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70831
epoch first part time 2.384185791015625e-06
replay_buffer._size: [111000]
collect time 0.0022554397583007812
inner_dict_sum {'sac_diff0': 0.0002009868621826172, 'sac_diff1': 0.006772041320800781, 'sac_diff2': 0.008022069931030273, 'sac_diff3': 0.010254144668579102, 'sac_diff4': 0.006891489028930664, 'sac_diff5': 0.03176569938659668, 'sac_diff6': 0.0003883838653564453, 'all': 0.06429481506347656}
diff5_list [0.006627559661865234, 0.00623774528503418, 0.0064241886138916016, 0.006338596343994141, 0.0061376094818115234]
time3 0
time4 0.06504321098327637
time5 0.06509065628051758
time7 7.152557373046875e-07
gen_weight_change tensor(-18.8004)
policy weight change tensor(36.9398, grad_fn=<SumBackward0>)
time8 0.0019381046295166016
train_time 0.07638120651245117
eval time 0.14823460578918457
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:01,767 MainThread INFO: EPOCH:733
2024-01-23 01:04:01,767 MainThread INFO: Time Consumed:0.2291722297668457s
2024-01-23 01:04:01,767 MainThread INFO: Total Frames:110850s
  7%|▋         | 734/10000 [05:29<38:19,  4.03it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13918.12075
Train_Epoch_Reward                23357.17382
Running_Training_Average_Rewards  14103.69578
Explore_Time                      0.00225
Train___Time                      0.07638
Eval____Time                      0.14823
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14233.96101
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.05844     2.51869     90.74487     83.64285
alpha_0                           0.69279      0.00010     0.69293      0.69266
Alpha_loss                        -2.47070     0.00195     -2.46796     -2.47334
Training/policy_loss              -3.53631     0.00377     -3.53302     -3.54297
Training/qf1_loss                 6560.07637   1485.46510  9378.28320   5460.18359
Training/qf2_loss                 14828.08828  1802.46780  18038.10352  13071.63281
Training/pf_norm                  0.10541      0.02947     0.14817      0.06734
Training/qf1_norm                 430.90060    311.26220   1022.78717   111.75954
Training/qf2_norm                 1131.38013   32.16379    1165.75317   1075.02808
log_std/mean                      -0.13241     0.00005     -0.13232     -0.13245
log_probs/mean                    -2.73357     0.00539     -2.72707     -2.74260
mean/mean                         -0.00703     0.00016     -0.00684     -0.00728
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019197702407836914
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70831
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [111150]
collect time 0.0007874965667724609
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.006316661834716797, 'sac_diff2': 0.0076978206634521484, 'sac_diff3': 0.009722232818603516, 'sac_diff4': 0.0066721439361572266, 'sac_diff5': 0.03107476234436035, 'sac_diff6': 0.0003764629364013672, 'all': 0.06208205223083496}
diff5_list [0.006451845169067383, 0.006406545639038086, 0.006028175354003906, 0.006264448165893555, 0.005923748016357422]
time3 0
time4 0.06284642219543457
time5 0.06288957595825195
time7 7.152557373046875e-07
gen_weight_change tensor(-18.8004)
policy weight change tensor(36.9020, grad_fn=<SumBackward0>)
time8 0.0019049644470214844
train_time 0.07376718521118164
eval time 0.14547228813171387
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:02,011 MainThread INFO: EPOCH:734
2024-01-23 01:04:02,012 MainThread INFO: Time Consumed:0.22222447395324707s
2024-01-23 01:04:02,012 MainThread INFO: Total Frames:111000s
  7%|▋         | 735/10000 [05:29<38:09,  4.05it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           13988.84859
Train_Epoch_Reward                8862.66035
Running_Training_Average_Rewards  13875.65814
Explore_Time                      0.00078
Train___Time                      0.07377
Eval____Time                      0.14547
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14224.48100
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.71851     1.20961     91.34242     88.08659
alpha_0                           0.69245      0.00010     0.69259      0.69231
Alpha_loss                        -2.47494     0.00222     -2.47143     -2.47784
Training/policy_loss              -3.44753     0.00417     -3.44389     -3.45367
Training/qf1_loss                 6875.09893   876.69501   8260.08691   5630.02783
Training/qf2_loss                 15299.21016  1126.98920  17059.01367  13674.67773
Training/pf_norm                  0.10125      0.01355     0.12771      0.08996
Training/qf1_norm                 2705.77720   292.40649   3168.14307   2289.66089
Training/qf2_norm                 1018.27206   13.85992    1036.24976   997.80951
log_std/mean                      -0.13093     0.00013     -0.13072     -0.13108
log_probs/mean                    -2.73593     0.00592     -2.73005     -2.74385
mean/mean                         -0.00894     0.00020     -0.00866     -0.00924
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01876544952392578
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70831
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [111300]
collect time 0.0008809566497802734
inside mustsac before update, task 0, sumup 70831
inside mustsac after update, task 0, sumup 70649
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.006631612777709961, 'sac_diff2': 0.007891654968261719, 'sac_diff3': 0.010033130645751953, 'sac_diff4': 0.0069882869720458984, 'sac_diff5': 0.05046200752258301, 'sac_diff6': 0.0003917217254638672, 'all': 0.08260345458984375}
diff5_list [0.010494709014892578, 0.01000213623046875, 0.010453939437866211, 0.009897470474243164, 0.009613752365112305]
time3 0.0008230209350585938
time4 0.08340668678283691
time5 0.08345484733581543
time7 0.009099960327148438
gen_weight_change tensor(-18.6475)
policy weight change tensor(36.8553, grad_fn=<SumBackward0>)
time8 0.0018723011016845703
train_time 0.11231279373168945
eval time 0.10527992248535156
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:02,254 MainThread INFO: EPOCH:735
2024-01-23 01:04:02,255 MainThread INFO: Time Consumed:0.22063350677490234s
2024-01-23 01:04:02,255 MainThread INFO: Total Frames:111150s
  7%|▋         | 736/10000 [05:29<37:55,  4.07it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14067.10218
Train_Epoch_Reward                22065.68249
Running_Training_Average_Rewards  14387.06223
Explore_Time                      0.00088
Train___Time                      0.11231
Eval____Time                      0.10528
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14196.02833
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.93228     2.91287     92.54948     83.77943
alpha_0                           0.69210      0.00010     0.69224      0.69196
Alpha_loss                        -2.47680     0.00138     -2.47546     -2.47939
Training/policy_loss              -3.59068     0.09235     -3.49925     -3.74563
Training/qf1_loss                 7292.07812   1248.28393  8823.38867   5536.83057
Training/qf2_loss                 15648.18281  1673.44770  17587.46484  13015.22168
Training/pf_norm                  0.11498      0.03646     0.18732      0.09093
Training/qf1_norm                 1137.92887   1074.71874  3222.56323   171.25897
Training/qf2_norm                 1099.23538   26.01598    1137.67334   1064.60193
log_std/mean                      -0.13282     0.00419     -0.12973     -0.14106
log_probs/mean                    -2.73184     0.00425     -2.72455     -2.73706
mean/mean                         -0.00733     0.00087     -0.00619     -0.00846
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018140077590942383
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70649
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [111450]
collect time 0.0009026527404785156
inner_dict_sum {'sac_diff0': 0.00020051002502441406, 'sac_diff1': 0.006899595260620117, 'sac_diff2': 0.007837295532226562, 'sac_diff3': 0.010183572769165039, 'sac_diff4': 0.006756782531738281, 'sac_diff5': 0.03200578689575195, 'sac_diff6': 0.00040721893310546875, 'all': 0.06429076194763184}
diff5_list [0.0065326690673828125, 0.006365060806274414, 0.006352424621582031, 0.006418704986572266, 0.00633692741394043]
time3 0
time4 0.06506013870239258
time5 0.06510710716247559
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6475)
policy weight change tensor(36.7795, grad_fn=<SumBackward0>)
time8 0.0018527507781982422
train_time 0.07610392570495605
eval time 0.14610075950622559
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:02,501 MainThread INFO: EPOCH:736
2024-01-23 01:04:02,502 MainThread INFO: Time Consumed:0.22551798820495605s
2024-01-23 01:04:02,502 MainThread INFO: Total Frames:111300s
  7%|▋         | 737/10000 [05:30<38:00,  4.06it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14122.28118
Train_Epoch_Reward                7588.13176
Running_Training_Average_Rewards  14219.30461
Explore_Time                      0.00090
Train___Time                      0.07610
Eval____Time                      0.14610
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14335.90428
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.28449     2.11389    92.58071     86.45884
alpha_0                           0.69176      0.00010    0.69189      0.69162
Alpha_loss                        -2.48089     0.00284    -2.47618     -2.48508
Training/policy_loss              -3.48310     0.00587    -3.47337     -3.49147
Training/qf1_loss                 6718.92598   533.18205  7388.65625   5825.94434
Training/qf2_loss                 15412.66465  794.59169  16500.83789  14507.13086
Training/pf_norm                  0.11507      0.03450    0.14823      0.07180
Training/qf1_norm                 1000.09138   419.81028  1447.20227   242.87512
Training/qf2_norm                 1059.71813   24.95379   1085.92725   1014.19427
log_std/mean                      -0.13646     0.00014    -0.13627     -0.13667
log_probs/mean                    -2.73382     0.00748    -2.72102     -2.74335
mean/mean                         -0.00823     0.00013    -0.00801     -0.00836
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018428325653076172
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70649
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [111600]
collect time 0.0008361339569091797
inner_dict_sum {'sac_diff0': 0.0002193450927734375, 'sac_diff1': 0.006662845611572266, 'sac_diff2': 0.007890939712524414, 'sac_diff3': 0.010218143463134766, 'sac_diff4': 0.007096529006958008, 'sac_diff5': 0.03172135353088379, 'sac_diff6': 0.00038504600524902344, 'all': 0.0641942024230957}
diff5_list [0.006570577621459961, 0.006218433380126953, 0.006142377853393555, 0.0066585540771484375, 0.006131410598754883]
time3 0
time4 0.06495404243469238
time5 0.06499600410461426
time7 9.5367431640625e-07
gen_weight_change tensor(-18.6475)
policy weight change tensor(36.7352, grad_fn=<SumBackward0>)
time8 0.0019173622131347656
train_time 0.0760488510131836
eval time 0.15793848037719727
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:02,760 MainThread INFO: EPOCH:737
2024-01-23 01:04:02,761 MainThread INFO: Time Consumed:0.23707175254821777s
2024-01-23 01:04:02,761 MainThread INFO: Total Frames:111450s
  7%|▋         | 738/10000 [05:30<38:36,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14168.40363
Train_Epoch_Reward                10425.36243
Running_Training_Average_Rewards  14335.90610
Explore_Time                      0.00083
Train___Time                      0.07605
Eval____Time                      0.15794
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14275.60976
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.09511     2.05793     93.12718     87.36369
alpha_0                           0.69141      0.00010     0.69155      0.69127
Alpha_loss                        -2.48373     0.00158     -2.48154     -2.48591
Training/policy_loss              -3.64248     0.00301     -3.63712     -3.64605
Training/qf1_loss                 7102.81465   1083.62826  8649.91309   5911.65381
Training/qf2_loss                 15935.82070  1314.24857  17837.85352  14750.37500
Training/pf_norm                  0.07938      0.02258     0.12354      0.06036
Training/qf1_norm                 492.90255    160.36324   702.59161    305.67923
Training/qf2_norm                 1226.76553   26.12897    1252.97754   1179.21497
log_std/mean                      -0.13164     0.00004     -0.13158     -0.13169
log_probs/mean                    -2.73237     0.00310     -2.72645     -2.73465
mean/mean                         -0.00736     0.00004     -0.00732     -0.00741
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018715620040893555
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70649
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [111750]
collect time 0.0009737014770507812
inner_dict_sum {'sac_diff0': 0.00020074844360351562, 'sac_diff1': 0.006681203842163086, 'sac_diff2': 0.007693767547607422, 'sac_diff3': 0.010092973709106445, 'sac_diff4': 0.006875276565551758, 'sac_diff5': 0.03238940238952637, 'sac_diff6': 0.00041413307189941406, 'all': 0.06434750556945801}
diff5_list [0.00674891471862793, 0.006255388259887695, 0.00640106201171875, 0.00651860237121582, 0.006465435028076172]
time3 0
time4 0.06512832641601562
time5 0.06517624855041504
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6475)
policy weight change tensor(36.6388, grad_fn=<SumBackward0>)
time8 0.001969575881958008
train_time 0.07635617256164551
eval time 0.1572585105895996
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:03,019 MainThread INFO: EPOCH:738
2024-01-23 01:04:03,020 MainThread INFO: Time Consumed:0.2367877960205078s
2024-01-23 01:04:03,020 MainThread INFO: Total Frames:111600s
  7%|▋         | 739/10000 [05:30<39:01,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14195.08477
Train_Epoch_Reward                2409.25739
Running_Training_Average_Rewards  13843.09612
Explore_Time                      0.00097
Train___Time                      0.07636
Eval____Time                      0.15726
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14199.11253
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.70511     2.12259     94.02040     88.35992
alpha_0                           0.69106      0.00010     0.69120      0.69093
Alpha_loss                        -2.48552     0.00154     -2.48348     -2.48712
Training/policy_loss              -3.72770     0.00274     -3.72417     -3.73172
Training/qf1_loss                 6903.41904   1065.29961  8191.53125   5516.93457
Training/qf2_loss                 15634.92266  1456.87912  17543.92969  13815.13086
Training/pf_norm                  0.11832      0.02222     0.16132      0.10248
Training/qf1_norm                 545.18562    402.22701   1184.36865   109.47072
Training/qf2_norm                 1210.04175   27.21679    1252.93347   1179.57764
log_std/mean                      -0.13428     0.00018     -0.13400     -0.13452
log_probs/mean                    -2.72812     0.00444     -2.72261     -2.73593
mean/mean                         -0.00890     0.00015     -0.00867     -0.00907
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018731355667114258
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70649
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [111900]
collect time 0.0008633136749267578
inner_dict_sum {'sac_diff0': 0.0002071857452392578, 'sac_diff1': 0.006844043731689453, 'sac_diff2': 0.008272886276245117, 'sac_diff3': 0.010727882385253906, 'sac_diff4': 0.006845951080322266, 'sac_diff5': 0.03253626823425293, 'sac_diff6': 0.00039124488830566406, 'all': 0.0658254623413086}
diff5_list [0.006849765777587891, 0.006392002105712891, 0.006829738616943359, 0.006272792816162109, 0.00619196891784668]
time3 0
time4 0.06659078598022461
time5 0.06663990020751953
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6475)
policy weight change tensor(36.5445, grad_fn=<SumBackward0>)
time8 0.0019137859344482422
train_time 0.07795500755310059
eval time 0.15598630905151367
epoch last part time 5.7220458984375e-06
2024-01-23 01:04:03,279 MainThread INFO: EPOCH:739
2024-01-23 01:04:03,279 MainThread INFO: Time Consumed:0.2371506690979004s
2024-01-23 01:04:03,280 MainThread INFO: Total Frames:111750s
  7%|▋         | 740/10000 [05:30<39:19,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14210.86885
Train_Epoch_Reward                2302.24280
Running_Training_Average_Rewards  13681.53042
Explore_Time                      0.00086
Train___Time                      0.07796
Eval____Time                      0.15599
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14247.35145
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.40932     1.46130    92.00306     88.13426
alpha_0                           0.69072      0.00010    0.69086      0.69058
Alpha_loss                        -2.49155     0.00170    -2.48868     -2.49362
Training/policy_loss              -3.50762     0.00272    -3.50285     -3.51108
Training/qf1_loss                 6576.87705   481.03295  7295.29199   6005.39111
Training/qf2_loss                 15239.92285  721.29551  16242.71387  14259.68750
Training/pf_norm                  0.09836      0.01184    0.11222      0.08429
Training/qf1_norm                 859.33641    245.80045  1098.00024   474.66345
Training/qf2_norm                 1073.61130   17.85258   1092.05652   1045.50146
log_std/mean                      -0.13333     0.00015    -0.13313     -0.13357
log_probs/mean                    -2.73533     0.00454    -2.72756     -2.74155
mean/mean                         -0.00623     0.00012    -0.00604     -0.00639
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01877617835998535
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70649
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [112050]
collect time 0.0009357929229736328
inside mustsac before update, task 0, sumup 70649
inside mustsac after update, task 0, sumup 70153
inner_dict_sum {'sac_diff0': 0.00021696090698242188, 'sac_diff1': 0.0069942474365234375, 'sac_diff2': 0.008372306823730469, 'sac_diff3': 0.010922670364379883, 'sac_diff4': 0.007429599761962891, 'sac_diff5': 0.05169320106506348, 'sac_diff6': 0.0004184246063232422, 'all': 0.08604741096496582}
diff5_list [0.010741472244262695, 0.010421991348266602, 0.010346651077270508, 0.01000833511352539, 0.010174751281738281]
time3 0.0008299350738525391
time4 0.08689332008361816
time5 0.0869438648223877
time7 0.009046792984008789
gen_weight_change tensor(-18.6140)
policy weight change tensor(36.5611, grad_fn=<SumBackward0>)
time8 0.0025930404663085938
train_time 0.1169281005859375
eval time 0.10318279266357422
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:03,525 MainThread INFO: EPOCH:740
2024-01-23 01:04:03,525 MainThread INFO: Time Consumed:0.22324848175048828s
2024-01-23 01:04:03,525 MainThread INFO: Total Frames:111900s
  7%|▋         | 741/10000 [05:31<39:00,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14217.99407
Train_Epoch_Reward                9574.40128
Running_Training_Average_Rewards  13379.75811
Explore_Time                      0.00093
Train___Time                      0.11693
Eval____Time                      0.10318
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14304.79557
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       87.55464     1.57519    89.42336     85.57013
alpha_0                           0.69037      0.00010    0.69051      0.69024
Alpha_loss                        -2.49165     0.00265    -2.48887     -2.49549
Training/policy_loss              -3.72956     0.14917    -3.47819     -3.92740
Training/qf1_loss                 5775.73047   198.50673  6043.91064   5499.32471
Training/qf2_loss                 13921.93848  405.51960  14466.77637  13287.32324
Training/pf_norm                  0.12540      0.05064    0.22135      0.07695
Training/qf1_norm                 890.80831    398.07489  1331.81555   402.85880
Training/qf2_norm                 1169.92283   97.00798   1312.50720   1015.23792
log_std/mean                      -0.12577     0.00499    -0.11809     -0.13357
log_probs/mean                    -2.72651     0.00496    -2.72048     -2.73324
mean/mean                         -0.00639     0.00131    -0.00415     -0.00743
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018278121948242188
epoch last part time3 0.0026350021362304688
inside rlalgo, task 0, sumup 70153
epoch first part time 2.86102294921875e-06
replay_buffer._size: [112200]
collect time 0.0009284019470214844
inner_dict_sum {'sac_diff0': 0.00022530555725097656, 'sac_diff1': 0.006577253341674805, 'sac_diff2': 0.007903575897216797, 'sac_diff3': 0.010090112686157227, 'sac_diff4': 0.006867647171020508, 'sac_diff5': 0.03169584274291992, 'sac_diff6': 0.0003859996795654297, 'all': 0.06374573707580566}
diff5_list [0.006464958190917969, 0.006200551986694336, 0.006146669387817383, 0.006725311279296875, 0.006158351898193359]
time3 0
time4 0.06447863578796387
time5 0.0645291805267334
time7 4.76837158203125e-07
gen_weight_change tensor(-18.6140)
policy weight change tensor(36.4545, grad_fn=<SumBackward0>)
time8 0.0018839836120605469
train_time 0.07541131973266602
eval time 0.14580035209655762
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:03,773 MainThread INFO: EPOCH:741
2024-01-23 01:04:03,774 MainThread INFO: Time Consumed:0.22443246841430664s
2024-01-23 01:04:03,774 MainThread INFO: Total Frames:112050s
  7%|▋         | 742/10000 [05:31<38:42,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14248.17065
Train_Epoch_Reward                2562.25498
Running_Training_Average_Rewards  12980.18902
Explore_Time                      0.00092
Train___Time                      0.07541
Eval____Time                      0.14580
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14281.61498
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.29246     1.01802    90.69228     87.74559
alpha_0                           0.69003      0.00010    0.69017      0.68989
Alpha_loss                        -2.49844     0.00362    -2.49416     -2.50225
Training/policy_loss              -3.64690     0.00624    -3.63759     -3.65367
Training/qf1_loss                 6466.53184   231.78419  6707.35791   6111.14795
Training/qf2_loss                 14937.32148  307.93784  15292.73633  14521.69727
Training/pf_norm                  0.10198      0.01232    0.11559      0.08166
Training/qf1_norm                 566.38154    205.94258  887.21198    279.03680
Training/qf2_norm                 1122.30830   12.28761   1138.96777   1103.42419
log_std/mean                      -0.12857     0.00017    -0.12828     -0.12877
log_probs/mean                    -2.73575     0.00895    -2.72256     -2.74577
mean/mean                         -0.00822     0.00002    -0.00819     -0.00825
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01877140998840332
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70153
epoch first part time 2.1457672119140625e-06
replay_buffer._size: [112350]
collect time 0.0008521080017089844
inner_dict_sum {'sac_diff0': 0.00024318695068359375, 'sac_diff1': 0.008025169372558594, 'sac_diff2': 0.009550809860229492, 'sac_diff3': 0.011065244674682617, 'sac_diff4': 0.007720947265625, 'sac_diff5': 0.0362858772277832, 'sac_diff6': 0.0004634857177734375, 'all': 0.07335472106933594}
diff5_list [0.006661415100097656, 0.006182670593261719, 0.006250619888305664, 0.010161161422729492, 0.007030010223388672]
time3 0
time4 0.0744316577911377
time5 0.07449030876159668
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6140)
policy weight change tensor(36.4187, grad_fn=<SumBackward0>)
time8 0.002251863479614258
train_time 0.08672904968261719
eval time 0.1409449577331543
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:04,027 MainThread INFO: EPOCH:742
2024-01-23 01:04:04,027 MainThread INFO: Time Consumed:0.23108291625976562s
2024-01-23 01:04:04,027 MainThread INFO: Total Frames:112200s
  7%|▋         | 743/10000 [05:31<38:49,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14254.99979
Train_Epoch_Reward                12185.98758
Running_Training_Average_Rewards  13094.55778
Explore_Time                      0.00085
Train___Time                      0.08673
Eval____Time                      0.14094
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14251.13902
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.63472     1.90202    92.74788     87.49072
alpha_0                           0.68968      0.00010    0.68982      0.68955
Alpha_loss                        -2.50000     0.00357    -2.49637     -2.50585
Training/policy_loss              -3.48115     0.00545    -3.47552     -3.49017
Training/qf1_loss                 6361.11504   461.76229  7251.90088   5925.95312
Training/qf2_loss                 14876.17227  789.49508  16355.24707  14250.40039
Training/pf_norm                  0.11458      0.02241    0.14000      0.07466
Training/qf1_norm                 678.95209    334.12085  1223.22559   324.74268
Training/qf2_norm                 1066.83513   22.58697   1105.18933   1044.14478
log_std/mean                      -0.12117     0.00005    -0.12112     -0.12126
log_probs/mean                    -2.73088     0.00874    -2.72220     -2.74663
mean/mean                         -0.00227     0.00015    -0.00204     -0.00246
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01910090446472168
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70153
epoch first part time 5.0067901611328125e-06
replay_buffer._size: [112500]
collect time 0.0009379386901855469
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.0074920654296875, 'sac_diff2': 0.008730173110961914, 'sac_diff3': 0.011071443557739258, 'sac_diff4': 0.007616281509399414, 'sac_diff5': 0.03357720375061035, 'sac_diff6': 0.00042366981506347656, 'all': 0.06912541389465332}
diff5_list [0.006671428680419922, 0.006786346435546875, 0.007469892501831055, 0.006315708160400391, 0.006333827972412109]
time3 0
time4 0.0699770450592041
time5 0.07003188133239746
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6140)
policy weight change tensor(36.3991, grad_fn=<SumBackward0>)
time8 0.002037525177001953
train_time 0.0819239616394043
eval time 0.14374589920043945
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:04,279 MainThread INFO: EPOCH:743
2024-01-23 01:04:04,279 MainThread INFO: Time Consumed:0.22914719581604004s
2024-01-23 01:04:04,279 MainThread INFO: Total Frames:112350s
  7%|▋         | 744/10000 [05:31<38:51,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14275.00359
Train_Epoch_Reward                11274.41529
Running_Training_Average_Rewards  12851.01912
Explore_Time                      0.00093
Train___Time                      0.08192
Eval____Time                      0.14375
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14433.99897
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.50749     1.04889     91.52113     88.77345
alpha_0                           0.68934      0.00010     0.68948      0.68920
Alpha_loss                        -2.50506     0.00180     -2.50322     -2.50767
Training/policy_loss              -3.87579     0.00595     -3.86794     -3.88308
Training/qf1_loss                 7065.06572   1493.66339  9737.79688   5688.79785
Training/qf2_loss                 15729.68086  1625.78975  18529.56836  14030.39551
Training/pf_norm                  0.13251      0.03970     0.17876      0.07948
Training/qf1_norm                 993.78560    223.08445   1219.07300   660.97729
Training/qf2_norm                 1265.15881   15.19795    1279.79529   1240.18518
log_std/mean                      -0.13503     0.00011     -0.13493     -0.13520
log_probs/mean                    -2.73544     0.00719     -2.72687     -2.74426
mean/mean                         -0.00456     0.00026     -0.00418     -0.00490
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019481420516967773
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70153
epoch first part time 3.337860107421875e-06
replay_buffer._size: [112650]
collect time 0.0009245872497558594
inner_dict_sum {'sac_diff0': 0.0002162456512451172, 'sac_diff1': 0.007245540618896484, 'sac_diff2': 0.008402585983276367, 'sac_diff3': 0.010449886322021484, 'sac_diff4': 0.007044315338134766, 'sac_diff5': 0.033126115798950195, 'sac_diff6': 0.00043129920959472656, 'all': 0.06691598892211914}
diff5_list [0.006893634796142578, 0.0062160491943359375, 0.007091522216796875, 0.006574153900146484, 0.00635075569152832]
time3 0
time4 0.06770944595336914
time5 0.06775689125061035
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6140)
policy weight change tensor(36.4467, grad_fn=<SumBackward0>)
time8 0.0019788742065429688
train_time 0.07902145385742188
eval time 0.14298701286315918
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:04,527 MainThread INFO: EPOCH:744
2024-01-23 01:04:04,528 MainThread INFO: Time Consumed:0.22532391548156738s
2024-01-23 01:04:04,528 MainThread INFO: Total Frames:112500s
  7%|▋         | 745/10000 [05:32<38:40,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14316.06372
Train_Epoch_Reward                19798.32064
Running_Training_Average_Rewards  12715.83640
Explore_Time                      0.00092
Train___Time                      0.07902
Eval____Time                      0.14299
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14635.08232
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.81782     1.75064     92.77374     87.69566
alpha_0                           0.68899      0.00010     0.68913      0.68886
Alpha_loss                        -2.50706     0.00356     -2.50190     -2.51276
Training/policy_loss              -3.78154     0.00602     -3.77327     -3.79205
Training/qf1_loss                 6782.43311   775.43491   7995.47900   5791.50830
Training/qf2_loss                 15343.97187  1020.63786  16680.48828  13950.65332
Training/pf_norm                  0.11843      0.02297     0.13977      0.07475
Training/qf1_norm                 448.39917    285.15075   939.92206    114.54153
Training/qf2_norm                 1210.84746   23.25822    1250.37390   1182.72571
log_std/mean                      -0.12550     0.00011     -0.12535     -0.12567
log_probs/mean                    -2.73177     0.00836     -2.72153     -2.74707
mean/mean                         -0.00544     0.00006     -0.00540     -0.00555
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018822193145751953
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70153
epoch first part time 2.86102294921875e-06
replay_buffer._size: [112800]
collect time 0.0008568763732910156
inside mustsac before update, task 0, sumup 70153
inside mustsac after update, task 0, sumup 69732
inner_dict_sum {'sac_diff0': 0.0002193450927734375, 'sac_diff1': 0.006981372833251953, 'sac_diff2': 0.008369207382202148, 'sac_diff3': 0.010335922241210938, 'sac_diff4': 0.006963253021240234, 'sac_diff5': 0.053519487380981445, 'sac_diff6': 0.00041747093200683594, 'all': 0.08680605888366699}
diff5_list [0.013266563415527344, 0.010069608688354492, 0.01008462905883789, 0.010110616683959961, 0.009988069534301758]
time3 0.0008490085601806641
time4 0.08764863014221191
time5 0.08769965171813965
time7 0.009658575057983398
gen_weight_change tensor(-18.5701)
policy weight change tensor(36.4416, grad_fn=<SumBackward0>)
time8 0.0022940635681152344
train_time 0.11794614791870117
eval time 0.10848665237426758
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:04,779 MainThread INFO: EPOCH:745
2024-01-23 01:04:04,780 MainThread INFO: Time Consumed:0.22957944869995117s
2024-01-23 01:04:04,780 MainThread INFO: Total Frames:112650s
  7%|▋         | 746/10000 [05:32<38:46,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14358.17269
Train_Epoch_Reward                16265.27550
Running_Training_Average_Rewards  12714.31275
Explore_Time                      0.00085
Train___Time                      0.11795
Eval____Time                      0.10849
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14617.11806
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.84862     1.17344     92.43065     88.78207
alpha_0                           0.68865      0.00010     0.68879      0.68851
Alpha_loss                        -2.51209     0.00020     -2.51177     -2.51232
Training/policy_loss              -3.82445     0.11304     -3.65201     -3.99730
Training/qf1_loss                 7265.52266   1122.90407  8723.32422   5505.60645
Training/qf2_loss                 16013.34238  1271.39517  17548.83203  13855.10645
Training/pf_norm                  0.11739      0.03202     0.17392      0.07699
Training/qf1_norm                 933.71551    839.13209   2236.48779   124.97800
Training/qf2_norm                 1240.86611   67.93285    1356.50781   1144.22009
log_std/mean                      -0.12953     0.00298     -0.12497     -0.13213
log_probs/mean                    -2.73623     0.00215     -2.73323     -2.73899
mean/mean                         -0.00461     0.00096     -0.00306     -0.00604
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01961517333984375
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 69732
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [112950]
collect time 0.0009579658508300781
inner_dict_sum {'sac_diff0': 0.000240325927734375, 'sac_diff1': 0.0068585872650146484, 'sac_diff2': 0.007605791091918945, 'sac_diff3': 0.010152339935302734, 'sac_diff4': 0.00702977180480957, 'sac_diff5': 0.031273841857910156, 'sac_diff6': 0.00037860870361328125, 'all': 0.06353926658630371}
diff5_list [0.006649494171142578, 0.006195068359375, 0.006203413009643555, 0.006169557571411133, 0.006056308746337891]
time3 0
time4 0.06427764892578125
time5 0.06431889533996582
time7 7.152557373046875e-07
gen_weight_change tensor(-18.5701)
policy weight change tensor(36.5129, grad_fn=<SumBackward0>)
time8 0.0019381046295166016
train_time 0.07573699951171875
eval time 0.1445481777191162
epoch last part time 7.62939453125e-06
2024-01-23 01:04:05,027 MainThread INFO: EPOCH:746
2024-01-23 01:04:05,027 MainThread INFO: Time Consumed:0.22366642951965332s
2024-01-23 01:04:05,027 MainThread INFO: Total Frames:112800s
  7%|▋         | 747/10000 [05:32<38:32,  4.00it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14383.27944
Train_Epoch_Reward                13892.16872
Running_Training_Average_Rewards  12605.01139
Explore_Time                      0.00095
Train___Time                      0.07574
Eval____Time                      0.14455
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14586.97172
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.03214     1.19594    92.61701     89.98912
alpha_0                           0.68831      0.00010    0.68844      0.68817
Alpha_loss                        -2.51526     0.00292    -2.51207     -2.51972
Training/policy_loss              -3.63027     0.00615    -3.62420     -3.64151
Training/qf1_loss                 6915.58643   336.89139  7414.94141   6396.30811
Training/qf2_loss                 15750.12168  542.13632  16558.11914  15045.10449
Training/pf_norm                  0.09986      0.02454    0.13388      0.06783
Training/qf1_norm                 359.93845    199.84854  534.69080    108.90671
Training/qf2_norm                 1174.02090   15.27264   1194.31628   1160.88110
log_std/mean                      -0.12488     0.00013    -0.12475     -0.12510
log_probs/mean                    -2.73569     0.00853    -2.72715     -2.75127
mean/mean                         -0.00382     0.00006    -0.00371     -0.00388
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018763303756713867
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69732
epoch first part time 2.86102294921875e-06
replay_buffer._size: [113100]
collect time 0.0009465217590332031
inner_dict_sum {'sac_diff0': 0.00024056434631347656, 'sac_diff1': 0.006749153137207031, 'sac_diff2': 0.00725102424621582, 'sac_diff3': 0.009567737579345703, 'sac_diff4': 0.006607532501220703, 'sac_diff5': 0.032155752182006836, 'sac_diff6': 0.00040435791015625, 'all': 0.06297612190246582}
diff5_list [0.0066258907318115234, 0.006198406219482422, 0.006236553192138672, 0.007005214691162109, 0.006089687347412109]
time3 0
time4 0.06371164321899414
time5 0.0637521743774414
time7 7.152557373046875e-07
gen_weight_change tensor(-18.5701)
policy weight change tensor(36.5853, grad_fn=<SumBackward0>)
time8 0.0018031597137451172
train_time 0.07460284233093262
eval time 0.14612507820129395
epoch last part time 7.867813110351562e-06
2024-01-23 01:04:05,273 MainThread INFO: EPOCH:747
2024-01-23 01:04:05,274 MainThread INFO: Time Consumed:0.22415995597839355s
2024-01-23 01:04:05,274 MainThread INFO: Total Frames:112950s
  7%|▋         | 748/10000 [05:32<38:22,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14405.48284
Train_Epoch_Reward                12953.42353
Running_Training_Average_Rewards  12399.77246
Explore_Time                      0.00094
Train___Time                      0.07460
Eval____Time                      0.14613
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14497.64376
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.95774     0.63780    90.91329     89.17474
alpha_0                           0.68796      0.00010    0.68810      0.68782
Alpha_loss                        -2.52109     0.00418    -2.51591     -2.52815
Training/policy_loss              -3.78523     0.00643    -3.77656     -3.79605
Training/qf1_loss                 6380.94795   286.60163  6764.40332   5891.77734
Training/qf2_loss                 14977.56816  188.24304  15219.00488  14656.05176
Training/pf_norm                  0.10059      0.02397    0.12816      0.06831
Training/qf1_norm                 435.81530    87.87938   544.27362    322.25232
Training/qf2_norm                 1214.16553   8.75109    1227.28833   1203.39294
log_std/mean                      -0.12468     0.00013    -0.12449     -0.12487
log_probs/mean                    -2.74228     0.00952    -2.72843     -2.75754
mean/mean                         -0.00399     0.00003    -0.00396     -0.00403
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01834583282470703
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69732
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [113250]
collect time 0.0008535385131835938
inner_dict_sum {'sac_diff0': 0.00022649765014648438, 'sac_diff1': 0.006979465484619141, 'sac_diff2': 0.00766754150390625, 'sac_diff3': 0.010140419006347656, 'sac_diff4': 0.006556510925292969, 'sac_diff5': 0.03398752212524414, 'sac_diff6': 0.0003972053527832031, 'all': 0.06595516204833984}
diff5_list [0.006623029708862305, 0.0061779022216796875, 0.006460666656494141, 0.007439851760864258, 0.00728607177734375]
time3 0
time4 0.06673312187194824
time5 0.06677913665771484
time7 9.5367431640625e-07
gen_weight_change tensor(-18.5701)
policy weight change tensor(36.7093, grad_fn=<SumBackward0>)
time8 0.0019664764404296875
train_time 0.07800889015197754
eval time 0.1460890769958496
epoch last part time 7.62939453125e-06
2024-01-23 01:04:05,523 MainThread INFO: EPOCH:748
2024-01-23 01:04:05,523 MainThread INFO: Time Consumed:0.22729253768920898s
2024-01-23 01:04:05,523 MainThread INFO: Total Frames:113100s
  7%|▋         | 749/10000 [05:33<38:23,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14419.52896
Train_Epoch_Reward                17542.72454
Running_Training_Average_Rewards  12450.38356
Explore_Time                      0.00085
Train___Time                      0.07801
Eval____Time                      0.14609
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14339.57379
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.66537     1.43155    91.96780     87.96291
alpha_0                           0.68762      0.00010    0.68776      0.68748
Alpha_loss                        -2.52029     0.00269    -2.51687     -2.52383
Training/policy_loss              -3.59821     0.00420    -3.59053     -3.60228
Training/qf1_loss                 7304.53105   476.10020  8229.67578   6892.24707
Training/qf2_loss                 16079.67012  649.81897  17226.35938  15336.36230
Training/pf_norm                  0.10134      0.01272    0.12204      0.08879
Training/qf1_norm                 319.01499    252.57027  801.81458    115.69367
Training/qf2_norm                 1151.81362   18.15873   1168.30151   1117.26270
log_std/mean                      -0.12447     0.00024    -0.12415     -0.12484
log_probs/mean                    -2.73113     0.00666    -2.72021     -2.73832
mean/mean                         -0.00422     0.00005    -0.00413     -0.00427
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.017817258834838867
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69732
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [113400]
collect time 0.0008361339569091797
inner_dict_sum {'sac_diff0': 0.00023174285888671875, 'sac_diff1': 0.0071680545806884766, 'sac_diff2': 0.008059024810791016, 'sac_diff3': 0.010526895523071289, 'sac_diff4': 0.0064351558685302734, 'sac_diff5': 0.03240513801574707, 'sac_diff6': 0.000400543212890625, 'all': 0.06522655487060547}
diff5_list [0.006500720977783203, 0.0070416927337646484, 0.00644993782043457, 0.006342887878417969, 0.00606989860534668]
time3 0
time4 0.06601786613464355
time5 0.06606459617614746
time7 7.152557373046875e-07
gen_weight_change tensor(-18.5701)
policy weight change tensor(36.7891, grad_fn=<SumBackward0>)
time8 0.0018093585968017578
train_time 0.07710981369018555
eval time 0.15133905410766602
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:05,777 MainThread INFO: EPOCH:749
2024-01-23 01:04:05,777 MainThread INFO: Time Consumed:0.2316265106201172s
2024-01-23 01:04:05,777 MainThread INFO: Total Frames:113250s
  8%|▊         | 750/10000 [05:33<38:37,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14426.76458
Train_Epoch_Reward                5278.15248
Running_Training_Average_Rewards  11822.29988
Explore_Time                      0.00083
Train___Time                      0.07711
Eval____Time                      0.15134
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14319.70759
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.04808     2.39694     94.61897     87.20863
alpha_0                           0.68727      0.00010     0.68741      0.68714
Alpha_loss                        -2.52359     0.00120     -2.52178     -2.52496
Training/policy_loss              -3.69717     0.00274     -3.69307     -3.70073
Training/qf1_loss                 7108.19902   993.35366   8817.96680   5731.25488
Training/qf2_loss                 15901.31895  1468.41641  18349.57031  13739.97168
Training/pf_norm                  0.11279      0.02857     0.16537      0.07896
Training/qf1_norm                 938.46000    481.05096   1665.47229   185.82887
Training/qf2_norm                 1229.39844   30.81468    1275.22253   1180.14502
log_std/mean                      -0.14270     0.00018     -0.14237     -0.14287
log_probs/mean                    -2.73096     0.00402     -2.72434     -2.73675
mean/mean                         -0.00771     0.00018     -0.00742     -0.00792
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01894974708557129
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69732
epoch first part time 2.86102294921875e-06
replay_buffer._size: [113550]
collect time 0.0008561611175537109
inside mustsac before update, task 0, sumup 69732
inside mustsac after update, task 0, sumup 69006
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.007355451583862305, 'sac_diff2': 0.008065462112426758, 'sac_diff3': 0.010790348052978516, 'sac_diff4': 0.006959199905395508, 'sac_diff5': 0.052359819412231445, 'sac_diff6': 0.0004189014434814453, 'all': 0.08615660667419434}
diff5_list [0.011869430541992188, 0.010163068771362305, 0.010727643966674805, 0.009936094284057617, 0.009663581848144531]
time3 0.0008826255798339844
time4 0.08698654174804688
time5 0.08703780174255371
time7 0.009079217910766602
gen_weight_change tensor(-18.5900)
policy weight change tensor(36.8511, grad_fn=<SumBackward0>)
time8 0.002504110336303711
train_time 0.11716032028198242
eval time 0.1099853515625
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:06,030 MainThread INFO: EPOCH:750
2024-01-23 01:04:06,030 MainThread INFO: Time Consumed:0.23039817810058594s
2024-01-23 01:04:06,030 MainThread INFO: Total Frames:113400s
  8%|▊         | 751/10000 [05:33<38:51,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14432.85825
Train_Epoch_Reward                42814.68498
Running_Training_Average_Rewards  13134.15058
Explore_Time                      0.00085
Train___Time                      0.11716
Eval____Time                      0.10999
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14365.73229
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.34359     1.24007    90.81348     87.08780
alpha_0                           0.68693      0.00010    0.68707      0.68679
Alpha_loss                        -2.52814     0.00113    -2.52609     -2.52956
Training/policy_loss              -3.71583     0.09649    -3.58811     -3.81724
Training/qf1_loss                 6233.72012   556.91393  7262.64551   5754.07812
Training/qf2_loss                 14683.75352  686.77691  15757.96875  13808.34082
Training/pf_norm                  0.09636      0.00749    0.10421      0.08232
Training/qf1_norm                 935.37121    822.74370  2456.71558   150.66336
Training/qf2_norm                 1172.59790   53.62626   1236.18701   1102.45837
log_std/mean                      -0.12870     0.00563    -0.12048     -0.13750
log_probs/mean                    -2.73411     0.00432    -2.72687     -2.73789
mean/mean                         -0.00492     0.00105    -0.00384     -0.00663
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018513917922973633
epoch last part time3 0.0029692649841308594
inside rlalgo, task 0, sumup 69006
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [113700]
collect time 0.001100301742553711
inner_dict_sum {'sac_diff0': 0.0002262592315673828, 'sac_diff1': 0.006725788116455078, 'sac_diff2': 0.007439851760864258, 'sac_diff3': 0.009495258331298828, 'sac_diff4': 0.006331920623779297, 'sac_diff5': 0.0317683219909668, 'sac_diff6': 0.0003845691680908203, 'all': 0.06237196922302246}
diff5_list [0.006688833236694336, 0.006330728530883789, 0.006337165832519531, 0.006406068801879883, 0.006005525588989258]
time3 0
time4 0.06308460235595703
time5 0.0631248950958252
time7 7.152557373046875e-07
gen_weight_change tensor(-18.5900)
policy weight change tensor(36.8810, grad_fn=<SumBackward0>)
time8 0.0017366409301757812
train_time 0.07408761978149414
eval time 0.1485610008239746
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:06,281 MainThread INFO: EPOCH:751
2024-01-23 01:04:06,281 MainThread INFO: Time Consumed:0.22630906105041504s
2024-01-23 01:04:06,281 MainThread INFO: Total Frames:113550s
  8%|▊         | 752/10000 [05:33<38:40,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14448.64644
Train_Epoch_Reward                3490.37672
Running_Training_Average_Rewards  12995.40293
Explore_Time                      0.00109
Train___Time                      0.07409
Eval____Time                      0.14856
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14439.49689
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.36265     3.21587     95.68134     86.95414
alpha_0                           0.68659      0.00010     0.68672      0.68645
Alpha_loss                        -2.53059     0.00373     -2.52642     -2.53684
Training/policy_loss              -3.62210     0.00609     -3.61660     -3.63152
Training/qf1_loss                 6685.70029   2147.39797  10882.72461  5078.82080
Training/qf2_loss                 15191.60078  2752.74596  20632.47461  13421.28711
Training/pf_norm                  0.08818      0.01038     0.10287      0.07477
Training/qf1_norm                 531.66556    400.55319   1294.69312   163.04147
Training/qf2_norm                 1101.41108   39.80559    1179.54297   1071.49988
log_std/mean                      -0.12755     0.00009     -0.12745     -0.12770
log_probs/mean                    -2.73169     0.00905     -2.72317     -2.74652
mean/mean                         -0.00384     0.00016     -0.00359     -0.00406
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018732547760009766
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69006
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [113850]
collect time 0.0008389949798583984
inner_dict_sum {'sac_diff0': 0.00022840499877929688, 'sac_diff1': 0.007109642028808594, 'sac_diff2': 0.007380485534667969, 'sac_diff3': 0.009720087051391602, 'sac_diff4': 0.006314277648925781, 'sac_diff5': 0.03354644775390625, 'sac_diff6': 0.0004088878631591797, 'all': 0.06470823287963867}
diff5_list [0.006256580352783203, 0.008675098419189453, 0.00636601448059082, 0.006199836730957031, 0.006048917770385742]
time3 0
time4 0.06548666954040527
time5 0.06554293632507324
time7 7.152557373046875e-07
gen_weight_change tensor(-18.5900)
policy weight change tensor(36.9086, grad_fn=<SumBackward0>)
time8 0.001852273941040039
train_time 0.07665276527404785
eval time 0.15717220306396484
epoch last part time 8.58306884765625e-06
2024-01-23 01:04:06,540 MainThread INFO: EPOCH:752
2024-01-23 01:04:06,540 MainThread INFO: Time Consumed:0.23705577850341797s
2024-01-23 01:04:06,541 MainThread INFO: Total Frames:113700s
  8%|▊         | 753/10000 [05:34<39:03,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14475.67883
Train_Epoch_Reward                12617.26335
Running_Training_Average_Rewards  13195.66465
Explore_Time                      0.00083
Train___Time                      0.07665
Eval____Time                      0.15717
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14521.46292
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.61728     1.58490    91.97285     88.08244
alpha_0                           0.68624      0.00010    0.68638      0.68611
Alpha_loss                        -2.53566     0.00322    -2.53138     -2.54012
Training/policy_loss              -3.79591     0.00422    -3.79153     -3.80245
Training/qf1_loss                 6457.62139   360.32647  6768.93604   5781.97705
Training/qf2_loss                 14941.66211  548.39054  15722.72363  14094.55273
Training/pf_norm                  0.09457      0.01032    0.10976      0.07877
Training/qf1_norm                 1525.42566   297.42582  1868.40356   1057.38318
Training/qf2_norm                 1237.72554   21.29154   1269.74377   1217.76282
log_std/mean                      -0.12905     0.00003    -0.12901     -0.12910
log_probs/mean                    -2.73621     0.00630    -2.72841     -2.74626
mean/mean                         -0.00249     0.00012    -0.00232     -0.00266
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018611669540405273
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69006
epoch first part time 2.86102294921875e-06
replay_buffer._size: [114000]
collect time 0.0009541511535644531
inner_dict_sum {'sac_diff0': 0.00024271011352539062, 'sac_diff1': 0.006894588470458984, 'sac_diff2': 0.007236957550048828, 'sac_diff3': 0.009975910186767578, 'sac_diff4': 0.0061969757080078125, 'sac_diff5': 0.031206846237182617, 'sac_diff6': 0.0003876686096191406, 'all': 0.06214165687561035}
diff5_list [0.006703615188598633, 0.006108999252319336, 0.0063190460205078125, 0.006273746490478516, 0.00580143928527832]
time3 0
time4 0.06288456916809082
time5 0.06292963027954102
time7 4.76837158203125e-07
gen_weight_change tensor(-18.5900)
policy weight change tensor(36.9002, grad_fn=<SumBackward0>)
time8 0.001959085464477539
train_time 0.07426142692565918
eval time 0.15988516807556152
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:06,800 MainThread INFO: EPOCH:753
2024-01-23 01:04:06,800 MainThread INFO: Time Consumed:0.2375345230102539s
2024-01-23 01:04:06,800 MainThread INFO: Total Frames:113850s
  8%|▊         | 754/10000 [05:34<39:20,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14496.23913
Train_Epoch_Reward                13550.20711
Running_Training_Average_Rewards  13078.37846
Explore_Time                      0.00094
Train___Time                      0.07426
Eval____Time                      0.15989
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14639.60200
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.05201     2.15214     93.71365     87.13268
alpha_0                           0.68590      0.00010     0.68604      0.68576
Alpha_loss                        -2.53877     0.00469     -2.53494     -2.54740
Training/policy_loss              -4.05993     0.00875     -4.05044     -4.07621
Training/qf1_loss                 7085.66113   1741.88634  9260.37109   5567.93262
Training/qf2_loss                 15593.18301  2055.97604  18405.74609  13679.25488
Training/pf_norm                  0.11701      0.02905     0.16242      0.07958
Training/qf1_norm                 1796.59536   393.50176   2454.87988   1254.63440
Training/qf2_norm                 1363.29963   32.15125    1418.16675   1319.96655
log_std/mean                      -0.13639     0.00006     -0.13629     -0.13646
log_probs/mean                    -2.73553     0.01194     -2.72493     -2.75842
mean/mean                         -0.00163     0.00007     -0.00156     -0.00174
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01842784881591797
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69006
epoch first part time 2.86102294921875e-06
replay_buffer._size: [114150]
collect time 0.0008580684661865234
inner_dict_sum {'sac_diff0': 0.0002231597900390625, 'sac_diff1': 0.007299661636352539, 'sac_diff2': 0.007669210433959961, 'sac_diff3': 0.010298490524291992, 'sac_diff4': 0.006389617919921875, 'sac_diff5': 0.03219866752624512, 'sac_diff6': 0.0003998279571533203, 'all': 0.06447863578796387}
diff5_list [0.006519794464111328, 0.0061571598052978516, 0.006663084030151367, 0.00656437873840332, 0.00629425048828125]
time3 0
time4 0.06530499458312988
time5 0.06535506248474121
time7 7.152557373046875e-07
gen_weight_change tensor(-18.5900)
policy weight change tensor(36.8840, grad_fn=<SumBackward0>)
time8 0.0018591880798339844
train_time 0.07654452323913574
eval time 0.14977717399597168
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:07,051 MainThread INFO: EPOCH:754
2024-01-23 01:04:07,052 MainThread INFO: Time Consumed:0.22956156730651855s
2024-01-23 01:04:07,052 MainThread INFO: Total Frames:114000s
  8%|▊         | 755/10000 [05:34<39:09,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14495.29450
Train_Epoch_Reward                12379.96270
Running_Training_Average_Rewards  13033.08096
Explore_Time                      0.00085
Train___Time                      0.07654
Eval____Time                      0.14978
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14625.63603
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.19030     2.68504     94.21566     85.79652
alpha_0                           0.68556      0.00010     0.68569      0.68542
Alpha_loss                        -2.54066     0.00297     -2.53676     -2.54580
Training/policy_loss              -3.81886     0.00685     -3.80981     -3.82663
Training/qf1_loss                 7125.40010   1033.11568  8365.20898   5398.94336
Training/qf2_loss                 15819.28184  1528.40826  17816.88086  13256.54492
Training/pf_norm                  0.10369      0.01491     0.12315      0.08859
Training/qf1_norm                 593.85604    281.25166   1143.08118   352.20828
Training/qf2_norm                 1239.28296   36.86727    1295.94922   1180.46875
log_std/mean                      -0.12884     0.00002     -0.12880     -0.12886
log_probs/mean                    -2.73161     0.00802     -2.72127     -2.74343
mean/mean                         -0.00453     0.00003     -0.00450     -0.00459
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0183255672454834
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69006
epoch first part time 3.337860107421875e-06
replay_buffer._size: [114300]
collect time 0.0008440017700195312
inside mustsac before update, task 0, sumup 69006
inside mustsac after update, task 0, sumup 71089
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.007001638412475586, 'sac_diff2': 0.007390499114990234, 'sac_diff3': 0.009901285171508789, 'sac_diff4': 0.0064945220947265625, 'sac_diff5': 0.05173349380493164, 'sac_diff6': 0.0004258155822753906, 'all': 0.08316183090209961}
diff5_list [0.010643243789672852, 0.010082721710205078, 0.011145591735839844, 0.010142087936401367, 0.0097198486328125]
time3 0.0008816719055175781
time4 0.08404064178466797
time5 0.0840919017791748
time7 0.009000301361083984
gen_weight_change tensor(-18.6569)
policy weight change tensor(36.8760, grad_fn=<SumBackward0>)
time8 0.001890420913696289
train_time 0.11322999000549316
eval time 0.11431050300598145
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:07,304 MainThread INFO: EPOCH:755
2024-01-23 01:04:07,304 MainThread INFO: Time Consumed:0.23079633712768555s
2024-01-23 01:04:07,305 MainThread INFO: Total Frames:114150s
  8%|▊         | 756/10000 [05:34<39:06,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14496.77687
Train_Epoch_Reward                10276.43239
Running_Training_Average_Rewards  13005.53498
Explore_Time                      0.00084
Train___Time                      0.11323
Eval____Time                      0.11431
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14631.94175
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.77458     3.09494     94.53969     85.45232
alpha_0                           0.68522      0.00010     0.68535      0.68508
Alpha_loss                        -2.54331     0.00298     -2.53851     -2.54730
Training/policy_loss              -3.74426     0.10385     -3.59569     -3.85951
Training/qf1_loss                 6226.52314   1106.40070  8155.96875   4726.64111
Training/qf2_loss                 14605.12500  1663.25770  17570.12891  12483.58496
Training/pf_norm                  0.11886      0.01893     0.15482      0.10062
Training/qf1_norm                 607.17044    506.88297   1555.74426   131.60808
Training/qf2_norm                 1175.20974   90.12509    1308.95825   1066.36255
log_std/mean                      -0.13436     0.00677     -0.12749     -0.14304
log_probs/mean                    -2.72970     0.00736     -2.71880     -2.73848
mean/mean                         -0.00334     0.00189     -0.00026     -0.00600
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018819093704223633
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 2.384185791015625e-06
replay_buffer._size: [114450]
collect time 0.0009436607360839844
inner_dict_sum {'sac_diff0': 0.00022554397583007812, 'sac_diff1': 0.007219076156616211, 'sac_diff2': 0.00796818733215332, 'sac_diff3': 0.010454654693603516, 'sac_diff4': 0.007002115249633789, 'sac_diff5': 0.033342838287353516, 'sac_diff6': 0.00042366981506347656, 'all': 0.0666360855102539}
diff5_list [0.006780862808227539, 0.007615566253662109, 0.00630640983581543, 0.006223201751708984, 0.006416797637939453]
time3 0
time4 0.06743097305297852
time5 0.06748151779174805
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6569)
policy weight change tensor(36.8569, grad_fn=<SumBackward0>)
time8 0.0019195079803466797
train_time 0.07857871055603027
eval time 0.14927291870117188
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:07,558 MainThread INFO: EPOCH:756
2024-01-23 01:04:07,558 MainThread INFO: Time Consumed:0.2311725616455078s
2024-01-23 01:04:07,558 MainThread INFO: Total Frames:114300s
  8%|▊         | 757/10000 [05:35<39:08,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14502.92630
Train_Epoch_Reward                46002.44543
Running_Training_Average_Rewards  14324.45918
Explore_Time                      0.00093
Train___Time                      0.07858
Eval____Time                      0.14927
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14648.46595
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.90306     1.49574    91.78081     88.12441
alpha_0                           0.68487      0.00010    0.68501      0.68474
Alpha_loss                        -2.54810     0.00406    -2.54007     -2.55128
Training/policy_loss              -4.04464     0.00633    -4.03232     -4.04935
Training/qf1_loss                 6636.13066   629.64513  7752.50146   6123.10059
Training/qf2_loss                 15256.64980  803.83493  16763.85742  14462.32812
Training/pf_norm                  0.13701      0.02095    0.16941      0.10467
Training/qf1_norm                 307.19427    118.54720  490.28262    136.27484
Training/qf2_norm                 1365.59185   23.52902   1396.68921   1339.04858
log_std/mean                      -0.13908     0.00004    -0.13902     -0.13915
log_probs/mean                    -2.73347     0.00904    -2.71580     -2.74009
mean/mean                         -0.00581     0.00005    -0.00573     -0.00586
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01998305320739746
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [114600]
collect time 0.0010051727294921875
inner_dict_sum {'sac_diff0': 0.0002529621124267578, 'sac_diff1': 0.006720066070556641, 'sac_diff2': 0.0074231624603271484, 'sac_diff3': 0.009724140167236328, 'sac_diff4': 0.006356954574584961, 'sac_diff5': 0.031523704528808594, 'sac_diff6': 0.0003814697265625, 'all': 0.06238245964050293}
diff5_list [0.006483793258666992, 0.00630640983581543, 0.006124258041381836, 0.00631403923034668, 0.006295204162597656]
time3 0
time4 0.06312203407287598
time5 0.06316351890563965
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6569)
policy weight change tensor(36.7702, grad_fn=<SumBackward0>)
time8 0.001863241195678711
train_time 0.07401680946350098
eval time 0.15769457817077637
epoch last part time 8.344650268554688e-06
2024-01-23 01:04:07,817 MainThread INFO: EPOCH:757
2024-01-23 01:04:07,817 MainThread INFO: Time Consumed:0.23514342308044434s
2024-01-23 01:04:07,817 MainThread INFO: Total Frames:114450s
  8%|▊         | 758/10000 [05:35<39:17,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14522.52300
Train_Epoch_Reward                8730.60509
Running_Training_Average_Rewards  14352.80445
Explore_Time                      0.00100
Train___Time                      0.07402
Eval____Time                      0.15769
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14693.61081
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.81532     1.97610     95.51694     90.37486
alpha_0                           0.68453      0.00010     0.68467      0.68439
Alpha_loss                        -2.55021     0.00254     -2.54718     -2.55408
Training/policy_loss              -3.81859     0.00537     -3.81398     -3.82876
Training/qf1_loss                 7779.33838   1326.50572  8815.88574   5419.88623
Training/qf2_loss                 16916.63672  1673.05586  18471.54688  14063.65039
Training/pf_norm                  0.12944      0.03074     0.16087      0.07916
Training/qf1_norm                 920.76552    380.13436   1437.58630   432.56836
Training/qf2_norm                 1303.83635   27.74818    1342.43103   1271.03210
log_std/mean                      -0.13813     0.00020     -0.13786     -0.13840
log_probs/mean                    -2.73016     0.00641     -2.72039     -2.74038
mean/mean                         -0.00316     0.00004     -0.00311     -0.00322
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0187070369720459
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [114750]
collect time 0.0008604526519775391
inner_dict_sum {'sac_diff0': 0.000217437744140625, 'sac_diff1': 0.006783246994018555, 'sac_diff2': 0.007233142852783203, 'sac_diff3': 0.00988459587097168, 'sac_diff4': 0.006273984909057617, 'sac_diff5': 0.03155016899108887, 'sac_diff6': 0.0003955364227294922, 'all': 0.06233811378479004}
diff5_list [0.0065593719482421875, 0.006110429763793945, 0.006369113922119141, 0.006462574005126953, 0.006048679351806641]
time3 0
time4 0.06306147575378418
time5 0.06310248374938965
time7 4.76837158203125e-07
gen_weight_change tensor(-18.6569)
policy weight change tensor(36.7808, grad_fn=<SumBackward0>)
time8 0.0018126964569091797
train_time 0.07392597198486328
eval time 0.14896368980407715
epoch last part time 7.867813110351562e-06
2024-01-23 01:04:08,065 MainThread INFO: EPOCH:758
2024-01-23 01:04:08,065 MainThread INFO: Time Consumed:0.22615385055541992s
2024-01-23 01:04:08,065 MainThread INFO: Total Frames:114600s
  8%|▊         | 759/10000 [05:35<38:59,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14576.17734
Train_Epoch_Reward                27545.67487
Running_Training_Average_Rewards  14805.49975
Explore_Time                      0.00086
Train___Time                      0.07393
Eval____Time                      0.14896
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14876.11715
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.95730     1.50413    92.40226     88.28430
alpha_0                           0.68419      0.00010    0.68432      0.68405
Alpha_loss                        -2.55600     0.00151    -2.55303     -2.55721
Training/policy_loss              -3.85574     0.00392    -3.85136     -3.86303
Training/qf1_loss                 6521.15811   686.22880  7228.43848   5240.97998
Training/qf2_loss                 15132.69746  869.11070  16328.58789  13612.98242
Training/pf_norm                  0.17173      0.01305    0.18605      0.14917
Training/qf1_norm                 666.87380    291.33937  996.93884    216.28687
Training/qf2_norm                 1271.64978   19.18496   1298.19214   1247.40210
log_std/mean                      -0.12921     0.00006    -0.12916     -0.12931
log_probs/mean                    -2.73655     0.00380    -2.73051     -2.74243
mean/mean                         -0.00368     0.00036    -0.00327     -0.00427
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01905369758605957
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [114900]
collect time 0.0008699893951416016
inner_dict_sum {'sac_diff0': 0.00025177001953125, 'sac_diff1': 0.007314205169677734, 'sac_diff2': 0.00765681266784668, 'sac_diff3': 0.010170459747314453, 'sac_diff4': 0.00645136833190918, 'sac_diff5': 0.032305240631103516, 'sac_diff6': 0.00040030479431152344, 'all': 0.06455016136169434}
diff5_list [0.0064868927001953125, 0.006341218948364258, 0.006908416748046875, 0.006513357162475586, 0.006055355072021484]
time3 0
time4 0.06535696983337402
time5 0.06540870666503906
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6569)
policy weight change tensor(36.8323, grad_fn=<SumBackward0>)
time8 0.0018506050109863281
train_time 0.07655501365661621
eval time 0.14426183700561523
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:08,312 MainThread INFO: EPOCH:759
2024-01-23 01:04:08,312 MainThread INFO: Time Consumed:0.22424936294555664s
2024-01-23 01:04:08,312 MainThread INFO: Total Frames:114750s
  8%|▊         | 760/10000 [05:35<38:40,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14656.36773
Train_Epoch_Reward                11632.57511
Running_Training_Average_Rewards  14832.74621
Explore_Time                      0.00086
Train___Time                      0.07656
Eval____Time                      0.14426
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15121.61152
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.64920     2.01508     93.99559     88.19895
alpha_0                           0.68385      0.00010     0.68398      0.68371
Alpha_loss                        -2.55819     0.00300     -2.55408     -2.56253
Training/policy_loss              -4.01737     0.00368     -4.01143     -4.02238
Training/qf1_loss                 6858.28760   1138.53229  8854.37793   5747.02539
Training/qf2_loss                 15524.06836  1432.40791  18116.27148  14128.98145
Training/pf_norm                  0.13387      0.02395     0.16313      0.10323
Training/qf1_norm                 1398.77142   385.17824   2055.51807   965.29608
Training/qf2_norm                 1339.90186   28.77654    1387.64221   1304.10339
log_std/mean                      -0.13319     0.00007     -0.13310     -0.13327
log_probs/mean                    -2.73345     0.00667     -2.72263     -2.74312
mean/mean                         -0.00285     0.00033     -0.00236     -0.00327
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0185391902923584
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71089
epoch first part time 3.337860107421875e-06
replay_buffer._size: [115050]
collect time 0.0008831024169921875
inside mustsac before update, task 0, sumup 71089
inside mustsac after update, task 0, sumup 69789
inner_dict_sum {'sac_diff0': 0.00021958351135253906, 'sac_diff1': 0.0072176456451416016, 'sac_diff2': 0.007524967193603516, 'sac_diff3': 0.010314702987670898, 'sac_diff4': 0.0066912174224853516, 'sac_diff5': 0.05309629440307617, 'sac_diff6': 0.0004012584686279297, 'all': 0.08546566963195801}
diff5_list [0.011638641357421875, 0.010179996490478516, 0.010461807250976562, 0.010299205780029297, 0.010516643524169922]
time3 0.0008633136749267578
time4 0.08629441261291504
time5 0.08634567260742188
time7 0.008860349655151367
gen_weight_change tensor(-18.7943)
policy weight change tensor(36.8509, grad_fn=<SumBackward0>)
time8 0.0027115345001220703
train_time 0.11646294593811035
eval time 0.10861682891845703
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:08,562 MainThread INFO: EPOCH:760
2024-01-23 01:04:08,563 MainThread INFO: Time Consumed:0.22835063934326172s
2024-01-23 01:04:08,563 MainThread INFO: Total Frames:114900s
  8%|▊         | 761/10000 [05:36<38:47,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14761.37158
Train_Epoch_Reward                17868.83711
Running_Training_Average_Rewards  15037.84653
Explore_Time                      0.00088
Train___Time                      0.11646
Eval____Time                      0.10862
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15415.77078
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.41220     0.94319     89.38993     86.76218
alpha_0                           0.68350      0.00010     0.68364      0.68337
Alpha_loss                        -2.56033     0.00257     -2.55570     -2.56260
Training/policy_loss              -3.80834     0.14237     -3.67528     -4.07394
Training/qf1_loss                 6361.59111   836.32404   6901.45312   4721.06250
Training/qf2_loss                 14669.41230  1001.83801  15380.74707  12716.44727
Training/pf_norm                  0.12694      0.01930     0.15137      0.10169
Training/qf1_norm                 860.79456    485.75625   1621.80383   168.02608
Training/qf2_norm                 1195.86870   71.97721    1325.93201   1127.08508
log_std/mean                      -0.12897     0.00840     -0.11665     -0.14304
log_probs/mean                    -2.73023     0.00896     -2.71452     -2.73796
mean/mean                         -0.00556     0.00136     -0.00370     -0.00711
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01852869987487793
epoch last part time3 0.00289154052734375
inside rlalgo, task 0, sumup 69789
epoch first part time 2.86102294921875e-06
replay_buffer._size: [115200]
collect time 0.0009922981262207031
inner_dict_sum {'sac_diff0': 0.00024008750915527344, 'sac_diff1': 0.007017374038696289, 'sac_diff2': 0.007616519927978516, 'sac_diff3': 0.010202884674072266, 'sac_diff4': 0.006444215774536133, 'sac_diff5': 0.03218984603881836, 'sac_diff6': 0.0003921985626220703, 'all': 0.0641031265258789}
diff5_list [0.007049560546875, 0.006295919418334961, 0.0061342716217041016, 0.006521940231323242, 0.006188154220581055]
time3 0
time4 0.06484842300415039
time5 0.06489205360412598
time7 7.152557373046875e-07
gen_weight_change tensor(-18.7943)
policy weight change tensor(36.9426, grad_fn=<SumBackward0>)
time8 0.0018458366394042969
train_time 0.07614278793334961
eval time 0.15241765975952148
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:08,819 MainThread INFO: EPOCH:761
2024-01-23 01:04:08,819 MainThread INFO: Time Consumed:0.23198866844177246s
2024-01-23 01:04:08,820 MainThread INFO: Total Frames:115050s
  8%|▊         | 762/10000 [05:36<38:52,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14799.43550
Train_Epoch_Reward                11574.97063
Running_Training_Average_Rewards  14788.01742
Explore_Time                      0.00099
Train___Time                      0.07614
Eval____Time                      0.15242
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14820.13613
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.55088     1.15996     92.63020     89.49562
alpha_0                           0.68316      0.00010     0.68330      0.68303
Alpha_loss                        -2.56500     0.00266     -2.56216     -2.56941
Training/policy_loss              -3.78178     0.00688     -3.77544     -3.79203
Training/qf1_loss                 7177.62988   869.33218   8208.55957   6059.90088
Training/qf2_loss                 15875.07832  1083.74563  17329.48242  14529.44336
Training/pf_norm                  0.14125      0.02566     0.17493      0.10729
Training/qf1_norm                 1090.65878   255.42141   1313.17700   621.54376
Training/qf2_norm                 1209.83542   14.74591    1236.59241   1195.16516
log_std/mean                      -0.12719     0.00011     -0.12709     -0.12739
log_probs/mean                    -2.73365     0.00644     -2.72797     -2.74170
mean/mean                         -0.00205     0.00004     -0.00199     -0.00209
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018494844436645508
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69789
epoch first part time 2.86102294921875e-06
replay_buffer._size: [115350]
collect time 0.0008590221405029297
inner_dict_sum {'sac_diff0': 0.0002167224884033203, 'sac_diff1': 0.006863832473754883, 'sac_diff2': 0.007546424865722656, 'sac_diff3': 0.010305404663085938, 'sac_diff4': 0.006767749786376953, 'sac_diff5': 0.03371572494506836, 'sac_diff6': 0.00040841102600097656, 'all': 0.06582427024841309}
diff5_list [0.006556034088134766, 0.007033586502075195, 0.006456851959228516, 0.006472587585449219, 0.007196664810180664]
time3 0
time4 0.0666661262512207
time5 0.06671404838562012
time7 1.1920928955078125e-06
gen_weight_change tensor(-18.7943)
policy weight change tensor(37.0229, grad_fn=<SumBackward0>)
time8 0.0019826889038085938
train_time 0.0777900218963623
eval time 0.15460777282714844
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:09,077 MainThread INFO: EPOCH:762
2024-01-23 01:04:09,077 MainThread INFO: Time Consumed:0.2356259822845459s
2024-01-23 01:04:09,077 MainThread INFO: Total Frames:115200s
  8%|▊         | 763/10000 [05:36<39:06,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14862.35464
Train_Epoch_Reward                13220.58331
Running_Training_Average_Rewards  14334.74181
Explore_Time                      0.00085
Train___Time                      0.07779
Eval____Time                      0.15461
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15150.65429
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.75027     2.21117     93.27077     86.56618
alpha_0                           0.68282      0.00010     0.68296      0.68268
Alpha_loss                        -2.56903     0.00410     -2.56268     -2.57417
Training/policy_loss              -3.93789     0.00413     -3.93109     -3.94349
Training/qf1_loss                 6609.18877   650.51715   7429.04590   5649.08594
Training/qf2_loss                 15157.03125  1027.03420  16596.89844  13632.33398
Training/pf_norm                  0.11075      0.02255     0.13941      0.07180
Training/qf1_norm                 909.88528    379.55157   1498.70129   341.65491
Training/qf2_norm                 1300.78965   31.96148    1350.72864   1253.65491
log_std/mean                      -0.13424     0.00005     -0.13417     -0.13431
log_probs/mean                    -2.73540     0.00845     -2.72228     -2.74712
mean/mean                         -0.00186     0.00001     -0.00185     -0.00187
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018407583236694336
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69789
epoch first part time 2.86102294921875e-06
replay_buffer._size: [115500]
collect time 0.0008544921875
inner_dict_sum {'sac_diff0': 0.0002307891845703125, 'sac_diff1': 0.006963014602661133, 'sac_diff2': 0.0075151920318603516, 'sac_diff3': 0.009955406188964844, 'sac_diff4': 0.0063953399658203125, 'sac_diff5': 0.033358097076416016, 'sac_diff6': 0.0003883838653564453, 'all': 0.06480622291564941}
diff5_list [0.007653713226318359, 0.006723880767822266, 0.006580352783203125, 0.006226778030395508, 0.006173372268676758]
time3 0
time4 0.0655670166015625
time5 0.0656130313873291
time7 4.76837158203125e-07
gen_weight_change tensor(-18.7943)
policy weight change tensor(37.0623, grad_fn=<SumBackward0>)
time8 0.0018570423126220703
train_time 0.07693719863891602
eval time 0.14563870429992676
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:09,325 MainThread INFO: EPOCH:763
2024-01-23 01:04:09,325 MainThread INFO: Time Consumed:0.2259058952331543s
2024-01-23 01:04:09,325 MainThread INFO: Total Frames:115350s
  8%|▊         | 764/10000 [05:36<38:50,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14934.95380
Train_Epoch_Reward                15005.27508
Running_Training_Average_Rewards  14056.34519
Explore_Time                      0.00085
Train___Time                      0.07694
Eval____Time                      0.14564
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15365.59355
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.76462     3.01537     93.61155     84.99597
alpha_0                           0.68248      0.00010     0.68262      0.68234
Alpha_loss                        -2.57281     0.00254     -2.56948     -2.57630
Training/policy_loss              -3.88178     0.00460     -3.87592     -3.88704
Training/qf1_loss                 6678.26582   1026.22505  7925.47559   5314.67676
Training/qf2_loss                 15045.75039  1527.29584  16953.28516  13017.37402
Training/pf_norm                  0.16559      0.03160     0.19446      0.10841
Training/qf1_norm                 1292.59537   574.08188   2185.26343   559.90094
Training/qf2_norm                 1248.41951   41.79450    1315.52417   1195.54443
log_std/mean                      -0.12861     0.00006     -0.12851     -0.12866
log_probs/mean                    -2.73647     0.00556     -2.73116     -2.74386
mean/mean                         -0.00561     0.00038     -0.00514     -0.00621
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018933773040771484
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69789
epoch first part time 2.86102294921875e-06
replay_buffer._size: [115650]
collect time 0.00091552734375
inner_dict_sum {'sac_diff0': 0.00022673606872558594, 'sac_diff1': 0.007131814956665039, 'sac_diff2': 0.007634639739990234, 'sac_diff3': 0.009980201721191406, 'sac_diff4': 0.0065708160400390625, 'sac_diff5': 0.032698869705200195, 'sac_diff6': 0.00041413307189941406, 'all': 0.06465721130371094}
diff5_list [0.0066187381744384766, 0.006247997283935547, 0.006119251251220703, 0.006340980529785156, 0.0073719024658203125]
time3 0
time4 0.06548190116882324
time5 0.06552958488464355
time7 9.5367431640625e-07
gen_weight_change tensor(-18.7943)
policy weight change tensor(37.1183, grad_fn=<SumBackward0>)
time8 0.002087831497192383
train_time 0.07726263999938965
eval time 0.15208053588867188
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:09,580 MainThread INFO: EPOCH:764
2024-01-23 01:04:09,581 MainThread INFO: Time Consumed:0.23266005516052246s
2024-01-23 01:04:09,581 MainThread INFO: Total Frames:115500s
  8%|▊         | 765/10000 [05:37<38:57,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15006.72961
Train_Epoch_Reward                42275.06708
Running_Training_Average_Rewards  15170.09208
Explore_Time                      0.00091
Train___Time                      0.07726
Eval____Time                      0.15208
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15343.39413
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.93287     2.17889    93.61066     87.74645
alpha_0                           0.68214      0.00010    0.68228      0.68200
Alpha_loss                        -2.57476     0.00244    -2.57247     -2.57869
Training/policy_loss              -3.85469     0.00319    -3.85061     -3.85925
Training/qf1_loss                 6884.97852   454.67136  7560.95605   6317.71387
Training/qf2_loss                 15473.69316  849.66784  16544.32422  14467.33105
Training/pf_norm                  0.09387      0.03147    0.12603      0.04491
Training/qf1_norm                 1157.26468   434.49293  1578.88843   440.50473
Training/qf2_norm                 1274.46985   30.85402   1326.25500   1243.26379
log_std/mean                      -0.13111     0.00011    -0.13099     -0.13129
log_probs/mean                    -2.73278     0.00475    -2.72767     -2.73951
mean/mean                         -0.00703     0.00027    -0.00662     -0.00738
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018551111221313477
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69789
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [115800]
collect time 0.0008485317230224609
inside mustsac before update, task 0, sumup 69789
inside mustsac after update, task 0, sumup 70591
inner_dict_sum {'sac_diff0': 0.0002186298370361328, 'sac_diff1': 0.007013559341430664, 'sac_diff2': 0.007681131362915039, 'sac_diff3': 0.010067462921142578, 'sac_diff4': 0.006790637969970703, 'sac_diff5': 0.05301499366760254, 'sac_diff6': 0.0004210472106933594, 'all': 0.08520746231079102}
diff5_list [0.011313676834106445, 0.011557340621948242, 0.01015162467956543, 0.009914875030517578, 0.010077476501464844]
time3 0.0008978843688964844
time4 0.0861060619354248
time5 0.08617043495178223
time7 0.009151458740234375
gen_weight_change tensor(-18.9171)
policy weight change tensor(37.0811, grad_fn=<SumBackward0>)
time8 0.0018680095672607422
train_time 0.1163935661315918
eval time 0.1110224723815918
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:09,833 MainThread INFO: EPOCH:765
2024-01-23 01:04:09,833 MainThread INFO: Time Consumed:0.2306528091430664s
2024-01-23 01:04:09,834 MainThread INFO: Total Frames:115650s
  8%|▊         | 766/10000 [05:37<38:57,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15066.64252
Train_Epoch_Reward                18215.42033
Running_Training_Average_Rewards  15041.75001
Explore_Time                      0.00084
Train___Time                      0.11639
Eval____Time                      0.11102
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15231.07093
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.83099     3.74536     96.13402     85.53901
alpha_0                           0.68180      0.00010     0.68193      0.68166
Alpha_loss                        -2.57691     0.00300     -2.57227     -2.58003
Training/policy_loss              -3.79013     0.14919     -3.60508     -3.99058
Training/qf1_loss                 6256.76113   928.06678   7593.03369   4949.30371
Training/qf2_loss                 15027.47051  1540.21132  17323.13867  13181.55664
Training/pf_norm                  0.11437      0.02570     0.14433      0.07508
Training/qf1_norm                 742.86254    616.90283   1918.65173   173.09340
Training/qf2_norm                 1242.09033   67.78665    1351.64758   1156.75513
log_std/mean                      -0.13451     0.00618     -0.12601     -0.14511
log_probs/mean                    -2.72958     0.00551     -2.72097     -2.73599
mean/mean                         -0.00717     0.00191     -0.00441     -0.01007
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018893957138061523
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [115950]
collect time 0.0008490085601806641
inner_dict_sum {'sac_diff0': 0.00022459030151367188, 'sac_diff1': 0.007525920867919922, 'sac_diff2': 0.008526086807250977, 'sac_diff3': 0.01073908805847168, 'sac_diff4': 0.007439374923706055, 'sac_diff5': 0.03404879570007324, 'sac_diff6': 0.0004062652587890625, 'all': 0.06891012191772461}
diff5_list [0.006787538528442383, 0.0074579715728759766, 0.0065402984619140625, 0.006516695022583008, 0.0067462921142578125]
time3 0
time4 0.06972908973693848
time5 0.06978321075439453
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9171)
policy weight change tensor(37.1211, grad_fn=<SumBackward0>)
time8 0.001969575881958008
train_time 0.0810098648071289
eval time 0.1443939208984375
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:10,084 MainThread INFO: EPOCH:766
2024-01-23 01:04:10,084 MainThread INFO: Time Consumed:0.22859907150268555s
2024-01-23 01:04:10,084 MainThread INFO: Total Frames:115800s
  8%|▊         | 767/10000 [05:37<38:50,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15184.90372
Train_Epoch_Reward                17721.19827
Running_Training_Average_Rewards  15379.51889
Explore_Time                      0.00084
Train___Time                      0.08101
Eval____Time                      0.14439
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15831.07792
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.43743     2.42639     91.69921     85.22206
alpha_0                           0.68146      0.00010     0.68159      0.68132
Alpha_loss                        -2.58303     0.00111     -2.58114     -2.58430
Training/policy_loss              -3.84722     0.00292     -3.84368     -3.85164
Training/qf1_loss                 6851.82217   875.49026   8066.67285   5492.16064
Training/qf2_loss                 15153.63770  1362.58238  17050.75195  13130.48926
Training/pf_norm                  0.14766      0.02284     0.17453      0.10585
Training/qf1_norm                 1496.96990   531.91323   2206.91040   802.32629
Training/qf2_norm                 1289.02813   34.08943    1334.67981   1243.66833
log_std/mean                      -0.12451     0.00006     -0.12442     -0.12459
log_probs/mean                    -2.73677     0.00405     -2.73186     -2.74360
mean/mean                         -0.00899     0.00041     -0.00848     -0.00964
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01858973503112793
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 2.86102294921875e-06
replay_buffer._size: [116100]
collect time 0.0009009838104248047
inner_dict_sum {'sac_diff0': 0.00022530555725097656, 'sac_diff1': 0.007415056228637695, 'sac_diff2': 0.008829116821289062, 'sac_diff3': 0.01099538803100586, 'sac_diff4': 0.007323741912841797, 'sac_diff5': 0.0325932502746582, 'sac_diff6': 0.0004134178161621094, 'all': 0.0677952766418457}
diff5_list [0.007756948471069336, 0.006295919418334961, 0.0062448978424072266, 0.006083250045776367, 0.0062122344970703125]
time3 0
time4 0.06860017776489258
time5 0.06865048408508301
time7 4.76837158203125e-07
gen_weight_change tensor(-18.9171)
policy weight change tensor(37.1221, grad_fn=<SumBackward0>)
time8 0.001958608627319336
train_time 0.08006477355957031
eval time 0.1490764617919922
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:10,339 MainThread INFO: EPOCH:767
2024-01-23 01:04:10,339 MainThread INFO: Time Consumed:0.2325141429901123s
2024-01-23 01:04:10,339 MainThread INFO: Total Frames:115950s
  8%|▊         | 768/10000 [05:37<38:58,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15279.34426
Train_Epoch_Reward                24591.88145
Running_Training_Average_Rewards  15851.73619
Explore_Time                      0.00090
Train___Time                      0.08006
Eval____Time                      0.14908
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15638.01617
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.21914     2.28397    94.97841     88.68212
alpha_0                           0.68112      0.00010    0.68125      0.68098
Alpha_loss                        -2.58600     0.00106    -2.58441     -2.58747
Training/policy_loss              -4.09792     0.00190    -4.09478     -4.10015
Training/qf1_loss                 7676.36855   622.32327  8664.51172   6802.53027
Training/qf2_loss                 16705.38457  979.69934  18222.11719  15577.55762
Training/pf_norm                  0.13757      0.02591    0.18094      0.10585
Training/qf1_norm                 471.02866    243.45435  772.03619    122.68559
Training/qf2_norm                 1403.49346   33.96481   1443.46777   1349.18250
log_std/mean                      -0.13671     0.00003    -0.13666     -0.13674
log_probs/mean                    -2.73573     0.00399    -2.73204     -2.74307
mean/mean                         -0.00514     0.00045    -0.00451     -0.00578
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019348621368408203
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [116250]
collect time 0.0009016990661621094
inner_dict_sum {'sac_diff0': 0.0002257823944091797, 'sac_diff1': 0.008243083953857422, 'sac_diff2': 0.010397672653198242, 'sac_diff3': 0.011541366577148438, 'sac_diff4': 0.00791621208190918, 'sac_diff5': 0.03884148597717285, 'sac_diff6': 0.00048351287841796875, 'all': 0.07764911651611328}
diff5_list [0.006615400314331055, 0.011153459548950195, 0.00853872299194336, 0.00632166862487793, 0.0062122344970703125]
time3 0
time4 0.0786433219909668
time5 0.07870888710021973
time7 7.152557373046875e-07
gen_weight_change tensor(-18.9171)
policy weight change tensor(37.0027, grad_fn=<SumBackward0>)
time8 0.0022687911987304688
train_time 0.09102201461791992
eval time 0.1351003646850586
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:10,592 MainThread INFO: EPOCH:768
2024-01-23 01:04:10,592 MainThread INFO: Time Consumed:0.2297074794769287s
2024-01-23 01:04:10,592 MainThread INFO: Total Frames:116100s
  8%|▊         | 769/10000 [05:38<38:58,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15336.25020
Train_Epoch_Reward                12086.24604
Running_Training_Average_Rewards  16174.30248
Explore_Time                      0.00090
Train___Time                      0.09102
Eval____Time                      0.13510
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15445.17654
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       88.69049     1.29795    90.37005     86.87422
alpha_0                           0.68078      0.00010    0.68091      0.68064
Alpha_loss                        -2.58691     0.00263    -2.58271     -2.59015
Training/policy_loss              -3.93163     0.00590    -3.92338     -3.93819
Training/qf1_loss                 6272.66406   512.29217  7189.03271   5636.75293
Training/qf2_loss                 14657.13496  639.69388  15683.40820  13693.72070
Training/pf_norm                  0.10485      0.03262    0.13918      0.05531
Training/qf1_norm                 848.16665    223.79506  1163.81299   543.15521
Training/qf2_norm                 1309.54089   18.60266   1333.69995   1283.52319
log_std/mean                      -0.13980     0.00029    -0.13938     -0.14020
log_probs/mean                    -2.72935     0.00778    -2.71841     -2.73815
mean/mean                         -0.00766     0.00032    -0.00721     -0.00811
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01988387107849121
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [116400]
collect time 0.00098419189453125
inner_dict_sum {'sac_diff0': 0.00022721290588378906, 'sac_diff1': 0.006876707077026367, 'sac_diff2': 0.008398056030273438, 'sac_diff3': 0.010684728622436523, 'sac_diff4': 0.007009267807006836, 'sac_diff5': 0.03210759162902832, 'sac_diff6': 0.00039458274841308594, 'all': 0.06569814682006836}
diff5_list [0.007063150405883789, 0.006347179412841797, 0.0062444210052490234, 0.006269693374633789, 0.006183147430419922]
time3 0
time4 0.06649279594421387
time5 0.06654143333435059
time7 9.5367431640625e-07
gen_weight_change tensor(-18.9171)
policy weight change tensor(36.8842, grad_fn=<SumBackward0>)
time8 0.0019292831420898438
train_time 0.07829761505126953
eval time 0.15088796615600586
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:10,848 MainThread INFO: EPOCH:769
2024-01-23 01:04:10,848 MainThread INFO: Time Consumed:0.2325756549835205s
2024-01-23 01:04:10,848 MainThread INFO: Total Frames:116250s
  8%|▊         | 770/10000 [05:38<39:02,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15356.80440
Train_Epoch_Reward                14860.88835
Running_Training_Average_Rewards  16592.92400
Explore_Time                      0.00098
Train___Time                      0.07830
Eval____Time                      0.15089
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15327.15355
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.22431     2.14443     91.09155     84.79724
alpha_0                           0.68044      0.00010     0.68057      0.68030
Alpha_loss                        -2.59311     0.00194     -2.59066     -2.59622
Training/policy_loss              -4.03316     0.00302     -4.02957     -4.03769
Training/qf1_loss                 6716.12334   1099.41016  8696.72754   5649.05908
Training/qf2_loss                 15036.27090  1360.89701  17331.06250  13321.22656
Training/pf_norm                  0.13478      0.05543     0.22463      0.07165
Training/qf1_norm                 391.30503    200.98825   714.87781    184.10643
Training/qf2_norm                 1344.16702   31.89678    1387.86487   1294.01807
log_std/mean                      -0.13688     0.00018     -0.13664     -0.13716
log_probs/mean                    -2.73670     0.00514     -2.73036     -2.74303
mean/mean                         -0.01294     0.00008     -0.01281     -0.01302
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018781661987304688
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70591
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [116550]
collect time 0.0008702278137207031
inside mustsac before update, task 0, sumup 70591
inside mustsac after update, task 0, sumup 70829
inner_dict_sum {'sac_diff0': 0.0002262592315673828, 'sac_diff1': 0.0073964595794677734, 'sac_diff2': 0.009101152420043945, 'sac_diff3': 0.011391878128051758, 'sac_diff4': 0.008021116256713867, 'sac_diff5': 0.05586123466491699, 'sac_diff6': 0.0004112720489501953, 'all': 0.09240937232971191}
diff5_list [0.011475324630737305, 0.010508298873901367, 0.012607097625732422, 0.010798931121826172, 0.010471582412719727]
time3 0.0009126663208007812
time4 0.0932765007019043
time5 0.09332799911499023
time7 0.009263992309570312
gen_weight_change tensor(-18.9503)
policy weight change tensor(36.8180, grad_fn=<SumBackward0>)
time8 0.0027196407318115234
train_time 0.1241145133972168
eval time 0.10225272178649902
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:11,100 MainThread INFO: EPOCH:770
2024-01-23 01:04:11,100 MainThread INFO: Time Consumed:0.22969412803649902s
2024-01-23 01:04:11,100 MainThread INFO: Total Frames:116400s
  8%|▊         | 771/10000 [05:38<39:06,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15347.51264
Train_Epoch_Reward                18919.57864
Running_Training_Average_Rewards  16904.42991
Explore_Time                      0.00087
Train___Time                      0.12411
Eval____Time                      0.10225
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15322.85315
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.60383     2.72790    95.26521     88.22275
alpha_0                           0.68010      0.00010    0.68023      0.67996
Alpha_loss                        -2.59481     0.00175    -2.59288     -2.59808
Training/policy_loss              -3.96938     0.04683    -3.90394     -4.02390
Training/qf1_loss                 6724.28213   400.77860  7308.75098   6104.89355
Training/qf2_loss                 15467.13594  879.51180  16957.10352  14583.53711
Training/pf_norm                  0.10108      0.01309    0.11466      0.08560
Training/qf1_norm                 777.26269    622.29227  1885.64709   128.71303
Training/qf2_norm                 1319.04661   33.11956   1370.89258   1275.96399
log_std/mean                      -0.13064     0.00468    -0.12501     -0.13620
log_probs/mean                    -2.73237     0.00528    -2.72740     -2.74262
mean/mean                         -0.01138     0.00134    -0.00915     -0.01314
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01902914047241211
epoch last part time3 0.0028221607208251953
inside rlalgo, task 0, sumup 70829
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [116700]
collect time 0.0009381771087646484
inner_dict_sum {'sac_diff0': 0.0002181529998779297, 'sac_diff1': 0.0073473453521728516, 'sac_diff2': 0.008743047714233398, 'sac_diff3': 0.01101994514465332, 'sac_diff4': 0.007090330123901367, 'sac_diff5': 0.03317546844482422, 'sac_diff6': 0.00039005279541015625, 'all': 0.06798434257507324}
diff5_list [0.007750988006591797, 0.006915569305419922, 0.0063915252685546875, 0.005960941314697266, 0.006156444549560547]
time3 0
time4 0.0687706470489502
time5 0.06882166862487793
time7 9.5367431640625e-07
gen_weight_change tensor(-18.9503)
policy weight change tensor(36.7156, grad_fn=<SumBackward0>)
time8 0.0018360614776611328
train_time 0.08003044128417969
eval time 0.1436784267425537
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:11,352 MainThread INFO: EPOCH:771
2024-01-23 01:04:11,352 MainThread INFO: Time Consumed:0.22713494300842285s
2024-01-23 01:04:11,352 MainThread INFO: Total Frames:116550s
  8%|▊         | 772/10000 [05:38<38:54,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15283.07049
Train_Epoch_Reward                22886.36077
Running_Training_Average_Rewards  17581.90010
Explore_Time                      0.00093
Train___Time                      0.08003
Eval____Time                      0.14368
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14175.71463
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.33742     2.56787     94.44657     87.45549
alpha_0                           0.67976      0.00010     0.67989      0.67962
Alpha_loss                        -2.59863     0.00250     -2.59568     -2.60316
Training/policy_loss              -3.96890     0.00403     -3.96605     -3.97679
Training/qf1_loss                 6719.43818   912.82850   7784.52051   5180.26562
Training/qf2_loss                 15390.44922  1307.10159  16933.67773  13612.13477
Training/pf_norm                  0.08005      0.01125     0.09394      0.06509
Training/qf1_norm                 1014.15387   500.08754   1609.20007   247.00069
Training/qf2_norm                 1327.21294   37.47328    1387.31262   1284.64941
log_std/mean                      -0.13105     0.00013     -0.13091     -0.13126
log_probs/mean                    -2.73355     0.00616     -2.72767     -2.74529
mean/mean                         -0.01285     0.00002     -0.01281     -0.01288
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019929170608520508
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70829
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [116850]
collect time 0.0009970664978027344
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.007232666015625, 'sac_diff2': 0.00891876220703125, 'sac_diff3': 0.010895252227783203, 'sac_diff4': 0.007227897644042969, 'sac_diff5': 0.03342914581298828, 'sac_diff6': 0.00039505958557128906, 'all': 0.06831979751586914}
diff5_list [0.0075533390045166016, 0.00637054443359375, 0.006880521774291992, 0.006344795227050781, 0.006279945373535156]
time3 0
time4 0.06909751892089844
time5 0.06914496421813965
time7 4.76837158203125e-07
gen_weight_change tensor(-18.9503)
policy weight change tensor(36.5844, grad_fn=<SumBackward0>)
time8 0.0018460750579833984
train_time 0.08046627044677734
eval time 0.14535188674926758
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:11,605 MainThread INFO: EPOCH:772
2024-01-23 01:04:11,605 MainThread INFO: Time Consumed:0.22919368743896484s
2024-01-23 01:04:11,605 MainThread INFO: Total Frames:116700s
  8%|▊         | 773/10000 [05:39<38:50,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15187.50466
Train_Epoch_Reward                2204.85953
Running_Training_Average_Rewards  17249.19583
Explore_Time                      0.00099
Train___Time                      0.08047
Eval____Time                      0.14535
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14194.99599
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.23542     3.09910     96.45819     87.68974
alpha_0                           0.67942      0.00010     0.67955      0.67928
Alpha_loss                        -2.60399     0.00300     -2.60017     -2.60885
Training/policy_loss              -4.00404     0.00572     -3.99888     -4.01519
Training/qf1_loss                 7110.69326   1680.08152  9465.16992   4590.00391
Training/qf2_loss                 15965.94180  2257.16261  19338.40625  12730.65625
Training/pf_norm                  0.15328      0.01602     0.17681      0.13458
Training/qf1_norm                 542.48583    334.33441   1058.53601   112.96870
Training/qf2_norm                 1351.22542   46.09852    1428.75110   1297.73413
log_std/mean                      -0.13181     0.00027     -0.13140     -0.13218
log_probs/mean                    -2.73871     0.00834     -2.72709     -2.75305
mean/mean                         -0.01188     0.00009     -0.01173     -0.01201
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018497467041015625
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70829
epoch first part time 2.86102294921875e-06
replay_buffer._size: [117000]
collect time 0.0008740425109863281
inner_dict_sum {'sac_diff0': 0.00022673606872558594, 'sac_diff1': 0.007650613784790039, 'sac_diff2': 0.008839130401611328, 'sac_diff3': 0.011118412017822266, 'sac_diff4': 0.007568836212158203, 'sac_diff5': 0.03364872932434082, 'sac_diff6': 0.0004076957702636719, 'all': 0.06946015357971191}
diff5_list [0.007019519805908203, 0.006283998489379883, 0.006340980529785156, 0.0075664520263671875, 0.006437778472900391]
time3 0
time4 0.07028031349182129
time5 0.07033205032348633
time7 4.76837158203125e-07
gen_weight_change tensor(-18.9503)
policy weight change tensor(36.4652, grad_fn=<SumBackward0>)
time8 0.0018694400787353516
train_time 0.08153843879699707
eval time 0.14547395706176758
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:11,857 MainThread INFO: EPOCH:773
2024-01-23 01:04:11,858 MainThread INFO: Time Consumed:0.23029494285583496s
2024-01-23 01:04:11,858 MainThread INFO: Total Frames:116850s
  8%|▊         | 774/10000 [05:39<38:53,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15075.58864
Train_Epoch_Reward                14104.17347
Running_Training_Average_Rewards  17343.52111
Explore_Time                      0.00087
Train___Time                      0.08154
Eval____Time                      0.14547
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14246.43336
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       87.81215     1.57175     89.30218     85.89541
alpha_0                           0.67908      0.00010     0.67921      0.67894
Alpha_loss                        -2.60546     0.00088     -2.60421     -2.60682
Training/policy_loss              -3.52464     0.00208     -3.52166     -3.52704
Training/qf1_loss                 6187.77236   828.40139   7705.84668   5259.61182
Training/qf2_loss                 14432.46836  1077.77919  16238.52148  13127.09375
Training/pf_norm                  0.12081      0.01597     0.14113      0.10132
Training/qf1_norm                 309.80316    219.03800   581.54108    116.32690
Training/qf2_norm                 1062.86643   19.74606    1080.77808   1038.23535
log_std/mean                      -0.12561     0.00018     -0.12535     -0.12585
log_probs/mean                    -2.73381     0.00294     -2.73056     -2.73863
mean/mean                         -0.01258     0.00007     -0.01252     -0.01271
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01953291893005371
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70829
epoch first part time 2.86102294921875e-06
replay_buffer._size: [117150]
collect time 0.0008757114410400391
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.007483959197998047, 'sac_diff2': 0.009078741073608398, 'sac_diff3': 0.011615991592407227, 'sac_diff4': 0.007764577865600586, 'sac_diff5': 0.03431129455566406, 'sac_diff6': 0.0004134178161621094, 'all': 0.07088994979858398}
diff5_list [0.007525205612182617, 0.007498979568481445, 0.0067255496978759766, 0.0062563419342041016, 0.006305217742919922]
time3 0
time4 0.071746826171875
time5 0.07179999351501465
time7 4.76837158203125e-07
gen_weight_change tensor(-18.9503)
policy weight change tensor(36.4068, grad_fn=<SumBackward0>)
time8 0.0019974708557128906
train_time 0.08356022834777832
eval time 0.14417695999145508
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:12,112 MainThread INFO: EPOCH:774
2024-01-23 01:04:12,112 MainThread INFO: Time Consumed:0.2310636043548584s
2024-01-23 01:04:12,112 MainThread INFO: Total Frames:117000s
  8%|▊         | 775/10000 [05:39<38:54,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14966.60522
Train_Epoch_Reward                16959.72316
Running_Training_Average_Rewards  17248.90119
Explore_Time                      0.00087
Train___Time                      0.08356
Eval____Time                      0.14418
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14253.55993
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.94498     2.87231     95.76858     88.58380
alpha_0                           0.67874      0.00010     0.67887      0.67860
Alpha_loss                        -2.60781     0.00206     -2.60569     -2.61088
Training/policy_loss              -3.49433     0.00407     -3.48996     -3.50152
Training/qf1_loss                 7423.39453   936.14986   8690.42188   6087.08740
Training/qf2_loss                 16403.07539  1433.65589  18009.06836  14431.24707
Training/pf_norm                  0.10294      0.02483     0.13134      0.06611
Training/qf1_norm                 741.51786    526.02114   1461.72070   125.16319
Training/qf2_norm                 1137.86794   34.41311    1184.17981   1097.65259
log_std/mean                      -0.12606     0.00002     -0.12604     -0.12609
log_probs/mean                    -2.73117     0.00562     -2.72571     -2.74083
mean/mean                         -0.01258     0.00007     -0.01247     -0.01266
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018719196319580078
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70829
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [117300]
collect time 0.0008685588836669922
inside mustsac before update, task 0, sumup 70829
inside mustsac after update, task 0, sumup 70724
inner_dict_sum {'sac_diff0': 0.00021600723266601562, 'sac_diff1': 0.006799936294555664, 'sac_diff2': 0.008211135864257812, 'sac_diff3': 0.011150598526000977, 'sac_diff4': 0.007348060607910156, 'sac_diff5': 0.05345273017883301, 'sac_diff6': 0.0004153251647949219, 'all': 0.08759379386901855}
diff5_list [0.011676788330078125, 0.010323047637939453, 0.0104827880859375, 0.010729312896728516, 0.010240793228149414]
time3 0.0008549690246582031
time4 0.08843278884887695
time5 0.08848166465759277
time7 0.008900642395019531
gen_weight_change tensor(-18.8176)
policy weight change tensor(36.3606, grad_fn=<SumBackward0>)
time8 0.0017852783203125
train_time 0.11743521690368652
eval time 0.10776448249816895
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:12,363 MainThread INFO: EPOCH:775
2024-01-23 01:04:12,363 MainThread INFO: Time Consumed:0.22849726676940918s
2024-01-23 01:04:12,363 MainThread INFO: Total Frames:117150s
  8%|▊         | 776/10000 [05:39<38:47,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14866.50795
Train_Epoch_Reward                35621.91527
Running_Training_Average_Rewards  17894.12252
Explore_Time                      0.00086
Train___Time                      0.11744
Eval____Time                      0.10776
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14230.09829
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.71277     2.01714     92.93567     86.99946
alpha_0                           0.67840      0.00010     0.67853      0.67826
Alpha_loss                        -2.60980     0.00410     -2.60424     -2.61599
Training/policy_loss              -3.90514     0.15594     -3.74376     -4.14214
Training/qf1_loss                 7253.54062   928.31965   8549.40820   6102.34473
Training/qf2_loss                 16012.98691  1274.93611  17746.91016  14165.07812
Training/pf_norm                  0.10733      0.02775     0.14596      0.06432
Training/qf1_norm                 761.26290    482.10351   1599.39526   192.17010
Training/qf2_norm                 1284.30864   33.16664    1326.95447   1232.07336
log_std/mean                      -0.13526     0.00656     -0.12489     -0.14324
log_probs/mean                    -2.72762     0.00938     -2.71676     -2.74185
mean/mean                         -0.01193     0.00134     -0.01056     -0.01415
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018340587615966797
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70724
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [117450]
collect time 0.0009045600891113281
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.0069217681884765625, 'sac_diff2': 0.008269548416137695, 'sac_diff3': 0.011215686798095703, 'sac_diff4': 0.007260560989379883, 'sac_diff5': 0.03280973434448242, 'sac_diff6': 0.0005366802215576172, 'all': 0.06722688674926758}
diff5_list [0.0065555572509765625, 0.006488800048828125, 0.00733494758605957, 0.006315946578979492, 0.006114482879638672]
time3 0
time4 0.0680546760559082
time5 0.06810927391052246
time7 7.152557373046875e-07
gen_weight_change tensor(-18.8176)
policy weight change tensor(36.3436, grad_fn=<SumBackward0>)
time8 0.0018374919891357422
train_time 0.07953453063964844
eval time 0.1481945514678955
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:12,616 MainThread INFO: EPOCH:776
2024-01-23 01:04:12,616 MainThread INFO: Time Consumed:0.23103618621826172s
2024-01-23 01:04:12,616 MainThread INFO: Total Frames:117300s
  8%|▊         | 777/10000 [05:40<38:50,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14785.98698
Train_Epoch_Reward                4863.75502
Running_Training_Average_Rewards  17593.17539
Explore_Time                      0.00090
Train___Time                      0.07953
Eval____Time                      0.14819
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15025.86821
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.56746     1.36279    91.23991     87.55083
alpha_0                           0.67806      0.00010    0.67819      0.67792
Alpha_loss                        -2.61452     0.00145    -2.61194     -2.61594
Training/policy_loss              -3.99566     0.00206    -3.99302     -3.99758
Training/qf1_loss                 6712.41211   421.95786  7084.79053   6087.69287
Training/qf2_loss                 15190.55508  686.83138  15838.21094  14322.11914
Training/pf_norm                  0.08694      0.02521    0.12898      0.06638
Training/qf1_norm                 1896.36353   292.83383  2289.60352   1551.81580
Training/qf2_norm                 1303.17830   20.11970   1327.22986   1272.78125
log_std/mean                      -0.13645     0.00004    -0.13641     -0.13650
log_probs/mean                    -2.73113     0.00362    -2.72622     -2.73477
mean/mean                         -0.01372     0.00006    -0.01365     -0.01382
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018750667572021484
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70724
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [117600]
collect time 0.0009453296661376953
inner_dict_sum {'sac_diff0': 0.00022268295288085938, 'sac_diff1': 0.007449626922607422, 'sac_diff2': 0.008917570114135742, 'sac_diff3': 0.011029243469238281, 'sac_diff4': 0.0075757503509521484, 'sac_diff5': 0.03373265266418457, 'sac_diff6': 0.00042128562927246094, 'all': 0.06934881210327148}
diff5_list [0.006806612014770508, 0.006698131561279297, 0.007529735565185547, 0.006295204162597656, 0.0064029693603515625]
time3 0
time4 0.07019877433776855
time5 0.0702507495880127
time7 9.5367431640625e-07
gen_weight_change tensor(-18.8176)
policy weight change tensor(36.2755, grad_fn=<SumBackward0>)
time8 0.002122163772583008
train_time 0.08209776878356934
eval time 0.14743804931640625
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:12,871 MainThread INFO: EPOCH:777
2024-01-23 01:04:12,872 MainThread INFO: Time Consumed:0.23299622535705566s
2024-01-23 01:04:12,872 MainThread INFO: Total Frames:117450s
  8%|▊         | 778/10000 [05:40<39:00,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14720.57825
Train_Epoch_Reward                8682.43939
Running_Training_Average_Rewards  17450.80926
Explore_Time                      0.00094
Train___Time                      0.08210
Eval____Time                      0.14744
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14983.92882
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.13718     2.32627     92.53260     86.07515
alpha_0                           0.67772      0.00010     0.67785      0.67758
Alpha_loss                        -2.61883     0.00331     -2.61593     -2.62503
Training/policy_loss              -3.98057     0.00443     -3.97596     -3.98893
Training/qf1_loss                 7037.24150   1393.77071  9413.29883   5195.82568
Training/qf2_loss                 15528.44902  1829.05287  18556.68945  13084.76660
Training/pf_norm                  0.08688      0.02628     0.12661      0.04697
Training/qf1_norm                 440.08632    299.03422   906.32837    126.09686
Training/qf2_norm                 1337.21672   35.23290    1390.27246   1291.47485
log_std/mean                      -0.13236     0.00011     -0.13220     -0.13253
log_probs/mean                    -2.73355     0.00743     -2.72610     -2.74777
mean/mean                         -0.01140     0.00008     -0.01128     -0.01149
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019550800323486328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70724
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [117750]
collect time 0.0008540153503417969
inner_dict_sum {'sac_diff0': 0.0002205371856689453, 'sac_diff1': 0.007019519805908203, 'sac_diff2': 0.008764982223510742, 'sac_diff3': 0.010583877563476562, 'sac_diff4': 0.007245302200317383, 'sac_diff5': 0.032723426818847656, 'sac_diff6': 0.00041866302490234375, 'all': 0.06697630882263184}
diff5_list [0.007140636444091797, 0.006612539291381836, 0.006324291229248047, 0.0063915252685546875, 0.006254434585571289]
time3 0
time4 0.06784963607788086
time5 0.06790280342102051
time7 7.152557373046875e-07
gen_weight_change tensor(-18.8176)
policy weight change tensor(36.2415, grad_fn=<SumBackward0>)
time8 0.0018596649169921875
train_time 0.07901191711425781
eval time 0.14998531341552734
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:13,127 MainThread INFO: EPOCH:778
2024-01-23 01:04:13,127 MainThread INFO: Time Consumed:0.23223638534545898s
2024-01-23 01:04:13,127 MainThread INFO: Total Frames:117600s
  8%|▊         | 779/10000 [05:40<39:01,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14681.71943
Train_Epoch_Reward                15051.63834
Running_Training_Average_Rewards  17367.77305
Explore_Time                      0.00085
Train___Time                      0.07901
Eval____Time                      0.14999
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15056.58832
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.52840     1.38032    91.12423     87.40508
alpha_0                           0.67738      0.00010    0.67752      0.67725
Alpha_loss                        -2.62337     0.00248    -2.61984     -2.62692
Training/policy_loss              -3.63387     0.00395    -3.62901     -3.63941
Training/qf1_loss                 6720.83438   809.94885  8042.47510   5860.86279
Training/qf2_loss                 14998.98359  915.77491  16535.83008  14026.28320
Training/pf_norm                  0.09719      0.02079    0.13720      0.07711
Training/qf1_norm                 2728.75527   239.59789  3012.22705   2371.33740
Training/qf2_norm                 1186.35259   17.49171   1205.49634   1160.54480
log_std/mean                      -0.13578     0.00005    -0.13570     -0.13583
log_probs/mean                    -2.73658     0.00580    -2.73097     -2.74395
mean/mean                         -0.00913     0.00006    -0.00903     -0.00920
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018446922302246094
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70724
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [117900]
collect time 0.0009448528289794922
inner_dict_sum {'sac_diff0': 0.0002334117889404297, 'sac_diff1': 0.0068073272705078125, 'sac_diff2': 0.008266925811767578, 'sac_diff3': 0.010815143585205078, 'sac_diff4': 0.007204532623291016, 'sac_diff5': 0.03269171714782715, 'sac_diff6': 0.00038552284240722656, 'all': 0.06640458106994629}
diff5_list [0.007195472717285156, 0.006327390670776367, 0.00643157958984375, 0.006306171417236328, 0.006431102752685547]
time3 0
time4 0.06715917587280273
time5 0.06720137596130371
time7 9.5367431640625e-07
gen_weight_change tensor(-18.8176)
policy weight change tensor(36.1542, grad_fn=<SumBackward0>)
time8 0.0018324851989746094
train_time 0.07819199562072754
eval time 0.14868974685668945
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:13,379 MainThread INFO: EPOCH:779
2024-01-23 01:04:13,380 MainThread INFO: Time Consumed:0.23029708862304688s
2024-01-23 01:04:13,380 MainThread INFO: Total Frames:117750s
  8%|▊         | 780/10000 [05:40<38:57,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14670.00355
Train_Epoch_Reward                17149.30551
Running_Training_Average_Rewards  17763.47815
Explore_Time                      0.00094
Train___Time                      0.07819
Eval____Time                      0.14869
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15209.99477
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.37657     1.27434    93.16494     89.64666
alpha_0                           0.67704      0.00010    0.67718      0.67691
Alpha_loss                        -2.62651     0.00173    -2.62374     -2.62857
Training/policy_loss              -4.00105     0.00395    -3.99549     -4.00727
Training/qf1_loss                 6630.93252   699.62442  7306.57666   5374.47314
Training/qf2_loss                 15520.74082  920.36840  16510.72461  13903.20801
Training/pf_norm                  0.12135      0.02068    0.14561      0.08522
Training/qf1_norm                 327.44723    187.37823  631.65930    154.66641
Training/qf2_norm                 1336.27844   19.30081   1363.92517   1309.10938
log_std/mean                      -0.13337     0.00021    -0.13305     -0.13364
log_probs/mean                    -2.73599     0.00436    -2.73060     -2.74128
mean/mean                         -0.01105     0.00009    -0.01090     -0.01115
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018517017364501953
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70724
epoch first part time 2.86102294921875e-06
replay_buffer._size: [118050]
collect time 0.0009257793426513672
inside mustsac before update, task 0, sumup 70724
inside mustsac after update, task 0, sumup 70247
inner_dict_sum {'sac_diff0': 0.00026154518127441406, 'sac_diff1': 0.008059263229370117, 'sac_diff2': 0.009740829467773438, 'sac_diff3': 0.012836694717407227, 'sac_diff4': 0.008769750595092773, 'sac_diff5': 0.06221413612365723, 'sac_diff6': 0.0004801750183105469, 'all': 0.10236239433288574}
diff5_list [0.011491060256958008, 0.011944293975830078, 0.01253509521484375, 0.013008594512939453, 0.013235092163085938]
time3 0.0009524822235107422
time4 0.10338449478149414
time5 0.10344672203063965
time7 0.009203672409057617
gen_weight_change tensor(-18.6696)
policy weight change tensor(36.1321, grad_fn=<SumBackward0>)
time8 0.0026254653930664062
train_time 0.1358470916748047
eval time 0.10375690460205078
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:13,644 MainThread INFO: EPOCH:780
2024-01-23 01:04:13,645 MainThread INFO: Time Consumed:0.24282002449035645s
2024-01-23 01:04:13,645 MainThread INFO: Total Frames:117900s
  8%|▊         | 781/10000 [05:41<39:37,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14674.50586
Train_Epoch_Reward                19224.37633
Running_Training_Average_Rewards  16977.13453
Explore_Time                      0.00092
Train___Time                      0.13585
Eval____Time                      0.10376
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15367.87624
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.14649     1.68031    93.35164     88.54085
alpha_0                           0.67670      0.00010    0.67684      0.67657
Alpha_loss                        -2.62984     0.00124    -2.62861     -2.63202
Training/policy_loss              -4.10810     0.14589    -3.88823     -4.28896
Training/qf1_loss                 6687.46455   628.36239  7545.00781   5927.67871
Training/qf2_loss                 15445.84258  903.18263  16581.88867  14197.34961
Training/pf_norm                  0.14029      0.04440    0.17836      0.07556
Training/qf1_norm                 1265.01273   856.28736  2685.09351   357.79285
Training/qf2_norm                 1380.48743   59.37546   1465.87170   1290.67273
log_std/mean                      -0.12608     0.00816    -0.11893     -0.14028
log_probs/mean                    -2.73588     0.00286    -2.73302     -2.73975
mean/mean                         -0.00976     0.00072    -0.00878     -0.01082
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018778562545776367
epoch last part time3 0.0025849342346191406
inside rlalgo, task 0, sumup 70247
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [118200]
collect time 0.0008802413940429688
inner_dict_sum {'sac_diff0': 0.00021123886108398438, 'sac_diff1': 0.0065572261810302734, 'sac_diff2': 0.00797581672668457, 'sac_diff3': 0.010544061660766602, 'sac_diff4': 0.007035970687866211, 'sac_diff5': 0.03158116340637207, 'sac_diff6': 0.0003809928894042969, 'all': 0.06428647041320801}
diff5_list [0.006568193435668945, 0.006474733352661133, 0.0064165592193603516, 0.006144285202026367, 0.0059773921966552734]
time3 0
time4 0.06503033638000488
time5 0.06507563591003418
time7 7.152557373046875e-07
gen_weight_change tensor(-18.6696)
policy weight change tensor(35.9947, grad_fn=<SumBackward0>)
time8 0.0019769668579101562
train_time 0.07606792449951172
eval time 0.15404629707336426
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:13,902 MainThread INFO: EPOCH:781
2024-01-23 01:04:13,903 MainThread INFO: Time Consumed:0.2332773208618164s
2024-01-23 01:04:13,903 MainThread INFO: Total Frames:118050s
  8%|▊         | 782/10000 [05:41<39:29,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14903.43355
Train_Epoch_Reward                1661.32618
Running_Training_Average_Rewards  16916.16618
Explore_Time                      0.00088
Train___Time                      0.07607
Eval____Time                      0.15405
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16464.99158
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.35864     1.04873    92.66119     89.85870
alpha_0                           0.67637      0.00010    0.67650      0.67623
Alpha_loss                        -2.63253     0.00295    -2.62905     -2.63799
Training/policy_loss              -4.15777     0.00467    -4.15344     -4.16653
Training/qf1_loss                 7366.05352   340.91998  7912.70703   6970.60645
Training/qf2_loss                 16206.98906  515.46985  16989.74609  15636.81543
Training/pf_norm                  0.11459      0.02644    0.14182      0.06815
Training/qf1_norm                 802.46748    194.19204  1046.99426   532.48267
Training/qf2_norm                 1424.98323   15.65971   1444.25354   1401.97192
log_std/mean                      -0.12916     0.00017    -0.12890     -0.12939
log_probs/mean                    -2.73415     0.00726    -2.72835     -2.74811
mean/mean                         -0.00940     0.00013    -0.00923     -0.00960
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01879119873046875
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70247
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [118350]
collect time 0.0008833408355712891
inner_dict_sum {'sac_diff0': 0.00022220611572265625, 'sac_diff1': 0.00731348991394043, 'sac_diff2': 0.008548498153686523, 'sac_diff3': 0.011357784271240234, 'sac_diff4': 0.007617950439453125, 'sac_diff5': 0.033391714096069336, 'sac_diff6': 0.0003979206085205078, 'all': 0.06884956359863281}
diff5_list [0.006899118423461914, 0.006409406661987305, 0.006484270095825195, 0.006267547607421875, 0.007331371307373047]
time3 0
time4 0.06963133811950684
time5 0.06967878341674805
time7 4.76837158203125e-07
gen_weight_change tensor(-18.6696)
policy weight change tensor(35.9431, grad_fn=<SumBackward0>)
time8 0.001913309097290039
train_time 0.08099007606506348
eval time 0.14533352851867676
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:14,154 MainThread INFO: EPOCH:782
2024-01-23 01:04:14,154 MainThread INFO: Time Consumed:0.2294154167175293s
2024-01-23 01:04:14,155 MainThread INFO: Total Frames:118200s
  8%|▊         | 783/10000 [05:41<39:13,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15135.74627
Train_Epoch_Reward                15288.11043
Running_Training_Average_Rewards  17005.19441
Explore_Time                      0.00088
Train___Time                      0.08099
Eval____Time                      0.14533
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16518.12316
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.04852     2.97360     95.24203     87.15090
alpha_0                           0.67603      0.00010     0.67616      0.67589
Alpha_loss                        -2.63389     0.00324     -2.62874     -2.63731
Training/policy_loss              -3.83780     0.00513     -3.83142     -3.84435
Training/qf1_loss                 7613.67754   1826.38339  10843.38770  5426.99854
Training/qf2_loss                 16478.72461  2395.58515  20561.78516  13506.45605
Training/pf_norm                  0.11404      0.02131     0.14151      0.08307
Training/qf1_norm                 597.94454    237.59621   926.78937    270.16867
Training/qf2_norm                 1284.19819   42.36096    1344.26831   1229.34790
log_std/mean                      -0.12320     0.00005     -0.12313     -0.12327
log_probs/mean                    -2.72902     0.00826     -2.71929     -2.73948
mean/mean                         -0.01214     0.00006     -0.01203     -0.01220
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01833200454711914
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70247
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [118500]
collect time 0.0008645057678222656
inner_dict_sum {'sac_diff0': 0.0002155303955078125, 'sac_diff1': 0.0070798397064208984, 'sac_diff2': 0.008118867874145508, 'sac_diff3': 0.010648727416992188, 'sac_diff4': 0.006878852844238281, 'sac_diff5': 0.032874107360839844, 'sac_diff6': 0.0003905296325683594, 'all': 0.06620645523071289}
diff5_list [0.006444454193115234, 0.006266593933105469, 0.0072443485260009766, 0.0065844058990478516, 0.0063343048095703125]
time3 0
time4 0.06695795059204102
time5 0.06700325012207031
time7 4.76837158203125e-07
gen_weight_change tensor(-18.6696)
policy weight change tensor(35.9560, grad_fn=<SumBackward0>)
time8 0.0020444393157958984
train_time 0.07987499237060547
eval time 0.144561767578125
epoch last part time 1.049041748046875e-05
2024-01-23 01:04:14,404 MainThread INFO: EPOCH:783
2024-01-23 01:04:14,404 MainThread INFO: Time Consumed:0.22765111923217773s
2024-01-23 01:04:14,404 MainThread INFO: Total Frames:118350s
  8%|▊         | 784/10000 [05:42<38:58,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15366.39928
Train_Epoch_Reward                8332.34184
Running_Training_Average_Rewards  16831.26557
Explore_Time                      0.00086
Train___Time                      0.07987
Eval____Time                      0.14456
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16552.96352
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.04704     1.67106     93.71280     89.90119
alpha_0                           0.67569      0.00010     0.67582      0.67555
Alpha_loss                        -2.64065     0.00132     -2.63845     -2.64204
Training/policy_loss              -4.00438     0.00454     -3.99579     -4.00861
Training/qf1_loss                 7109.48008   1129.11241  9239.24023   5916.65723
Training/qf2_loss                 16115.00586  1318.29245  18407.86523  14551.04102
Training/pf_norm                  0.14544      0.02802     0.19955      0.11916
Training/qf1_norm                 390.01871    122.58606   566.46692    246.54533
Training/qf2_norm                 1374.07527   23.74407    1397.87927   1344.28381
log_std/mean                      -0.12507     0.00002     -0.12503     -0.12509
log_probs/mean                    -2.73769     0.00363     -2.73231     -2.74209
mean/mean                         -0.00714     0.00018     -0.00686     -0.00737
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01901555061340332
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70247
epoch first part time 7.152557373046875e-06
replay_buffer._size: [118650]
collect time 0.0009219646453857422
inner_dict_sum {'sac_diff0': 0.0002448558807373047, 'sac_diff1': 0.0070383548736572266, 'sac_diff2': 0.007965326309204102, 'sac_diff3': 0.01037740707397461, 'sac_diff4': 0.0067980289459228516, 'sac_diff5': 0.032500267028808594, 'sac_diff6': 0.0003998279571533203, 'all': 0.06532406806945801}
diff5_list [0.006929874420166016, 0.00632166862487793, 0.006508588790893555, 0.006620645523071289, 0.006119489669799805]
time3 0
time4 0.0660855770111084
time5 0.0661308765411377
time7 4.76837158203125e-07
gen_weight_change tensor(-18.6696)
policy weight change tensor(35.9718, grad_fn=<SumBackward0>)
time8 0.001917123794555664
train_time 0.07775330543518066
eval time 0.1457078456878662
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:14,653 MainThread INFO: EPOCH:784
2024-01-23 01:04:14,653 MainThread INFO: Time Consumed:0.2266221046447754s
2024-01-23 01:04:14,653 MainThread INFO: Total Frames:118500s
  8%|▊         | 785/10000 [05:42<38:44,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15627.61485
Train_Epoch_Reward                10093.19142
Running_Training_Average_Rewards  16755.03986
Explore_Time                      0.00091
Train___Time                      0.07775
Eval____Time                      0.14571
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16865.71563
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.92594     2.26674     92.83075     85.86400
alpha_0                           0.67535      0.00010     0.67549      0.67522
Alpha_loss                        -2.64187     0.00172     -2.64034     -2.64512
Training/policy_loss              -3.79420     0.00475     -3.79000     -3.80314
Training/qf1_loss                 7058.22207   776.69202   8086.62109   6205.60645
Training/qf2_loss                 15511.27930  1160.09744  16979.71289  13795.22852
Training/pf_norm                  0.09903      0.01368     0.11776      0.08460
Training/qf1_norm                 2707.10020   473.38331   3544.28735   2083.77808
Training/qf2_norm                 1237.02810   30.24380    1275.83655   1183.13440
log_std/mean                      -0.13034     0.00002     -0.13032     -0.13038
log_probs/mean                    -2.73222     0.00592     -2.72778     -2.74394
mean/mean                         -0.00997     0.00020     -0.00966     -0.01021
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018564462661743164
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70247
epoch first part time 3.337860107421875e-06
replay_buffer._size: [118800]
collect time 0.0008146762847900391
inside mustsac before update, task 0, sumup 70247
inside mustsac after update, task 0, sumup 70236
inner_dict_sum {'sac_diff0': 0.0002067089080810547, 'sac_diff1': 0.0064239501953125, 'sac_diff2': 0.007907629013061523, 'sac_diff3': 0.01013636589050293, 'sac_diff4': 0.006951332092285156, 'sac_diff5': 0.051416873931884766, 'sac_diff6': 0.0003936290740966797, 'all': 0.08343648910522461}
diff5_list [0.010563850402832031, 0.009980201721191406, 0.010287761688232422, 0.010387659072875977, 0.01019740104675293]
time3 0.0008580684661865234
time4 0.08424067497253418
time5 0.08428812026977539
time7 0.008784294128417969
gen_weight_change tensor(-18.5265)
policy weight change tensor(35.9492, grad_fn=<SumBackward0>)
time8 0.0017921924591064453
train_time 0.11278939247131348
eval time 0.10767507553100586
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:14,898 MainThread INFO: EPOCH:785
2024-01-23 01:04:14,899 MainThread INFO: Time Consumed:0.22353863716125488s
2024-01-23 01:04:14,899 MainThread INFO: Total Frames:118650s
  8%|▊         | 786/10000 [05:42<38:28,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15931.27876
Train_Epoch_Reward                54654.80415
Running_Training_Average_Rewards  18234.31892
Explore_Time                      0.00081
Train___Time                      0.11279
Eval____Time                      0.10768
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17266.73730
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.89976     1.09054    94.83025     91.68174
alpha_0                           0.67501      0.00010    0.67515      0.67488
Alpha_loss                        -2.64567     0.00203    -2.64287     -2.64845
Training/policy_loss              -4.09459     0.13823    -3.93459     -4.28380
Training/qf1_loss                 7080.91943   656.79631  8356.42090   6617.27246
Training/qf2_loss                 16201.00742  693.95335  17406.91016  15481.84668
Training/pf_norm                  0.10202      0.01976    0.13069      0.07566
Training/qf1_norm                 1523.97974   397.57676  2118.90112   887.36340
Training/qf2_norm                 1422.71794   60.97700   1502.25403   1334.68689
log_std/mean                      -0.12333     0.00317    -0.11919     -0.12766
log_probs/mean                    -2.73332     0.00393    -2.72785     -2.73764
mean/mean                         -0.00779     0.00265    -0.00553     -0.01279
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019285202026367188
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70236
epoch first part time 2.86102294921875e-06
replay_buffer._size: [118950]
collect time 0.0009806156158447266
inner_dict_sum {'sac_diff0': 0.00023436546325683594, 'sac_diff1': 0.006777286529541016, 'sac_diff2': 0.007821798324584961, 'sac_diff3': 0.010146379470825195, 'sac_diff4': 0.006783485412597656, 'sac_diff5': 0.0313115119934082, 'sac_diff6': 0.0003876686096191406, 'all': 0.06346249580383301}
diff5_list [0.006441831588745117, 0.006369590759277344, 0.006214618682861328, 0.006198406219482422, 0.006087064743041992]
time3 0
time4 0.06420326232910156
time5 0.06424784660339355
time7 9.5367431640625e-07
gen_weight_change tensor(-18.5265)
policy weight change tensor(35.9419, grad_fn=<SumBackward0>)
time8 0.001790761947631836
train_time 0.07529115676879883
eval time 0.15659213066101074
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:15,157 MainThread INFO: EPOCH:786
2024-01-23 01:04:15,157 MainThread INFO: Time Consumed:0.23508071899414062s
2024-01-23 01:04:15,157 MainThread INFO: Total Frames:118800s
  8%|▊         | 787/10000 [05:42<38:47,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16096.19784
Train_Epoch_Reward                10260.86861
Running_Training_Average_Rewards  17042.93303
Explore_Time                      0.00098
Train___Time                      0.07529
Eval____Time                      0.15659
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16675.05903
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.72089     1.53320    93.69559     89.41383
alpha_0                           0.67468      0.00010    0.67481      0.67454
Alpha_loss                        -2.64849     0.00198    -2.64607     -2.65119
Training/policy_loss              -3.97852     0.00455    -3.97349     -3.98509
Training/qf1_loss                 6653.36680   241.18651  6881.27051   6221.79004
Training/qf2_loss                 15420.11582  179.67306  15606.05664  15116.94824
Training/pf_norm                  0.12506      0.02603    0.15296      0.08153
Training/qf1_norm                 258.55697    183.98159  612.93610    113.20667
Training/qf2_norm                 1339.04790   22.56483   1382.15515   1317.71484
log_std/mean                      -0.12211     0.00001    -0.12210     -0.12212
log_probs/mean                    -2.73192     0.00595    -2.72487     -2.74222
mean/mean                         -0.00897     0.00011    -0.00884     -0.00914
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018462181091308594
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70236
epoch first part time 2.384185791015625e-06
replay_buffer._size: [119100]
collect time 0.0009062290191650391
inner_dict_sum {'sac_diff0': 0.00024509429931640625, 'sac_diff1': 0.006537675857543945, 'sac_diff2': 0.007489919662475586, 'sac_diff3': 0.009956598281860352, 'sac_diff4': 0.007134914398193359, 'sac_diff5': 0.032926082611083984, 'sac_diff6': 0.00039577484130859375, 'all': 0.06468605995178223}
diff5_list [0.006833553314208984, 0.006186723709106445, 0.006922006607055664, 0.006401538848876953, 0.0065822601318359375]
time3 0
time4 0.06542181968688965
time5 0.06546497344970703
time7 7.152557373046875e-07
gen_weight_change tensor(-18.5265)
policy weight change tensor(35.9282, grad_fn=<SumBackward0>)
time8 0.0018732547760009766
train_time 0.0761420726776123
eval time 0.1481785774230957
epoch last part time 5.7220458984375e-06
2024-01-23 01:04:15,406 MainThread INFO: EPOCH:787
2024-01-23 01:04:15,407 MainThread INFO: Time Consumed:0.22751235961914062s
2024-01-23 01:04:15,407 MainThread INFO: Total Frames:118950s
  8%|▊         | 788/10000 [05:43<38:39,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16276.30671
Train_Epoch_Reward                15328.51965
Running_Training_Average_Rewards  17262.86351
Explore_Time                      0.00090
Train___Time                      0.07614
Eval____Time                      0.14818
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16785.01755
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.27232     1.48593    93.85276     89.54035
alpha_0                           0.67434      0.00010    0.67447      0.67420
Alpha_loss                        -2.65289     0.00077    -2.65142     -2.65350
Training/policy_loss              -4.03528     0.00277    -4.03173     -4.03898
Training/qf1_loss                 7143.78457   475.56673  7588.92188   6292.82373
Training/qf2_loss                 15966.23203  717.35971  16680.59766  14740.47852
Training/pf_norm                  0.10969      0.01780    0.13109      0.07806
Training/qf1_norm                 1113.46005   292.30249  1410.60962   619.14246
Training/qf2_norm                 1394.69556   22.81387   1433.90820   1367.13489
log_std/mean                      -0.12256     0.00003    -0.12253     -0.12261
log_probs/mean                    -2.73456     0.00307    -2.73082     -2.73952
mean/mean                         -0.01012     0.00024    -0.00972     -0.01039
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01871037483215332
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70236
epoch first part time 2.86102294921875e-06
replay_buffer._size: [119250]
collect time 0.0009908676147460938
inner_dict_sum {'sac_diff0': 0.0002319812774658203, 'sac_diff1': 0.0065746307373046875, 'sac_diff2': 0.0077381134033203125, 'sac_diff3': 0.010326862335205078, 'sac_diff4': 0.0070421695709228516, 'sac_diff5': 0.031171560287475586, 'sac_diff6': 0.0003788471221923828, 'all': 0.06346416473388672}
diff5_list [0.006779909133911133, 0.006102561950683594, 0.0060520172119140625, 0.006216287612915039, 0.006020784378051758]
time3 0
time4 0.06420612335205078
time5 0.06424975395202637
time7 7.152557373046875e-07
gen_weight_change tensor(-18.5265)
policy weight change tensor(35.8555, grad_fn=<SumBackward0>)
time8 0.0018389225006103516
train_time 0.07535982131958008
eval time 0.15112876892089844
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:15,659 MainThread INFO: EPOCH:788
2024-01-23 01:04:15,659 MainThread INFO: Time Consumed:0.2297220230102539s
2024-01-23 01:04:15,659 MainThread INFO: Total Frames:119100s
  8%|▊         | 789/10000 [05:43<38:39,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16448.28838
Train_Epoch_Reward                18205.49781
Running_Training_Average_Rewards  16951.52428
Explore_Time                      0.00099
Train___Time                      0.07536
Eval____Time                      0.15113
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16776.40505
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.34404     1.69884     92.34620     88.39400
alpha_0                           0.67400      0.00010     0.67414      0.67387
Alpha_loss                        -2.65373     0.00254     -2.64969     -2.65714
Training/policy_loss              -4.17068     0.00344     -4.16615     -4.17545
Training/qf1_loss                 7102.03281   949.05296   8673.08789   6114.61230
Training/qf2_loss                 15783.46016  1195.98877  17755.23242  14435.30469
Training/pf_norm                  0.14949      0.02972     0.20005      0.11939
Training/qf1_norm                 500.58811    305.29339   892.54541    145.52049
Training/qf2_norm                 1408.40789   26.07459    1439.79993   1378.39148
log_std/mean                      -0.14133     0.00013     -0.14109     -0.14146
log_probs/mean                    -2.72815     0.00561     -2.72132     -2.73509
mean/mean                         -0.00715     0.00020     -0.00689     -0.00744
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018498659133911133
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70236
epoch first part time 2.86102294921875e-06
replay_buffer._size: [119400]
collect time 0.0008256435394287109
inner_dict_sum {'sac_diff0': 0.00024628639221191406, 'sac_diff1': 0.0065059661865234375, 'sac_diff2': 0.0074901580810546875, 'sac_diff3': 0.00978231430053711, 'sac_diff4': 0.006747722625732422, 'sac_diff5': 0.031161069869995117, 'sac_diff6': 0.00037670135498046875, 'all': 0.062310218811035156}
diff5_list [0.006481170654296875, 0.006173133850097656, 0.006035566329956055, 0.0063228607177734375, 0.006148338317871094]
time3 0
time4 0.06305766105651855
time5 0.06310057640075684
time7 4.76837158203125e-07
gen_weight_change tensor(-18.5265)
policy weight change tensor(35.7550, grad_fn=<SumBackward0>)
time8 0.0018684864044189453
train_time 0.0738685131072998
eval time 0.15323281288146973
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:15,911 MainThread INFO: EPOCH:789
2024-01-23 01:04:15,911 MainThread INFO: Time Consumed:0.23010754585266113s
2024-01-23 01:04:15,911 MainThread INFO: Total Frames:119250s
  8%|▊         | 790/10000 [05:43<38:46,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16590.04043
Train_Epoch_Reward                13650.31351
Running_Training_Average_Rewards  17018.78222
Explore_Time                      0.00082
Train___Time                      0.07387
Eval____Time                      0.15323
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16627.51522
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.27605     2.42276     94.87718     88.23934
alpha_0                           0.67367      0.00010     0.67380      0.67353
Alpha_loss                        -2.65794     0.00369     -2.65268     -2.66325
Training/policy_loss              -4.07344     0.00542     -4.06604     -4.08261
Training/qf1_loss                 7110.23467   796.36078   8289.77734   6119.41113
Training/qf2_loss                 15939.50488  1281.64956  17844.98828  14334.52148
Training/pf_norm                  0.13143      0.03473     0.17398      0.08878
Training/qf1_norm                 872.73730    457.43616   1481.53101   243.30672
Training/qf2_norm                 1380.53132   37.01705    1437.09790   1335.14795
log_std/mean                      -0.14024     0.00010     -0.14013     -0.14039
log_probs/mean                    -2.73029     0.00870     -2.71528     -2.74033
mean/mean                         -0.00610     0.00018     -0.00584     -0.00635
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020622730255126953
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70236
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [119550]
collect time 0.0009424686431884766
inside mustsac before update, task 0, sumup 70236
inside mustsac after update, task 0, sumup 70597
inner_dict_sum {'sac_diff0': 0.00022268295288085938, 'sac_diff1': 0.0070612430572509766, 'sac_diff2': 0.008456230163574219, 'sac_diff3': 0.010821104049682617, 'sac_diff4': 0.007214784622192383, 'sac_diff5': 0.051872968673706055, 'sac_diff6': 0.0004107952117919922, 'all': 0.0860598087310791}
diff5_list [0.011268377304077148, 0.010179519653320312, 0.010390758514404297, 0.009957313537597656, 0.01007699966430664]
time3 0.0008699893951416016
time4 0.08687472343444824
time5 0.08692455291748047
time7 0.009032249450683594
gen_weight_change tensor(-18.3903)
policy weight change tensor(35.7903, grad_fn=<SumBackward0>)
time8 0.002577543258666992
train_time 0.11674737930297852
eval time 0.1065523624420166
epoch last part time 4.0531158447265625e-06
2024-01-23 01:04:16,162 MainThread INFO: EPOCH:790
2024-01-23 01:04:16,162 MainThread INFO: Time Consumed:0.2264270782470703s
2024-01-23 01:04:16,163 MainThread INFO: Total Frames:119400s
  8%|▊         | 791/10000 [05:43<38:59,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16694.62675
Train_Epoch_Reward                1495.35718
Running_Training_Average_Rewards  16472.99956
Explore_Time                      0.00094
Train___Time                      0.11675
Eval____Time                      0.10655
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16413.73941
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.38761     2.37280     94.22656     86.82691
alpha_0                           0.67333      0.00010     0.67346      0.67319
Alpha_loss                        -2.66115     0.00216     -2.65869     -2.66511
Training/policy_loss              -4.13114     0.20916     -3.86357     -4.49943
Training/qf1_loss                 7236.89668   880.22446   8722.42383   6371.53955
Training/qf2_loss                 15891.40801  1245.23508  18007.03906  14457.11426
Training/pf_norm                  0.12196      0.02485     0.15548      0.09070
Training/qf1_norm                 1118.75372   724.04987   2374.26904   385.88824
Training/qf2_norm                 1391.60337   130.60645   1573.69678   1185.79858
log_std/mean                      -0.12388     0.00265     -0.11931     -0.12655
log_probs/mean                    -2.72989     0.00383     -2.72538     -2.73650
mean/mean                         -0.00638     0.00192     -0.00420     -0.00969
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.024360179901123047
epoch last part time3 0.002465963363647461
inside rlalgo, task 0, sumup 70597
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [119700]
collect time 0.00086212158203125
inner_dict_sum {'sac_diff0': 0.00023436546325683594, 'sac_diff1': 0.0065920352935791016, 'sac_diff2': 0.007962703704833984, 'sac_diff3': 0.01035308837890625, 'sac_diff4': 0.006972312927246094, 'sac_diff5': 0.03320717811584473, 'sac_diff6': 0.00040149688720703125, 'all': 0.06572318077087402}
diff5_list [0.006529331207275391, 0.007502317428588867, 0.006795644760131836, 0.006221294403076172, 0.006158590316772461]
time3 0
time4 0.06648731231689453
time5 0.06653380393981934
time7 4.76837158203125e-07
gen_weight_change tensor(-18.3903)
policy weight change tensor(35.7016, grad_fn=<SumBackward0>)
time8 0.0019512176513671875
train_time 0.07755231857299805
eval time 0.14044499397277832
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:16,413 MainThread INFO: EPOCH:791
2024-01-23 01:04:16,413 MainThread INFO: Time Consumed:0.22121477127075195s
2024-01-23 01:04:16,414 MainThread INFO: Total Frames:119550s
  8%|▊         | 792/10000 [05:44<38:27,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16614.34361
Train_Epoch_Reward                39123.24680
Running_Training_Average_Rewards  17391.27543
Explore_Time                      0.00086
Train___Time                      0.07755
Eval____Time                      0.14044
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15662.16027
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.27262     1.60632    92.02296     87.06895
alpha_0                           0.67299      0.00010    0.67313      0.67286
Alpha_loss                        -2.66701     0.00044    -2.66650     -2.66755
Training/policy_loss              -3.89102     0.00244    -3.88819     -3.89519
Training/qf1_loss                 6548.26045   735.56817  7696.74707   5801.09131
Training/qf2_loss                 15004.49707  981.00708  16653.67188  13960.66113
Training/pf_norm                  0.09941      0.01189    0.11527      0.08611
Training/qf1_norm                 597.70674    304.84659  1107.35986   160.06036
Training/qf2_norm                 1266.95498   22.25137   1305.45093   1236.88086
log_std/mean                      -0.12545     0.00009    -0.12532     -0.12557
log_probs/mean                    -2.73621     0.00331    -2.73155     -2.74064
mean/mean                         -0.00910     0.00026    -0.00871     -0.00944
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018375635147094727
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70597
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [119850]
collect time 0.0008900165557861328
inner_dict_sum {'sac_diff0': 0.00021576881408691406, 'sac_diff1': 0.006556987762451172, 'sac_diff2': 0.007810831069946289, 'sac_diff3': 0.010027170181274414, 'sac_diff4': 0.006842136383056641, 'sac_diff5': 0.03253817558288574, 'sac_diff6': 0.00040221214294433594, 'all': 0.06439328193664551}
diff5_list [0.0063898563385009766, 0.0063211917877197266, 0.006303548812866211, 0.007368326187133789, 0.006155252456665039]
time3 0
time4 0.06514859199523926
time5 0.06519389152526855
time7 4.76837158203125e-07
gen_weight_change tensor(-18.3903)
policy weight change tensor(35.6819, grad_fn=<SumBackward0>)
time8 0.0018892288208007812
train_time 0.07611322402954102
eval time 0.1515510082244873
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:16,666 MainThread INFO: EPOCH:792
2024-01-23 01:04:16,666 MainThread INFO: Time Consumed:0.23082470893859863s
2024-01-23 01:04:16,666 MainThread INFO: Total Frames:119700s
  8%|▊         | 793/10000 [05:44<38:32,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16510.85280
Train_Epoch_Reward                4284.54224
Running_Training_Average_Rewards  17093.40740
Explore_Time                      0.00089
Train___Time                      0.07611
Eval____Time                      0.15155
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15483.21505
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.21329     1.08317    92.73078     89.41075
alpha_0                           0.67266      0.00010    0.67279      0.67252
Alpha_loss                        -2.67037     0.00135    -2.66902     -2.67297
Training/policy_loss              -4.14143     0.00152    -4.13977     -4.14390
Training/qf1_loss                 6866.95361   534.98043  7488.99902   6234.06201
Training/qf2_loss                 15752.69141  587.89831  16457.54297  14824.00293
Training/pf_norm                  0.10840      0.01426    0.12626      0.09070
Training/qf1_norm                 873.07091    201.22906  1142.60449   557.72583
Training/qf2_norm                 1404.95002   17.53505   1430.82971   1376.33093
log_std/mean                      -0.12685     0.00005    -0.12679     -0.12694
log_probs/mean                    -2.73618     0.00346    -2.73176     -2.74105
mean/mean                         -0.00442     0.00018    -0.00417     -0.00467
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018150806427001953
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70597
epoch first part time 2.86102294921875e-06
replay_buffer._size: [120000]
collect time 0.00099945068359375
inner_dict_sum {'sac_diff0': 0.00022411346435546875, 'sac_diff1': 0.0064182281494140625, 'sac_diff2': 0.007597446441650391, 'sac_diff3': 0.009653329849243164, 'sac_diff4': 0.0066070556640625, 'sac_diff5': 0.031089305877685547, 'sac_diff6': 0.0003771781921386719, 'all': 0.061966657638549805}
diff5_list [0.006337165832519531, 0.006115913391113281, 0.0059795379638671875, 0.0064961910247802734, 0.0061604976654052734]
time3 0
time4 0.06270146369934082
time5 0.062744140625
time7 4.76837158203125e-07
gen_weight_change tensor(-18.3903)
policy weight change tensor(35.6233, grad_fn=<SumBackward0>)
time8 0.001865386962890625
train_time 0.07359981536865234
eval time 0.1522068977355957
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:16,917 MainThread INFO: EPOCH:793
2024-01-23 01:04:16,917 MainThread INFO: Time Consumed:0.22904753684997559s
2024-01-23 01:04:16,917 MainThread INFO: Total Frames:119850s
  8%|▊         | 794/10000 [05:44<38:32,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16391.58371
Train_Epoch_Reward                13650.76859
Running_Training_Average_Rewards  17048.25718
Explore_Time                      0.00099
Train___Time                      0.07360
Eval____Time                      0.15221
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15360.27261
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.04675     1.25434    92.12410     89.02325
alpha_0                           0.67232      0.00010    0.67245      0.67218
Alpha_loss                        -2.67332     0.00195    -2.67130     -2.67626
Training/policy_loss              -4.08560     0.00436    -4.07895     -4.09151
Training/qf1_loss                 7085.05146   206.77142  7338.82715   6814.18262
Training/qf2_loss                 15814.00605  364.43770  16258.54883  15197.15430
Training/pf_norm                  0.09361      0.01302    0.10632      0.07491
Training/qf1_norm                 1774.05327   259.74152  2030.94019   1342.05542
Training/qf2_norm                 1372.60630   19.12114   1388.53516   1341.30347
log_std/mean                      -0.13637     0.00012    -0.13619     -0.13653
log_probs/mean                    -2.73514     0.00517    -2.72745     -2.74101
mean/mean                         -0.00507     0.00016    -0.00484     -0.00528
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018179893493652344
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70597
epoch first part time 2.86102294921875e-06
replay_buffer._size: [120150]
collect time 0.001007080078125
inner_dict_sum {'sac_diff0': 0.0002396106719970703, 'sac_diff1': 0.006742238998413086, 'sac_diff2': 0.008078813552856445, 'sac_diff3': 0.010358572006225586, 'sac_diff4': 0.007241725921630859, 'sac_diff5': 0.032244205474853516, 'sac_diff6': 0.0004029273986816406, 'all': 0.0653080940246582}
diff5_list [0.007464885711669922, 0.0061452388763427734, 0.00612187385559082, 0.00622868537902832, 0.00628352165222168]
time3 0
time4 0.06607532501220703
time5 0.06612038612365723
time7 7.152557373046875e-07
gen_weight_change tensor(-18.3903)
policy weight change tensor(35.5113, grad_fn=<SumBackward0>)
time8 0.0018186569213867188
train_time 0.07728362083435059
eval time 0.15469145774841309
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:17,174 MainThread INFO: EPOCH:794
2024-01-23 01:04:17,175 MainThread INFO: Time Consumed:0.2352437973022461s
2024-01-23 01:04:17,175 MainThread INFO: Total Frames:120000s
  8%|▊         | 795/10000 [05:44<38:49,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16236.53263
Train_Epoch_Reward                23320.78823
Running_Training_Average_Rewards  16416.44788
Explore_Time                      0.00100
Train___Time                      0.07728
Eval____Time                      0.15469
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15315.20481
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.67024     2.10785     94.35390     88.50852
alpha_0                           0.67198      0.00010     0.67212      0.67185
Alpha_loss                        -2.67567     0.00249     -2.67238     -2.67951
Training/policy_loss              -4.09700     0.00349     -4.09145     -4.10229
Training/qf1_loss                 7096.29072   1169.50195  9355.96875   6162.13770
Training/qf2_loss                 15822.39355  1504.48852  18753.10547  14502.44824
Training/pf_norm                  0.09605      0.01204     0.11101      0.07699
Training/qf1_norm                 1108.77198   414.46191   1822.38831   665.98022
Training/qf2_norm                 1394.63472   32.06896    1449.69128   1360.51453
log_std/mean                      -0.13637     0.00022     -0.13607     -0.13668
log_probs/mean                    -2.73257     0.00432     -2.72599     -2.73884
mean/mean                         -0.00562     0.00005     -0.00558     -0.00572
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01816534996032715
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70597
epoch first part time 2.86102294921875e-06
replay_buffer._size: [120300]
collect time 0.0008137226104736328
inside mustsac before update, task 0, sumup 70597
inside mustsac after update, task 0, sumup 69993
inner_dict_sum {'sac_diff0': 0.000213623046875, 'sac_diff1': 0.006570577621459961, 'sac_diff2': 0.007871627807617188, 'sac_diff3': 0.010291337966918945, 'sac_diff4': 0.006845712661743164, 'sac_diff5': 0.05169081687927246, 'sac_diff6': 0.0004181861877441406, 'all': 0.08390188217163086}
diff5_list [0.011081933975219727, 0.01015019416809082, 0.010157585144042969, 0.010206460952758789, 0.010094642639160156]
time3 0.0008380413055419922
time4 0.08477163314819336
time5 0.0848240852355957
time7 0.008946895599365234
gen_weight_change tensor(-18.3113)
policy weight change tensor(35.5801, grad_fn=<SumBackward0>)
time8 0.0018126964569091797
train_time 0.1136176586151123
eval time 0.11806201934814453
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:17,431 MainThread INFO: EPOCH:795
2024-01-23 01:04:17,431 MainThread INFO: Time Consumed:0.23469924926757812s
2024-01-23 01:04:17,431 MainThread INFO: Total Frames:120150s
  8%|▊         | 796/10000 [05:45<38:59,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16042.66632
Train_Epoch_Reward                4603.09503
Running_Training_Average_Rewards  15962.70371
Explore_Time                      0.00081
Train___Time                      0.11362
Eval____Time                      0.11806
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15328.07422
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.28041     3.58604     94.90745     86.34653
alpha_0                           0.67165      0.00009     0.67178      0.67151
Alpha_loss                        -2.68092     0.00223     -2.67832     -2.68469
Training/policy_loss              -4.03034     0.12463     -3.83087     -4.13256
Training/qf1_loss                 6655.60957   1207.30986  8427.24023   5274.22754
Training/qf2_loss                 15258.53340  1862.37120  17785.43750  13255.91602
Training/pf_norm                  0.10266      0.02080     0.12685      0.06689
Training/qf1_norm                 1595.68953   1378.61125  4201.65771   358.87350
Training/qf2_norm                 1357.43870   90.93565    1466.81982   1215.72278
log_std/mean                      -0.12666     0.00384     -0.12259     -0.13326
log_probs/mean                    -2.73730     0.00521     -2.73143     -2.74678
mean/mean                         -0.00323     0.00131     -0.00179     -0.00522
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01853466033935547
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69993
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [120450]
collect time 0.0009222030639648438
inner_dict_sum {'sac_diff0': 0.0002281665802001953, 'sac_diff1': 0.006665706634521484, 'sac_diff2': 0.007639646530151367, 'sac_diff3': 0.00970911979675293, 'sac_diff4': 0.006606101989746094, 'sac_diff5': 0.030948877334594727, 'sac_diff6': 0.00036907196044921875, 'all': 0.062166690826416016}
diff5_list [0.006512880325317383, 0.0060977935791015625, 0.006245136260986328, 0.006079435348510742, 0.006013631820678711]
time3 0
time4 0.06290411949157715
time5 0.06294775009155273
time7 7.152557373046875e-07
gen_weight_change tensor(-18.3113)
policy weight change tensor(35.5629, grad_fn=<SumBackward0>)
time8 0.0017826557159423828
train_time 0.07357621192932129
eval time 0.15197467803955078
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:17,682 MainThread INFO: EPOCH:796
2024-01-23 01:04:17,682 MainThread INFO: Time Consumed:0.22872662544250488s
2024-01-23 01:04:17,682 MainThread INFO: Total Frames:120300s
  8%|▊         | 797/10000 [05:45<38:49,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16042.51102
Train_Epoch_Reward                14283.25369
Running_Training_Average_Rewards  15848.10555
Explore_Time                      0.00092
Train___Time                      0.07358
Eval____Time                      0.15197
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16673.50599
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.62592     1.99579     92.62971     87.12717
alpha_0                           0.67131      0.00009     0.67145      0.67118
Alpha_loss                        -2.68299     0.00390     -2.67672     -2.68795
Training/policy_loss              -4.23177     0.00565     -4.22600     -4.24144
Training/qf1_loss                 6987.98809   1026.94169  8272.25977   5408.24023
Training/qf2_loss                 15704.50156  1376.44978  17357.42969  13480.10645
Training/pf_norm                  0.10016      0.02824     0.14847      0.07007
Training/qf1_norm                 1082.10593   355.70043   1455.89624   450.57025
Training/qf2_norm                 1416.76023   30.70551    1450.39673   1363.75488
log_std/mean                      -0.11921     0.00007     -0.11913     -0.11929
log_probs/mean                    -2.73404     0.00856     -2.71999     -2.74650
mean/mean                         -0.00567     0.00009     -0.00554     -0.00581
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0185854434967041
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69993
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [120600]
collect time 0.0008587837219238281
inner_dict_sum {'sac_diff0': 0.000247955322265625, 'sac_diff1': 0.006540536880493164, 'sac_diff2': 0.0073468685150146484, 'sac_diff3': 0.009828805923461914, 'sac_diff4': 0.006509065628051758, 'sac_diff5': 0.032074689865112305, 'sac_diff6': 0.00037789344787597656, 'all': 0.06292581558227539}
diff5_list [0.006356239318847656, 0.006181955337524414, 0.006391048431396484, 0.006976127624511719, 0.006169319152832031]
time3 0
time4 0.06365323066711426
time5 0.06370282173156738
time7 4.76837158203125e-07
gen_weight_change tensor(-18.3113)
policy weight change tensor(35.5768, grad_fn=<SumBackward0>)
time8 0.0018000602722167969
train_time 0.07436251640319824
eval time 0.15261626243591309
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:17,934 MainThread INFO: EPOCH:797
2024-01-23 01:04:17,934 MainThread INFO: Time Consumed:0.23005938529968262s
2024-01-23 01:04:17,934 MainThread INFO: Total Frames:120450s
  8%|▊         | 798/10000 [05:45<38:46,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16035.81729
Train_Epoch_Reward                5213.21780
Running_Training_Average_Rewards  15202.15010
Explore_Time                      0.00085
Train___Time                      0.07436
Eval____Time                      0.15262
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16718.08032
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.82023     1.94182     93.47703     87.52007
alpha_0                           0.67098      0.00009     0.67111      0.67084
Alpha_loss                        -2.68494     0.00128     -2.68306     -2.68694
Training/policy_loss              -3.95610     0.00306     -3.95109     -3.95969
Training/qf1_loss                 7873.51562   1408.23108  10133.34180  5681.96191
Training/qf2_loss                 16632.13750  1751.58393  19378.00977  13841.05273
Training/pf_norm                  0.13266      0.03519     0.18766      0.08660
Training/qf1_norm                 1540.37257   373.70692   2115.50879   966.72491
Training/qf2_norm                 1307.59509   27.51553    1345.20203   1260.10876
log_std/mean                      -0.13517     0.00001     -0.13515     -0.13519
log_probs/mean                    -2.73049     0.00264     -2.72851     -2.73550
mean/mean                         -0.00639     0.00030     -0.00606     -0.00688
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01833033561706543
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69993
epoch first part time 2.86102294921875e-06
replay_buffer._size: [120750]
collect time 0.0010230541229248047
inner_dict_sum {'sac_diff0': 0.0002319812774658203, 'sac_diff1': 0.00667881965637207, 'sac_diff2': 0.00798177719116211, 'sac_diff3': 0.010710716247558594, 'sac_diff4': 0.00690913200378418, 'sac_diff5': 0.03226494789123535, 'sac_diff6': 0.00038623809814453125, 'all': 0.06516361236572266}
diff5_list [0.007832050323486328, 0.006130218505859375, 0.006062030792236328, 0.006306886672973633, 0.0059337615966796875]
time3 0
time4 0.06589293479919434
time5 0.06593728065490723
time7 4.76837158203125e-07
gen_weight_change tensor(-18.3113)
policy weight change tensor(35.6058, grad_fn=<SumBackward0>)
time8 0.001903533935546875
train_time 0.07727503776550293
eval time 0.15176129341125488
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:18,189 MainThread INFO: EPOCH:798
2024-01-23 01:04:18,189 MainThread INFO: Time Consumed:0.23234248161315918s
2024-01-23 01:04:18,189 MainThread INFO: Total Frames:120600s
  8%|▊         | 799/10000 [05:45<38:51,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16028.56682
Train_Epoch_Reward                9791.05710
Running_Training_Average_Rewards  15125.64380
Explore_Time                      0.00102
Train___Time                      0.07728
Eval____Time                      0.15176
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16703.90030
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.00065     2.11561     92.67051     87.33514
alpha_0                           0.67064      0.00009     0.67077      0.67051
Alpha_loss                        -2.68923     0.00348     -2.68322     -2.69320
Training/policy_loss              -3.98346     0.00576     -3.97511     -3.99154
Training/qf1_loss                 6901.26768   793.81928   8040.98682   5909.23926
Training/qf2_loss                 15383.44805  1223.86871  16940.97461  13936.97852
Training/pf_norm                  0.11693      0.02031     0.13718      0.08209
Training/qf1_norm                 2478.05713   427.70836   3028.06299   1944.53101
Training/qf2_norm                 1371.14121   32.39484    1411.70837   1330.08850
log_std/mean                      -0.12372     0.00007     -0.12363     -0.12383
log_probs/mean                    -2.73280     0.00770     -2.72113     -2.74274
mean/mean                         -0.00458     0.00013     -0.00437     -0.00473
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018604040145874023
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69993
epoch first part time 2.86102294921875e-06
replay_buffer._size: [120900]
collect time 0.0009431838989257812
inner_dict_sum {'sac_diff0': 0.0002193450927734375, 'sac_diff1': 0.006701946258544922, 'sac_diff2': 0.007900476455688477, 'sac_diff3': 0.010150909423828125, 'sac_diff4': 0.006800174713134766, 'sac_diff5': 0.03214120864868164, 'sac_diff6': 0.00038313865661621094, 'all': 0.06429719924926758}
diff5_list [0.007140159606933594, 0.006544351577758789, 0.006095409393310547, 0.0060160160064697266, 0.006345272064208984]
time3 0
time4 0.06506013870239258
time5 0.06510376930236816
time7 7.152557373046875e-07
gen_weight_change tensor(-18.3113)
policy weight change tensor(35.6631, grad_fn=<SumBackward0>)
time8 0.0018465518951416016
train_time 0.07606387138366699
eval time 0.16188645362854004
epoch last part time 5.4836273193359375e-06
2024-01-23 01:04:18,452 MainThread INFO: EPOCH:799
2024-01-23 01:04:18,452 MainThread INFO: Time Consumed:0.2413315773010254s
2024-01-23 01:04:18,453 MainThread INFO: Total Frames:120750s
  8%|▊         | 800/10000 [05:46<39:23,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16030.66907
Train_Epoch_Reward                13887.66879
Running_Training_Average_Rewards  15093.20315
Explore_Time                      0.00094
Train___Time                      0.07606
Eval____Time                      0.16189
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16648.53771
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.24849     1.35252    93.09318     89.50277
alpha_0                           0.67031      0.00009    0.67044      0.67017
Alpha_loss                        -2.69420     0.00221    -2.69059     -2.69725
Training/policy_loss              -4.01114     0.00353    -4.00579     -4.01626
Training/qf1_loss                 7306.50371   707.52527  8294.82715   6631.53369
Training/qf2_loss                 16166.27773  891.31505  17525.49023  15126.34082
Training/pf_norm                  0.09745      0.01869    0.12723      0.07904
Training/qf1_norm                 795.81134    302.15383  1222.67810   449.14310
Training/qf2_norm                 1333.71907   19.66555   1359.62903   1308.05225
log_std/mean                      -0.12228     0.00012    -0.12213     -0.12249
log_probs/mean                    -2.73682     0.00487    -2.72948     -2.74443
mean/mean                         -0.00661     0.00009    -0.00648     -0.00674
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019884586334228516
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69993
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [121050]
collect time 0.0009217262268066406
inside mustsac before update, task 0, sumup 69993
inside mustsac after update, task 0, sumup 71107
inner_dict_sum {'sac_diff0': 0.00021696090698242188, 'sac_diff1': 0.00769352912902832, 'sac_diff2': 0.008795499801635742, 'sac_diff3': 0.011494159698486328, 'sac_diff4': 0.008051872253417969, 'sac_diff5': 0.0559999942779541, 'sac_diff6': 0.0004353523254394531, 'all': 0.09268736839294434}
diff5_list [0.012031078338623047, 0.011119842529296875, 0.011307954788208008, 0.011079072952270508, 0.010462045669555664]
time3 0.0008652210235595703
time4 0.09355974197387695
time5 0.09361481666564941
time7 0.009109020233154297
gen_weight_change tensor(-18.2638)
policy weight change tensor(35.7699, grad_fn=<SumBackward0>)
time8 0.002579927444458008
train_time 0.12430357933044434
eval time 0.1302173137664795
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:18,734 MainThread INFO: EPOCH:800
2024-01-23 01:04:18,734 MainThread INFO: Time Consumed:0.2576735019683838s
2024-01-23 01:04:18,734 MainThread INFO: Total Frames:120900s
  8%|▊         | 801/10000 [05:46<40:35,  3.78it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16043.17527
Train_Epoch_Reward                11676.29206
Running_Training_Average_Rewards  14851.76026
Explore_Time                      0.00092
Train___Time                      0.12430
Eval____Time                      0.13022
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16538.80146
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.96657     1.55472     92.03344     88.45634
alpha_0                           0.66997      0.00009     0.67010      0.66984
Alpha_loss                        -2.69578     0.00237     -2.69213     -2.69870
Training/policy_loss              -4.08679     0.12495     -3.92882     -4.30794
Training/qf1_loss                 6614.98262   854.68540   8059.08838   5459.77197
Training/qf2_loss                 15194.58418  1135.83661  17101.11523  13760.41699
Training/pf_norm                  0.11917      0.03302     0.17918      0.09202
Training/qf1_norm                 1100.29969   460.99739   1754.41162   485.39630
Training/qf2_norm                 1353.99531   36.18513    1405.13000   1293.54944
log_std/mean                      -0.12753     0.00518     -0.11919     -0.13519
log_probs/mean                    -2.73234     0.00508     -2.72491     -2.73835
mean/mean                         -0.00470     0.00230     -0.00053     -0.00749
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01837778091430664
epoch last part time3 0.0027718544006347656
inside rlalgo, task 0, sumup 71107
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [121200]
collect time 0.0009808540344238281
inner_dict_sum {'sac_diff0': 0.00023651123046875, 'sac_diff1': 0.00683283805847168, 'sac_diff2': 0.007872581481933594, 'sac_diff3': 0.01028299331665039, 'sac_diff4': 0.007161140441894531, 'sac_diff5': 0.03183102607727051, 'sac_diff6': 0.00038504600524902344, 'all': 0.06460213661193848}
diff5_list [0.0069293975830078125, 0.006274700164794922, 0.00621485710144043, 0.006191253662109375, 0.006220817565917969]
time3 0
time4 0.06535935401916504
time5 0.06540536880493164
time7 4.76837158203125e-07
gen_weight_change tensor(-18.2638)
policy weight change tensor(35.8891, grad_fn=<SumBackward0>)
time8 0.001965761184692383
train_time 0.07644343376159668
eval time 0.5108952522277832
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:19,349 MainThread INFO: EPOCH:801
2024-01-23 01:04:19,350 MainThread INFO: Time Consumed:0.590813159942627s
2024-01-23 01:04:19,350 MainThread INFO: Total Frames:121050s
  8%|▊         | 802/10000 [05:46<56:36,  2.71it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16137.61289
Train_Epoch_Reward                9449.36994
Running_Training_Average_Rewards  14403.86057
Explore_Time                      0.00098
Train___Time                      0.07644
Eval____Time                      0.51090
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16606.53638
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.75185     2.37045     93.42386     86.95949
alpha_0                           0.66964      0.00009     0.66977      0.66950
Alpha_loss                        -2.69982     0.00317     -2.69532     -2.70478
Training/policy_loss              -4.53723     0.00399     -4.53166     -4.54354
Training/qf1_loss                 6724.15586   802.61257   7734.42871   5309.92871
Training/qf2_loss                 15267.15566  1178.10331  16942.72656  13346.83594
Training/pf_norm                  0.12930      0.01643     0.15528      0.10511
Training/qf1_norm                 1096.49058   445.68441   1805.69348   566.78467
Training/qf2_norm                 1566.19019   41.96271    1630.92444   1517.93762
log_std/mean                      -0.12346     0.00019     -0.12319     -0.12373
log_probs/mean                    -2.73403     0.00718     -2.72449     -2.74640
mean/mean                         -0.00654     0.00028     -0.00621     -0.00700
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019014596939086914
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71107
epoch first part time 2.86102294921875e-06
replay_buffer._size: [121350]
collect time 0.0009560585021972656
inner_dict_sum {'sac_diff0': 0.00020885467529296875, 'sac_diff1': 0.007375001907348633, 'sac_diff2': 0.008794307708740234, 'sac_diff3': 0.011742591857910156, 'sac_diff4': 0.007478475570678711, 'sac_diff5': 0.03369641304016113, 'sac_diff6': 0.0003914833068847656, 'all': 0.0696871280670166}
diff5_list [0.006560564041137695, 0.007068634033203125, 0.007388591766357422, 0.0064182281494140625, 0.006260395050048828]
time3 0
time4 0.07046270370483398
time5 0.0705101490020752
time7 7.152557373046875e-07
gen_weight_change tensor(-18.2638)
policy weight change tensor(36.0371, grad_fn=<SumBackward0>)
time8 0.0018298625946044922
train_time 0.08168435096740723
eval time 0.21112990379333496
epoch last part time 5.4836273193359375e-06
2024-01-23 01:04:19,668 MainThread INFO: EPOCH:802
2024-01-23 01:04:19,669 MainThread INFO: Time Consumed:0.29609012603759766s
2024-01-23 01:04:19,669 MainThread INFO: Total Frames:121200s
  8%|▊         | 803/10000 [05:47<54:15,  2.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16229.31526
Train_Epoch_Reward                8813.36887
Running_Training_Average_Rewards  14624.14421
Explore_Time                      0.00095
Train___Time                      0.08168
Eval____Time                      0.21113
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16400.23878
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.09890     2.27629     92.37440     86.49088
alpha_0                           0.66930      0.00009     0.66943      0.66917
Alpha_loss                        -2.70212     0.00329     -2.69732     -2.70660
Training/policy_loss              -4.11845     0.00434     -4.11089     -4.12306
Training/qf1_loss                 6872.97041   869.86683   8093.36963   5789.93848
Training/qf2_loss                 15490.44531  1096.00299  17203.79102  13918.41211
Training/pf_norm                  0.11544      0.02732     0.15723      0.07334
Training/qf1_norm                 1270.88793   465.77497   2048.03320   765.05328
Training/qf2_norm                 1411.54612   36.16360    1448.49268   1354.56592
log_std/mean                      -0.12859     0.00023     -0.12829     -0.12893
log_probs/mean                    -2.73137     0.00690     -2.71942     -2.73919
mean/mean                         -0.00843     0.00028     -0.00802     -0.00881
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01852893829345703
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71107
epoch first part time 2.86102294921875e-06
replay_buffer._size: [121369]
collect time 0.0008301734924316406
inner_dict_sum {'sac_diff0': 0.0002377033233642578, 'sac_diff1': 0.006452083587646484, 'sac_diff2': 0.0075092315673828125, 'sac_diff3': 0.009829282760620117, 'sac_diff4': 0.006472349166870117, 'sac_diff5': 0.03171825408935547, 'sac_diff6': 0.00042438507080078125, 'all': 0.06264328956604004}
diff5_list [0.006488800048828125, 0.006178855895996094, 0.0062177181243896484, 0.006433725357055664, 0.0063991546630859375]
time3 0
time4 0.06344199180603027
time5 0.06348729133605957
time7 7.152557373046875e-07
gen_weight_change tensor(-18.2638)
policy weight change tensor(36.1891, grad_fn=<SumBackward0>)
time8 0.0018110275268554688
train_time 0.0742654800415039
eval time 0.1512465476989746
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:19,919 MainThread INFO: EPOCH:803
2024-01-23 01:04:19,919 MainThread INFO: Time Consumed:0.22854208946228027s
2024-01-23 01:04:19,919 MainThread INFO: Total Frames:121350s
  8%|▊         | 804/10000 [05:47<49:30,  3.10it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16326.79635
Train_Epoch_Reward                92844.24226
Running_Training_Average_Rewards  17248.81317
Explore_Time                      0.00083
Train___Time                      0.07427
Eval____Time                      0.15125
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16335.08348
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.53971     1.70701     93.37705     88.53036
alpha_0                           0.66897      0.00009     0.66910      0.66883
Alpha_loss                        -2.70751     0.00185     -2.70454     -2.71001
Training/policy_loss              -4.25266     0.00518     -4.24645     -4.25949
Training/qf1_loss                 7316.42578   1161.56529  9316.77051   6222.04199
Training/qf2_loss                 16019.93418  1295.79164  18143.99414  14499.45898
Training/pf_norm                  0.16460      0.02049     0.19479      0.14581
Training/qf1_norm                 1403.97505   337.58838   1784.08167   880.19946
Training/qf2_norm                 1474.43704   29.37410    1522.78699   1438.81384
log_std/mean                      -0.13224     0.00015     -0.13204     -0.13248
log_probs/mean                    -2.73640     0.00390     -2.73069     -2.74010
mean/mean                         -0.00893     0.00021     -0.00862     -0.00923
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018748760223388672
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71107
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [121650]
collect time 0.0009324550628662109
inner_dict_sum {'sac_diff0': 0.00023365020751953125, 'sac_diff1': 0.0066263675689697266, 'sac_diff2': 0.007820606231689453, 'sac_diff3': 0.010328054428100586, 'sac_diff4': 0.0069310665130615234, 'sac_diff5': 0.034039974212646484, 'sac_diff6': 0.00041961669921875, 'all': 0.06639933586120605}
diff5_list [0.0063478946685791016, 0.006095170974731445, 0.0061261653900146484, 0.00773310661315918, 0.007737636566162109]
time3 0
time4 0.0671846866607666
time5 0.06723189353942871
time7 7.152557373046875e-07
gen_weight_change tensor(-18.2638)
policy weight change tensor(36.3403, grad_fn=<SumBackward0>)
time8 0.0018863677978515625
train_time 0.07803153991699219
eval time 0.14930152893066406
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:20,172 MainThread INFO: EPOCH:804
2024-01-23 01:04:20,172 MainThread INFO: Time Consumed:0.23043537139892578s
2024-01-23 01:04:20,172 MainThread INFO: Total Frames:121500s
  8%|▊         | 805/10000 [05:47<46:15,  3.31it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16439.92375
Train_Epoch_Reward                16925.18204
Running_Training_Average_Rewards  17247.66180
Explore_Time                      0.00092
Train___Time                      0.07803
Eval____Time                      0.14930
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16446.47882
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       88.80758     1.69233     90.60184     86.04512
alpha_0                           0.66863      0.00009     0.66877      0.66850
Alpha_loss                        -2.71109     0.00407     -2.70699     -2.71849
Training/policy_loss              -4.40043     0.00576     -4.39139     -4.40931
Training/qf1_loss                 7040.66445   1274.93993  9156.47070   5165.47168
Training/qf2_loss                 15432.37305  1592.55075  17935.58398  12982.26367
Training/pf_norm                  0.11544      0.02022     0.14299      0.08943
Training/qf1_norm                 1290.70171   330.41243   1790.47681   948.53088
Training/qf2_norm                 1493.08145   29.16581    1522.09827   1446.05725
log_std/mean                      -0.12599     0.00016     -0.12581     -0.12627
log_probs/mean                    -2.73694     0.00901     -2.72508     -2.75196
mean/mean                         -0.00987     0.00013     -0.00965     -0.01000
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0180966854095459
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71107
epoch first part time 2.86102294921875e-06
replay_buffer._size: [121800]
collect time 0.0008180141448974609
inside mustsac before update, task 0, sumup 71107
inside mustsac after update, task 0, sumup 70352
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.0066928863525390625, 'sac_diff2': 0.008179903030395508, 'sac_diff3': 0.010695934295654297, 'sac_diff4': 0.007284402847290039, 'sac_diff5': 0.05150294303894043, 'sac_diff6': 0.00040531158447265625, 'all': 0.08497428894042969}
diff5_list [0.011016368865966797, 0.01019597053527832, 0.010160684585571289, 0.010317564010620117, 0.009812355041503906]
time3 0.0008075237274169922
time4 0.08578681945800781
time5 0.08583521842956543
time7 0.009064912796020508
gen_weight_change tensor(-18.2639)
policy weight change tensor(36.4037, grad_fn=<SumBackward0>)
time8 0.0018863677978515625
train_time 0.11505556106567383
eval time 0.11805081367492676
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:20,430 MainThread INFO: EPOCH:805
2024-01-23 01:04:20,430 MainThread INFO: Time Consumed:0.23615169525146484s
2024-01-23 01:04:20,430 MainThread INFO: Total Frames:121650s
  8%|▊         | 806/10000 [05:48<44:14,  3.46it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16571.44273
Train_Epoch_Reward                9412.07655
Running_Training_Average_Rewards  16374.00051
Explore_Time                      0.00081
Train___Time                      0.11506
Eval____Time                      0.11805
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16643.26409
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.34810     1.49865    94.10362     90.14437
alpha_0                           0.66830      0.00009    0.66843      0.66816
Alpha_loss                        -2.71222     0.00284    -2.70813     -2.71474
Training/policy_loss              -4.24201     0.21221    -3.87809     -4.52865
Training/qf1_loss                 8066.91182   585.45037  8629.97363   7071.38770
Training/qf2_loss                 17098.69453  653.75394  18051.61328  16035.80469
Training/pf_norm                  0.10902      0.02482    0.13839      0.07543
Training/qf1_norm                 1057.42949   515.63652  1964.12122   560.37433
Training/qf2_norm                 1500.23601   94.25204   1656.29346   1361.69958
log_std/mean                      -0.13220     0.00566    -0.12477     -0.14106
log_probs/mean                    -2.73138     0.00496    -2.72457     -2.73595
mean/mean                         -0.00812     0.00221    -0.00560     -0.01164
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018329620361328125
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70352
epoch first part time 2.384185791015625e-06
replay_buffer._size: [121950]
collect time 0.0009322166442871094
inner_dict_sum {'sac_diff0': 0.00022983551025390625, 'sac_diff1': 0.0063991546630859375, 'sac_diff2': 0.007730722427368164, 'sac_diff3': 0.00957942008972168, 'sac_diff4': 0.006575345993041992, 'sac_diff5': 0.031591176986694336, 'sac_diff6': 0.0003745555877685547, 'all': 0.06248021125793457}
diff5_list [0.0063855648040771484, 0.006161928176879883, 0.006022453308105469, 0.006434202194213867, 0.006587028503417969]
time3 0
time4 0.0632164478302002
time5 0.06325984001159668
time7 4.76837158203125e-07
gen_weight_change tensor(-18.2639)
policy weight change tensor(36.5159, grad_fn=<SumBackward0>)
time8 0.0017962455749511719
train_time 0.0739893913269043
eval time 0.15204620361328125
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:20,681 MainThread INFO: EPOCH:806
2024-01-23 01:04:20,681 MainThread INFO: Time Consumed:0.22922396659851074s
2024-01-23 01:04:20,681 MainThread INFO: Total Frames:121800s
  8%|▊         | 807/10000 [05:48<42:31,  3.60it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16475.09988
Train_Epoch_Reward                39186.49255
Running_Training_Average_Rewards  17518.09176
Explore_Time                      0.00093
Train___Time                      0.07399
Eval____Time                      0.15205
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15710.07747
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.59274     2.46885     94.92985     88.56824
alpha_0                           0.66796      0.00009     0.66810      0.66783
Alpha_loss                        -2.71252     0.00261     -2.70844     -2.71604
Training/policy_loss              -3.78201     0.00386     -3.77573     -3.78746
Training/qf1_loss                 7822.62422   1181.85346  8829.27148   5768.86084
Training/qf2_loss                 16963.32910  1648.64386  18299.20312  14135.37695
Training/pf_norm                  0.12110      0.03198     0.16157      0.07033
Training/qf1_norm                 779.39345    430.95829   1209.33020   120.16182
Training/qf2_norm                 1255.54236   33.44261    1286.60156   1201.28015
log_std/mean                      -0.13044     0.00014     -0.13021     -0.13059
log_probs/mean                    -2.72378     0.00564     -2.71368     -2.73085
mean/mean                         -0.01076     0.00003     -0.01070     -0.01079
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018547773361206055
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70352
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [122100]
collect time 0.0008647441864013672
inner_dict_sum {'sac_diff0': 0.00022268295288085938, 'sac_diff1': 0.006407260894775391, 'sac_diff2': 0.007606029510498047, 'sac_diff3': 0.009879827499389648, 'sac_diff4': 0.006717681884765625, 'sac_diff5': 0.030679702758789062, 'sac_diff6': 0.00038504600524902344, 'all': 0.061898231506347656}
diff5_list [0.006200313568115234, 0.0060465335845947266, 0.00592041015625, 0.006409883499145508, 0.006102561950683594]
time3 0
time4 0.06264400482177734
time5 0.06268978118896484
time7 4.76837158203125e-07
gen_weight_change tensor(-18.2639)
policy weight change tensor(36.4572, grad_fn=<SumBackward0>)
time8 0.0018074512481689453
train_time 0.07336592674255371
eval time 0.15309691429138184
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:20,933 MainThread INFO: EPOCH:807
2024-01-23 01:04:20,933 MainThread INFO: Time Consumed:0.2295529842376709s
2024-01-23 01:04:20,933 MainThread INFO: Total Frames:121950s
  8%|▊         | 808/10000 [05:48<41:19,  3.71it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16375.68933
Train_Epoch_Reward                14313.95682
Running_Training_Average_Rewards  17705.80901
Explore_Time                      0.00086
Train___Time                      0.07337
Eval____Time                      0.15310
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15723.97481
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.75706     2.02279     95.75621     90.32482
alpha_0                           0.66763      0.00009     0.66776      0.66750
Alpha_loss                        -2.71721     0.00158     -2.71439     -2.71927
Training/policy_loss              -4.00231     0.00243     -3.99909     -4.00596
Training/qf1_loss                 7885.58271   934.35324   9314.25586   6686.34912
Training/qf2_loss                 16992.48086  1271.64893  18994.27734  15447.08789
Training/pf_norm                  0.12261      0.05293     0.22324      0.07170
Training/qf1_norm                 1050.11360   404.50825   1647.76526   533.62439
Training/qf2_norm                 1370.18970   29.64457    1414.63269   1336.20959
log_std/mean                      -0.14220     0.00018     -0.14190     -0.14238
log_probs/mean                    -2.72708     0.00399     -2.72008     -2.73152
mean/mean                         -0.00915     0.00019     -0.00885     -0.00936
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01833963394165039
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70352
epoch first part time 2.86102294921875e-06
replay_buffer._size: [122250]
collect time 0.0009088516235351562
inner_dict_sum {'sac_diff0': 0.00020885467529296875, 'sac_diff1': 0.0065631866455078125, 'sac_diff2': 0.007944583892822266, 'sac_diff3': 0.010128498077392578, 'sac_diff4': 0.0070307254791259766, 'sac_diff5': 0.03269243240356445, 'sac_diff6': 0.00038695335388183594, 'all': 0.06495523452758789}
diff5_list [0.006471872329711914, 0.006025791168212891, 0.006300926208496094, 0.0066373348236083984, 0.007256507873535156]
time3 0
time4 0.06572818756103516
time5 0.06577348709106445
time7 7.152557373046875e-07
gen_weight_change tensor(-18.2639)
policy weight change tensor(36.4519, grad_fn=<SumBackward0>)
time8 0.002103090286254883
train_time 0.07714509963989258
eval time 0.15211129188537598
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:21,187 MainThread INFO: EPOCH:808
2024-01-23 01:04:21,202 MainThread INFO: Time Consumed:0.23246192932128906s
2024-01-23 01:04:21,203 MainThread INFO: Total Frames:122100s
  8%|▊         | 809/10000 [05:48<41:19,  3.71it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16271.44125
Train_Epoch_Reward                21566.67707
Running_Training_Average_Rewards  17922.97697
Explore_Time                      0.00090
Train___Time                      0.07715
Eval____Time                      0.15211
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15661.41947
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.76708     3.16954     96.00981     87.83348
alpha_0                           0.66730      0.00009     0.66743      0.66716
Alpha_loss                        -2.72090     0.00217     -2.71890     -2.72503
Training/policy_loss              -4.22757     0.00203     -4.22504     -4.23032
Training/qf1_loss                 7876.31797   1073.63817  9073.55469   6216.05469
Training/qf2_loss                 16834.68652  1640.33535  18334.11914  14489.17676
Training/pf_norm                  0.10907      0.00588     0.11326      0.09747
Training/qf1_norm                 623.35401    227.45919   964.78723    288.95554
Training/qf2_norm                 1564.54666   52.64654    1635.20544   1499.59253
log_std/mean                      -0.12412     0.00010     -0.12405     -0.12430
log_probs/mean                    -2.72787     0.00379     -2.72414     -2.73477
mean/mean                         -0.01024     0.00007     -0.01018     -0.01036
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0339961051940918
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70352
epoch first part time 7.152557373046875e-06
replay_buffer._size: [122400]
collect time 0.0007932186126708984
inner_dict_sum {'sac_diff0': 0.00021910667419433594, 'sac_diff1': 0.0078582763671875, 'sac_diff2': 0.008777856826782227, 'sac_diff3': 0.010864019393920898, 'sac_diff4': 0.007745504379272461, 'sac_diff5': 0.036151885986328125, 'sac_diff6': 0.00043582916259765625, 'all': 0.0720524787902832}
diff5_list [0.006417512893676758, 0.0062334537506103516, 0.011033773422241211, 0.0065326690673828125, 0.005934476852416992]
time3 0
time4 0.07293868064880371
time5 0.07299518585205078
time7 4.76837158203125e-07
gen_weight_change tensor(-18.2639)
policy weight change tensor(36.5510, grad_fn=<SumBackward0>)
time8 0.0022635459899902344
train_time 0.08523845672607422
eval time 0.13066697120666504
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:21,444 MainThread INFO: EPOCH:809
2024-01-23 01:04:21,444 MainThread INFO: Time Consumed:0.21928644180297852s
2024-01-23 01:04:21,444 MainThread INFO: Total Frames:122250s
  8%|▊         | 810/10000 [05:49<40:03,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16149.25919
Train_Epoch_Reward                6255.73173
Running_Training_Average_Rewards  17559.85784
Explore_Time                      0.00079
Train___Time                      0.08524
Eval____Time                      0.13067
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15426.71709
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.51580     2.07999     93.37577     88.03426
alpha_0                           0.66696      0.00009     0.66710      0.66683
Alpha_loss                        -2.72714     0.00310     -2.72458     -2.73298
Training/policy_loss              -4.05165     0.00524     -4.04488     -4.06107
Training/qf1_loss                 7962.47988   1276.73350  9420.75977   5897.42529
Training/qf2_loss                 16695.82988  1613.53456  18480.07422  14066.53125
Training/pf_norm                  0.09137      0.01780     0.10819      0.05772
Training/qf1_norm                 2462.41770   389.06952   2786.38208   1802.33191
Training/qf2_norm                 1373.96582   31.04952    1401.33472   1321.86841
log_std/mean                      -0.12542     0.00027     -0.12509     -0.12583
log_probs/mean                    -2.73500     0.00731     -2.72536     -2.74774
mean/mean                         -0.00977     0.00005     -0.00969     -0.00983
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01966238021850586
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70352
epoch first part time 4.5299530029296875e-06
replay_buffer._size: [122550]
collect time 0.0009775161743164062
inside mustsac before update, task 0, sumup 70352
inside mustsac after update, task 0, sumup 70099
inner_dict_sum {'sac_diff0': 0.00019788742065429688, 'sac_diff1': 0.006569385528564453, 'sac_diff2': 0.007668972015380859, 'sac_diff3': 0.010133981704711914, 'sac_diff4': 0.007199525833129883, 'sac_diff5': 0.05205178260803223, 'sac_diff6': 0.0003914833068847656, 'all': 0.0842130184173584}
diff5_list [0.01159358024597168, 0.01008749008178711, 0.01036977767944336, 0.009833812713623047, 0.010167121887207031]
time3 0.0008478164672851562
time4 0.08505678176879883
time5 0.08510804176330566
time7 0.009292364120483398
gen_weight_change tensor(-18.1764)
policy weight change tensor(36.5536, grad_fn=<SumBackward0>)
time8 0.0025606155395507812
train_time 0.11532378196716309
eval time 0.11124277114868164
epoch last part time 3.814697265625e-06
2024-01-23 01:04:21,697 MainThread INFO: EPOCH:810
2024-01-23 01:04:21,697 MainThread INFO: Time Consumed:0.22971820831298828s
2024-01-23 01:04:21,697 MainThread INFO: Total Frames:122400s
  8%|▊         | 811/10000 [05:49<39:43,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16011.57111
Train_Epoch_Reward                8444.51007
Running_Training_Average_Rewards  17200.52897
Explore_Time                      0.00097
Train___Time                      0.11532
Eval____Time                      0.11124
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15161.92073
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.78627     1.79684     93.56406     89.02818
alpha_0                           0.66663      0.00009     0.66676      0.66650
Alpha_loss                        -2.72881     0.00260     -2.72575     -2.73326
Training/policy_loss              -4.29168     0.18079     -3.99693     -4.54512
Training/qf1_loss                 7198.98682   781.15943   8264.41113   5901.47119
Training/qf2_loss                 16165.91562  1078.23630  17627.77344  14589.60449
Training/pf_norm                  0.10621      0.02102     0.12493      0.07367
Training/qf1_norm                 593.85018    355.29207   1104.13489   141.54906
Training/qf2_norm                 1496.95559   108.74889   1663.79382   1324.47266
log_std/mean                      -0.13406     0.00739     -0.12383     -0.14294
log_probs/mean                    -2.73082     0.00664     -2.71994     -2.74012
mean/mean                         -0.00884     0.00222     -0.00595     -0.01129
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018390417098999023
epoch last part time3 0.0025014877319335938
inside rlalgo, task 0, sumup 70099
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [122700]
collect time 0.000926971435546875
inner_dict_sum {'sac_diff0': 0.0002391338348388672, 'sac_diff1': 0.006479978561401367, 'sac_diff2': 0.007608652114868164, 'sac_diff3': 0.010010004043579102, 'sac_diff4': 0.006681203842163086, 'sac_diff5': 0.03155183792114258, 'sac_diff6': 0.00038433074951171875, 'all': 0.06295514106750488}
diff5_list [0.0063915252685546875, 0.006345987319946289, 0.006205558776855469, 0.006486177444458008, 0.006122589111328125]
time3 0
time4 0.06369709968566895
time5 0.06374096870422363
time7 7.152557373046875e-07
gen_weight_change tensor(-18.1764)
policy weight change tensor(36.6625, grad_fn=<SumBackward0>)
time8 0.0018188953399658203
train_time 0.07435822486877441
eval time 0.148817777633667
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:21,947 MainThread INFO: EPOCH:811
2024-01-23 01:04:21,948 MainThread INFO: Time Consumed:0.22632861137390137s
2024-01-23 01:04:21,948 MainThread INFO: Total Frames:122550s
  8%|▊         | 812/10000 [05:49<39:11,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15851.31226
Train_Epoch_Reward                45603.56011
Running_Training_Average_Rewards  18665.27010
Explore_Time                      0.00092
Train___Time                      0.07436
Eval____Time                      0.14882
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15003.94789
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.76174     0.84483    94.29662     91.83832
alpha_0                           0.66630      0.00009    0.66643      0.66616
Alpha_loss                        -2.73028     0.00153    -2.72799     -2.73202
Training/policy_loss              -4.36436     0.00205    -4.36041     -4.36625
Training/qf1_loss                 7812.04658   766.63199  8897.82812   6713.32568
Training/qf2_loss                 16978.20859  800.71785  17875.45312  15754.82520
Training/pf_norm                  0.11815      0.02476    0.14981      0.08554
Training/qf1_norm                 411.02290    155.84964  597.28857    149.19405
Training/qf2_norm                 1555.61187   15.17030   1582.87927   1542.53284
log_std/mean                      -0.13861     0.00019    -0.13836     -0.13888
log_probs/mean                    -2.72613     0.00517    -2.71885     -2.73375
mean/mean                         -0.00972     0.00008    -0.00960     -0.00982
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01816534996032715
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70099
epoch first part time 2.86102294921875e-06
replay_buffer._size: [122850]
collect time 0.0007991790771484375
inner_dict_sum {'sac_diff0': 0.00023031234741210938, 'sac_diff1': 0.006472587585449219, 'sac_diff2': 0.007483482360839844, 'sac_diff3': 0.010045528411865234, 'sac_diff4': 0.006847381591796875, 'sac_diff5': 0.0321345329284668, 'sac_diff6': 0.00038123130798339844, 'all': 0.06359505653381348}
diff5_list [0.006493330001831055, 0.006154298782348633, 0.006074190139770508, 0.006318569183349609, 0.007094144821166992]
time3 0
time4 0.06433343887329102
time5 0.0643768310546875
time7 4.76837158203125e-07
gen_weight_change tensor(-18.1764)
policy weight change tensor(36.7339, grad_fn=<SumBackward0>)
time8 0.0020585060119628906
train_time 0.07542920112609863
eval time 0.15509724617004395
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:22,203 MainThread INFO: EPOCH:812
2024-01-23 01:04:22,203 MainThread INFO: Time Consumed:0.23352932929992676s
2024-01-23 01:04:22,203 MainThread INFO: Total Frames:122700s
  8%|▊         | 813/10000 [05:49<39:10,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15707.16860
Train_Epoch_Reward                19411.57285
Running_Training_Average_Rewards  18802.71885
Explore_Time                      0.00080
Train___Time                      0.07543
Eval____Time                      0.15510
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14958.80211
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.84058     1.36674    92.34335     88.80401
alpha_0                           0.66596      0.00009    0.66610      0.66583
Alpha_loss                        -2.73479     0.00199    -2.73209     -2.73764
Training/policy_loss              -4.21794     0.00631    -4.21234     -4.22733
Training/qf1_loss                 6978.74297   326.19677  7278.74365   6387.34912
Training/qf2_loss                 15748.93359  493.46523  16330.98242  14955.21680
Training/pf_norm                  0.20002      0.04651    0.24579      0.11557
Training/qf1_norm                 495.33087    283.50002  877.47620    163.99411
Training/qf2_norm                 1477.14177   21.78203   1499.04846   1441.98621
log_std/mean                      -0.14117     0.00013    -0.14100     -0.14135
log_probs/mean                    -2.72898     0.00426    -2.72233     -2.73429
mean/mean                         -0.00609     0.00025    -0.00589     -0.00654
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018522024154663086
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70099
epoch first part time 2.384185791015625e-06
replay_buffer._size: [123000]
collect time 0.0008306503295898438
inner_dict_sum {'sac_diff0': 0.00021839141845703125, 'sac_diff1': 0.006623506546020508, 'sac_diff2': 0.007929086685180664, 'sac_diff3': 0.009632587432861328, 'sac_diff4': 0.006541728973388672, 'sac_diff5': 0.031164884567260742, 'sac_diff6': 0.00037789344787597656, 'all': 0.06248807907104492}
diff5_list [0.0062906742095947266, 0.006437778472900391, 0.006081104278564453, 0.006230831146240234, 0.0061244964599609375]
time3 0
time4 0.06324076652526855
time5 0.06328487396240234
time7 7.152557373046875e-07
gen_weight_change tensor(-18.1764)
policy weight change tensor(36.7911, grad_fn=<SumBackward0>)
time8 0.0018451213836669922
train_time 0.07407021522521973
eval time 0.14895939826965332
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:22,451 MainThread INFO: EPOCH:813
2024-01-23 01:04:22,451 MainThread INFO: Time Consumed:0.22618651390075684s
2024-01-23 01:04:22,451 MainThread INFO: Total Frames:122850s
  8%|▊         | 814/10000 [05:50<38:47,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15565.47359
Train_Epoch_Reward                7626.17542
Running_Training_Average_Rewards  18779.17996
Explore_Time                      0.00083
Train___Time                      0.07407
Eval____Time                      0.14896
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14918.13341
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.75789     3.81919     97.79935     86.85918
alpha_0                           0.66563      0.00009     0.66576      0.66550
Alpha_loss                        -2.73982     0.00197     -2.73809     -2.74354
Training/policy_loss              -4.19603     0.00412     -4.19131     -4.20355
Training/qf1_loss                 6935.15010   942.49363   8499.52246   6020.12988
Training/qf2_loss                 15714.10176  1679.24964  18670.05664  14017.61719
Training/pf_norm                  0.10731      0.01045     0.11990      0.09030
Training/qf1_norm                 845.44348    302.15635   1199.06995   368.25882
Training/qf2_norm                 1437.19609   59.81277    1547.03223   1375.43311
log_std/mean                      -0.12529     0.00013     -0.12516     -0.12551
log_probs/mean                    -2.73307     0.00681     -2.72550     -2.74552
mean/mean                         -0.01131     0.00019     -0.01103     -0.01157
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.017777204513549805
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70099
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [123150]
collect time 0.0008008480072021484
inner_dict_sum {'sac_diff0': 0.0002410411834716797, 'sac_diff1': 0.0064601898193359375, 'sac_diff2': 0.007580757141113281, 'sac_diff3': 0.009459257125854492, 'sac_diff4': 0.006504535675048828, 'sac_diff5': 0.03027486801147461, 'sac_diff6': 0.00037980079650878906, 'all': 0.06090044975280762}
diff5_list [0.006316661834716797, 0.005838155746459961, 0.006217479705810547, 0.006028175354003906, 0.0058743953704833984]
time3 0
time4 0.06160330772399902
time5 0.061644792556762695
time7 9.5367431640625e-07
gen_weight_change tensor(-18.1764)
policy weight change tensor(36.7828, grad_fn=<SumBackward0>)
time8 0.0017883777618408203
train_time 0.07225918769836426
eval time 0.15499162673950195
epoch last part time 4.0531158447265625e-06
2024-01-23 01:04:22,703 MainThread INFO: EPOCH:814
2024-01-23 01:04:22,703 MainThread INFO: Time Consumed:0.23026514053344727s
2024-01-23 01:04:22,703 MainThread INFO: Total Frames:123000s
  8%|▊         | 815/10000 [05:50<38:42,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15398.18293
Train_Epoch_Reward                24981.15550
Running_Training_Average_Rewards  19275.44543
Explore_Time                      0.00080
Train___Time                      0.07226
Eval____Time                      0.15499
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14773.57223
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.15696     0.47951    90.83018     89.35186
alpha_0                           0.66530      0.00009    0.66543      0.66517
Alpha_loss                        -2.74381     0.00171    -2.74137     -2.74669
Training/policy_loss              -4.40620     0.00232    -4.40213     -4.40915
Training/qf1_loss                 6807.61729   350.58499  7266.57178   6282.24268
Training/qf2_loss                 15359.12832  391.90115  15950.47461  14837.81543
Training/pf_norm                  0.09279      0.03258    0.14369      0.04229
Training/qf1_norm                 1851.05581   82.26965   1993.43823   1740.55969
Training/qf2_norm                 1537.29402   7.72195    1548.66626   1524.51636
log_std/mean                      -0.13171     0.00005    -0.13164     -0.13178
log_probs/mean                    -2.73460     0.00265    -2.73192     -2.73837
mean/mean                         -0.00750     0.00020    -0.00723     -0.00779
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018010377883911133
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70099
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [123300]
collect time 0.0008835792541503906
inside mustsac before update, task 0, sumup 70099
inside mustsac after update, task 0, sumup 70906
inner_dict_sum {'sac_diff0': 0.00020003318786621094, 'sac_diff1': 0.00635528564453125, 'sac_diff2': 0.007557868957519531, 'sac_diff3': 0.009681463241577148, 'sac_diff4': 0.0068013668060302734, 'sac_diff5': 0.05040764808654785, 'sac_diff6': 0.0003879070281982422, 'all': 0.08139157295227051}
diff5_list [0.010255098342895508, 0.010173559188842773, 0.010370254516601562, 0.009887933731079102, 0.009720802307128906]
time3 0.0008246898651123047
time4 0.08217811584472656
time5 0.08222436904907227
time7 0.008983135223388672
gen_weight_change tensor(-18.0370)
policy weight change tensor(36.7576, grad_fn=<SumBackward0>)
time8 0.0018496513366699219
train_time 0.11065530776977539
eval time 0.1165778636932373
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:22,954 MainThread INFO: EPOCH:815
2024-01-23 01:04:22,955 MainThread INFO: Time Consumed:0.2303166389465332s
2024-01-23 01:04:22,955 MainThread INFO: Total Frames:123150s
  8%|▊         | 816/10000 [05:50<38:42,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15194.04662
Train_Epoch_Reward                12157.30532
Running_Training_Average_Rewards  17858.86214
Explore_Time                      0.00088
Train___Time                      0.11066
Eval____Time                      0.11658
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14601.90098
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.20995     1.01768    93.17249     90.28854
alpha_0                           0.66497      0.00009    0.66510      0.66483
Alpha_loss                        -2.74687     0.00304    -2.74213     -2.75035
Training/policy_loss              -4.39284     0.12458    -4.20269     -4.56628
Training/qf1_loss                 7121.95723   437.96491  7952.26953   6727.80859
Training/qf2_loss                 16133.29395  577.68865  17157.75391  15401.03613
Training/pf_norm                  0.12315      0.02395    0.15197      0.08316
Training/qf1_norm                 1225.72601   738.94278  1959.26086   173.22044
Training/qf2_norm                 1558.11858   57.16476   1634.88855   1503.70068
log_std/mean                      -0.13229     0.01004    -0.11783     -0.14500
log_probs/mean                    -2.73385     0.00824    -2.72223     -2.74487
mean/mean                         -0.00966     0.00100    -0.00833     -0.01080
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018825769424438477
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70906
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [123450]
collect time 0.0007572174072265625
inner_dict_sum {'sac_diff0': 0.00023126602172851562, 'sac_diff1': 0.0065691471099853516, 'sac_diff2': 0.0075871944427490234, 'sac_diff3': 0.01013326644897461, 'sac_diff4': 0.006776094436645508, 'sac_diff5': 0.03142666816711426, 'sac_diff6': 0.0003902912139892578, 'all': 0.06311392784118652}
diff5_list [0.006506681442260742, 0.006150484085083008, 0.006240129470825195, 0.006432771682739258, 0.006096601486206055]
time3 0
time4 0.06383967399597168
time5 0.06388235092163086
time7 4.76837158203125e-07
gen_weight_change tensor(-18.0370)
policy weight change tensor(36.7279, grad_fn=<SumBackward0>)
time8 0.0019207000732421875
train_time 0.07454109191894531
eval time 0.15837955474853516
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:23,212 MainThread INFO: EPOCH:816
2024-01-23 01:04:23,213 MainThread INFO: Time Consumed:0.23590517044067383s
2024-01-23 01:04:23,213 MainThread INFO: Total Frames:123300s
  8%|▊         | 817/10000 [05:50<38:55,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15058.22297
Train_Epoch_Reward                28741.61130
Running_Training_Average_Rewards  18474.88690
Explore_Time                      0.00075
Train___Time                      0.07454
Eval____Time                      0.15838
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14351.84094
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.34124     1.51659     95.78558     91.83604
alpha_0                           0.66463      0.00009     0.66477      0.66450
Alpha_loss                        -2.74960     0.00310     -2.74386     -2.75318
Training/policy_loss              -4.29825     0.00551     -4.28849     -4.30320
Training/qf1_loss                 7779.60391   814.66497   9103.83496   7041.95947
Training/qf2_loss                 17060.63320  1099.10185  18846.19531  16025.05664
Training/pf_norm                  0.16109      0.01983     0.19607      0.13985
Training/qf1_norm                 320.85747    199.17950   673.06091    159.52861
Training/qf2_norm                 1495.59048   23.18931    1532.06580   1470.68958
log_std/mean                      -0.13820     0.00008     -0.13807     -0.13830
log_probs/mean                    -2.73228     0.00805     -2.71660     -2.73775
mean/mean                         -0.01074     0.00012     -0.01056     -0.01088
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018606901168823242
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70906
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [123600]
collect time 0.0008883476257324219
inner_dict_sum {'sac_diff0': 0.0002377033233642578, 'sac_diff1': 0.0065534114837646484, 'sac_diff2': 0.0076863765716552734, 'sac_diff3': 0.010062932968139648, 'sac_diff4': 0.007028341293334961, 'sac_diff5': 0.031209945678710938, 'sac_diff6': 0.0003752708435058594, 'all': 0.06315398216247559}
diff5_list [0.006531715393066406, 0.0061757564544677734, 0.0065195560455322266, 0.005912303924560547, 0.006070613861083984]
time3 0
time4 0.06389117240905762
time5 0.0639348030090332
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0370)
policy weight change tensor(36.6900, grad_fn=<SumBackward0>)
time8 0.0017998218536376953
train_time 0.07453608512878418
eval time 0.14841580390930176
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:23,461 MainThread INFO: EPOCH:817
2024-01-23 01:04:23,461 MainThread INFO: Time Consumed:0.22608661651611328s
2024-01-23 01:04:23,461 MainThread INFO: Total Frames:123450s
  8%|▊         | 818/10000 [05:51<38:37,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14919.88717
Train_Epoch_Reward                14511.25077
Running_Training_Average_Rewards  18447.64460
Explore_Time                      0.00088
Train___Time                      0.07454
Eval____Time                      0.14842
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14340.61688
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.80011     1.61726     92.46695     88.15552
alpha_0                           0.66430      0.00009     0.66443      0.66417
Alpha_loss                        -2.75332     0.00372     -2.74960     -2.76041
Training/policy_loss              -4.33779     0.00507     -4.33296     -4.34669
Training/qf1_loss                 7915.21650   1052.64781  8796.04883   6143.82764
Training/qf2_loss                 16719.69727  1312.35567  17800.21484  14712.59961
Training/pf_norm                  0.12424      0.02177     0.15162      0.09328
Training/qf1_norm                 772.81968    337.40585   1102.78198   267.69824
Training/qf2_norm                 1485.62842   25.24560    1511.82251   1444.50647
log_std/mean                      -0.13537     0.00008     -0.13528     -0.13548
log_probs/mean                    -2.73316     0.00728     -2.72734     -2.74719
mean/mean                         -0.01107     0.00004     -0.01100     -0.01110
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018165111541748047
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70906
epoch first part time 2.86102294921875e-06
replay_buffer._size: [123750]
collect time 0.0009090900421142578
inner_dict_sum {'sac_diff0': 0.00023889541625976562, 'sac_diff1': 0.006306886672973633, 'sac_diff2': 0.007532596588134766, 'sac_diff3': 0.009479761123657227, 'sac_diff4': 0.006341457366943359, 'sac_diff5': 0.031017541885375977, 'sac_diff6': 0.0003724098205566406, 'all': 0.06128954887390137}
diff5_list [0.0062906742095947266, 0.005959749221801758, 0.006116151809692383, 0.006475687026977539, 0.00617527961730957]
time3 0
time4 0.06201052665710449
time5 0.062052011489868164
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0370)
policy weight change tensor(36.5531, grad_fn=<SumBackward0>)
time8 0.0017352104187011719
train_time 0.07258391380310059
eval time 0.15473127365112305
epoch last part time 3.814697265625e-06
2024-01-23 01:04:23,713 MainThread INFO: EPOCH:818
2024-01-23 01:04:23,713 MainThread INFO: Time Consumed:0.23044443130493164s
2024-01-23 01:04:23,713 MainThread INFO: Total Frames:123600s
  8%|▊         | 819/10000 [05:51<38:36,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14787.75120
Train_Epoch_Reward                12376.54027
Running_Training_Average_Rewards  18253.34602
Explore_Time                      0.00091
Train___Time                      0.07258
Eval____Time                      0.15473
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14340.05970
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.87055     2.25383     94.05145     87.66740
alpha_0                           0.66397      0.00009     0.66410      0.66384
Alpha_loss                        -2.75447     0.00402     -2.74984     -2.75997
Training/policy_loss              -4.04491     0.00507     -4.03931     -4.05166
Training/qf1_loss                 7101.82402   1128.39427  8418.92090   5743.88477
Training/qf2_loss                 15866.82734  1561.09221  17696.12500  13867.13867
Training/pf_norm                  0.12892      0.02933     0.17600      0.09563
Training/qf1_norm                 792.46419    426.25099   1376.88159   201.51291
Training/qf2_norm                 1408.18691   35.05525    1458.55969   1358.63464
log_std/mean                      -0.14351     0.00030     -0.14302     -0.14382
log_probs/mean                    -2.72776     0.00888     -2.71643     -2.73953
mean/mean                         -0.01424     0.00015     -0.01399     -0.01439
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018290996551513672
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70906
epoch first part time 2.86102294921875e-06
replay_buffer._size: [123900]
collect time 0.0009677410125732422
inner_dict_sum {'sac_diff0': 0.0002281665802001953, 'sac_diff1': 0.006352901458740234, 'sac_diff2': 0.007947444915771484, 'sac_diff3': 0.010676383972167969, 'sac_diff4': 0.007075786590576172, 'sac_diff5': 0.031564950942993164, 'sac_diff6': 0.0003864765167236328, 'all': 0.06423211097717285}
diff5_list [0.0063419342041015625, 0.006259918212890625, 0.006169557571411133, 0.0065615177154541016, 0.006232023239135742]
time3 0
time4 0.06497502326965332
time5 0.06501984596252441
time7 4.76837158203125e-07
gen_weight_change tensor(-18.0370)
policy weight change tensor(36.4219, grad_fn=<SumBackward0>)
time8 0.0018668174743652344
train_time 0.0758674144744873
eval time 0.14861154556274414
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:23,962 MainThread INFO: EPOCH:819
2024-01-23 01:04:23,963 MainThread INFO: Time Consumed:0.22771096229553223s
2024-01-23 01:04:23,963 MainThread INFO: Total Frames:123750s
  8%|▊         | 820/10000 [05:51<38:28,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14679.73308
Train_Epoch_Reward                21022.88367
Running_Training_Average_Rewards  18499.09835
Explore_Time                      0.00096
Train___Time                      0.07587
Eval____Time                      0.14861
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14346.53597
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.70936     3.04545     96.35162     88.55221
alpha_0                           0.66364      0.00009     0.66377      0.66350
Alpha_loss                        -2.76101     0.00485     -2.75439     -2.76920
Training/policy_loss              -4.47646     0.00589     -4.47051     -4.48735
Training/qf1_loss                 7933.75059   1228.73441  9679.76758   6479.27832
Training/qf2_loss                 16923.56328  1809.67698  19191.23242  14840.89062
Training/pf_norm                  0.10502      0.01666     0.13027      0.08903
Training/qf1_norm                 662.08893    242.83335   941.97949    251.40903
Training/qf2_norm                 1601.26265   51.60164    1679.44263   1546.94043
log_std/mean                      -0.12661     0.00012     -0.12648     -0.12681
log_probs/mean                    -2.73549     0.01041     -2.72098     -2.75219
mean/mean                         -0.00941     0.00003     -0.00938     -0.00946
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0182802677154541
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70906
epoch first part time 3.337860107421875e-06
replay_buffer._size: [124050]
collect time 0.0008883476257324219
inside mustsac before update, task 0, sumup 70906
inside mustsac after update, task 0, sumup 70565
inner_dict_sum {'sac_diff0': 0.00022268295288085938, 'sac_diff1': 0.007784128189086914, 'sac_diff2': 0.009063005447387695, 'sac_diff3': 0.011484384536743164, 'sac_diff4': 0.008049964904785156, 'sac_diff5': 0.05590033531188965, 'sac_diff6': 0.0004475116729736328, 'all': 0.09295201301574707}
diff5_list [0.011975765228271484, 0.010585784912109375, 0.010803461074829102, 0.010561227798461914, 0.011974096298217773]
time3 0.0008804798126220703
time4 0.0938868522644043
time5 0.09394645690917969
time7 0.009385108947753906
gen_weight_change tensor(-17.9741)
policy weight change tensor(36.3934, grad_fn=<SumBackward0>)
time8 0.002604961395263672
train_time 0.12587857246398926
eval time 0.11789727210998535
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:24,231 MainThread INFO: EPOCH:820
2024-01-23 01:04:24,231 MainThread INFO: Time Consumed:0.246826171875s
2024-01-23 01:04:24,232 MainThread INFO: Total Frames:123900s
  8%|▊         | 821/10000 [05:51<39:24,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14591.35229
Train_Epoch_Reward                12536.81290
Running_Training_Average_Rewards  18867.14688
Explore_Time                      0.00088
Train___Time                      0.12588
Eval____Time                      0.11790
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14278.11284
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.69531     1.43499    91.64736     87.78207
alpha_0                           0.66331      0.00009    0.66344      0.66317
Alpha_loss                        -2.76471     0.00095    -2.76363     -2.76605
Training/policy_loss              -4.37611     0.21177    -4.08502     -4.74032
Training/qf1_loss                 6666.73848   346.33195  7338.42578   6360.18506
Training/qf2_loss                 15220.62500  547.06448  16230.13867  14594.02539
Training/pf_norm                  0.11485      0.01280    0.13035      0.10000
Training/qf1_norm                 827.36853    610.81021  1838.77820   145.30089
Training/qf2_norm                 1488.58933   98.30085   1670.69189   1395.99536
log_std/mean                      -0.13477     0.00436    -0.12936     -0.13930
log_probs/mean                    -2.73630     0.00282    -2.73243     -2.74002
mean/mean                         -0.00872     0.00240    -0.00508     -0.01178
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018218517303466797
epoch last part time3 0.002765655517578125
inside rlalgo, task 0, sumup 70565
epoch first part time 2.86102294921875e-06
replay_buffer._size: [124200]
collect time 0.0009250640869140625
inner_dict_sum {'sac_diff0': 0.00020194053649902344, 'sac_diff1': 0.007691383361816406, 'sac_diff2': 0.008878231048583984, 'sac_diff3': 0.011716842651367188, 'sac_diff4': 0.007699012756347656, 'sac_diff5': 0.03541302680969238, 'sac_diff6': 0.00041484832763671875, 'all': 0.07201528549194336}
diff5_list [0.007275104522705078, 0.007321834564208984, 0.006902456283569336, 0.006969451904296875, 0.006944179534912109]
time3 0
time4 0.07281923294067383
time5 0.07286715507507324
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9741)
policy weight change tensor(36.3810, grad_fn=<SumBackward0>)
time8 0.0020067691802978516
train_time 0.08426737785339355
eval time 0.15811872482299805
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:24,501 MainThread INFO: EPOCH:821
2024-01-23 01:04:24,502 MainThread INFO: Time Consumed:0.24565720558166504s
2024-01-23 01:04:24,502 MainThread INFO: Total Frames:124050s
  8%|▊         | 822/10000 [05:52<39:52,  3.84it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14559.87623
Train_Epoch_Reward                10943.56466
Running_Training_Average_Rewards  17927.82414
Explore_Time                      0.00092
Train___Time                      0.08427
Eval____Time                      0.15812
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14689.18730
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.53548     1.34680    94.65868     90.95721
alpha_0                           0.66297      0.00009    0.66311      0.66284
Alpha_loss                        -2.76596     0.00135    -2.76388     -2.76715
Training/policy_loss              -4.16597     0.00357    -4.15934     -4.16982
Training/qf1_loss                 7215.12178   639.14250  7896.89941   6186.79980
Training/qf2_loss                 16312.41270  790.66862  17430.27930  15130.56250
Training/pf_norm                  0.14330      0.01369    0.16531      0.13021
Training/qf1_norm                 692.65947    238.58901  1032.87451   331.47760
Training/qf2_norm                 1499.48369   20.89543   1530.67529   1473.52136
log_std/mean                      -0.12701     0.00005    -0.12694     -0.12709
log_probs/mean                    -2.73115     0.00385    -2.72445     -2.73570
mean/mean                         -0.00755     0.00002    -0.00752     -0.00757
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018770933151245117
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70565
epoch first part time 2.86102294921875e-06
replay_buffer._size: [124350]
collect time 0.0008554458618164062
inner_dict_sum {'sac_diff0': 0.00020837783813476562, 'sac_diff1': 0.0067596435546875, 'sac_diff2': 0.008080720901489258, 'sac_diff3': 0.010262012481689453, 'sac_diff4': 0.007088899612426758, 'sac_diff5': 0.031406402587890625, 'sac_diff6': 0.00040984153747558594, 'all': 0.06421589851379395}
diff5_list [0.00652623176574707, 0.0063974857330322266, 0.006222724914550781, 0.006270408630371094, 0.005989551544189453]
time3 0
time4 0.06501388549804688
time5 0.06506228446960449
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9741)
policy weight change tensor(36.4542, grad_fn=<SumBackward0>)
time8 0.001935720443725586
train_time 0.0761861801147461
eval time 0.15708518028259277
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:24,760 MainThread INFO: EPOCH:822
2024-01-23 01:04:24,761 MainThread INFO: Time Consumed:0.23641133308410645s
2024-01-23 01:04:24,761 MainThread INFO: Total Frames:124200s
  8%|▊         | 823/10000 [05:52<39:45,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14544.92125
Train_Epoch_Reward                15233.77020
Running_Training_Average_Rewards  18292.79841
Explore_Time                      0.00085
Train___Time                      0.07619
Eval____Time                      0.15709
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14809.25227
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.98821     2.89803     94.93266     87.11798
alpha_0                           0.66264      0.00009     0.66278      0.66251
Alpha_loss                        -2.77005     0.00243     -2.76576     -2.77211
Training/policy_loss              -4.44011     0.00464     -4.43218     -4.44555
Training/qf1_loss                 7610.46855   1322.01590  9139.21875   5833.00098
Training/qf2_loss                 16617.31523  1858.23434  18734.16992  13876.38477
Training/pf_norm                  0.14018      0.02820     0.16621      0.08828
Training/qf1_norm                 557.31931    416.62419   1323.75903   150.57857
Training/qf2_norm                 1546.56680   48.18895    1594.84595   1465.81091
log_std/mean                      -0.12477     0.00007     -0.12469     -0.12488
log_probs/mean                    -2.73292     0.00606     -2.72249     -2.74120
mean/mean                         -0.00736     0.00008     -0.00723     -0.00746
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018085718154907227
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70565
epoch first part time 2.86102294921875e-06
replay_buffer._size: [124500]
collect time 0.0008816719055175781
inner_dict_sum {'sac_diff0': 0.0002276897430419922, 'sac_diff1': 0.006597757339477539, 'sac_diff2': 0.0077228546142578125, 'sac_diff3': 0.009673118591308594, 'sac_diff4': 0.006627559661865234, 'sac_diff5': 0.03139376640319824, 'sac_diff6': 0.0003745555877685547, 'all': 0.06261730194091797}
diff5_list [0.006472110748291016, 0.006211996078491211, 0.006205558776855469, 0.006399869918823242, 0.006104230880737305]
time3 0
time4 0.06337666511535645
time5 0.06342267990112305
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9741)
policy weight change tensor(36.5453, grad_fn=<SumBackward0>)
time8 0.0018298625946044922
train_time 0.07423877716064453
eval time 0.15208959579467773
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:25,011 MainThread INFO: EPOCH:823
2024-01-23 01:04:25,012 MainThread INFO: Time Consumed:0.22942805290222168s
2024-01-23 01:04:25,012 MainThread INFO: Total Frames:124350s
  8%|▊         | 824/10000 [05:52<39:21,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14545.47575
Train_Epoch_Reward                40071.51174
Running_Training_Average_Rewards  19173.48984
Explore_Time                      0.00088
Train___Time                      0.07424
Eval____Time                      0.15209
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             14923.67844
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.80540     2.22487     93.30638     86.67553
alpha_0                           0.66231      0.00009     0.66244      0.66218
Alpha_loss                        -2.77330     0.00201     -2.76975     -2.77583
Training/policy_loss              -4.44223     0.00289     -4.44038     -4.44798
Training/qf1_loss                 7152.96572   714.94180   7907.60840   6254.18262
Training/qf2_loss                 15807.16816  1063.07894  17206.35547  14310.01855
Training/pf_norm                  0.11130      0.02444     0.13522      0.07098
Training/qf1_norm                 456.35592    228.83491   817.91003    170.82300
Training/qf2_norm                 1562.24775   39.06008    1624.62598   1508.56030
log_std/mean                      -0.13104     0.00006     -0.13097     -0.13114
log_probs/mean                    -2.73264     0.00532     -2.72565     -2.74205
mean/mean                         -0.00911     0.00007     -0.00900     -0.00921
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0185549259185791
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70565
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [124650]
collect time 0.0009629726409912109
inner_dict_sum {'sac_diff0': 0.00023031234741210938, 'sac_diff1': 0.0068798065185546875, 'sac_diff2': 0.007997274398803711, 'sac_diff3': 0.010589838027954102, 'sac_diff4': 0.0069386959075927734, 'sac_diff5': 0.033594608306884766, 'sac_diff6': 0.0004057884216308594, 'all': 0.06663632392883301}
diff5_list [0.006389617919921875, 0.006232023239135742, 0.0061876773834228516, 0.007062435150146484, 0.0077228546142578125]
time3 0
time4 0.0674123764038086
time5 0.0674586296081543
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9741)
policy weight change tensor(36.5556, grad_fn=<SumBackward0>)
time8 0.002002239227294922
train_time 0.07862401008605957
eval time 0.15021443367004395
epoch last part time 5.4836273193359375e-06
2024-01-23 01:04:25,266 MainThread INFO: EPOCH:824
2024-01-23 01:04:25,266 MainThread INFO: Time Consumed:0.23206281661987305s
2024-01-23 01:04:25,266 MainThread INFO: Total Frames:124500s
  8%|▊         | 825/10000 [05:52<39:12,  3.90it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14570.00627
Train_Epoch_Reward                8375.35665
Running_Training_Average_Rewards  18675.30879
Explore_Time                      0.00096
Train___Time                      0.07862
Eval____Time                      0.15021
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15018.87736
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.81872     1.60257    93.99787     89.65962
alpha_0                           0.66198      0.00009    0.66211      0.66185
Alpha_loss                        -2.77727     0.00116    -2.77547     -2.77876
Training/policy_loss              -4.08655     0.00179    -4.08313     -4.08828
Training/qf1_loss                 6644.90420   483.47811  7323.67578   5994.67285
Training/qf2_loss                 15580.96621  665.84089  16644.07812  14755.89551
Training/pf_norm                  0.13031      0.03409    0.16912      0.06698
Training/qf1_norm                 1334.80895   279.14420  1726.71130   964.69855
Training/qf2_norm                 1386.72297   22.98689   1418.53528   1354.80872
log_std/mean                      -0.13633     0.00013    -0.13610     -0.13645
log_probs/mean                    -2.73409     0.00199    -2.73236     -2.73782
mean/mean                         -0.00846     0.00003    -0.00844     -0.00851
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018442869186401367
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70565
epoch first part time 2.86102294921875e-06
replay_buffer._size: [124800]
collect time 0.0009214878082275391
inside mustsac before update, task 0, sumup 70565
inside mustsac after update, task 0, sumup 70385
inner_dict_sum {'sac_diff0': 0.00021791458129882812, 'sac_diff1': 0.0072438716888427734, 'sac_diff2': 0.00823211669921875, 'sac_diff3': 0.010975360870361328, 'sac_diff4': 0.0074193477630615234, 'sac_diff5': 0.05301356315612793, 'sac_diff6': 0.00040149688720703125, 'all': 0.08750367164611816}
diff5_list [0.011176109313964844, 0.010094881057739258, 0.010521650314331055, 0.010693788528442383, 0.01052713394165039]
time3 0.0008585453033447266
time4 0.08833956718444824
time5 0.08839225769042969
time7 0.009072303771972656
gen_weight_change tensor(-17.9826)
policy weight change tensor(36.5401, grad_fn=<SumBackward0>)
time8 0.0018451213836669922
train_time 0.11772322654724121
eval time 0.11047124862670898
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:25,519 MainThread INFO: EPOCH:825
2024-01-23 01:04:25,519 MainThread INFO: Time Consumed:0.23134350776672363s
2024-01-23 01:04:25,520 MainThread INFO: Total Frames:124650s
  8%|▊         | 826/10000 [05:53<39:05,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           14631.74845
Train_Epoch_Reward                14750.14942
Running_Training_Average_Rewards  19013.54394
Explore_Time                      0.00092
Train___Time                      0.11772
Eval____Time                      0.11047
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15219.32282
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.53412     2.02952    92.93532     86.84415
alpha_0                           0.66165      0.00009    0.66178      0.66152
Alpha_loss                        -2.78174     0.00281    -2.77660     -2.78491
Training/policy_loss              -4.39641     0.11792    -4.26886     -4.60642
Training/qf1_loss                 6912.06055   649.85263  8055.82129   6189.68066
Training/qf2_loss                 15615.15117  937.14189  17011.84570  14410.06836
Training/pf_norm                  0.10119      0.01568    0.11740      0.07380
Training/qf1_norm                 991.18654    563.07414  1742.14258   123.51744
Training/qf2_norm                 1528.03833   81.41619   1647.88928   1399.37708
log_std/mean                      -0.13017     0.00611    -0.12222     -0.13868
log_probs/mean                    -2.73677     0.00608    -2.72758     -2.74608
mean/mean                         -0.00859     0.00073    -0.00769     -0.00936
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01871466636657715
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70385
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [124950]
collect time 0.0009179115295410156
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.00716090202331543, 'sac_diff2': 0.008424997329711914, 'sac_diff3': 0.010622978210449219, 'sac_diff4': 0.007312774658203125, 'sac_diff5': 0.032421112060546875, 'sac_diff6': 0.0004074573516845703, 'all': 0.06657528877258301}
diff5_list [0.00648045539855957, 0.006215333938598633, 0.00598597526550293, 0.00732731819152832, 0.006412029266357422]
time3 0
time4 0.06738662719726562
time5 0.06743907928466797
time7 9.5367431640625e-07
gen_weight_change tensor(-17.9826)
policy weight change tensor(36.5537, grad_fn=<SumBackward0>)
time8 0.0018427371978759766
train_time 0.07835030555725098
eval time 0.14988374710083008
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:25,773 MainThread INFO: EPOCH:826
2024-01-23 01:04:25,773 MainThread INFO: Time Consumed:0.2315526008605957s
2024-01-23 01:04:25,773 MainThread INFO: Total Frames:124800s
  8%|▊         | 827/10000 [05:53<39:00,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           14850.98735
Train_Epoch_Reward                9617.24122
Running_Training_Average_Rewards  18858.01019
Explore_Time                      0.00091
Train___Time                      0.07835
Eval____Time                      0.14988
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16544.22989
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.89524     2.75902     97.01772     88.92853
alpha_0                           0.66132      0.00009     0.66145      0.66119
Alpha_loss                        -2.78339     0.00203     -2.78020     -2.78565
Training/policy_loss              -4.48847     0.00497     -4.48370     -4.49602
Training/qf1_loss                 7106.43340   538.37891   8093.95947   6479.32520
Training/qf2_loss                 16072.38730  1004.12284  17988.26562  15260.48145
Training/pf_norm                  0.14312      0.03424     0.18489      0.08642
Training/qf1_norm                 924.94200    507.41568   1857.79968   405.95554
Training/qf2_norm                 1595.13203   47.00873    1682.09009   1543.37573
log_std/mean                      -0.12089     0.00004     -0.12083     -0.12092
log_probs/mean                    -2.73261     0.00348     -2.72816     -2.73807
mean/mean                         -0.00982     0.00013     -0.00965     -0.01001
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018909692764282227
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70385
epoch first part time 2.86102294921875e-06
replay_buffer._size: [125100]
collect time 0.0008714199066162109
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.0070536136627197266, 'sac_diff2': 0.008339643478393555, 'sac_diff3': 0.010271072387695312, 'sac_diff4': 0.007118940353393555, 'sac_diff5': 0.031662940979003906, 'sac_diff6': 0.0004062652587890625, 'all': 0.0650629997253418}
diff5_list [0.006335020065307617, 0.005876302719116211, 0.0070095062255859375, 0.0063381195068359375, 0.006103992462158203]
time3 0
time4 0.06586551666259766
time5 0.06591558456420898
time7 9.5367431640625e-07
gen_weight_change tensor(-17.9826)
policy weight change tensor(36.6282, grad_fn=<SumBackward0>)
time8 0.0018427371978759766
train_time 0.07708740234375
eval time 0.1470928192138672
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:26,023 MainThread INFO: EPOCH:827
2024-01-23 01:04:26,023 MainThread INFO: Time Consumed:0.2274317741394043s
2024-01-23 01:04:26,023 MainThread INFO: Total Frames:124950s
  8%|▊         | 828/10000 [05:53<38:44,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15105.36895
Train_Epoch_Reward                21416.43092
Running_Training_Average_Rewards  19398.11729
Explore_Time                      0.00087
Train___Time                      0.07709
Eval____Time                      0.14709
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16884.43295
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.24962     2.07698     94.54205     88.55011
alpha_0                           0.66099      0.00009     0.66112      0.66086
Alpha_loss                        -2.78567     0.00229     -2.78251     -2.78892
Training/policy_loss              -4.68443     0.00443     -4.67667     -4.69000
Training/qf1_loss                 7329.38896   1463.37589  10233.99414  6280.76025
Training/qf2_loss                 16135.20156  1776.69460  19650.62109  14850.38477
Training/pf_norm                  0.10428      0.02366     0.13956      0.07812
Training/qf1_norm                 1375.51545   407.26500   2026.75073   834.46082
Training/qf2_norm                 1675.11165   38.45108    1734.93506   1624.17493
log_std/mean                      -0.12749     0.00008     -0.12739     -0.12762
log_probs/mean                    -2.72998     0.00464     -2.72397     -2.73512
mean/mean                         -0.01110     0.00011     -0.01094     -0.01125
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018331050872802734
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70385
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [125250]
collect time 0.0009357929229736328
inner_dict_sum {'sac_diff0': 0.00021195411682128906, 'sac_diff1': 0.007265567779541016, 'sac_diff2': 0.008781671524047852, 'sac_diff3': 0.011281967163085938, 'sac_diff4': 0.007854223251342773, 'sac_diff5': 0.03484964370727539, 'sac_diff6': 0.00041985511779785156, 'all': 0.07066488265991211}
diff5_list [0.007176876068115234, 0.007199525833129883, 0.006582498550415039, 0.00621485710144043, 0.007675886154174805]
time3 0
time4 0.0715019702911377
time5 0.07155776023864746
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9826)
policy weight change tensor(36.6861, grad_fn=<SumBackward0>)
time8 0.0019078254699707031
train_time 0.08310532569885254
eval time 0.1445460319519043
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:26,276 MainThread INFO: EPOCH:828
2024-01-23 01:04:26,276 MainThread INFO: Time Consumed:0.23098349571228027s
2024-01-23 01:04:26,276 MainThread INFO: Total Frames:125100s
  8%|▊         | 829/10000 [05:53<38:44,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15395.10931
Train_Epoch_Reward                25915.31729
Running_Training_Average_Rewards  19935.59263
Explore_Time                      0.00093
Train___Time                      0.08311
Eval____Time                      0.14455
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17237.46325
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.76013     2.46802    95.60828     89.15016
alpha_0                           0.66066      0.00009    0.66079      0.66053
Alpha_loss                        -2.78716     0.00350    -2.78255     -2.79150
Training/policy_loss              -4.53001     0.00214    -4.52662     -4.53325
Training/qf1_loss                 7236.56260   524.18139  7861.78271   6433.56494
Training/qf2_loss                 16267.76387  932.91954  17483.28516  14998.41406
Training/pf_norm                  0.11778      0.01991    0.14666      0.08995
Training/qf1_norm                 495.99672    214.41276  878.77264    214.02158
Training/qf2_norm                 1637.17793   43.81270   1704.58569   1588.77307
log_std/mean                      -0.13693     0.00003    -0.13688     -0.13696
log_probs/mean                    -2.72546     0.00629    -2.71758     -2.73268
mean/mean                         -0.01093     0.00003    -0.01088     -0.01099
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018805503845214844
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70385
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [125400]
collect time 0.0008814334869384766
inner_dict_sum {'sac_diff0': 0.0002090930938720703, 'sac_diff1': 0.007194042205810547, 'sac_diff2': 0.008584260940551758, 'sac_diff3': 0.01020956039428711, 'sac_diff4': 0.007130146026611328, 'sac_diff5': 0.03403043746948242, 'sac_diff6': 0.0004093647003173828, 'all': 0.06776690483093262}
diff5_list [0.006543636322021484, 0.007337331771850586, 0.006579875946044922, 0.006331920623779297, 0.007237672805786133]
time3 0
time4 0.06863570213317871
time5 0.06869029998779297
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9826)
policy weight change tensor(36.7844, grad_fn=<SumBackward0>)
time8 0.001974821090698242
train_time 0.08039307594299316
eval time 0.14760661125183105
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:26,530 MainThread INFO: EPOCH:829
2024-01-23 01:04:26,530 MainThread INFO: Time Consumed:0.23121333122253418s
2024-01-23 01:04:26,530 MainThread INFO: Total Frames:125250s
  8%|▊         | 830/10000 [05:54<38:45,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15718.66466
Train_Epoch_Reward                6364.22012
Running_Training_Average_Rewards  19684.81101
Explore_Time                      0.00088
Train___Time                      0.08039
Eval____Time                      0.14761
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17582.08947
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.63027     1.59525    92.85563     88.01262
alpha_0                           0.66033      0.00009    0.66046      0.66020
Alpha_loss                        -2.79116     0.00223    -2.78851     -2.79479
Training/policy_loss              -4.16955     0.00311    -4.16668     -4.17544
Training/qf1_loss                 6625.17471   724.30899  7382.75244   5650.83350
Training/qf2_loss                 15371.05371  954.36973  16495.40430  14141.22363
Training/pf_norm                  0.09374      0.01240    0.10518      0.07668
Training/qf1_norm                 750.25635    328.75321  1218.82947   214.59908
Training/qf2_norm                 1473.32258   25.74102   1509.95996   1431.30090
log_std/mean                      -0.12190     0.00016    -0.12174     -0.12216
log_probs/mean                    -2.72700     0.00633    -2.72132     -2.73899
mean/mean                         -0.01320     0.00007    -0.01310     -0.01331
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01916217803955078
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70385
epoch first part time 3.337860107421875e-06
replay_buffer._size: [125550]
collect time 0.0009744167327880859
inside mustsac before update, task 0, sumup 70385
inside mustsac after update, task 0, sumup 71407
inner_dict_sum {'sac_diff0': 0.0002117156982421875, 'sac_diff1': 0.0070285797119140625, 'sac_diff2': 0.008388280868530273, 'sac_diff3': 0.01074838638305664, 'sac_diff4': 0.007210493087768555, 'sac_diff5': 0.0505368709564209, 'sac_diff6': 0.0003998279571533203, 'all': 0.08452415466308594}
diff5_list [0.010743141174316406, 0.009975910186767578, 0.009851694107055664, 0.010103464126586914, 0.009862661361694336]
time3 0.0008399486541748047
time4 0.0853431224822998
time5 0.08539199829101562
time7 0.009043455123901367
gen_weight_change tensor(-17.9893)
policy weight change tensor(36.7442, grad_fn=<SumBackward0>)
time8 0.002580404281616211
train_time 0.11562275886535645
eval time 0.10860085487365723
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:26,781 MainThread INFO: EPOCH:830
2024-01-23 01:04:26,781 MainThread INFO: Time Consumed:0.2275245189666748s
2024-01-23 01:04:26,781 MainThread INFO: Total Frames:125400s
  8%|▊         | 831/10000 [05:54<38:48,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16076.91455
Train_Epoch_Reward                15373.22580
Running_Training_Average_Rewards  19808.04213
Explore_Time                      0.00097
Train___Time                      0.11562
Eval____Time                      0.10860
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17860.61171
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.17098     0.79647    91.01385     88.80282
alpha_0                           0.66000      0.00009    0.66013      0.65987
Alpha_loss                        -2.79745     0.00196    -2.79494     -2.79988
Training/policy_loss              -4.20559     0.10743    -4.06206     -4.32438
Training/qf1_loss                 6593.94092   487.87960  7280.42090   5791.91309
Training/qf2_loss                 15243.23477  636.62505  16115.16895  14177.98145
Training/pf_norm                  0.09788      0.02013    0.12863      0.07446
Training/qf1_norm                 780.45832    694.80849  1960.35193   123.67789
Training/qf2_norm                 1418.46226   30.54585   1469.76099   1380.36243
log_std/mean                      -0.13357     0.00733    -0.12090     -0.14087
log_probs/mean                    -2.73406     0.00533    -2.72478     -2.74006
mean/mean                         -0.00977     0.00213    -0.00711     -0.01358
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019726991653442383
epoch last part time3 0.0031044483184814453
inside rlalgo, task 0, sumup 71407
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [125700]
collect time 0.0008752346038818359
inner_dict_sum {'sac_diff0': 0.00023555755615234375, 'sac_diff1': 0.00661158561706543, 'sac_diff2': 0.0077855587005615234, 'sac_diff3': 0.009859085083007812, 'sac_diff4': 0.006922006607055664, 'sac_diff5': 0.03192877769470215, 'sac_diff6': 0.00037741661071777344, 'all': 0.0637199878692627}
diff5_list [0.00656890869140625, 0.006209611892700195, 0.0062007904052734375, 0.006445407867431641, 0.006504058837890625]
time3 0
time4 0.06445550918579102
time5 0.0644996166229248
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9893)
policy weight change tensor(36.7992, grad_fn=<SumBackward0>)
time8 0.001847982406616211
train_time 0.0754396915435791
eval time 0.1445157527923584
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:27,030 MainThread INFO: EPOCH:831
2024-01-23 01:04:27,030 MainThread INFO: Time Consumed:0.22323369979858398s
2024-01-23 01:04:27,031 MainThread INFO: Total Frames:125550s
  8%|▊         | 832/10000 [05:54<38:24,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16278.61579
Train_Epoch_Reward                8694.20997
Running_Training_Average_Rewards  19782.87014
Explore_Time                      0.00087
Train___Time                      0.07544
Eval____Time                      0.14452
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16706.19968
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       89.53177     1.71353    91.82524     87.22588
alpha_0                           0.65967      0.00009    0.65980      0.65954
Alpha_loss                        -2.80133     0.00185    -2.79790     -2.80305
Training/policy_loss              -4.26528     0.00345    -4.26147     -4.27027
Training/qf1_loss                 6568.17549   384.39209  7013.19678   6040.45215
Training/qf2_loss                 15105.35762  536.17135  15854.57617  14323.85449
Training/pf_norm                  0.09583      0.02022    0.12075      0.06544
Training/qf1_norm                 878.52408    341.61233  1325.81812   458.39868
Training/qf2_norm                 1433.49229   26.79887   1468.85767   1397.11169
log_std/mean                      -0.13323     0.00009    -0.13308     -0.13334
log_probs/mean                    -2.73529     0.00378    -2.73028     -2.74059
mean/mean                         -0.01324     0.00009    -0.01309     -0.01332
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018790006637573242
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71407
epoch first part time 2.86102294921875e-06
replay_buffer._size: [125850]
collect time 0.0009675025939941406
inner_dict_sum {'sac_diff0': 0.0002377033233642578, 'sac_diff1': 0.0065577030181884766, 'sac_diff2': 0.0076673030853271484, 'sac_diff3': 0.009786367416381836, 'sac_diff4': 0.0065250396728515625, 'sac_diff5': 0.030594587326049805, 'sac_diff6': 0.000385284423828125, 'all': 0.06175398826599121}
diff5_list [0.006280422210693359, 0.006021261215209961, 0.0060536861419677734, 0.00608062744140625, 0.006158590316772461]
time3 0
time4 0.062479257583618164
time5 0.06251955032348633
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9893)
policy weight change tensor(36.7840, grad_fn=<SumBackward0>)
time8 0.00205230712890625
train_time 0.07376432418823242
eval time 0.1547393798828125
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:27,284 MainThread INFO: EPOCH:832
2024-01-23 01:04:27,285 MainThread INFO: Time Consumed:0.23192882537841797s
2024-01-23 01:04:27,285 MainThread INFO: Total Frames:125700s
  8%|▊         | 833/10000 [05:54<38:32,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16483.26891
Train_Epoch_Reward                17125.54335
Running_Training_Average_Rewards  20059.94262
Explore_Time                      0.00096
Train___Time                      0.07376
Eval____Time                      0.15474
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16855.78351
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.41599     2.24092     96.36598     89.93870
alpha_0                           0.65934      0.00009     0.65947      0.65921
Alpha_loss                        -2.80118     0.00239     -2.79754     -2.80334
Training/policy_loss              -4.48347     0.00330     -4.48088     -4.48990
Training/qf1_loss                 7366.26768   689.15861   8159.13965   6476.99658
Training/qf2_loss                 16678.42129  1088.23922  18019.86133  15122.84082
Training/pf_norm                  0.08354      0.02343     0.12207      0.04809
Training/qf1_norm                 971.92698    432.78532   1545.27856   317.28720
Training/qf2_norm                 1552.18430   37.37101    1601.29895   1493.71631
log_std/mean                      -0.12892     0.00002     -0.12890     -0.12894
log_probs/mean                    -2.72683     0.00455     -2.72133     -2.73265
mean/mean                         -0.00717     0.00013     -0.00697     -0.00734
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0187833309173584
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71407
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [126000]
collect time 0.0009601116180419922
inner_dict_sum {'sac_diff0': 0.00020956993103027344, 'sac_diff1': 0.00735020637512207, 'sac_diff2': 0.00877833366394043, 'sac_diff3': 0.010867595672607422, 'sac_diff4': 0.007182121276855469, 'sac_diff5': 0.0329737663269043, 'sac_diff6': 0.0004115104675292969, 'all': 0.06777310371398926}
diff5_list [0.006693363189697266, 0.007052183151245117, 0.006793975830078125, 0.006308794021606445, 0.006125450134277344]
time3 0
time4 0.06859874725341797
time5 0.06865167617797852
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9893)
policy weight change tensor(36.7278, grad_fn=<SumBackward0>)
time8 0.001814126968383789
train_time 0.07973217964172363
eval time 0.1437091827392578
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:27,534 MainThread INFO: EPOCH:833
2024-01-23 01:04:27,534 MainThread INFO: Time Consumed:0.2267904281616211s
2024-01-23 01:04:27,534 MainThread INFO: Total Frames:125850s
  8%|▊         | 834/10000 [05:55<38:24,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16685.15552
Train_Epoch_Reward                39029.59501
Running_Training_Average_Rewards  18266.12104
Explore_Time                      0.00096
Train___Time                      0.07973
Eval____Time                      0.14371
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16942.54460
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.41820     2.08622    94.07380     88.79031
alpha_0                           0.65901      0.00009    0.65914      0.65888
Alpha_loss                        -2.80597     0.00294    -2.80060     -2.80921
Training/policy_loss              -4.82592     0.00433    -4.81766     -4.83018
Training/qf1_loss                 7129.92471   646.81056  7946.15527   6270.61865
Training/qf2_loss                 16019.67617  992.74717  17239.04297  14635.58789
Training/pf_norm                  0.17472      0.02076    0.19654      0.14727
Training/qf1_norm                 794.82556    411.72380  1335.21741   295.13528
Training/qf2_norm                 1719.09500   38.44890   1767.71924   1671.33032
log_std/mean                      -0.12740     0.00006    -0.12731     -0.12748
log_probs/mean                    -2.73026     0.00548    -2.72060     -2.73641
mean/mean                         -0.00881     0.00033    -0.00830     -0.00922
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018996715545654297
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71407
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [126150]
collect time 0.0009212493896484375
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.007055759429931641, 'sac_diff2': 0.008441925048828125, 'sac_diff3': 0.010601043701171875, 'sac_diff4': 0.007049560546875, 'sac_diff5': 0.03221011161804199, 'sac_diff6': 0.00039267539978027344, 'all': 0.06596088409423828}
diff5_list [0.006799221038818359, 0.006968021392822266, 0.006226301193237305, 0.006033182144165039, 0.0061833858489990234]
time3 0
time4 0.06675481796264648
time5 0.06681299209594727
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9893)
policy weight change tensor(36.7408, grad_fn=<SumBackward0>)
time8 0.001912832260131836
train_time 0.07837367057800293
eval time 0.14357972145080566
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:27,782 MainThread INFO: EPOCH:834
2024-01-23 01:04:27,782 MainThread INFO: Time Consumed:0.22526788711547852s
2024-01-23 01:04:27,782 MainThread INFO: Total Frames:126000s
  8%|▊         | 835/10000 [05:55<38:15,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16875.80804
Train_Epoch_Reward                12968.77162
Running_Training_Average_Rewards  18134.24070
Explore_Time                      0.00092
Train___Time                      0.07837
Eval____Time                      0.14358
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16925.40255
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.48129     1.44924     94.21236     90.27245
alpha_0                           0.65868      0.00009     0.65881      0.65855
Alpha_loss                        -2.80952     0.00251     -2.80536     -2.81304
Training/policy_loss              -4.44390     0.00448     -4.43838     -4.45157
Training/qf1_loss                 7834.98477   865.38239   8705.11914   6480.95557
Training/qf2_loss                 16784.83848  1058.98664  18030.54297  15153.42480
Training/pf_norm                  0.13291      0.02608     0.17096      0.10237
Training/qf1_norm                 341.64970    267.53518   845.84760    133.19125
Training/qf2_norm                 1539.02070   22.84619    1582.04089   1520.38770
log_std/mean                      -0.13567     0.00008     -0.13557     -0.13577
log_probs/mean                    -2.73071     0.00454     -2.72235     -2.73593
mean/mean                         -0.00437     0.00019     -0.00409     -0.00464
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01907801628112793
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71407
epoch first part time 3.337860107421875e-06
replay_buffer._size: [126300]
collect time 0.0009264945983886719
inside mustsac before update, task 0, sumup 71407
inside mustsac after update, task 0, sumup 70570
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.0071527957916259766, 'sac_diff2': 0.008170366287231445, 'sac_diff3': 0.010240316390991211, 'sac_diff4': 0.0071563720703125, 'sac_diff5': 0.05166459083557129, 'sac_diff6': 0.0004012584686279297, 'all': 0.08499622344970703}
diff5_list [0.01249837875366211, 0.009900093078613281, 0.00989389419555664, 0.009781837463378906, 0.009590387344360352]
time3 0.0008668899536132812
time4 0.08582258224487305
time5 0.08587503433227539
time7 0.009199857711791992
gen_weight_change tensor(-17.9382)
policy weight change tensor(36.7191, grad_fn=<SumBackward0>)
time8 0.0017740726470947266
train_time 0.1149897575378418
eval time 0.10994601249694824
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:28,033 MainThread INFO: EPOCH:835
2024-01-23 01:04:28,033 MainThread INFO: Time Consumed:0.22823643684387207s
2024-01-23 01:04:28,033 MainThread INFO: Total Frames:126150s
  8%|▊         | 836/10000 [05:55<38:17,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17050.61706
Train_Epoch_Reward                6852.50684
Running_Training_Average_Rewards  18048.92171
Explore_Time                      0.00092
Train___Time                      0.11499
Eval____Time                      0.10995
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16967.41298
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.71978     1.59611     92.87308     88.79665
alpha_0                           0.65835      0.00009     0.65848      0.65822
Alpha_loss                        -2.81332     0.00414     -2.80742     -2.81844
Training/policy_loss              -4.39504     0.17115     -4.07707     -4.57223
Training/qf1_loss                 6682.82949   908.46113   7958.33105   5505.02148
Training/qf2_loss                 15415.17461  1195.73746  17023.12305  14076.90820
Training/pf_norm                  0.14383      0.04400     0.20333      0.07892
Training/qf1_norm                 696.02640    763.78519   2191.40161   102.92537
Training/qf2_norm                 1535.54358   80.16428    1638.68396   1432.94922
log_std/mean                      -0.13390     0.00820     -0.12362     -0.14872
log_probs/mean                    -2.73176     0.00857     -2.71765     -2.74240
mean/mean                         -0.00740     0.00324     -0.00383     -0.01228
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019247770309448242
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70570
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [126450]
collect time 0.0010046958923339844
inner_dict_sum {'sac_diff0': 0.00023627281188964844, 'sac_diff1': 0.006715297698974609, 'sac_diff2': 0.007798671722412109, 'sac_diff3': 0.00990915298461914, 'sac_diff4': 0.006590604782104492, 'sac_diff5': 0.030770540237426758, 'sac_diff6': 0.00037407875061035156, 'all': 0.06239461898803711}
diff5_list [0.006526947021484375, 0.0061304569244384766, 0.005982875823974609, 0.006048679351806641, 0.006081581115722656]
time3 0
time4 0.06312727928161621
time5 0.0631711483001709
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9382)
policy weight change tensor(36.6843, grad_fn=<SumBackward0>)
time8 0.0017614364624023438
train_time 0.07402324676513672
eval time 0.15429234504699707
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:28,288 MainThread INFO: EPOCH:836
2024-01-23 01:04:28,288 MainThread INFO: Time Consumed:0.23172831535339355s
2024-01-23 01:04:28,288 MainThread INFO: Total Frames:126300s
  8%|▊         | 837/10000 [05:55<38:26,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17028.86090
Train_Epoch_Reward                6913.20378
Running_Training_Average_Rewards  16973.14541
Explore_Time                      0.00099
Train___Time                      0.07402
Eval____Time                      0.15429
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16326.66831
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.56269     3.23748     95.83858     87.05865
alpha_0                           0.65802      0.00009     0.65815      0.65789
Alpha_loss                        -2.81887     0.00103     -2.81716     -2.82012
Training/policy_loss              -4.43006     0.00174     -4.42715     -4.43245
Training/qf1_loss                 7499.78320   1390.90770  9295.68164   5212.13770
Training/qf2_loss                 16596.34590  2022.57372  19096.35547  13162.50488
Training/pf_norm                  0.11951      0.03115     0.17430      0.09025
Training/qf1_norm                 873.61606    669.46247   2030.96912   236.89992
Training/qf2_norm                 1627.46702   55.87602    1682.09314   1532.04419
log_std/mean                      -0.13274     0.00009     -0.13261     -0.13286
log_probs/mean                    -2.73698     0.00235     -2.73508     -2.74158
mean/mean                         -0.00679     0.00005     -0.00674     -0.00685
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018566370010375977
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70570
epoch first part time 2.384185791015625e-06
replay_buffer._size: [126600]
collect time 0.0008714199066162109
inner_dict_sum {'sac_diff0': 0.0002071857452392578, 'sac_diff1': 0.00742030143737793, 'sac_diff2': 0.008694171905517578, 'sac_diff3': 0.010994434356689453, 'sac_diff4': 0.007069110870361328, 'sac_diff5': 0.0323481559753418, 'sac_diff6': 0.0004062652587890625, 'all': 0.0671396255493164}
diff5_list [0.006310939788818359, 0.006167173385620117, 0.0072972774505615234, 0.0062885284423828125, 0.006284236907958984]
time3 0
time4 0.06794595718383789
time5 0.06799721717834473
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9382)
policy weight change tensor(36.5804, grad_fn=<SumBackward0>)
time8 0.0018310546875
train_time 0.07911181449890137
eval time 0.14451193809509277
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:28,537 MainThread INFO: EPOCH:837
2024-01-23 01:04:28,537 MainThread INFO: Time Consumed:0.22689270973205566s
2024-01-23 01:04:28,537 MainThread INFO: Total Frames:126450s
  8%|▊         | 838/10000 [05:56<38:18,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16967.83588
Train_Epoch_Reward                9805.74687
Running_Training_Average_Rewards  16822.87175
Explore_Time                      0.00087
Train___Time                      0.07911
Eval____Time                      0.14451
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16274.18280
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.33552     2.13120     95.75296     90.47884
alpha_0                           0.65769      0.00009     0.65782      0.65756
Alpha_loss                        -2.82046     0.00259     -2.81725     -2.82490
Training/policy_loss              -4.36645     0.00218     -4.36348     -4.36942
Training/qf1_loss                 7204.08643   877.33797   8817.28320   6215.32715
Training/qf2_loss                 16257.42656  1119.96587  18173.28906  14933.66699
Training/pf_norm                  0.10788      0.00662     0.11446      0.09538
Training/qf1_norm                 1501.29900   399.59691   2130.70117   1096.05505
Training/qf2_norm                 1564.16570   36.38037    1622.19739   1532.58740
log_std/mean                      -0.13165     0.00016     -0.13142     -0.13186
log_probs/mean                    -2.73274     0.00409     -2.72828     -2.74012
mean/mean                         -0.01129     0.00014     -0.01110     -0.01148
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018628358840942383
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70570
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [126750]
collect time 0.0009381771087646484
inner_dict_sum {'sac_diff0': 0.0002181529998779297, 'sac_diff1': 0.0073506832122802734, 'sac_diff2': 0.008426904678344727, 'sac_diff3': 0.010926485061645508, 'sac_diff4': 0.006988048553466797, 'sac_diff5': 0.03180885314941406, 'sac_diff6': 0.000396728515625, 'all': 0.0661158561706543}
diff5_list [0.006613969802856445, 0.006778717041015625, 0.006574392318725586, 0.006048440933227539, 0.005793333053588867]
time3 0
time4 0.06693744659423828
time5 0.06699109077453613
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9382)
policy weight change tensor(36.3948, grad_fn=<SumBackward0>)
time8 0.0019216537475585938
train_time 0.07833337783813477
eval time 0.14690184593200684
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:28,787 MainThread INFO: EPOCH:838
2024-01-23 01:04:28,788 MainThread INFO: Time Consumed:0.22855424880981445s
2024-01-23 01:04:28,788 MainThread INFO: Total Frames:126600s
  8%|▊         | 839/10000 [05:56<38:19,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16861.74040
Train_Epoch_Reward                5747.76509
Running_Training_Average_Rewards  16295.57468
Explore_Time                      0.00093
Train___Time                      0.07833
Eval____Time                      0.14690
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16176.50838
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.94961     2.02533     93.97490     87.93345
alpha_0                           0.65736      0.00009     0.65750      0.65723
Alpha_loss                        -2.82253     0.00201     -2.81887     -2.82483
Training/policy_loss              -4.39419     0.00307     -4.38894     -4.39846
Training/qf1_loss                 7831.65449   1026.63623  9010.49902   5951.37793
Training/qf2_loss                 16681.72148  1393.56311  18468.87109  14185.65625
Training/pf_norm                  0.16385      0.01867     0.19573      0.14848
Training/qf1_norm                 407.30521    234.04606   805.13336    161.56447
Training/qf2_norm                 1481.42764   31.41535    1529.06079   1435.71057
log_std/mean                      -0.14163     0.00028     -0.14116     -0.14195
log_probs/mean                    -2.72964     0.00541     -2.72252     -2.73834
mean/mean                         -0.01239     0.00005     -0.01231     -0.01245
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019031286239624023
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70570
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [126900]
collect time 0.0009331703186035156
inner_dict_sum {'sac_diff0': 0.000240325927734375, 'sac_diff1': 0.006742238998413086, 'sac_diff2': 0.007704734802246094, 'sac_diff3': 0.009765863418579102, 'sac_diff4': 0.0067560672760009766, 'sac_diff5': 0.03097987174987793, 'sac_diff6': 0.00038242340087890625, 'all': 0.06257152557373047}
diff5_list [0.00654149055480957, 0.0060062408447265625, 0.005991220474243164, 0.006240129470825195, 0.0062007904052734375]
time3 0
time4 0.06329560279846191
time5 0.0633394718170166
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9382)
policy weight change tensor(36.2585, grad_fn=<SumBackward0>)
time8 0.0018892288208007812
train_time 0.07493066787719727
eval time 0.15452837944030762
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:29,043 MainThread INFO: EPOCH:839
2024-01-23 01:04:29,043 MainThread INFO: Time Consumed:0.23273324966430664s
2024-01-23 01:04:29,043 MainThread INFO: Total Frames:126750s
  8%|▊         | 840/10000 [05:56<38:31,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16738.88702
Train_Epoch_Reward                12072.87179
Running_Training_Average_Rewards  16489.47935
Explore_Time                      0.00093
Train___Time                      0.07493
Eval____Time                      0.15453
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16353.55569
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.01876     2.06196     95.46010     89.36089
alpha_0                           0.65704      0.00009     0.65717      0.65690
Alpha_loss                        -2.82555     0.00358     -2.81963     -2.83064
Training/policy_loss              -4.13997     0.00483     -4.13339     -4.14667
Training/qf1_loss                 6895.23896   934.99315   8332.46973   5796.25000
Training/qf2_loss                 15849.52832  1311.32082  17924.43945  14264.18750
Training/pf_norm                  0.08998      0.01918     0.12588      0.07276
Training/qf1_norm                 1058.34924   366.40343   1654.96362   579.54504
Training/qf2_norm                 1396.38604   30.59860    1448.04016   1358.81799
log_std/mean                      -0.12825     0.00012     -0.12811     -0.12842
log_probs/mean                    -2.72882     0.00721     -2.71793     -2.73933
mean/mean                         -0.00833     0.00016     -0.00810     -0.00856
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01892375946044922
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70570
epoch first part time 3.337860107421875e-06
replay_buffer._size: [127050]
collect time 0.0009233951568603516
inside mustsac before update, task 0, sumup 70570
inside mustsac after update, task 0, sumup 71010
inner_dict_sum {'sac_diff0': 0.00021195411682128906, 'sac_diff1': 0.006944417953491211, 'sac_diff2': 0.00831460952758789, 'sac_diff3': 0.010264873504638672, 'sac_diff4': 0.007138729095458984, 'sac_diff5': 0.05004763603210449, 'sac_diff6': 0.00040650367736816406, 'all': 0.0833287239074707}
diff5_list [0.010766744613647461, 0.009824991226196289, 0.009823799133300781, 0.009701967239379883, 0.009930133819580078]
time3 0.0008547306060791016
time4 0.0841372013092041
time5 0.08419418334960938
time7 0.0091094970703125
gen_weight_change tensor(-17.9620)
policy weight change tensor(36.3187, grad_fn=<SumBackward0>)
time8 0.0026400089263916016
train_time 0.11417126655578613
eval time 0.11871600151062012
epoch last part time 7.867813110351562e-06
2024-01-23 01:04:29,302 MainThread INFO: EPOCH:840
2024-01-23 01:04:29,302 MainThread INFO: Time Consumed:0.23629975318908691s
2024-01-23 01:04:29,303 MainThread INFO: Total Frames:126900s
  8%|▊         | 841/10000 [05:56<38:57,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16602.78081
Train_Epoch_Reward                20931.41645
Running_Training_Average_Rewards  16905.70956
Explore_Time                      0.00092
Train___Time                      0.11417
Eval____Time                      0.11872
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16499.54965
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.91898     2.30629     97.19875     90.34361
alpha_0                           0.65671      0.00009     0.65684      0.65658
Alpha_loss                        -2.82863     0.00252     -2.82595     -2.83240
Training/policy_loss              -4.56705     0.20982     -4.22677     -4.80866
Training/qf1_loss                 7595.62451   827.64103   8870.46289   6562.21240
Training/qf2_loss                 16751.98984  1247.15942  18877.90234  15250.57031
Training/pf_norm                  0.15303      0.04676     0.21811      0.09626
Training/qf1_norm                 1174.36674   311.07662   1720.44116   837.07727
Training/qf2_norm                 1636.83223   54.23062    1722.94568   1578.25684
log_std/mean                      -0.12855     0.00331     -0.12450     -0.13202
log_probs/mean                    -2.72815     0.00464     -2.72301     -2.73552
mean/mean                         -0.00921     0.00063     -0.00829     -0.01013
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0186917781829834
epoch last part time3 0.002749919891357422
inside rlalgo, task 0, sumup 71010
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [127200]
collect time 0.0008494853973388672
inner_dict_sum {'sac_diff0': 0.00022530555725097656, 'sac_diff1': 0.006907463073730469, 'sac_diff2': 0.0075418949127197266, 'sac_diff3': 0.010320425033569336, 'sac_diff4': 0.00684046745300293, 'sac_diff5': 0.031432151794433594, 'sac_diff6': 0.00038552284240722656, 'all': 0.06365323066711426}
diff5_list [0.006679534912109375, 0.00623631477355957, 0.006266355514526367, 0.0065805912017822266, 0.005669355392456055]
time3 0
time4 0.0644228458404541
time5 0.06446623802185059
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9620)
policy weight change tensor(36.2746, grad_fn=<SumBackward0>)
time8 0.0018568038940429688
train_time 0.0754392147064209
eval time 0.14722609519958496
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:29,553 MainThread INFO: EPOCH:841
2024-01-23 01:04:29,553 MainThread INFO: Time Consumed:0.22590303421020508s
2024-01-23 01:04:29,553 MainThread INFO: Total Frames:127050s
  8%|▊         | 842/10000 [05:57<38:35,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16445.66832
Train_Epoch_Reward                15978.78083
Running_Training_Average_Rewards  15918.21692
Explore_Time                      0.00084
Train___Time                      0.07544
Eval____Time                      0.14723
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15135.07478
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.87350     0.58641     94.70337     93.08139
alpha_0                           0.65638      0.00009     0.65651      0.65625
Alpha_loss                        -2.83223     0.00300     -2.82778     -2.83625
Training/policy_loss              -5.00307     0.00156     -5.00150     -5.00569
Training/qf1_loss                 7890.13135   1159.63552  9544.40430   6091.92188
Training/qf2_loss                 17215.28691  1252.37816  19056.51172  15344.92480
Training/pf_norm                  0.09111      0.01745     0.12001      0.06747
Training/qf1_norm                 1337.65261   114.03954   1541.05859   1205.18433
Training/qf2_norm                 1842.50569   9.91357     1858.48230   1829.71448
log_std/mean                      -0.13486     0.00007     -0.13475     -0.13495
log_probs/mean                    -2.72871     0.00568     -2.72133     -2.73826
mean/mean                         -0.00605     0.00012     -0.00589     -0.00622
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01859450340270996
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71010
epoch first part time 2.86102294921875e-06
replay_buffer._size: [127350]
collect time 0.000873565673828125
inner_dict_sum {'sac_diff0': 0.00022077560424804688, 'sac_diff1': 0.00693202018737793, 'sac_diff2': 0.008144140243530273, 'sac_diff3': 0.010794639587402344, 'sac_diff4': 0.007203102111816406, 'sac_diff5': 0.03243899345397949, 'sac_diff6': 0.0004184246063232422, 'all': 0.06615209579467773}
diff5_list [0.006510019302368164, 0.0067844390869140625, 0.006471872329711914, 0.0064547061920166016, 0.00621795654296875]
time3 0
time4 0.06699419021606445
time5 0.06704831123352051
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9620)
policy weight change tensor(36.3044, grad_fn=<SumBackward0>)
time8 0.0018074512481689453
train_time 0.07835769653320312
eval time 0.14816975593566895
epoch last part time 7.62939453125e-06
2024-01-23 01:04:29,805 MainThread INFO: EPOCH:842
2024-01-23 01:04:29,805 MainThread INFO: Time Consumed:0.2297992706298828s
2024-01-23 01:04:29,805 MainThread INFO: Total Frames:127200s
  8%|▊         | 843/10000 [05:57<38:32,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16274.76834
Train_Epoch_Reward                17405.45387
Running_Training_Average_Rewards  15851.34629
Explore_Time                      0.00087
Train___Time                      0.07836
Eval____Time                      0.14817
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15146.78370
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.04850     1.61072     94.51804     89.93206
alpha_0                           0.65605      0.00009     0.65618      0.65592
Alpha_loss                        -2.83772     0.00276     -2.83351     -2.84207
Training/policy_loss              -4.59328     0.00366     -4.58823     -4.59936
Training/qf1_loss                 7007.28408   1038.12888  8720.76270   5972.92969
Training/qf2_loss                 16023.43867  1308.74729  18224.69922  14570.15918
Training/pf_norm                  0.09502      0.02680     0.12895      0.05997
Training/qf1_norm                 442.00950    323.42433   994.60675    103.87406
Training/qf2_norm                 1632.69829   29.06717    1676.92444   1594.97424
log_std/mean                      -0.13172     0.00011     -0.13159     -0.13189
log_probs/mean                    -2.73375     0.00443     -2.72696     -2.74088
mean/mean                         -0.00981     0.00002     -0.00978     -0.00983
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018515348434448242
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71010
epoch first part time 2.86102294921875e-06
replay_buffer._size: [127500]
collect time 0.0008525848388671875
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.007138490676879883, 'sac_diff2': 0.008156776428222656, 'sac_diff3': 0.010623455047607422, 'sac_diff4': 0.00699162483215332, 'sac_diff5': 0.032301902770996094, 'sac_diff6': 0.0004000663757324219, 'all': 0.06583428382873535}
diff5_list [0.0074100494384765625, 0.006510019302368164, 0.0062255859375, 0.0060062408447265625, 0.006150007247924805]
time3 0
time4 0.06661868095397949
time5 0.06666946411132812
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9620)
policy weight change tensor(36.4006, grad_fn=<SumBackward0>)
time8 0.0018184185028076172
train_time 0.07759761810302734
eval time 0.14970135688781738
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:30,057 MainThread INFO: EPOCH:843
2024-01-23 01:04:30,057 MainThread INFO: Time Consumed:0.23048925399780273s
2024-01-23 01:04:30,058 MainThread INFO: Total Frames:127350s
  8%|▊         | 844/10000 [05:57<38:34,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16106.71077
Train_Epoch_Reward                13985.74928
Running_Training_Average_Rewards  16063.33208
Explore_Time                      0.00085
Train___Time                      0.07760
Eval____Time                      0.14970
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15261.96892
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.27254     2.42670     93.74627     87.10686
alpha_0                           0.65572      0.00009     0.65585      0.65559
Alpha_loss                        -2.84052     0.00193     -2.83813     -2.84292
Training/policy_loss              -4.63728     0.00307     -4.63157     -4.63969
Training/qf1_loss                 6785.99160   763.68783   7537.30469   5812.01074
Training/qf2_loss                 15581.46602  1236.44520  16768.48633  13831.73438
Training/pf_norm                  0.10427      0.01888     0.12758      0.08333
Training/qf1_norm                 1792.17849   504.35422   2651.01978   1317.59888
Training/qf2_norm                 1629.19626   42.96227    1676.20850   1556.30310
log_std/mean                      -0.12659     0.00015     -0.12643     -0.12683
log_probs/mean                    -2.73242     0.00575     -2.72530     -2.74051
mean/mean                         -0.00741     0.00011     -0.00727     -0.00759
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01929616928100586
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71010
epoch first part time 1.8835067749023438e-05
replay_buffer._size: [127650]
collect time 0.0009870529174804688
inner_dict_sum {'sac_diff0': 0.00024437904357910156, 'sac_diff1': 0.006854057312011719, 'sac_diff2': 0.007876873016357422, 'sac_diff3': 0.010373115539550781, 'sac_diff4': 0.0069539546966552734, 'sac_diff5': 0.03189659118652344, 'sac_diff6': 0.0003829002380371094, 'all': 0.06458187103271484}
diff5_list [0.006689786911010742, 0.006309032440185547, 0.006265163421630859, 0.006294965744018555, 0.006337642669677734]
time3 0
time4 0.06532073020935059
time5 0.06536579132080078
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9620)
policy weight change tensor(36.4778, grad_fn=<SumBackward0>)
time8 0.0017998218536376953
train_time 0.07666921615600586
eval time 0.1540052890777588
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:30,314 MainThread INFO: EPOCH:844
2024-01-23 01:04:30,315 MainThread INFO: Time Consumed:0.23415565490722656s
2024-01-23 01:04:30,315 MainThread INFO: Total Frames:127500s
  8%|▊         | 845/10000 [05:57<38:45,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15948.13858
Train_Epoch_Reward                12257.19276
Running_Training_Average_Rewards  15639.19999
Explore_Time                      0.00098
Train___Time                      0.07667
Eval____Time                      0.15401
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15339.68057
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.80379     2.36575     97.07358     91.06590
alpha_0                           0.65540      0.00009     0.65553      0.65526
Alpha_loss                        -2.84559     0.00391     -2.83802     -2.84911
Training/policy_loss              -4.40734     0.00542     -4.39672     -4.41150
Training/qf1_loss                 8020.27559   839.39759   9186.04004   7119.95605
Training/qf2_loss                 17399.89180  1284.17379  19019.34570  16104.91602
Training/pf_norm                  0.10759      0.03060     0.15528      0.07398
Training/qf1_norm                 595.93300    425.61904   1171.51355   133.18613
Training/qf2_norm                 1571.66729   39.43456    1627.02783   1526.67126
log_std/mean                      -0.12766     0.00005     -0.12760     -0.12774
log_probs/mean                    -2.73644     0.00853     -2.72014     -2.74322
mean/mean                         -0.00876     0.00003     -0.00870     -0.00879
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01891493797302246
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71010
epoch first part time 3.337860107421875e-06
replay_buffer._size: [127800]
collect time 0.0008702278137207031
inside mustsac before update, task 0, sumup 71010
inside mustsac after update, task 0, sumup 70483
inner_dict_sum {'sac_diff0': 0.00022530555725097656, 'sac_diff1': 0.00730586051940918, 'sac_diff2': 0.008631229400634766, 'sac_diff3': 0.011492013931274414, 'sac_diff4': 0.007647991180419922, 'sac_diff5': 0.05227375030517578, 'sac_diff6': 0.00041604042053222656, 'all': 0.08799219131469727}
diff5_list [0.010617494583129883, 0.010432720184326172, 0.010653495788574219, 0.010516166687011719, 0.010053873062133789]
time3 0.0009114742279052734
time4 0.08890247344970703
time5 0.08895492553710938
time7 0.008917570114135742
gen_weight_change tensor(-18.0263)
policy weight change tensor(36.5375, grad_fn=<SumBackward0>)
time8 0.0018694400787353516
train_time 0.11802363395690918
eval time 0.10698676109313965
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:30,565 MainThread INFO: EPOCH:845
2024-01-23 01:04:30,565 MainThread INFO: Time Consumed:0.22828054428100586s
2024-01-23 01:04:30,566 MainThread INFO: Total Frames:127650s
  8%|▊         | 846/10000 [05:58<38:36,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           15784.81567
Train_Epoch_Reward                11270.94550
Running_Training_Average_Rewards  15609.65467
Explore_Time                      0.00087
Train___Time                      0.11802
Eval____Time                      0.10699
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15334.18393
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.29247     1.10005    91.51070     88.63945
alpha_0                           0.65507      0.00009    0.65520      0.65494
Alpha_loss                        -2.84695     0.00417    -2.83932     -2.85186
Training/policy_loss              -4.48331     0.12771    -4.34197     -4.70273
Training/qf1_loss                 7424.74951   719.08610  8418.16211   6270.25146
Training/qf2_loss                 16093.80859  902.41196  17324.44336  14760.39551
Training/pf_norm                  0.12213      0.02288    0.14986      0.08503
Training/qf1_norm                 613.28071    440.44152  1457.86182   190.02621
Training/qf2_norm                 1552.67114   51.40101   1644.24072   1503.36035
log_std/mean                      -0.13293     0.00283    -0.12796     -0.13609
log_probs/mean                    -2.73170     0.00993    -2.71526     -2.74650
mean/mean                         -0.01010     0.00242    -0.00812     -0.01438
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018764972686767578
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70483
epoch first part time 2.86102294921875e-06
replay_buffer._size: [127950]
collect time 0.0008552074432373047
inner_dict_sum {'sac_diff0': 0.00021839141845703125, 'sac_diff1': 0.007283449172973633, 'sac_diff2': 0.008706808090209961, 'sac_diff3': 0.011254549026489258, 'sac_diff4': 0.007180213928222656, 'sac_diff5': 0.03218698501586914, 'sac_diff6': 0.0004107952117919922, 'all': 0.06724119186401367}
diff5_list [0.006513118743896484, 0.006493330001831055, 0.006895542144775391, 0.006157875061035156, 0.006127119064331055]
time3 0
time4 0.06809663772583008
time5 0.06815099716186523
time7 4.76837158203125e-07
gen_weight_change tensor(-18.0263)
policy weight change tensor(36.5648, grad_fn=<SumBackward0>)
time8 0.001898050308227539
train_time 0.07928133010864258
eval time 0.14897489547729492
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:30,819 MainThread INFO: EPOCH:846
2024-01-23 01:04:30,819 MainThread INFO: Time Consumed:0.23147845268249512s
2024-01-23 01:04:30,820 MainThread INFO: Total Frames:127800s
  8%|▊         | 847/10000 [05:58<38:37,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           15914.60197
Train_Epoch_Reward                26033.69139
Running_Training_Average_Rewards  15519.39067
Explore_Time                      0.00085
Train___Time                      0.07928
Eval____Time                      0.14897
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17624.53126
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.40233     1.56431     95.85338     90.93795
alpha_0                           0.65474      0.00009     0.65487      0.65461
Alpha_loss                        -2.85167     0.00253     -2.84950     -2.85629
Training/policy_loss              -4.86468     0.00376     -4.86141     -4.87166
Training/qf1_loss                 7718.98877   829.18106   8997.79102   6796.93506
Training/qf2_loss                 16959.54492  1077.27905  18728.76172  15654.18164
Training/pf_norm                  0.13720      0.03435     0.19771      0.09815
Training/qf1_norm                 639.68879    295.75572   1168.41333   258.12195
Training/qf2_norm                 1821.24353   30.71241    1870.41956   1773.71289
log_std/mean                      -0.12762     0.00003     -0.12758     -0.12765
log_probs/mean                    -2.73491     0.00469     -2.73137     -2.74421
mean/mean                         -0.00607     0.00008     -0.00599     -0.00621
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018456459045410156
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70483
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [128100]
collect time 0.0008335113525390625
inner_dict_sum {'sac_diff0': 0.00022339820861816406, 'sac_diff1': 0.0073108673095703125, 'sac_diff2': 0.008762836456298828, 'sac_diff3': 0.010787248611450195, 'sac_diff4': 0.0073206424713134766, 'sac_diff5': 0.03256583213806152, 'sac_diff6': 0.0004093647003173828, 'all': 0.06738018989562988}
diff5_list [0.00695037841796875, 0.006857395172119141, 0.00647282600402832, 0.006119728088378906, 0.006165504455566406]
time3 0
time4 0.06821107864379883
time5 0.06826186180114746
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0263)
policy weight change tensor(36.6206, grad_fn=<SumBackward0>)
time8 0.0018069744110107422
train_time 0.07939696311950684
eval time 0.14491796493530273
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:31,069 MainThread INFO: EPOCH:847
2024-01-23 01:04:31,069 MainThread INFO: Time Consumed:0.2274494171142578s
2024-01-23 01:04:31,069 MainThread INFO: Total Frames:127950s
  8%|▊         | 848/10000 [05:58<38:27,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16023.94326
Train_Epoch_Reward                7480.18049
Running_Training_Average_Rewards  15285.02166
Explore_Time                      0.00083
Train___Time                      0.07940
Eval____Time                      0.14492
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17367.59576
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.74368     1.56503    93.44320     89.30586
alpha_0                           0.65441      0.00009    0.65454      0.65428
Alpha_loss                        -2.85384     0.00297    -2.85001     -2.85695
Training/policy_loss              -4.49022     0.00485    -4.48343     -4.49814
Training/qf1_loss                 6978.18506   667.31563  8136.73535   6331.38477
Training/qf2_loss                 15941.92754  928.86214  17442.30469  14903.12988
Training/pf_norm                  0.09552      0.02149    0.12929      0.06145
Training/qf1_norm                 330.55324    152.80294  601.59711    164.49496
Training/qf2_norm                 1632.58005   27.21143   1661.86560   1589.93469
log_std/mean                      -0.12011     0.00015    -0.11991     -0.12034
log_probs/mean                    -2.73208     0.00680    -2.72145     -2.74100
mean/mean                         -0.00700     0.00006    -0.00690     -0.00706
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018732070922851562
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70483
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [128250]
collect time 0.0008270740509033203
inner_dict_sum {'sac_diff0': 0.00022721290588378906, 'sac_diff1': 0.006810903549194336, 'sac_diff2': 0.008021354675292969, 'sac_diff3': 0.010439157485961914, 'sac_diff4': 0.006852149963378906, 'sac_diff5': 0.0318453311920166, 'sac_diff6': 0.0003883838653564453, 'all': 0.06458449363708496}
diff5_list [0.00660252571105957, 0.006206512451171875, 0.006273508071899414, 0.006566524505615234, 0.006196260452270508]
time3 0
time4 0.06535148620605469
time5 0.06539702415466309
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0263)
policy weight change tensor(36.7041, grad_fn=<SumBackward0>)
time8 0.0018644332885742188
train_time 0.07638716697692871
eval time 0.15501785278320312
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:31,326 MainThread INFO: EPOCH:848
2024-01-23 01:04:31,326 MainThread INFO: Time Consumed:0.2346351146697998s
2024-01-23 01:04:31,326 MainThread INFO: Total Frames:128100s
  8%|▊         | 849/10000 [05:58<38:40,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16108.09745
Train_Epoch_Reward                12071.26632
Running_Training_Average_Rewards  15274.84586
Explore_Time                      0.00082
Train___Time                      0.07639
Eval____Time                      0.15502
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17018.05021
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.16831     1.31123    92.71326     88.80124
alpha_0                           0.65409      0.00009    0.65422      0.65396
Alpha_loss                        -2.85898     0.00421    -2.85286     -2.86382
Training/policy_loss              -4.41882     0.00528    -4.41155     -4.42430
Training/qf1_loss                 6875.54385   717.04987  8014.85010   5920.30859
Training/qf2_loss                 15697.29551  878.81080  17002.45898  14718.19238
Training/pf_norm                  0.11588      0.00446    0.12170      0.10832
Training/qf1_norm                 626.94958    262.75319  953.05450    185.27367
Training/qf2_norm                 1525.95383   22.98925   1552.90466   1484.25806
log_std/mean                      -0.12763     0.00013    -0.12746     -0.12783
log_probs/mean                    -2.73627     0.00789    -2.72502     -2.74448
mean/mean                         -0.01022     0.00012    -0.01004     -0.01039
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01859736442565918
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70483
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [128400]
collect time 0.0008618831634521484
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.006948947906494141, 'sac_diff2': 0.008469343185424805, 'sac_diff3': 0.010841608047485352, 'sac_diff4': 0.007190227508544922, 'sac_diff5': 0.03403472900390625, 'sac_diff6': 0.0004012584686279297, 'all': 0.0681002140045166}
diff5_list [0.00662541389465332, 0.007132768630981445, 0.0065860748291015625, 0.006218433380126953, 0.007472038269042969]
time3 0
time4 0.06892609596252441
time5 0.06897950172424316
time7 1.1920928955078125e-06
gen_weight_change tensor(-18.0263)
policy weight change tensor(36.7454, grad_fn=<SumBackward0>)
time8 0.002023458480834961
train_time 0.08033084869384766
eval time 0.1559755802154541
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:31,587 MainThread INFO: EPOCH:849
2024-01-23 01:04:31,588 MainThread INFO: Time Consumed:0.2396090030670166s
2024-01-23 01:04:31,588 MainThread INFO: Total Frames:128250s
  8%|▊         | 850/10000 [05:59<39:03,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16168.32678
Train_Epoch_Reward                9976.90776
Running_Training_Average_Rewards  14906.64666
Explore_Time                      0.00086
Train___Time                      0.08033
Eval____Time                      0.15598
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16955.84901
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.26693     3.51132     96.54169     86.57065
alpha_0                           0.65376      0.00009     0.65389      0.65363
Alpha_loss                        -2.86032     0.00211     -2.85711     -2.86286
Training/policy_loss              -4.55595     0.00332     -4.55114     -4.56115
Training/qf1_loss                 7440.60039   1541.34245  9311.89258   5130.98584
Training/qf2_loss                 16707.29941  2195.82810  19236.52344  13070.59180
Training/pf_norm                  0.10902      0.00961     0.12050      0.09838
Training/qf1_norm                 579.63922    398.59339   1279.89832   158.32224
Training/qf2_norm                 1656.72273   62.00314    1713.94263   1538.62964
log_std/mean                      -0.13332     0.00006     -0.13319     -0.13337
log_probs/mean                    -2.73149     0.00406     -2.72394     -2.73587
mean/mean                         -0.01177     0.00023     -0.01142     -0.01207
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01853632926940918
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70483
epoch first part time 2.86102294921875e-06
replay_buffer._size: [128550]
collect time 0.0008440017700195312
inside mustsac before update, task 0, sumup 70483
inside mustsac after update, task 0, sumup 69847
inner_dict_sum {'sac_diff0': 0.0002148151397705078, 'sac_diff1': 0.007070779800415039, 'sac_diff2': 0.008723258972167969, 'sac_diff3': 0.01055002212524414, 'sac_diff4': 0.0074977874755859375, 'sac_diff5': 0.052176713943481445, 'sac_diff6': 0.00041556358337402344, 'all': 0.08664894104003906}
diff5_list [0.010701894760131836, 0.011913299560546875, 0.010030269622802734, 0.009830713272094727, 0.009700536727905273]
time3 0.0008690357208251953
time4 0.08754301071166992
time5 0.08759951591491699
time7 0.008992671966552734
gen_weight_change tensor(-18.0343)
policy weight change tensor(36.7878, grad_fn=<SumBackward0>)
time8 0.002604961395263672
train_time 0.11762571334838867
eval time 0.11377859115600586
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:31,844 MainThread INFO: EPOCH:850
2024-01-23 01:04:31,845 MainThread INFO: Time Consumed:0.23474907875061035s
2024-01-23 01:04:31,845 MainThread INFO: Total Frames:128400s
  9%|▊         | 851/10000 [05:59<39:13,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16220.77295
Train_Epoch_Reward                6344.76162
Running_Training_Average_Rewards  14700.24495
Explore_Time                      0.00084
Train___Time                      0.11763
Eval____Time                      0.11378
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17024.01139
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.00275     1.61521     95.37301     90.37044
alpha_0                           0.65343      0.00009     0.65356      0.65330
Alpha_loss                        -2.86282     0.00210     -2.86003     -2.86605
Training/policy_loss              -4.88280     0.26590     -4.62623     -5.36915
Training/qf1_loss                 8309.58613   1367.19274  11006.12500  7327.26318
Training/qf2_loss                 17528.02148  1621.65296  20768.65234  16623.48438
Training/pf_norm                  0.12157      0.00902     0.13112      0.10550
Training/qf1_norm                 933.05518    546.66279   1779.76038   326.76874
Training/qf2_norm                 1783.99561   132.49331   2033.67004   1657.20410
log_std/mean                      -0.13194     0.00528     -0.12420     -0.14079
log_probs/mean                    -2.72946     0.00411     -2.72448     -2.73388
mean/mean                         -0.00883     0.00155     -0.00697     -0.01082
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018618106842041016
epoch last part time3 0.002882719039916992
inside rlalgo, task 0, sumup 69847
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [128700]
collect time 0.0008828639984130859
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.006735801696777344, 'sac_diff2': 0.00794672966003418, 'sac_diff3': 0.010755538940429688, 'sac_diff4': 0.007240772247314453, 'sac_diff5': 0.03321504592895508, 'sac_diff6': 0.0004124641418457031, 'all': 0.06652402877807617}
diff5_list [0.006665945053100586, 0.006291389465332031, 0.006566524505615234, 0.006537675857543945, 0.007153511047363281]
time3 0
time4 0.06735944747924805
time5 0.06740784645080566
time7 9.5367431640625e-07
gen_weight_change tensor(-18.0343)
policy weight change tensor(36.7922, grad_fn=<SumBackward0>)
time8 0.0020329952239990234
train_time 0.0787651538848877
eval time 0.14576506614685059
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:32,097 MainThread INFO: EPOCH:851
2024-01-23 01:04:32,097 MainThread INFO: Time Consumed:0.2278146743774414s
2024-01-23 01:04:32,097 MainThread INFO: Total Frames:128550s
  9%|▊         | 852/10000 [05:59<38:52,  3.92it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16351.00262
Train_Epoch_Reward                8966.89985
Running_Training_Average_Rewards  14634.35613
Explore_Time                      0.00088
Train___Time                      0.07877
Eval____Time                      0.14577
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16437.37145
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.80613     1.79013    93.94167     88.43346
alpha_0                           0.65311      0.00009    0.65324      0.65298
Alpha_loss                        -2.86465     0.00207    -2.86136     -2.86691
Training/policy_loss              -4.50869     0.00307    -4.50491     -4.51386
Training/qf1_loss                 6630.44307   493.33436  7557.56494   6198.48096
Training/qf2_loss                 15345.00117  849.69649  16923.56250  14458.24707
Training/pf_norm                  0.13565      0.01614    0.16362      0.11893
Training/qf1_norm                 941.30867    355.80635  1421.83691   319.47223
Training/qf2_norm                 1572.42275   29.90690   1625.21265   1534.56970
log_std/mean                      -0.13271     0.00004    -0.13265     -0.13277
log_probs/mean                    -2.72585     0.00460    -2.71970     -2.73063
mean/mean                         -0.00938     0.00025    -0.00901     -0.00972
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01880025863647461
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69847
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [128850]
collect time 0.0008525848388671875
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.007157325744628906, 'sac_diff2': 0.008772134780883789, 'sac_diff3': 0.010663509368896484, 'sac_diff4': 0.007498502731323242, 'sac_diff5': 0.032965898513793945, 'sac_diff6': 0.0004131793975830078, 'all': 0.06768298149108887}
diff5_list [0.006957292556762695, 0.006819725036621094, 0.006561756134033203, 0.006395578384399414, 0.006231546401977539]
time3 0
time4 0.06852054595947266
time5 0.06857585906982422
time7 4.76837158203125e-07
gen_weight_change tensor(-18.0343)
policy weight change tensor(36.8003, grad_fn=<SumBackward0>)
time8 0.0018303394317626953
train_time 0.0797574520111084
eval time 0.1481313705444336
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:32,351 MainThread INFO: EPOCH:852
2024-01-23 01:04:32,351 MainThread INFO: Time Consumed:0.23114967346191406s
2024-01-23 01:04:32,351 MainThread INFO: Total Frames:128700s
  9%|▊         | 853/10000 [05:59<38:49,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16486.17237
Train_Epoch_Reward                15392.59275
Running_Training_Average_Rewards  14639.65021
Explore_Time                      0.00085
Train___Time                      0.07976
Eval____Time                      0.14813
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16498.48124
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.36235     1.16899    92.74683     89.33664
alpha_0                           0.65278      0.00009    0.65291      0.65265
Alpha_loss                        -2.87149     0.00438    -2.86547     -2.87534
Training/policy_loss              -4.60050     0.00472    -4.59375     -4.60547
Training/qf1_loss                 7007.39648   635.33865  8216.15332   6418.31982
Training/qf2_loss                 15881.17305  737.47380  17103.79297  14902.52051
Training/pf_norm                  0.12611      0.02565    0.16956      0.09421
Training/qf1_norm                 334.54418    214.30153  718.27441    131.71449
Training/qf2_norm                 1601.80996   19.97839   1624.96509   1567.46448
log_std/mean                      -0.11979     0.00004    -0.11972     -0.11983
log_probs/mean                    -2.73400     0.00842    -2.72147     -2.74130
mean/mean                         -0.01356     0.00015    -0.01339     -0.01380
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018929719924926758
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69847
epoch first part time 2.86102294921875e-06
replay_buffer._size: [129000]
collect time 0.0008835792541503906
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.006938457489013672, 'sac_diff2': 0.008405208587646484, 'sac_diff3': 0.010761260986328125, 'sac_diff4': 0.006945610046386719, 'sac_diff5': 0.032868385314941406, 'sac_diff6': 0.0004010200500488281, 'all': 0.06654191017150879}
diff5_list [0.0071523189544677734, 0.006418466567993164, 0.006364345550537109, 0.007010459899902344, 0.005922794342041016]
time3 0
time4 0.06732916831970215
time5 0.06737422943115234
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0343)
policy weight change tensor(36.7473, grad_fn=<SumBackward0>)
time8 0.0020596981048583984
train_time 0.07911300659179688
eval time 0.14139080047607422
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:32,597 MainThread INFO: EPOCH:853
2024-01-23 01:04:32,597 MainThread INFO: Time Consumed:0.2238311767578125s
2024-01-23 01:04:32,598 MainThread INFO: Total Frames:128850s
  9%|▊         | 854/10000 [06:00<38:25,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16619.53084
Train_Epoch_Reward                13862.58455
Running_Training_Average_Rewards  13766.01931
Explore_Time                      0.00088
Train___Time                      0.07911
Eval____Time                      0.14139
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16595.55360
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.29021     1.56887    92.98987     88.70665
alpha_0                           0.65245      0.00009    0.65258      0.65232
Alpha_loss                        -2.87480     0.00255    -2.87173     -2.87817
Training/policy_loss              -5.03946     0.00303    -5.03576     -5.04364
Training/qf1_loss                 7380.82900   627.00765  8140.08447   6381.27832
Training/qf2_loss                 16055.29531  765.03545  17122.14453  14893.51367
Training/pf_norm                  0.13863      0.01707    0.16468      0.11378
Training/qf1_norm                 417.40259    277.58574  903.55511    140.96384
Training/qf2_norm                 1808.01863   29.75214   1859.30127   1776.79871
log_std/mean                      -0.13794     0.00010    -0.13776     -0.13802
log_probs/mean                    -2.73388     0.00387    -2.72876     -2.73862
mean/mean                         -0.01060     0.00004    -0.01056     -0.01067
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01866769790649414
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69847
epoch first part time 2.86102294921875e-06
replay_buffer._size: [129150]
collect time 0.0008690357208251953
inner_dict_sum {'sac_diff0': 0.00022864341735839844, 'sac_diff1': 0.0067424774169921875, 'sac_diff2': 0.00815439224243164, 'sac_diff3': 0.010260343551635742, 'sac_diff4': 0.007059574127197266, 'sac_diff5': 0.03254556655883789, 'sac_diff6': 0.0004296302795410156, 'all': 0.06542062759399414}
diff5_list [0.006674051284790039, 0.0063304901123046875, 0.006193399429321289, 0.006257534027099609, 0.007090091705322266]
time3 0
time4 0.06624174118041992
time5 0.06629586219787598
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0343)
policy weight change tensor(36.6789, grad_fn=<SumBackward0>)
time8 0.0019941329956054688
train_time 0.07756781578063965
eval time 0.14426159858703613
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:32,844 MainThread INFO: EPOCH:854
2024-01-23 01:04:32,845 MainThread INFO: Time Consumed:0.22508716583251953s
2024-01-23 01:04:32,845 MainThread INFO: Total Frames:129000s
  9%|▊         | 855/10000 [06:00<38:12,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16754.68223
Train_Epoch_Reward                4288.25901
Running_Training_Average_Rewards  13629.78272
Explore_Time                      0.00086
Train___Time                      0.07757
Eval____Time                      0.14426
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16691.19448
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.59249     1.00640     93.50013     90.62722
alpha_0                           0.65213      0.00009     0.65226      0.65200
Alpha_loss                        -2.87883     0.00286     -2.87579     -2.88240
Training/policy_loss              -4.38663     0.00544     -4.37971     -4.39567
Training/qf1_loss                 7095.49219   972.97151   8489.10156   5690.16943
Training/qf2_loss                 16277.25820  1091.20756  17881.49023  14909.82812
Training/pf_norm                  0.10803      0.03769     0.16276      0.04572
Training/qf1_norm                 353.59532    131.82398   543.54309    129.09210
Training/qf2_norm                 1513.49263   17.00469    1529.73828   1480.51831
log_std/mean                      -0.12770     0.00007     -0.12761     -0.12779
log_probs/mean                    -2.73543     0.00672     -2.72674     -2.74460
mean/mean                         -0.00825     0.00004     -0.00819     -0.00830
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01867508888244629
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69847
epoch first part time 3.337860107421875e-06
replay_buffer._size: [129300]
collect time 0.0009000301361083984
inside mustsac before update, task 0, sumup 69847
inside mustsac after update, task 0, sumup 70910
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.007178544998168945, 'sac_diff2': 0.008460760116577148, 'sac_diff3': 0.01105952262878418, 'sac_diff4': 0.007747173309326172, 'sac_diff5': 0.05177807807922363, 'sac_diff6': 0.0004086494445800781, 'all': 0.08685779571533203}
diff5_list [0.010812044143676758, 0.01147317886352539, 0.010011672973632812, 0.009589672088623047, 0.009891510009765625]
time3 0.0008623600006103516
time4 0.08773088455200195
time5 0.08779263496398926
time7 0.009160518646240234
gen_weight_change tensor(-17.9437)
policy weight change tensor(36.6721, grad_fn=<SumBackward0>)
time8 0.0018672943115234375
train_time 0.11734557151794434
eval time 0.10628080368041992
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:33,094 MainThread INFO: EPOCH:855
2024-01-23 01:04:33,094 MainThread INFO: Time Consumed:0.22688961029052734s
2024-01-23 01:04:33,094 MainThread INFO: Total Frames:129150s
  9%|▊         | 856/10000 [06:00<38:07,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16904.51185
Train_Epoch_Reward                34232.68368
Running_Training_Average_Rewards  14279.20053
Explore_Time                      0.00090
Train___Time                      0.11735
Eval____Time                      0.10628
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16832.48005
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.96699     2.52515     96.74218     89.84884
alpha_0                           0.65180      0.00009     0.65193      0.65167
Alpha_loss                        -2.88040     0.00311     -2.87611     -2.88400
Training/policy_loss              -4.75112     0.11545     -4.57299     -4.88280
Training/qf1_loss                 6796.38877   1089.45983  8254.77441   5062.84619
Training/qf2_loss                 15765.05215  1520.20037  18171.85352  13677.21094
Training/pf_norm                  0.11686      0.02548     0.14631      0.06974
Training/qf1_norm                 1133.03445   468.14807   1863.70789   442.15942
Training/qf2_norm                 1698.42642   17.24614    1727.41785   1679.46582
log_std/mean                      -0.13061     0.00534     -0.12515     -0.13887
log_probs/mean                    -2.73123     0.00562     -2.72435     -2.73808
mean/mean                         -0.00831     0.00174     -0.00654     -0.01132
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018421173095703125
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70910
epoch first part time 2.86102294921875e-06
replay_buffer._size: [129450]
collect time 0.0008523464202880859
inner_dict_sum {'sac_diff0': 0.00021910667419433594, 'sac_diff1': 0.007838726043701172, 'sac_diff2': 0.009493350982666016, 'sac_diff3': 0.01192164421081543, 'sac_diff4': 0.00814509391784668, 'sac_diff5': 0.0347440242767334, 'sac_diff6': 0.0004131793975830078, 'all': 0.07277512550354004}
diff5_list [0.006848812103271484, 0.0073659420013427734, 0.0067708492279052734, 0.006879568099975586, 0.006878852844238281]
time3 0
time4 0.07361173629760742
time5 0.07366633415222168
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9437)
policy weight change tensor(36.5207, grad_fn=<SumBackward0>)
time8 0.0020055770874023438
train_time 0.08505988121032715
eval time 0.14225316047668457
epoch last part time 8.344650268554688e-06
2024-01-23 01:04:33,346 MainThread INFO: EPOCH:856
2024-01-23 01:04:33,347 MainThread INFO: Time Consumed:0.2305898666381836s
2024-01-23 01:04:33,347 MainThread INFO: Total Frames:129300s
  9%|▊         | 857/10000 [06:00<38:14,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16820.66892
Train_Epoch_Reward                25200.28655
Running_Training_Average_Rewards  14798.63537
Explore_Time                      0.00085
Train___Time                      0.08506
Eval____Time                      0.14225
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16786.10198
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       89.08604     1.93187     92.08165     86.63627
alpha_0                           0.65148      0.00009     0.65161      0.65134
Alpha_loss                        -2.88307     0.00188     -2.88078     -2.88497
Training/policy_loss              -4.63223     0.00270     -4.62899     -4.63649
Training/qf1_loss                 6690.59805   1199.90408  7954.36572   5068.56152
Training/qf2_loss                 15122.51758  1561.96416  16855.09570  13205.53613
Training/pf_norm                  0.12612      0.01673     0.15407      0.10738
Training/qf1_norm                 1333.30410   418.63962   1857.82788   710.64581
Training/qf2_norm                 1585.53770   34.03519    1636.20288   1540.64880
log_std/mean                      -0.13806     0.00025     -0.13765     -0.13836
log_probs/mean                    -2.72961     0.00424     -2.72425     -2.73583
mean/mean                         -0.00563     0.00003     -0.00558     -0.00568
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018558025360107422
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70910
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [129600]
collect time 0.0010144710540771484
inner_dict_sum {'sac_diff0': 0.00021886825561523438, 'sac_diff1': 0.007336616516113281, 'sac_diff2': 0.00870203971862793, 'sac_diff3': 0.010457754135131836, 'sac_diff4': 0.006723880767822266, 'sac_diff5': 0.031149864196777344, 'sac_diff6': 0.00040078163146972656, 'all': 0.06498980522155762}
diff5_list [0.0066492557525634766, 0.006289005279541016, 0.006092548370361328, 0.006052970886230469, 0.006066083908081055]
time3 0
time4 0.06580877304077148
time5 0.06585812568664551
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9437)
policy weight change tensor(36.4380, grad_fn=<SumBackward0>)
time8 0.001834869384765625
train_time 0.07750415802001953
eval time 0.14689183235168457
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:33,597 MainThread INFO: EPOCH:857
2024-01-23 01:04:33,597 MainThread INFO: Time Consumed:0.22780632972717285s
2024-01-23 01:04:33,597 MainThread INFO: Total Frames:129450s
  9%|▊         | 858/10000 [06:01<38:10,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16774.68693
Train_Epoch_Reward                4889.26495
Running_Training_Average_Rewards  14247.72984
Explore_Time                      0.00101
Train___Time                      0.07750
Eval____Time                      0.14689
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16907.77593
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.84777     3.27089     95.08464     86.06576
alpha_0                           0.65115      0.00009     0.65128      0.65102
Alpha_loss                        -2.88547     0.00294     -2.87987     -2.88790
Training/policy_loss              -4.54129     0.00431     -4.53473     -4.54705
Training/qf1_loss                 6797.05762   494.84138   7245.93359   5987.13379
Training/qf2_loss                 15810.47520  1056.37363  16882.50195  14322.29395
Training/pf_norm                  0.11472      0.01806     0.13072      0.09018
Training/qf1_norm                 543.59541    484.90255   1450.94043   145.65077
Training/qf2_norm                 1643.46619   57.11422    1700.92236   1541.81970
log_std/mean                      -0.13078     0.00007     -0.13071     -0.13090
log_probs/mean                    -2.72736     0.00535     -2.71742     -2.73303
mean/mean                         -0.00806     0.00011     -0.00795     -0.00825
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018112659454345703
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70910
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [129750]
collect time 0.0008549690246582031
inner_dict_sum {'sac_diff0': 0.0002162456512451172, 'sac_diff1': 0.007212162017822266, 'sac_diff2': 0.0086212158203125, 'sac_diff3': 0.011037826538085938, 'sac_diff4': 0.007277250289916992, 'sac_diff5': 0.03303647041320801, 'sac_diff6': 0.0004048347473144531, 'all': 0.06780600547790527}
diff5_list [0.006952524185180664, 0.006301403045654297, 0.006287574768066406, 0.006842851638793945, 0.006652116775512695]
time3 0
time4 0.06866216659545898
time5 0.06871223449707031
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9437)
policy weight change tensor(36.3513, grad_fn=<SumBackward0>)
time8 0.0018756389617919922
train_time 0.07996559143066406
eval time 0.1430809497833252
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:33,844 MainThread INFO: EPOCH:858
2024-01-23 01:04:33,845 MainThread INFO: Time Consumed:0.2262868881225586s
2024-01-23 01:04:33,845 MainThread INFO: Total Frames:129600s
  9%|▊         | 859/10000 [06:01<38:03,  4.00it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16776.78683
Train_Epoch_Reward                24465.44747
Running_Training_Average_Rewards  14199.40084
Explore_Time                      0.00085
Train___Time                      0.07997
Eval____Time                      0.14308
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17039.04914
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.09582     1.06157    93.27928     90.36322
alpha_0                           0.65082      0.00009    0.65095      0.65069
Alpha_loss                        -2.88931     0.00196    -2.88560     -2.89120
Training/policy_loss              -4.65747     0.00350    -4.65335     -4.66200
Training/qf1_loss                 6724.27646   576.92778  7777.93262   6112.64111
Training/qf2_loss                 15749.72754  623.08822  16691.16406  14790.65332
Training/pf_norm                  0.10482      0.00405    0.11095      0.09951
Training/qf1_norm                 318.86309    153.85071  575.93860    168.52937
Training/qf2_norm                 1651.67361   19.44557   1672.44446   1619.82495
log_std/mean                      -0.13616     0.00022    -0.13584     -0.13643
log_probs/mean                    -2.72846     0.00292    -2.72296     -2.73131
mean/mean                         -0.00509     0.00017    -0.00486     -0.00535
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018301963806152344
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70910
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [129900]
collect time 0.0008637905120849609
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.007303953170776367, 'sac_diff2': 0.008712530136108398, 'sac_diff3': 0.010906457901000977, 'sac_diff4': 0.0073812007904052734, 'sac_diff5': 0.03352046012878418, 'sac_diff6': 0.00040602684020996094, 'all': 0.06845259666442871}
diff5_list [0.0070629119873046875, 0.006779193878173828, 0.007200956344604492, 0.006316423416137695, 0.0061609745025634766]
time3 0
time4 0.06926536560058594
time5 0.06931710243225098
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9437)
policy weight change tensor(36.3393, grad_fn=<SumBackward0>)
time8 0.0019421577453613281
train_time 0.08058857917785645
eval time 0.15065622329711914
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:34,101 MainThread INFO: EPOCH:859
2024-01-23 01:04:34,101 MainThread INFO: Time Consumed:0.23448562622070312s
2024-01-23 01:04:34,101 MainThread INFO: Total Frames:129750s
  9%|▊         | 860/10000 [06:01<38:22,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16784.53772
Train_Epoch_Reward                8418.15407
Running_Training_Average_Rewards  14267.86531
Explore_Time                      0.00086
Train___Time                      0.08059
Eval____Time                      0.15066
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17033.35791
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.87442     1.79695    93.59595     88.43376
alpha_0                           0.65050      0.00009    0.65063      0.65037
Alpha_loss                        -2.89429     0.00393    -2.88888     -2.90114
Training/policy_loss              -4.70328     0.00377    -4.70039     -4.71066
Training/qf1_loss                 7216.77949   645.69902  7868.82178   6008.14307
Training/qf2_loss                 16225.72363  962.02294  17039.08008  14347.87793
Training/pf_norm                  0.12236      0.01960    0.14696      0.09763
Training/qf1_norm                 463.57357    102.35238  647.87665    352.25754
Training/qf2_norm                 1713.37556   34.41201   1746.63147   1647.61060
log_std/mean                      -0.12676     0.00006    -0.12670     -0.12683
log_probs/mean                    -2.73222     0.00786    -2.72120     -2.74503
mean/mean                         -0.00465     0.00021    -0.00438     -0.00498
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018545150756835938
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70910
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [130050]
collect time 0.0008730888366699219
inside mustsac before update, task 0, sumup 70910
inside mustsac after update, task 0, sumup 70894
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.007077455520629883, 'sac_diff2': 0.008950948715209961, 'sac_diff3': 0.011172771453857422, 'sac_diff4': 0.007924318313598633, 'sac_diff5': 0.05418086051940918, 'sac_diff6': 0.0004436969757080078, 'all': 0.08996772766113281}
diff5_list [0.010747909545898438, 0.011193275451660156, 0.011469841003417969, 0.010205745697021484, 0.010564088821411133]
time3 0.0008943080902099609
time4 0.09090542793273926
time5 0.09095931053161621
time7 0.00886678695678711
gen_weight_change tensor(-17.8725)
policy weight change tensor(36.2542, grad_fn=<SumBackward0>)
time8 0.0027098655700683594
train_time 0.12143397331237793
eval time 0.11040163040161133
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:34,358 MainThread INFO: EPOCH:860
2024-01-23 01:04:34,359 MainThread INFO: Time Consumed:0.23516321182250977s
2024-01-23 01:04:34,359 MainThread INFO: Total Frames:129900s
  9%|▊         | 861/10000 [06:01<38:44,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16779.30298
Train_Epoch_Reward                7697.25016
Running_Training_Average_Rewards  14011.99945
Explore_Time                      0.00087
Train___Time                      0.12143
Eval____Time                      0.11040
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16971.66399
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.36091     0.76950    92.24272     90.23456
alpha_0                           0.65017      0.00009    0.65030      0.65004
Alpha_loss                        -2.89708     0.00231    -2.89347     -2.90069
Training/policy_loss              -4.77814     0.13801    -4.67510     -5.04581
Training/qf1_loss                 7047.65762   482.37921  7738.97266   6362.37402
Training/qf2_loss                 15929.34414  567.61789  16735.00977  15012.93750
Training/pf_norm                  0.12838      0.02593    0.15407      0.08198
Training/qf1_norm                 414.10155    291.66404  947.28741    121.10116
Training/qf2_norm                 1680.16135   80.86680   1829.80896   1608.48535
log_std/mean                      -0.13423     0.00197    -0.13134     -0.13700
log_probs/mean                    -2.73089     0.00520    -2.72406     -2.73928
mean/mean                         -0.01087     0.00315    -0.00538     -0.01432
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018155574798583984
epoch last part time3 0.002916574478149414
inside rlalgo, task 0, sumup 70894
epoch first part time 3.337860107421875e-06
replay_buffer._size: [130200]
collect time 0.0009229183197021484
inner_dict_sum {'sac_diff0': 0.00023031234741210938, 'sac_diff1': 0.007558345794677734, 'sac_diff2': 0.00853109359741211, 'sac_diff3': 0.010612010955810547, 'sac_diff4': 0.006952047348022461, 'sac_diff5': 0.03251838684082031, 'sac_diff6': 0.0004150867462158203, 'all': 0.0668172836303711}
diff5_list [0.0069544315338134766, 0.0066454410552978516, 0.0067021846771240234, 0.006005525588989258, 0.006210803985595703]
time3 0
time4 0.06769108772277832
time5 0.06774497032165527
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8725)
policy weight change tensor(36.3717, grad_fn=<SumBackward0>)
time8 0.0019524097442626953
train_time 0.07903933525085449
eval time 0.1462101936340332
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:34,611 MainThread INFO: EPOCH:861
2024-01-23 01:04:34,611 MainThread INFO: Time Consumed:0.22860479354858398s
2024-01-23 01:04:34,612 MainThread INFO: Total Frames:130050s
  9%|▊         | 862/10000 [06:02<38:33,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16894.35411
Train_Epoch_Reward                26038.46281
Running_Training_Average_Rewards  14590.14122
Explore_Time                      0.00092
Train___Time                      0.07904
Eval____Time                      0.14621
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17587.88277
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.54733     1.87858    95.62924     90.02125
alpha_0                           0.64985      0.00009    0.64998      0.64972
Alpha_loss                        -2.90180     0.00349    -2.89531     -2.90495
Training/policy_loss              -4.46687     0.00378    -4.45974     -4.46998
Training/qf1_loss                 6742.68359   542.48418  7301.69043   6016.34131
Training/qf2_loss                 15856.99512  782.59734  16979.48828  14876.18945
Training/pf_norm                  0.13964      0.01953    0.16163      0.10705
Training/qf1_norm                 534.27343    213.19511  842.77441    232.52939
Training/qf2_norm                 1588.48237   31.87305   1641.30798   1547.10376
log_std/mean                      -0.12234     0.00021    -0.12208     -0.12265
log_probs/mean                    -2.73405     0.00694    -2.72054     -2.74000
mean/mean                         -0.01140     0.00031    -0.01099     -0.01187
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018925189971923828
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70894
epoch first part time 3.337860107421875e-06
replay_buffer._size: [130350]
collect time 0.0008797645568847656
inner_dict_sum {'sac_diff0': 0.00023674964904785156, 'sac_diff1': 0.00678706169128418, 'sac_diff2': 0.008031845092773438, 'sac_diff3': 0.010421514511108398, 'sac_diff4': 0.00681304931640625, 'sac_diff5': 0.031024456024169922, 'sac_diff6': 0.0003783702850341797, 'all': 0.06369304656982422}
diff5_list [0.006494760513305664, 0.00628209114074707, 0.006109714508056641, 0.006003141403198242, 0.006134748458862305]
time3 0
time4 0.06445121765136719
time5 0.06449556350708008
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8725)
policy weight change tensor(36.4941, grad_fn=<SumBackward0>)
time8 0.0018830299377441406
train_time 0.07545232772827148
eval time 0.1472635269165039
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:34,860 MainThread INFO: EPOCH:862
2024-01-23 01:04:34,860 MainThread INFO: Time Consumed:0.22599244117736816s
2024-01-23 01:04:34,860 MainThread INFO: Total Frames:130200s
  9%|▊         | 863/10000 [06:02<38:19,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16978.32152
Train_Epoch_Reward                10054.55964
Running_Training_Average_Rewards  14354.44176
Explore_Time                      0.00087
Train___Time                      0.07545
Eval____Time                      0.14726
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17338.15539
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.41076     3.07416     96.42114     88.30016
alpha_0                           0.64952      0.00009     0.64965      0.64939
Alpha_loss                        -2.90602     0.00129     -2.90465     -2.90807
Training/policy_loss              -5.35324     0.00211     -5.35010     -5.35597
Training/qf1_loss                 7565.89014   1274.31043  10050.19141  6469.11670
Training/qf2_loss                 16543.40313  1657.44831  19759.14062  15143.98145
Training/pf_norm                  0.13958      0.01914     0.16330      0.11298
Training/qf1_norm                 1789.35325   536.77670   2499.60620   1080.75916
Training/qf2_norm                 1975.42893   64.96112    2061.83765   1889.71252
log_std/mean                      -0.12866     0.00015     -0.12840     -0.12882
log_probs/mean                    -2.73601     0.00513     -2.73025     -2.74389
mean/mean                         -0.00374     0.00031     -0.00332     -0.00417
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018356800079345703
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70894
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [130500]
collect time 0.000888824462890625
inner_dict_sum {'sac_diff0': 0.0002548694610595703, 'sac_diff1': 0.006898403167724609, 'sac_diff2': 0.008098363876342773, 'sac_diff3': 0.010645151138305664, 'sac_diff4': 0.007246494293212891, 'sac_diff5': 0.032068490982055664, 'sac_diff6': 0.000392913818359375, 'all': 0.06560468673706055}
diff5_list [0.006661891937255859, 0.006545305252075195, 0.00641632080078125, 0.006307840347290039, 0.00613713264465332]
time3 0
time4 0.06636834144592285
time5 0.06641244888305664
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8725)
policy weight change tensor(36.5563, grad_fn=<SumBackward0>)
time8 0.0018358230590820312
train_time 0.07758069038391113
eval time 0.15107178688049316
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:35,114 MainThread INFO: EPOCH:863
2024-01-23 01:04:35,114 MainThread INFO: Time Consumed:0.23186945915222168s
2024-01-23 01:04:35,114 MainThread INFO: Total Frames:130350s
  9%|▊         | 864/10000 [06:02<38:25,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           17010.16969
Train_Epoch_Reward                21626.99528
Running_Training_Average_Rewards  13774.35510
Explore_Time                      0.00088
Train___Time                      0.07758
Eval____Time                      0.15107
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16914.03528
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.30094     1.61373    94.14947     89.75362
alpha_0                           0.64920      0.00009    0.64933      0.64907
Alpha_loss                        -2.90508     0.00175    -2.90288     -2.90770
Training/policy_loss              -4.70513     0.00254    -4.70141     -4.70935
Training/qf1_loss                 6825.90039   609.77843  7948.44385   6273.70996
Training/qf2_loss                 15747.24395  687.80240  17082.47266  15193.01270
Training/pf_norm                  0.15505      0.01545    0.16994      0.13447
Training/qf1_norm                 2323.73926   269.95529  2673.24365   1940.14966
Training/qf2_norm                 1661.00393   30.30757   1696.17236   1612.73035
log_std/mean                      -0.13182     0.00003    -0.13178     -0.13187
log_probs/mean                    -2.72604     0.00544    -2.71784     -2.73368
mean/mean                         -0.01067     0.00029    -0.01027     -0.01110
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018543481826782227
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70894
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [130650]
collect time 0.0009129047393798828
inner_dict_sum {'sac_diff0': 0.00022912025451660156, 'sac_diff1': 0.0072438716888427734, 'sac_diff2': 0.008556365966796875, 'sac_diff3': 0.011026620864868164, 'sac_diff4': 0.007443428039550781, 'sac_diff5': 0.0328984260559082, 'sac_diff6': 0.00039267539978027344, 'all': 0.06779050827026367}
diff5_list [0.006689786911010742, 0.0062694549560546875, 0.006175518035888672, 0.0067861080169677734, 0.006977558135986328]
time3 0
time4 0.06856465339660645
time5 0.06861066818237305
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8725)
policy weight change tensor(36.5329, grad_fn=<SumBackward0>)
time8 0.0019345283508300781
train_time 0.07977557182312012
eval time 0.14963293075561523
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:35,368 MainThread INFO: EPOCH:864
2024-01-23 01:04:35,369 MainThread INFO: Time Consumed:0.23258233070373535s
2024-01-23 01:04:35,369 MainThread INFO: Total Frames:130500s
  9%|▊         | 865/10000 [06:02<38:33,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17009.29293
Train_Epoch_Reward                9015.14034
Running_Training_Average_Rewards  13642.56739
Explore_Time                      0.00091
Train___Time                      0.07978
Eval____Time                      0.14963
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16682.42680
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.20154     2.10643     94.46049     88.33176
alpha_0                           0.64888      0.00009     0.64900      0.64875
Alpha_loss                        -2.91267     0.00237     -2.90998     -2.91604
Training/policy_loss              -4.51776     0.00339     -4.51251     -4.52168
Training/qf1_loss                 7037.40967   693.96644   8160.59521   6126.25488
Training/qf2_loss                 16056.27734  1034.62481  17419.13672  14417.11230
Training/pf_norm                  0.11053      0.01320     0.12391      0.08700
Training/qf1_norm                 1206.22987   407.43937   1631.73193   461.45953
Training/qf2_norm                 1618.00132   36.34985    1657.40588   1550.90930
log_std/mean                      -0.14302     0.00019     -0.14274     -0.14328
log_probs/mean                    -2.73582     0.00609     -2.72650     -2.74263
mean/mean                         -0.01128     0.00032     -0.01086     -0.01177
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019234657287597656
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70894
epoch first part time 3.337860107421875e-06
replay_buffer._size: [130800]
collect time 0.0007932186126708984
inside mustsac before update, task 0, sumup 70894
inside mustsac after update, task 0, sumup 70998
inner_dict_sum {'sac_diff0': 0.0002186298370361328, 'sac_diff1': 0.006688833236694336, 'sac_diff2': 0.008261919021606445, 'sac_diff3': 0.010293006896972656, 'sac_diff4': 0.0073413848876953125, 'sac_diff5': 0.04932999610900879, 'sac_diff6': 0.0003955364227294922, 'all': 0.08252930641174316}
diff5_list [0.010653018951416016, 0.009912729263305664, 0.009786128997802734, 0.009583234786987305, 0.00939488410949707]
time3 0.0008478164672851562
time4 0.08335447311401367
time5 0.08340263366699219
time7 0.008817911148071289
gen_weight_change tensor(-17.8557)
policy weight change tensor(36.4870, grad_fn=<SumBackward0>)
time8 0.0017952919006347656
train_time 0.11202359199523926
eval time 0.1099853515625
epoch last part time 4.0531158447265625e-06
2024-01-23 01:04:35,616 MainThread INFO: EPOCH:865
2024-01-23 01:04:35,616 MainThread INFO: Time Consumed:0.22501397132873535s
2024-01-23 01:04:35,616 MainThread INFO: Total Frames:130650s
  9%|▊         | 866/10000 [06:03<38:15,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16983.66153
Train_Epoch_Reward                12462.03061
Running_Training_Average_Rewards  13829.55152
Explore_Time                      0.00079
Train___Time                      0.11202
Eval____Time                      0.10999
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16576.16614
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.62538     1.82146     94.26478     89.11427
alpha_0                           0.64855      0.00009     0.64868      0.64842
Alpha_loss                        -2.91158     0.00159     -2.90870     -2.91322
Training/policy_loss              -4.83960     0.27858     -4.38560     -5.11336
Training/qf1_loss                 7066.84414   719.00631   8096.70215   6103.28223
Training/qf2_loss                 15989.59668  1033.38963  17274.11914  14520.25781
Training/pf_norm                  0.12275      0.02248     0.15655      0.09428
Training/qf1_norm                 1019.42894   468.35641   1693.88708   385.12589
Training/qf2_norm                 1731.43782   132.73958   1900.71582   1498.55078
log_std/mean                      -0.13026     0.00291     -0.12688     -0.13524
log_probs/mean                    -2.72552     0.00245     -2.72198     -2.72932
mean/mean                         -0.01300     0.00235     -0.00933     -0.01643
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018219947814941406
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70998
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [130950]
collect time 0.0008232593536376953
inner_dict_sum {'sac_diff0': 0.00021219253540039062, 'sac_diff1': 0.006532907485961914, 'sac_diff2': 0.007742881774902344, 'sac_diff3': 0.010002374649047852, 'sac_diff4': 0.006703615188598633, 'sac_diff5': 0.031317710876464844, 'sac_diff6': 0.0003814697265625, 'all': 0.06289315223693848}
diff5_list [0.006460905075073242, 0.006178855895996094, 0.006114006042480469, 0.0065500736236572266, 0.0060138702392578125]
time3 0
time4 0.06363558769226074
time5 0.06367921829223633
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8557)
policy weight change tensor(36.4371, grad_fn=<SumBackward0>)
time8 0.0018765926361083984
train_time 0.07466793060302734
eval time 0.14658665657043457
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:35,862 MainThread INFO: EPOCH:866
2024-01-23 01:04:35,862 MainThread INFO: Time Consumed:0.22434186935424805s
2024-01-23 01:04:35,862 MainThread INFO: Total Frames:130800s
  9%|▊         | 867/10000 [06:03<38:00,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17022.73743
Train_Epoch_Reward                19237.49018
Running_Training_Average_Rewards  14240.36106
Explore_Time                      0.00082
Train___Time                      0.07467
Eval____Time                      0.14659
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17176.86092
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.66648     2.59528     96.69033     89.25930
alpha_0                           0.64823      0.00009     0.64836      0.64810
Alpha_loss                        -2.91923     0.00191     -2.91564     -2.92095
Training/policy_loss              -5.14874     0.00297     -5.14411     -5.15229
Training/qf1_loss                 7396.68525   916.21347   8735.38770   6504.47266
Training/qf2_loss                 16728.22324  1386.95901  18691.55078  14963.47656
Training/pf_norm                  0.11070      0.01466     0.12988      0.08477
Training/qf1_norm                 543.77817    236.96461   857.33179    240.15289
Training/qf2_norm                 1924.50562   53.99059    1988.89746   1832.93298
log_std/mean                      -0.13012     0.00006     -0.13001     -0.13020
log_probs/mean                    -2.73542     0.00408     -2.72868     -2.73974
mean/mean                         -0.01116     0.00017     -0.01090     -0.01140
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018318653106689453
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70998
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [131100]
collect time 0.0008006095886230469
inner_dict_sum {'sac_diff0': 0.00019812583923339844, 'sac_diff1': 0.0062901973724365234, 'sac_diff2': 0.00730133056640625, 'sac_diff3': 0.009384393692016602, 'sac_diff4': 0.006543874740600586, 'sac_diff5': 0.030011653900146484, 'sac_diff6': 0.00037932395935058594, 'all': 0.06010890007019043}
diff5_list [0.0063152313232421875, 0.005906581878662109, 0.005885124206542969, 0.006218671798706055, 0.005686044692993164]
time3 0
time4 0.06083202362060547
time5 0.06087923049926758
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8557)
policy weight change tensor(36.4464, grad_fn=<SumBackward0>)
time8 0.0017430782318115234
train_time 0.07161331176757812
eval time 0.1507885456085205
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:36,109 MainThread INFO: EPOCH:867
2024-01-23 01:04:36,109 MainThread INFO: Time Consumed:0.22542738914489746s
2024-01-23 01:04:36,110 MainThread INFO: Total Frames:130950s
  9%|▊         | 868/10000 [06:03<37:53,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           17042.94379
Train_Epoch_Reward                3106.83040
Running_Training_Average_Rewards  14017.06385
Explore_Time                      0.00080
Train___Time                      0.07161
Eval____Time                      0.15079
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17109.83955
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.79180     1.71607    94.59009     90.61815
alpha_0                           0.64790      0.00009    0.64803      0.64777
Alpha_loss                        -2.92072     0.00342    -2.91638     -2.92461
Training/policy_loss              -5.10299     0.00436    -5.09642     -5.10778
Training/qf1_loss                 7882.57373   669.43547  9127.19824   7182.80225
Training/qf2_loss                 17063.41738  935.44516  18667.02148  15917.23145
Training/pf_norm                  0.08007      0.01676    0.10173      0.05421
Training/qf1_norm                 629.29936    349.65710  1071.97388   222.73502
Training/qf2_norm                 1946.04404   34.73620   1984.45996   1900.86450
log_std/mean                      -0.12521     0.00009    -0.12511     -0.12533
log_probs/mean                    -2.73109     0.00630    -2.72279     -2.73724
mean/mean                         -0.00907     0.00017    -0.00883     -0.00932
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01811361312866211
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70998
epoch first part time 2.384185791015625e-06
replay_buffer._size: [131250]
collect time 0.0009086132049560547
inner_dict_sum {'sac_diff0': 0.00020122528076171875, 'sac_diff1': 0.0066165924072265625, 'sac_diff2': 0.007893085479736328, 'sac_diff3': 0.010148048400878906, 'sac_diff4': 0.0068264007568359375, 'sac_diff5': 0.03164267539978027, 'sac_diff6': 0.00038433074951171875, 'all': 0.06371235847473145}
diff5_list [0.0065038204193115234, 0.0061910152435302734, 0.006283283233642578, 0.006412029266357422, 0.0062525272369384766]
time3 0
time4 0.06445789337158203
time5 0.06450247764587402
time7 2.384185791015625e-07
gen_weight_change tensor(-17.8557)
policy weight change tensor(36.5208, grad_fn=<SumBackward0>)
time8 0.0017952919006347656
train_time 0.07529163360595703
eval time 0.15209317207336426
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:36,361 MainThread INFO: EPOCH:868
2024-01-23 01:04:36,402 MainThread INFO: Time Consumed:0.23057937622070312s
2024-01-23 01:04:36,402 MainThread INFO: Total Frames:131100s
  9%|▊         | 869/10000 [06:04<39:52,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17041.65435
Train_Epoch_Reward                33604.58489
Running_Training_Average_Rewards  14945.62451
Explore_Time                      0.00090
Train___Time                      0.07529
Eval____Time                      0.15209
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17026.15477
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.94027     2.77383     96.80492     89.04309
alpha_0                           0.64758      0.00009     0.64771      0.64745
Alpha_loss                        -2.92478     0.00294     -2.92249     -2.93055
Training/policy_loss              -5.05510     0.00265     -5.05139     -5.05965
Training/qf1_loss                 7089.45420   773.53426   8399.61230   6297.28369
Training/qf2_loss                 16044.07871  1298.06872  18326.80859  14799.32324
Training/pf_norm                  0.08897      0.01301     0.10322      0.07131
Training/qf1_norm                 917.54082    436.28366   1370.79944   217.81812
Training/qf2_norm                 1848.46558   54.61713    1944.02942   1791.04358
log_std/mean                      -0.13125     0.00014     -0.13108     -0.13146
log_probs/mean                    -2.73270     0.00662     -2.72558     -2.74443
mean/mean                         -0.01759     0.00021     -0.01731     -0.01791
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.05866694450378418
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70998
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [131400]
collect time 0.0009248256683349609
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.006516456604003906, 'sac_diff2': 0.007750511169433594, 'sac_diff3': 0.010264396667480469, 'sac_diff4': 0.006976127624511719, 'sac_diff5': 0.031108379364013672, 'sac_diff6': 0.00037598609924316406, 'all': 0.06320428848266602}
diff5_list [0.006705522537231445, 0.0060727596282958984, 0.00637507438659668, 0.005935192108154297, 0.0060198307037353516]
time3 0
time4 0.06393933296203613
time5 0.06398177146911621
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8557)
policy weight change tensor(36.6161, grad_fn=<SumBackward0>)
time8 0.0018444061279296875
train_time 0.07503747940063477
eval time 0.11180806159973145
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:36,614 MainThread INFO: EPOCH:869
2024-01-23 01:04:36,614 MainThread INFO: Time Consumed:0.19005227088928223s
2024-01-23 01:04:36,614 MainThread INFO: Total Frames:131250s
  9%|▊         | 870/10000 [06:04<37:34,  4.05it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           17031.68448
Train_Epoch_Reward                5446.24279
Running_Training_Average_Rewards  14724.73688
Explore_Time                      0.00092
Train___Time                      0.07504
Eval____Time                      0.11181
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16933.65917
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.15869     0.57061    92.89644     91.41040
alpha_0                           0.64726      0.00009    0.64738      0.64713
Alpha_loss                        -2.92714     0.00203    -2.92441     -2.92922
Training/policy_loss              -4.95986     0.00480    -4.95379     -4.96637
Training/qf1_loss                 6942.53008   915.88616  8526.07129   6054.01953
Training/qf2_loss                 15911.07910  998.45833  17595.25391  15026.22461
Training/pf_norm                  0.13237      0.01372    0.15053      0.11553
Training/qf1_norm                 1416.75906   121.63415  1571.82971   1274.02563
Training/qf2_norm                 1817.04866   11.74018   1834.32922   1800.03296
log_std/mean                      -0.12997     0.00003    -0.12992     -0.13000
log_probs/mean                    -2.73039     0.00627    -2.72101     -2.73674
mean/mean                         -0.01178     0.00025    -0.01144     -0.01215
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01813220977783203
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70998
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [131550]
collect time 0.0009238719940185547
inside mustsac before update, task 0, sumup 70998
inside mustsac after update, task 0, sumup 70499
inner_dict_sum {'sac_diff0': 0.00022077560424804688, 'sac_diff1': 0.006443977355957031, 'sac_diff2': 0.007782936096191406, 'sac_diff3': 0.010190725326538086, 'sac_diff4': 0.007114887237548828, 'sac_diff5': 0.050836801528930664, 'sac_diff6': 0.0004062652587890625, 'all': 0.08299636840820312}
diff5_list [0.010371208190917969, 0.009821414947509766, 0.010507822036743164, 0.009896993637084961, 0.010239362716674805]
time3 0.0008459091186523438
time4 0.08381795883178711
time5 0.08387351036071777
time7 0.009181022644042969
gen_weight_change tensor(-17.8937)
policy weight change tensor(36.5779, grad_fn=<SumBackward0>)
time8 0.0026073455810546875
train_time 0.11357736587524414
eval time 0.10963964462280273
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:36,861 MainThread INFO: EPOCH:870
2024-01-23 01:04:36,862 MainThread INFO: Time Consumed:0.22633123397827148s
2024-01-23 01:04:36,862 MainThread INFO: Total Frames:131400s
  9%|▊         | 871/10000 [06:04<37:44,  4.03it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17018.59782
Train_Epoch_Reward                16701.56771
Running_Training_Average_Rewards  14583.74192
Explore_Time                      0.00092
Train___Time                      0.11358
Eval____Time                      0.10964
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16840.79741
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.75600     3.54016     97.27455     87.64823
alpha_0                           0.64693      0.00009     0.64706      0.64680
Alpha_loss                        -2.92950     0.00133     -2.92731     -2.93089
Training/policy_loss              -5.03878     0.15558     -4.84750     -5.27444
Training/qf1_loss                 6577.41572   1243.39605  8149.26562   5097.84912
Training/qf2_loss                 15358.66250  1881.10940  18236.24023  13233.05957
Training/pf_norm                  0.13703      0.04886     0.21574      0.07809
Training/qf1_norm                 383.30157    244.05443   835.58838    122.26440
Training/qf2_norm                 1803.24116   78.41040    1890.03979   1669.34436
log_std/mean                      -0.13463     0.00299     -0.12942     -0.13776
log_probs/mean                    -2.72807     0.00409     -2.71996     -2.73075
mean/mean                         -0.01486     0.00295     -0.01081     -0.01867
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018014192581176758
epoch last part time3 0.0026068687438964844
inside rlalgo, task 0, sumup 70499
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [131700]
collect time 0.0009222030639648438
inner_dict_sum {'sac_diff0': 0.00020647048950195312, 'sac_diff1': 0.006742715835571289, 'sac_diff2': 0.007880687713623047, 'sac_diff3': 0.009863615036010742, 'sac_diff4': 0.006776094436645508, 'sac_diff5': 0.03175854682922363, 'sac_diff6': 0.0003788471221923828, 'all': 0.06360697746276855}
diff5_list [0.0065708160400390625, 0.0062825679779052734, 0.006410121917724609, 0.006533145904541016, 0.005961894989013672]
time3 0
time4 0.06433582305908203
time5 0.06437993049621582
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8937)
policy weight change tensor(36.7855, grad_fn=<SumBackward0>)
time8 0.001786947250366211
train_time 0.07511258125305176
eval time 0.14259910583496094
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:37,106 MainThread INFO: EPOCH:871
2024-01-23 01:04:37,107 MainThread INFO: Time Consumed:0.2208690643310547s
2024-01-23 01:04:37,107 MainThread INFO: Total Frames:131550s
  9%|▊         | 872/10000 [06:04<37:28,  4.06it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16848.64805
Train_Epoch_Reward                24168.32266
Running_Training_Average_Rewards  14856.72665
Explore_Time                      0.00092
Train___Time                      0.07511
Eval____Time                      0.14260
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15888.38510
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.80923     1.26518    94.41196     91.19284
alpha_0                           0.64661      0.00009    0.64674      0.64648
Alpha_loss                        -2.93410     0.00283    -2.93017     -2.93696
Training/policy_loss              -4.61293     0.00637    -4.60421     -4.62149
Training/qf1_loss                 7291.24424   466.52795  7684.70215   6391.17285
Training/qf2_loss                 16444.18906  560.59228  16941.21094  15354.54883
Training/pf_norm                  0.14474      0.01075    0.15511      0.12571
Training/qf1_norm                 740.01996    239.86246  1049.64954   442.25577
Training/qf2_norm                 1676.17249   20.93217   1704.91211   1650.29272
log_std/mean                      -0.12337     0.00023    -0.12311     -0.12375
log_probs/mean                    -2.73091     0.00702    -2.72274     -2.74056
mean/mean                         -0.01741     0.00031    -0.01701     -0.01788
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018038034439086914
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70499
epoch first part time 3.337860107421875e-06
replay_buffer._size: [131850]
collect time 0.0009219646453857422
inner_dict_sum {'sac_diff0': 0.00020170211791992188, 'sac_diff1': 0.0064640045166015625, 'sac_diff2': 0.007784128189086914, 'sac_diff3': 0.010101318359375, 'sac_diff4': 0.006754398345947266, 'sac_diff5': 0.031759023666381836, 'sac_diff6': 0.0003814697265625, 'all': 0.063446044921875}
diff5_list [0.00666499137878418, 0.006425619125366211, 0.00620722770690918, 0.006330966949462891, 0.006130218505859375]
time3 0
time4 0.06418871879577637
time5 0.06424188613891602
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8937)
policy weight change tensor(36.9220, grad_fn=<SumBackward0>)
time8 0.0018808841705322266
train_time 0.07499933242797852
eval time 0.15465784072875977
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:37,361 MainThread INFO: EPOCH:872
2024-01-23 01:04:37,361 MainThread INFO: Time Consumed:0.23294305801391602s
2024-01-23 01:04:37,361 MainThread INFO: Total Frames:131700s
  9%|▊         | 873/10000 [06:04<37:58,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16698.54190
Train_Epoch_Reward                11493.55519
Running_Training_Average_Rewards  14659.66336
Explore_Time                      0.00092
Train___Time                      0.07500
Eval____Time                      0.15466
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15837.09392
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.74900     0.66141     93.50342     91.72652
alpha_0                           0.64629      0.00009     0.64641      0.64616
Alpha_loss                        -2.93666     0.00347     -2.93045     -2.93951
Training/policy_loss              -4.79508     0.00513     -4.78537     -4.79937
Training/qf1_loss                 8174.01895   1615.91271  10073.40332  6185.84717
Training/qf2_loss                 17268.39570  1685.86171  19326.77539  15071.56836
Training/pf_norm                  0.13713      0.01174     0.15053      0.11901
Training/qf1_norm                 1636.28267   160.63377   1798.51074   1350.83997
Training/qf2_norm                 1719.91150   12.78782    1735.22620   1699.84399
log_std/mean                      -0.12775     0.00008     -0.12761     -0.12784
log_probs/mean                    -2.72907     0.00733     -2.71791     -2.73572
mean/mean                         -0.01424     0.00010     -0.01406     -0.01434
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020970582962036133
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70499
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [132000]
collect time 0.0009889602661132812
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.007680654525756836, 'sac_diff2': 0.009078025817871094, 'sac_diff3': 0.011778116226196289, 'sac_diff4': 0.007841348648071289, 'sac_diff5': 0.036195993423461914, 'sac_diff6': 0.00042128562927246094, 'all': 0.07320976257324219}
diff5_list [0.007470130920410156, 0.0070612430572509766, 0.007415294647216797, 0.0077631473541259766, 0.006486177444458008]
time3 0
time4 0.07402801513671875
time5 0.07407617568969727
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8937)
policy weight change tensor(37.0499, grad_fn=<SumBackward0>)
time8 0.0018768310546875
train_time 0.08577489852905273
eval time 0.15502595901489258
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:37,630 MainThread INFO: EPOCH:873
2024-01-23 01:04:37,630 MainThread INFO: Time Consumed:0.24413347244262695s
2024-01-23 01:04:37,630 MainThread INFO: Total Frames:131850s
  9%|▊         | 874/10000 [06:05<38:44,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16604.19921
Train_Epoch_Reward                7205.20601
Running_Training_Average_Rewards  14433.64525
Explore_Time                      0.00098
Train___Time                      0.08577
Eval____Time                      0.15503
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             15970.60831
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.94361     1.48191     94.60837     90.04931
alpha_0                           0.64596      0.00009     0.64609      0.64583
Alpha_loss                        -2.93947     0.00174     -2.93709     -2.94150
Training/policy_loss              -4.86157     0.00127     -4.85905     -4.86234
Training/qf1_loss                 7741.59082   1108.55742  9701.73145   6291.92480
Training/qf2_loss                 16671.77227  1369.61107  19112.62500  14885.63867
Training/pf_norm                  0.11719      0.03080     0.14746      0.06834
Training/qf1_norm                 1597.65432   289.03587   2079.93115   1169.92212
Training/qf2_norm                 1806.84124   29.45930    1859.83875   1768.85547
log_std/mean                      -0.11967     0.00027     -0.11933     -0.12009
log_probs/mean                    -2.72780     0.00343     -2.72399     -2.73398
mean/mean                         -0.01735     0.00002     -0.01733     -0.01738
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018364667892456055
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70499
epoch first part time 2.86102294921875e-06
replay_buffer._size: [132150]
collect time 0.0008285045623779297
inner_dict_sum {'sac_diff0': 0.00020933151245117188, 'sac_diff1': 0.0062754154205322266, 'sac_diff2': 0.007515430450439453, 'sac_diff3': 0.00965571403503418, 'sac_diff4': 0.006575822830200195, 'sac_diff5': 0.03145289421081543, 'sac_diff6': 0.0004086494445800781, 'all': 0.062093257904052734}
diff5_list [0.00637507438659668, 0.006038188934326172, 0.006767988204956055, 0.006215333938598633, 0.006056308746337891]
time3 0
time4 0.06282424926757812
time5 0.06287312507629395
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8937)
policy weight change tensor(37.1759, grad_fn=<SumBackward0>)
time8 0.0018875598907470703
train_time 0.0738222599029541
eval time 0.1511845588684082
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:37,880 MainThread INFO: EPOCH:874
2024-01-23 01:04:37,880 MainThread INFO: Time Consumed:0.22806000709533691s
2024-01-23 01:04:37,880 MainThread INFO: Total Frames:132000s
  9%|▉         | 875/10000 [06:05<38:30,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16552.51474
Train_Epoch_Reward                4788.84175
Running_Training_Average_Rewards  14184.70021
Explore_Time                      0.00082
Train___Time                      0.07382
Eval____Time                      0.15118
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16165.58210
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.41513     3.25937     95.59246     87.07712
alpha_0                           0.64564      0.00009     0.64577      0.64551
Alpha_loss                        -2.94460     0.00259     -2.93972     -2.94739
Training/policy_loss              -5.07684     0.00394     -5.06914     -5.08014
Training/qf1_loss                 7281.44609   1524.90161  9355.19629   4865.83301
Training/qf2_loss                 16336.87734  2167.49414  19046.96875  12830.62500
Training/pf_norm                  0.12345      0.02420     0.16693      0.09911
Training/qf1_norm                 634.58885    455.69941   1468.84534   166.89035
Training/qf2_norm                 1867.01418   65.05351    1930.19202   1760.40723
log_std/mean                      -0.12608     0.00017     -0.12585     -0.12633
log_probs/mean                    -2.73185     0.00641     -2.72070     -2.73978
mean/mean                         -0.01936     0.00016     -0.01912     -0.01955
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01828622817993164
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70499
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [132300]
collect time 0.0008101463317871094
inside mustsac before update, task 0, sumup 70499
inside mustsac after update, task 0, sumup 70532
inner_dict_sum {'sac_diff0': 0.00020241737365722656, 'sac_diff1': 0.0064356327056884766, 'sac_diff2': 0.007826089859008789, 'sac_diff3': 0.009921550750732422, 'sac_diff4': 0.007057666778564453, 'sac_diff5': 0.04945516586303711, 'sac_diff6': 0.0004019737243652344, 'all': 0.08130049705505371}
diff5_list [0.010117530822753906, 0.009669780731201172, 0.01001739501953125, 0.009659767150878906, 0.009990692138671875]
time3 0.0008530616760253906
time4 0.0821065902709961
time5 0.0821530818939209
time7 0.008593082427978516
gen_weight_change tensor(-17.8249)
policy weight change tensor(37.1010, grad_fn=<SumBackward0>)
time8 0.0018169879913330078
train_time 0.11032414436340332
eval time 0.11720895767211914
epoch last part time 1.4066696166992188e-05
2024-01-23 01:04:38,132 MainThread INFO: EPOCH:875
2024-01-23 01:04:38,132 MainThread INFO: Time Consumed:0.23052740097045898s
2024-01-23 01:04:38,132 MainThread INFO: Total Frames:132150s
  9%|▉         | 876/10000 [06:05<38:26,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16525.19137
Train_Epoch_Reward                12838.46347
Running_Training_Average_Rewards  14236.95081
Explore_Time                      0.00081
Train___Time                      0.11032
Eval____Time                      0.11721
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16302.93251
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.60908     2.31421     96.48555     90.07816
alpha_0                           0.64532      0.00009     0.64545      0.64519
Alpha_loss                        -2.94625     0.00179     -2.94436     -2.94876
Training/policy_loss              -5.03356     0.19102     -4.86022     -5.34606
Training/qf1_loss                 7204.53555   886.44609   8387.21191   5997.42041
Training/qf2_loss                 16291.91387  1355.40722  18265.15430  14537.12793
Training/pf_norm                  0.13111      0.05918     0.23450      0.05581
Training/qf1_norm                 1138.83084   819.14675   2490.00220   208.01031
Training/qf2_norm                 1827.22163   97.97501    2012.69861   1745.07788
log_std/mean                      -0.13016     0.00499     -0.12378     -0.13574
log_probs/mean                    -2.72794     0.00498     -2.72126     -2.73519
mean/mean                         -0.01528     0.00361     -0.01066     -0.01999
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.017928361892700195
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70532
epoch first part time 2.384185791015625e-06
replay_buffer._size: [132450]
collect time 0.0008220672607421875
inner_dict_sum {'sac_diff0': 0.00020956993103027344, 'sac_diff1': 0.006633281707763672, 'sac_diff2': 0.007855653762817383, 'sac_diff3': 0.009882926940917969, 'sac_diff4': 0.007067203521728516, 'sac_diff5': 0.031261444091796875, 'sac_diff6': 0.0003790855407714844, 'all': 0.06328916549682617}
diff5_list [0.006499052047729492, 0.006129741668701172, 0.006224632263183594, 0.006072282791137695, 0.006335735321044922]
time3 0
time4 0.0640413761138916
time5 0.0641028881072998
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8249)
policy weight change tensor(37.1520, grad_fn=<SumBackward0>)
time8 0.0018336772918701172
train_time 0.07500529289245605
eval time 0.1560070514678955
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:38,387 MainThread INFO: EPOCH:876
2024-01-23 01:04:38,388 MainThread INFO: Time Consumed:0.23409295082092285s
2024-01-23 01:04:38,388 MainThread INFO: Total Frames:132300s
  9%|▉         | 877/10000 [06:05<38:34,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16470.15782
Train_Epoch_Reward                15922.53039
Running_Training_Average_Rewards  13899.91211
Explore_Time                      0.00082
Train___Time                      0.07501
Eval____Time                      0.15601
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16626.52539
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.06291     0.97921     94.68379     91.59393
alpha_0                           0.64500      0.00009     0.64512      0.64487
Alpha_loss                        -2.94979     0.00209     -2.94781     -2.95360
Training/policy_loss              -5.00548     0.00565     -4.99561     -5.01105
Training/qf1_loss                 7695.39570   1121.34570  9634.89844   6195.35254
Training/qf2_loss                 16913.97305  1208.91488  18886.33789  15108.42383
Training/pf_norm                  0.17244      0.04232     0.23258      0.11692
Training/qf1_norm                 721.27304    166.45833   961.74225    439.40659
Training/qf2_norm                 1854.24058   19.19739    1885.91675   1825.62378
log_std/mean                      -0.12674     0.00011     -0.12653     -0.12683
log_probs/mean                    -2.72833     0.00374     -2.72229     -2.73396
mean/mean                         -0.01688     0.00047     -0.01613     -0.01745
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018233776092529297
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70532
epoch first part time 2.86102294921875e-06
replay_buffer._size: [132600]
collect time 0.0008521080017089844
inner_dict_sum {'sac_diff0': 0.00020384788513183594, 'sac_diff1': 0.006855964660644531, 'sac_diff2': 0.008467435836791992, 'sac_diff3': 0.010483026504516602, 'sac_diff4': 0.0071506500244140625, 'sac_diff5': 0.03531455993652344, 'sac_diff6': 0.0005154609680175781, 'all': 0.06899094581604004}
diff5_list [0.006491422653198242, 0.006830453872680664, 0.006440877914428711, 0.006619930267333984, 0.008931875228881836]
time3 0
time4 0.07010316848754883
time5 0.07016420364379883
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8249)
policy weight change tensor(37.0336, grad_fn=<SumBackward0>)
time8 0.002595663070678711
train_time 0.08250856399536133
eval time 0.13742399215698242
epoch last part time 1.0013580322265625e-05
2024-01-23 01:04:38,633 MainThread INFO: EPOCH:877
2024-01-23 01:04:38,633 MainThread INFO: Time Consumed:0.2233591079711914s
2024-01-23 01:04:38,633 MainThread INFO: Total Frames:132450s
  9%|▉         | 878/10000 [06:06<38:14,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16447.90737
Train_Epoch_Reward                4155.05682
Running_Training_Average_Rewards  13789.07466
Explore_Time                      0.00085
Train___Time                      0.08251
Eval____Time                      0.13742
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16887.33497
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.87132     2.02603     95.32510     90.22627
alpha_0                           0.64467      0.00009     0.64480      0.64454
Alpha_loss                        -2.95232     0.00273     -2.94919     -2.95721
Training/policy_loss              -4.89128     0.00243     -4.88790     -4.89470
Training/qf1_loss                 7320.95596   1240.57400  9678.12695   6459.58398
Training/qf2_loss                 16425.70430  1606.59794  19281.12109  15019.15137
Training/pf_norm                  0.12523      0.01604     0.15583      0.11243
Training/qf1_norm                 1810.20095   443.84206   2349.02148   1277.44177
Training/qf2_norm                 1797.30649   38.67067    1844.03186   1747.30640
log_std/mean                      -0.14619     0.00021     -0.14584     -0.14643
log_probs/mean                    -2.72644     0.00469     -2.72084     -2.73451
mean/mean                         -0.01130     0.00044     -0.01067     -0.01192
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01977825164794922
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70532
epoch first part time 4.291534423828125e-06
replay_buffer._size: [132750]
collect time 0.0008935928344726562
inner_dict_sum {'sac_diff0': 0.00020265579223632812, 'sac_diff1': 0.007580280303955078, 'sac_diff2': 0.009372472763061523, 'sac_diff3': 0.011038064956665039, 'sac_diff4': 0.007718324661254883, 'sac_diff5': 0.032932281494140625, 'sac_diff6': 0.0003807544708251953, 'all': 0.06922483444213867}
diff5_list [0.0075321197509765625, 0.00623321533203125, 0.006243467330932617, 0.006659030914306641, 0.006264448165893555]
time3 0
time4 0.06998062133789062
time5 0.07002544403076172
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8249)
policy weight change tensor(36.9588, grad_fn=<SumBackward0>)
time8 0.0019042491912841797
train_time 0.08169698715209961
eval time 0.13633179664611816
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:38,877 MainThread INFO: EPOCH:878
2024-01-23 01:04:38,877 MainThread INFO: Time Consumed:0.2211928367614746s
2024-01-23 01:04:38,877 MainThread INFO: Total Frames:132600s
  9%|▉         | 879/10000 [06:06<37:51,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           16459.39117
Train_Epoch_Reward                12561.27438
Running_Training_Average_Rewards  13805.40826
Explore_Time                      0.00089
Train___Time                      0.08170
Eval____Time                      0.13633
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17140.99277
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.96412     2.64861    94.87230     87.73058
alpha_0                           0.64435      0.00009    0.64448      0.64422
Alpha_loss                        -2.95783     0.00192    -2.95567     -2.96025
Training/policy_loss              -5.01100     0.00427    -5.00378     -5.01595
Training/qf1_loss                 6352.51631   397.08542  7052.07324   5930.33447
Training/qf2_loss                 15143.17090  807.72120  16110.67969  14184.42773
Training/pf_norm                  0.08290      0.02023    0.11002      0.05726
Training/qf1_norm                 483.51901    280.66792  774.76154    120.43439
Training/qf2_norm                 1786.12310   50.03095   1858.25793   1724.20190
log_std/mean                      -0.13899     0.00004    -0.13891     -0.13904
log_probs/mean                    -2.73133     0.00544    -2.72416     -2.73990
mean/mean                         -0.00736     0.00038    -0.00683     -0.00791
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018479347229003906
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70532
epoch first part time 2.86102294921875e-06
replay_buffer._size: [132900]
collect time 0.0008184909820556641
inner_dict_sum {'sac_diff0': 0.000202178955078125, 'sac_diff1': 0.0065958499908447266, 'sac_diff2': 0.008144617080688477, 'sac_diff3': 0.010175704956054688, 'sac_diff4': 0.0065767765045166016, 'sac_diff5': 0.0315701961517334, 'sac_diff6': 0.0003859996795654297, 'all': 0.06365132331848145}
diff5_list [0.006819486618041992, 0.006196260452270508, 0.006206035614013672, 0.006250143051147461, 0.006098270416259766]
time3 0
time4 0.06440019607543945
time5 0.06444430351257324
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8249)
policy weight change tensor(36.8747, grad_fn=<SumBackward0>)
time8 0.0019342899322509766
train_time 0.07561230659484863
eval time 0.15929484367370605
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:39,137 MainThread INFO: EPOCH:879
2024-01-23 01:04:39,137 MainThread INFO: Time Consumed:0.23798131942749023s
2024-01-23 01:04:39,137 MainThread INFO: Total Frames:132750s
  9%|▉         | 880/10000 [06:06<38:19,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16499.57947
Train_Epoch_Reward                12033.95035
Running_Training_Average_Rewards  13873.97634
Explore_Time                      0.00081
Train___Time                      0.07561
Eval____Time                      0.15929
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17335.54219
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.46125     3.12042     98.02723     88.60223
alpha_0                           0.64403      0.00009     0.64416      0.64390
Alpha_loss                        -2.96067     0.00116     -2.95949     -2.96286
Training/policy_loss              -4.72071     0.00295     -4.71621     -4.72494
Training/qf1_loss                 7362.04521   1459.39544  10099.85840  5832.86328
Training/qf2_loss                 16328.57871  2143.07537  20305.46094  13948.19824
Training/pf_norm                  0.07402      0.02211     0.10212      0.04810
Training/qf1_norm                 2335.39424   654.92571   3179.52417   1193.90771
Training/qf2_norm                 1682.95808   56.91826    1784.76099   1612.91174
log_std/mean                      -0.13765     0.00014     -0.13745     -0.13785
log_probs/mean                    -2.73014     0.00405     -2.72441     -2.73666
mean/mean                         -0.01150     0.00027     -0.01113     -0.01189
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01783895492553711
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70532
epoch first part time 2.86102294921875e-06
replay_buffer._size: [133050]
collect time 0.0007781982421875
inside mustsac before update, task 0, sumup 70532
inside mustsac after update, task 0, sumup 70866
inner_dict_sum {'sac_diff0': 0.00020623207092285156, 'sac_diff1': 0.0068624019622802734, 'sac_diff2': 0.008137941360473633, 'sac_diff3': 0.01002645492553711, 'sac_diff4': 0.007137775421142578, 'sac_diff5': 0.050684452056884766, 'sac_diff6': 0.0003905296325683594, 'all': 0.08344578742980957}
diff5_list [0.011239767074584961, 0.009939193725585938, 0.009764671325683594, 0.009943246841430664, 0.00979757308959961]
time3 0.0008733272552490234
time4 0.08424997329711914
time5 0.08429932594299316
time7 0.008855342864990234
gen_weight_change tensor(-17.7903)
policy weight change tensor(36.8118, grad_fn=<SumBackward0>)
time8 0.0027244091033935547
train_time 0.11383175849914551
eval time 0.11888766288757324
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:39,394 MainThread INFO: EPOCH:880
2024-01-23 01:04:39,394 MainThread INFO: Time Consumed:0.23581886291503906s
2024-01-23 01:04:39,394 MainThread INFO: Total Frames:132900s
  9%|▉         | 881/10000 [06:07<38:41,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16565.93051
Train_Epoch_Reward                18529.02530
Running_Training_Average_Rewards  14280.11847
Explore_Time                      0.00077
Train___Time                      0.11383
Eval____Time                      0.11889
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17504.30783
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.34158     1.90855     94.74709     89.00943
alpha_0                           0.64371      0.00009     0.64384      0.64358
Alpha_loss                        -2.96505     0.00274     -2.96245     -2.96933
Training/policy_loss              -4.98999     0.17148     -4.83406     -5.28632
Training/qf1_loss                 6821.52217   840.58013   8026.49854   5420.58936
Training/qf2_loss                 15885.86543  1146.44369  17251.59766  13822.41992
Training/pf_norm                  0.12525      0.03762     0.18015      0.06292
Training/qf1_norm                 909.38246    704.04616   2181.45142   120.92840
Training/qf2_norm                 1805.60103   99.34884    1944.37805   1657.54761
log_std/mean                      -0.13162     0.00415     -0.12735     -0.13696
log_probs/mean                    -2.73245     0.00480     -2.72743     -2.73912
mean/mean                         -0.00928     0.00141     -0.00673     -0.01099
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018421411514282227
epoch last part time3 0.0024552345275878906
inside rlalgo, task 0, sumup 70866
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [133200]
collect time 0.0008416175842285156
inner_dict_sum {'sac_diff0': 0.00020360946655273438, 'sac_diff1': 0.006974697113037109, 'sac_diff2': 0.008137226104736328, 'sac_diff3': 0.010230541229248047, 'sac_diff4': 0.006871938705444336, 'sac_diff5': 0.0319671630859375, 'sac_diff6': 0.0003783702850341797, 'all': 0.06476354598999023}
diff5_list [0.006850004196166992, 0.006541252136230469, 0.006402254104614258, 0.0061168670654296875, 0.006056785583496094]
time3 0
time4 0.06551218032836914
time5 0.06555604934692383
time7 4.76837158203125e-07
gen_weight_change tensor(-17.7903)
policy weight change tensor(36.7200, grad_fn=<SumBackward0>)
time8 0.0019392967224121094
train_time 0.07672953605651855
eval time 0.14879512786865234
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:39,647 MainThread INFO: EPOCH:881
2024-01-23 01:04:39,647 MainThread INFO: Time Consumed:0.2286205291748047s
2024-01-23 01:04:39,647 MainThread INFO: Total Frames:133050s
  9%|▉         | 882/10000 [06:07<38:30,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16710.90006
Train_Epoch_Reward                9382.41161
Running_Training_Average_Rewards  14293.96886
Explore_Time                      0.00084
Train___Time                      0.07673
Eval____Time                      0.14880
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17338.08059
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.85426     1.88174     95.24136     90.35260
alpha_0                           0.64339      0.00009     0.64351      0.64326
Alpha_loss                        -2.96470     0.00248     -2.96272     -2.96875
Training/policy_loss              -5.03817     0.00477     -5.03199     -5.04578
Training/qf1_loss                 7641.27979   1162.92250  9113.40137   6047.25586
Training/qf2_loss                 16807.70078  1501.58688  18608.88867  14860.04199
Training/pf_norm                  0.15662      0.02109     0.18368      0.12993
Training/qf1_norm                 581.94640    338.86492   1002.80444   155.69232
Training/qf2_norm                 1867.81599   36.94410    1910.09351   1812.21387
log_std/mean                      -0.13759     0.00020     -0.13729     -0.13783
log_probs/mean                    -2.72403     0.00421     -2.71975     -2.73169
mean/mean                         -0.01276     0.00041     -0.01215     -0.01328
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01909470558166504
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70866
epoch first part time 2.86102294921875e-06
replay_buffer._size: [133350]
collect time 0.0008132457733154297
inner_dict_sum {'sac_diff0': 0.00019860267639160156, 'sac_diff1': 0.006556987762451172, 'sac_diff2': 0.007595062255859375, 'sac_diff3': 0.009885311126708984, 'sac_diff4': 0.006592750549316406, 'sac_diff5': 0.03129220008850098, 'sac_diff6': 0.0003845691680908203, 'all': 0.06250548362731934}
diff5_list [0.006211757659912109, 0.006100177764892578, 0.006027698516845703, 0.006810665130615234, 0.0061419010162353516]
time3 0
time4 0.06324505805969238
time5 0.06328845024108887
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7903)
policy weight change tensor(36.7528, grad_fn=<SumBackward0>)
time8 0.0018072128295898438
train_time 0.07425141334533691
eval time 0.1534430980682373
epoch last part time 2.1457672119140625e-05
2024-01-23 01:04:39,900 MainThread INFO: EPOCH:882
2024-01-23 01:04:39,900 MainThread INFO: Time Consumed:0.23078060150146484s
2024-01-23 01:04:39,901 MainThread INFO: Total Frames:133200s
  9%|▉         | 883/10000 [06:07<38:29,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           16892.45162
Train_Epoch_Reward                31971.68811
Running_Training_Average_Rewards  14846.60537
Explore_Time                      0.00081
Train___Time                      0.07425
Eval____Time                      0.15344
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17652.60952
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.90045     1.53588     94.99768     91.00291
alpha_0                           0.64306      0.00009     0.64319      0.64294
Alpha_loss                        -2.96976     0.00358     -2.96663     -2.97636
Training/policy_loss              -5.33054     0.00680     -5.32428     -5.34378
Training/qf1_loss                 7472.71299   826.41590   8499.30469   6204.79492
Training/qf2_loss                 16679.66211  1099.77624  17907.54883  15053.63477
Training/pf_norm                  0.14724      0.01523     0.17064      0.13134
Training/qf1_norm                 428.77807    252.78598   798.12109    143.39076
Training/qf2_norm                 1978.10212   30.84411    2020.50867   1941.66077
log_std/mean                      -0.12905     0.00006     -0.12898     -0.12915
log_probs/mean                    -2.72788     0.00791     -2.71927     -2.74283
mean/mean                         -0.00503     0.00045     -0.00437     -0.00564
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018813133239746094
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70866
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [133500]
collect time 0.0008037090301513672
inner_dict_sum {'sac_diff0': 0.00020170211791992188, 'sac_diff1': 0.007067203521728516, 'sac_diff2': 0.008463621139526367, 'sac_diff3': 0.011067390441894531, 'sac_diff4': 0.0071451663970947266, 'sac_diff5': 0.03328442573547363, 'sac_diff6': 0.0003948211669921875, 'all': 0.06762433052062988}
diff5_list [0.0070858001708984375, 0.006552457809448242, 0.006191253662109375, 0.006655454635620117, 0.006799459457397461]
time3 0
time4 0.0684046745300293
time5 0.068450927734375
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7903)
policy weight change tensor(36.8263, grad_fn=<SumBackward0>)
time8 0.0019636154174804688
train_time 0.07959914207458496
eval time 0.15617847442626953
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:40,161 MainThread INFO: EPOCH:883
2024-01-23 01:04:40,161 MainThread INFO: Time Consumed:0.2388317584991455s
2024-01-23 01:04:40,162 MainThread INFO: Total Frames:133350s
  9%|▉         | 884/10000 [06:07<38:50,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17095.80745
Train_Epoch_Reward                14403.14873
Running_Training_Average_Rewards  14864.62418
Explore_Time                      0.00080
Train___Time                      0.07960
Eval____Time                      0.15618
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18004.16660
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.87407     1.01997     94.03426     91.14414
alpha_0                           0.64274      0.00009     0.64287      0.64261
Alpha_loss                        -2.97337     0.00450     -2.96906     -2.98191
Training/policy_loss              -5.25699     0.00526     -5.25003     -5.26519
Training/qf1_loss                 7540.30352   976.01480   9116.50781   6126.21875
Training/qf2_loss                 16739.56895  1106.40501  18465.23828  15217.12500
Training/pf_norm                  0.10388      0.00729     0.11690      0.09567
Training/qf1_norm                 384.78930    169.95539   582.67139    141.14833
Training/qf2_norm                 1918.21250   20.14429    1940.91638   1883.59827
log_std/mean                      -0.13703     0.00004     -0.13698     -0.13711
log_probs/mean                    -2.72844     0.00920     -2.71868     -2.74472
mean/mean                         -0.00606     0.00047     -0.00540     -0.00672
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018802404403686523
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70866
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [133650]
collect time 0.0008895397186279297
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.007184028625488281, 'sac_diff2': 0.008973836898803711, 'sac_diff3': 0.011677980422973633, 'sac_diff4': 0.007363557815551758, 'sac_diff5': 0.03377509117126465, 'sac_diff6': 0.0004162788391113281, 'all': 0.06959819793701172}
diff5_list [0.00752568244934082, 0.006692171096801758, 0.006586551666259766, 0.006560802459716797, 0.006409883499145508]
time3 0
time4 0.07040190696716309
time5 0.0704488754272461
time7 4.76837158203125e-07
gen_weight_change tensor(-17.7903)
policy weight change tensor(36.8829, grad_fn=<SumBackward0>)
time8 0.0018925666809082031
train_time 0.08235955238342285
eval time 0.1560816764831543
epoch last part time 5.4836273193359375e-06
2024-01-23 01:04:40,425 MainThread INFO: EPOCH:884
2024-01-23 01:04:40,426 MainThread INFO: Time Consumed:0.24168634414672852s
2024-01-23 01:04:40,426 MainThread INFO: Total Frames:133500s
  9%|▉         | 885/10000 [06:08<39:13,  3.87it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           17311.58054
Train_Epoch_Reward                22265.23630
Running_Training_Average_Rewards  15463.85675
Explore_Time                      0.00088
Train___Time                      0.08236
Eval____Time                      0.15608
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18323.31298
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.89677     2.11198    95.51249     89.92544
alpha_0                           0.64242      0.00009    0.64255      0.64229
Alpha_loss                        -2.97624     0.00180    -2.97375     -2.97866
Training/policy_loss              -4.83784     0.00530    -4.83059     -4.84565
Training/qf1_loss                 7028.28643   604.40251  7871.02588   6448.60840
Training/qf2_loss                 16212.92793  979.44988  17433.40430  15277.16895
Training/pf_norm                  0.12868      0.01148    0.14200      0.11473
Training/qf1_norm                 452.61133    211.57592  793.78986    231.07825
Training/qf2_norm                 1708.10972   38.49071   1755.51624   1653.83191
log_std/mean                      -0.12053     0.00011    -0.12038     -0.12069
log_probs/mean                    -2.72733     0.00618    -2.71866     -2.73584
mean/mean                         -0.00216     0.00032    -0.00174     -0.00264
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018665075302124023
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70866
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [133800]
collect time 0.0008873939514160156
inside mustsac before update, task 0, sumup 70866
inside mustsac after update, task 0, sumup 69929
inner_dict_sum {'sac_diff0': 0.00021028518676757812, 'sac_diff1': 0.0068433284759521484, 'sac_diff2': 0.008047342300415039, 'sac_diff3': 0.010454654693603516, 'sac_diff4': 0.007310390472412109, 'sac_diff5': 0.050986289978027344, 'sac_diff6': 0.0004203319549560547, 'all': 0.08427262306213379}
diff5_list [0.010997772216796875, 0.00990152359008789, 0.009856224060058594, 0.010320425033569336, 0.009910345077514648]
time3 0.0008385181427001953
time4 0.0850980281829834
time5 0.08514690399169922
time7 0.008819341659545898
gen_weight_change tensor(-17.8255)
policy weight change tensor(36.8098, grad_fn=<SumBackward0>)
time8 0.0019168853759765625
train_time 0.11408066749572754
eval time 0.11428189277648926
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:40,679 MainThread INFO: EPOCH:885
2024-01-23 01:04:40,680 MainThread INFO: Time Consumed:0.23169803619384766s
2024-01-23 01:04:40,680 MainThread INFO: Total Frames:133650s
  9%|▉         | 886/10000 [06:08<39:01,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17538.05204
Train_Epoch_Reward                18336.73552
Running_Training_Average_Rewards  14933.99182
Explore_Time                      0.00088
Train___Time                      0.11408
Eval____Time                      0.11428
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18567.64759
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.36347     0.95737     93.60164     91.06541
alpha_0                           0.64210      0.00009     0.64223      0.64197
Alpha_loss                        -2.98158     0.00309     -2.97690     -2.98536
Training/policy_loss              -5.07332     0.24718     -4.73170     -5.47790
Training/qf1_loss                 7668.84365   1234.31522  9495.19336   5814.48096
Training/qf2_loss                 16740.21543  1440.69727  18863.23438  14611.73730
Training/pf_norm                  0.14367      0.01472     0.16699      0.12503
Training/qf1_norm                 756.48120    642.98414   1953.72791   174.05026
Training/qf2_norm                 1832.18013   109.20439   1983.65356   1680.73401
log_std/mean                      -0.13002     0.00458     -0.12127     -0.13433
log_probs/mean                    -2.73179     0.00553     -2.72426     -2.73881
mean/mean                         -0.00559     0.00182     -0.00363     -0.00877
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018821001052856445
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 69929
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [133950]
collect time 0.0008845329284667969
inner_dict_sum {'sac_diff0': 0.00023221969604492188, 'sac_diff1': 0.007189035415649414, 'sac_diff2': 0.008668184280395508, 'sac_diff3': 0.010888338088989258, 'sac_diff4': 0.00743865966796875, 'sac_diff5': 0.033394575119018555, 'sac_diff6': 0.00040793418884277344, 'all': 0.06821894645690918}
diff5_list [0.006814479827880859, 0.006462574005126953, 0.0066678524017333984, 0.006743907928466797, 0.006705760955810547]
time3 0
time4 0.06905937194824219
time5 0.06911420822143555
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8255)
policy weight change tensor(36.7919, grad_fn=<SumBackward0>)
time8 0.0019211769104003906
train_time 0.08048582077026367
eval time 0.13884615898132324
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:40,924 MainThread INFO: EPOCH:886
2024-01-23 01:04:40,924 MainThread INFO: Time Consumed:0.2224588394165039s
2024-01-23 01:04:40,925 MainThread INFO: Total Frames:133800s
  9%|▉         | 887/10000 [06:08<38:27,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17753.99303
Train_Epoch_Reward                8368.58548
Running_Training_Average_Rewards  14372.93511
Explore_Time                      0.00088
Train___Time                      0.08049
Eval____Time                      0.13885
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18785.93526
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.96540     2.28242     96.38178     89.70995
alpha_0                           0.64178      0.00009     0.64191      0.64165
Alpha_loss                        -2.98508     0.00167     -2.98226     -2.98746
Training/policy_loss              -4.87614     0.00274     -4.87367     -4.88115
Training/qf1_loss                 7925.40020   1560.32674  10616.92383  6203.37354
Training/qf2_loss                 17103.55098  1945.76441  20469.74219  15295.62012
Training/pf_norm                  0.08795      0.03057     0.12830      0.05097
Training/qf1_norm                 1524.21917   450.77440   2195.75977   887.28003
Training/qf2_norm                 1814.26807   44.64355    1881.03247   1750.59851
log_std/mean                      -0.13311     0.00001     -0.13309     -0.13312
log_probs/mean                    -2.73210     0.00459     -2.72726     -2.74051
mean/mean                         -0.00781     0.00014     -0.00759     -0.00801
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01841139793395996
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69929
epoch first part time 2.86102294921875e-06
replay_buffer._size: [134100]
collect time 0.0008306503295898438
inner_dict_sum {'sac_diff0': 0.00020432472229003906, 'sac_diff1': 0.006605863571166992, 'sac_diff2': 0.007958412170410156, 'sac_diff3': 0.01041102409362793, 'sac_diff4': 0.006959676742553711, 'sac_diff5': 0.0325167179107666, 'sac_diff6': 0.0004074573516845703, 'all': 0.0650634765625}
diff5_list [0.006512641906738281, 0.00632786750793457, 0.006217002868652344, 0.006311178207397461, 0.007148027420043945]
time3 0
time4 0.06583714485168457
time5 0.06588125228881836
time7 2.384185791015625e-07
gen_weight_change tensor(-17.8255)
policy weight change tensor(36.7884, grad_fn=<SumBackward0>)
time8 0.0020279884338378906
train_time 0.07741999626159668
eval time 0.14869213104248047
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:41,175 MainThread INFO: EPOCH:887
2024-01-23 01:04:41,176 MainThread INFO: Time Consumed:0.22920751571655273s
2024-01-23 01:04:41,176 MainThread INFO: Total Frames:133950s
  9%|▉         | 888/10000 [06:08<38:21,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17961.29100
Train_Epoch_Reward                18707.13035
Running_Training_Average_Rewards  14833.53063
Explore_Time                      0.00082
Train___Time                      0.07742
Eval____Time                      0.14869
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18960.31467
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.39743     2.45132     95.82764     89.18208
alpha_0                           0.64146      0.00009     0.64159      0.64133
Alpha_loss                        -2.98441     0.00155     -2.98251     -2.98625
Training/policy_loss              -4.99982     0.00413     -4.99250     -5.00305
Training/qf1_loss                 7259.95088   721.08548   8046.71387   6151.15576
Training/qf2_loss                 16355.21406  1043.59716  17516.19922  14599.13867
Training/pf_norm                  0.16703      0.02192     0.19554      0.13955
Training/qf1_norm                 573.38263    350.83976   1029.72058   184.69798
Training/qf2_norm                 1790.64888   46.37045    1856.21021   1731.26721
log_std/mean                      -0.12830     0.00004     -0.12825     -0.12837
log_probs/mean                    -2.72301     0.00435     -2.71711     -2.72851
mean/mean                         -0.00693     0.00007     -0.00686     -0.00706
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018280506134033203
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69929
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [134250]
collect time 0.0008161067962646484
inner_dict_sum {'sac_diff0': 0.00020503997802734375, 'sac_diff1': 0.0067403316497802734, 'sac_diff2': 0.008139848709106445, 'sac_diff3': 0.010501861572265625, 'sac_diff4': 0.007219076156616211, 'sac_diff5': 0.0328061580657959, 'sac_diff6': 0.0003871917724609375, 'all': 0.06599950790405273}
diff5_list [0.006487846374511719, 0.006838560104370117, 0.006273508071899414, 0.00669407844543457, 0.006512165069580078]
time3 0
time4 0.06674981117248535
time5 0.06679415702819824
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8255)
policy weight change tensor(36.7819, grad_fn=<SumBackward0>)
time8 0.0018155574798583984
train_time 0.07773375511169434
eval time 0.1437361240386963
epoch last part time 5.245208740234375e-06
2024-01-23 01:04:41,422 MainThread INFO: EPOCH:888
2024-01-23 01:04:41,422 MainThread INFO: Time Consumed:0.2245786190032959s
2024-01-23 01:04:41,422 MainThread INFO: Total Frames:134100s
  9%|▉         | 889/10000 [06:09<38:06,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           18121.67197
Train_Epoch_Reward                19252.54068
Running_Training_Average_Rewards  14659.76707
Explore_Time                      0.00081
Train___Time                      0.07773
Eval____Time                      0.14374
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18744.80252
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.68966     2.41856    92.87836     86.27339
alpha_0                           0.64114      0.00009    0.64127      0.64101
Alpha_loss                        -2.99248     0.00234    -2.98882     -2.99510
Training/policy_loss              -5.67675     0.00350    -5.67115     -5.68146
Training/qf1_loss                 7102.88125   361.10790  7709.54004   6643.41309
Training/qf2_loss                 15794.21934  682.41937  16846.97656  14721.47461
Training/pf_norm                  0.10444      0.02819    0.13779      0.07437
Training/qf1_norm                 1444.04254   453.34324  2260.69116   1016.06317
Training/qf2_norm                 2094.82761   55.31572   2145.29419   1993.82971
log_std/mean                      -0.13133     0.00006    -0.13124     -0.13139
log_probs/mean                    -2.73361     0.00533    -2.72386     -2.73876
mean/mean                         -0.00667     0.00006    -0.00658     -0.00674
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019419431686401367
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69929
epoch first part time 2.384185791015625e-06
replay_buffer._size: [134400]
collect time 0.0007665157318115234
inner_dict_sum {'sac_diff0': 0.00019812583923339844, 'sac_diff1': 0.006591796875, 'sac_diff2': 0.008021831512451172, 'sac_diff3': 0.009804010391235352, 'sac_diff4': 0.0067596435546875, 'sac_diff5': 0.03156733512878418, 'sac_diff6': 0.0003819465637207031, 'all': 0.0633246898651123}
diff5_list [0.006469011306762695, 0.006163835525512695, 0.006285905838012695, 0.006670236587524414, 0.00597834587097168]
time3 0
time4 0.06407284736633301
time5 0.06411623954772949
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8255)
policy weight change tensor(36.7388, grad_fn=<SumBackward0>)
time8 0.0018296241760253906
train_time 0.07518935203552246
eval time 0.1495680809020996
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:41,672 MainThread INFO: EPOCH:889
2024-01-23 01:04:41,672 MainThread INFO: Time Consumed:0.22789454460144043s
2024-01-23 01:04:41,673 MainThread INFO: Total Frames:134250s
  9%|▉         | 890/10000 [06:09<38:01,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18250.85444
Train_Epoch_Reward                11801.80503
Running_Training_Average_Rewards  14772.55543
Explore_Time                      0.00076
Train___Time                      0.07519
Eval____Time                      0.14957
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18627.36686
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.35681     2.55350     96.57286     89.51814
alpha_0                           0.64082      0.00009     0.64095      0.64069
Alpha_loss                        -2.99295     0.00195     -2.99061     -2.99561
Training/policy_loss              -5.56360     0.00463     -5.55708     -5.57062
Training/qf1_loss                 7525.38193   1448.00957  9752.43555   5651.33447
Training/qf2_loss                 16569.68672  1937.24069  19600.80078  14130.02344
Training/pf_norm                  0.10944      0.03087     0.13317      0.04853
Training/qf1_norm                 885.70122    520.14808   1734.14941   308.62677
Training/qf2_norm                 2043.86777   56.92885    2137.73657   1979.86890
log_std/mean                      -0.14146     0.00011     -0.14127     -0.14156
log_probs/mean                    -2.72712     0.00634     -2.71947     -2.73610
mean/mean                         -0.00455     0.00011     -0.00445     -0.00473
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.017920255661010742
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69929
epoch first part time 2.86102294921875e-06
replay_buffer._size: [134550]
collect time 0.0008599758148193359
inside mustsac before update, task 0, sumup 69929
inside mustsac after update, task 0, sumup 71278
inner_dict_sum {'sac_diff0': 0.00020813941955566406, 'sac_diff1': 0.0069043636322021484, 'sac_diff2': 0.008164405822753906, 'sac_diff3': 0.010573625564575195, 'sac_diff4': 0.007729530334472656, 'sac_diff5': 0.05282402038574219, 'sac_diff6': 0.0004203319549560547, 'all': 0.08682441711425781}
diff5_list [0.01043701171875, 0.010607719421386719, 0.010846853256225586, 0.010508537292480469, 0.010423898696899414]
time3 0.0008540153503417969
time4 0.08769369125366211
time5 0.08774471282958984
time7 0.008984804153442383
gen_weight_change tensor(-17.8468)
policy weight change tensor(36.7096, grad_fn=<SumBackward0>)
time8 0.0025954246520996094
train_time 0.11768364906311035
eval time 0.1030421257019043
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:41,917 MainThread INFO: EPOCH:890
2024-01-23 01:04:41,917 MainThread INFO: Time Consumed:0.2237837314605713s
2024-01-23 01:04:41,918 MainThread INFO: Total Frames:134400s
  9%|▉         | 891/10000 [06:09<37:54,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18351.83818
Train_Epoch_Reward                30047.33355
Running_Training_Average_Rewards  15517.55821
Explore_Time                      0.00086
Train___Time                      0.11768
Eval____Time                      0.10304
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18514.14521
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.98546     0.71670     93.98418     92.28608
alpha_0                           0.64050      0.00009     0.64063      0.64037
Alpha_loss                        -2.99971     0.00408     -2.99379     -3.00309
Training/policy_loss              -5.25752     0.19242     -5.00281     -5.50243
Training/qf1_loss                 7406.49473   1102.90466  9601.94141   6734.70605
Training/qf2_loss                 16577.49668  1089.70225  18703.08203  15745.15820
Training/pf_norm                  0.14104      0.04499     0.20640      0.07195
Training/qf1_norm                 990.02213    677.86361   2071.49121   161.53841
Training/qf2_norm                 1939.61885   67.49101    2049.25977   1862.53381
log_std/mean                      -0.13128     0.00511     -0.12298     -0.13669
log_probs/mean                    -2.73474     0.00731     -2.72446     -2.74198
mean/mean                         -0.00527     0.00222     -0.00123     -0.00770
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018176555633544922
epoch last part time3 0.0025014877319335938
inside rlalgo, task 0, sumup 71278
epoch first part time 2.86102294921875e-06
replay_buffer._size: [134700]
collect time 0.00086212158203125
inner_dict_sum {'sac_diff0': 0.0002243518829345703, 'sac_diff1': 0.006575345993041992, 'sac_diff2': 0.00797271728515625, 'sac_diff3': 0.010228633880615234, 'sac_diff4': 0.006876945495605469, 'sac_diff5': 0.03169727325439453, 'sac_diff6': 0.0003819465637207031, 'all': 0.06395721435546875}
diff5_list [0.006531715393066406, 0.006191730499267578, 0.006140708923339844, 0.00677180290222168, 0.0060613155364990234]
time3 0
time4 0.064697265625
time5 0.06474184989929199
time7 9.5367431640625e-07
gen_weight_change tensor(-17.8468)
policy weight change tensor(36.6950, grad_fn=<SumBackward0>)
time8 0.0018954277038574219
train_time 0.0757298469543457
eval time 0.14071393013000488
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:42,160 MainThread INFO: EPOCH:891
2024-01-23 01:04:42,161 MainThread INFO: Time Consumed:0.21953821182250977s
2024-01-23 01:04:42,161 MainThread INFO: Total Frames:134550s
  9%|▉         | 892/10000 [06:09<37:30,  4.05it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18412.38133
Train_Epoch_Reward                24222.78573
Running_Training_Average_Rewards  15457.03564
Explore_Time                      0.00086
Train___Time                      0.07573
Eval____Time                      0.14071
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17943.51212
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.67433     2.62515     98.04305     89.90749
alpha_0                           0.64018      0.00009     0.64031      0.64005
Alpha_loss                        -3.00219     0.00438     -2.99787     -3.00952
Training/policy_loss              -5.02046     0.00337     -5.01591     -5.02535
Training/qf1_loss                 7751.48271   1028.60100  9315.45996   6521.35254
Training/qf2_loss                 17108.92305  1520.40524  19533.70312  15153.86523
Training/pf_norm                  0.09892      0.02528     0.13968      0.07069
Training/qf1_norm                 665.59060    413.08806   1450.05640   285.44720
Training/qf2_norm                 1906.97930   53.77446    1997.09753   1830.89990
log_std/mean                      -0.12829     0.00002     -0.12826     -0.12833
log_probs/mean                    -2.73275     0.00828     -2.72457     -2.74768
mean/mean                         0.00079      0.00004     0.00084      0.00074
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01869344711303711
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71278
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [134850]
collect time 0.0009255409240722656
inner_dict_sum {'sac_diff0': 0.0002148151397705078, 'sac_diff1': 0.0065228939056396484, 'sac_diff2': 0.007696866989135742, 'sac_diff3': 0.009919881820678711, 'sac_diff4': 0.006529569625854492, 'sac_diff5': 0.03168964385986328, 'sac_diff6': 0.000385284423828125, 'all': 0.06295895576477051}
diff5_list [0.0067441463470458984, 0.006239652633666992, 0.0067059993743896484, 0.0060007572174072266, 0.005999088287353516]
time3 0
time4 0.06369829177856445
time5 0.06375718116760254
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8468)
policy weight change tensor(36.7133, grad_fn=<SumBackward0>)
time8 0.0017938613891601562
train_time 0.07450747489929199
eval time 0.1501929759979248
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:42,410 MainThread INFO: EPOCH:892
2024-01-23 01:04:42,411 MainThread INFO: Time Consumed:0.22791194915771484s
2024-01-23 01:04:42,411 MainThread INFO: Total Frames:134700s
  9%|▉         | 893/10000 [06:10<37:40,  4.03it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           18434.96460
Train_Epoch_Reward                11537.02855
Running_Training_Average_Rewards  15506.45127
Explore_Time                      0.00092
Train___Time                      0.07451
Eval____Time                      0.15019
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17878.44223
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.33985     0.55928    92.93037     91.52653
alpha_0                           0.63986      0.00009    0.63999      0.63973
Alpha_loss                        -3.00638     0.00272    -3.00379     -3.01094
Training/policy_loss              -4.90001     0.00290    -4.89524     -4.90435
Training/qf1_loss                 7032.11816   544.63359  7547.65137   6324.91846
Training/qf2_loss                 15973.29316  557.42669  16539.00391  15154.90918
Training/pf_norm                  0.14037      0.01883    0.17690      0.12332
Training/qf1_norm                 2441.56104   77.04766   2543.15601   2363.11548
Training/qf2_norm                 1768.39243   9.19931    1777.86499   1755.26306
log_std/mean                      -0.13113     0.00004    -0.13107     -0.13118
log_probs/mean                    -2.73460     0.00464    -2.73030     -2.74330
mean/mean                         -0.00357     0.00024    -0.00332     -0.00396
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01917862892150879
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71278
epoch first part time 2.86102294921875e-06
replay_buffer._size: [135000]
collect time 0.0009691715240478516
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.007117748260498047, 'sac_diff2': 0.008224248886108398, 'sac_diff3': 0.010528087615966797, 'sac_diff4': 0.007058858871459961, 'sac_diff5': 0.03266501426696777, 'sac_diff6': 0.0003917217254638672, 'all': 0.06620001792907715}
diff5_list [0.007608175277709961, 0.00654149055480957, 0.0062410831451416016, 0.005948543548583984, 0.006325721740722656]
time3 0
time4 0.06701445579528809
time5 0.0670616626739502
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8468)
policy weight change tensor(36.6748, grad_fn=<SumBackward0>)
time8 0.0020704269409179688
train_time 0.07889175415039062
eval time 0.14351153373718262
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:42,659 MainThread INFO: EPOCH:893
2024-01-23 01:04:42,660 MainThread INFO: Time Consumed:0.22563457489013672s
2024-01-23 01:04:42,660 MainThread INFO: Total Frames:134850s
  9%|▉         | 894/10000 [06:10<37:38,  4.03it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18410.03185
Train_Epoch_Reward                13999.94290
Running_Training_Average_Rewards  15252.21619
Explore_Time                      0.00096
Train___Time                      0.07889
Eval____Time                      0.14351
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17754.83909
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.02440     3.29177     95.99104     86.47088
alpha_0                           0.63954      0.00009     0.63967      0.63941
Alpha_loss                        -3.00878     0.00281     -3.00420     -3.01260
Training/policy_loss              -5.12583     0.00547     -5.11687     -5.13194
Training/qf1_loss                 7007.74668   1006.10965  8402.36914   5339.18604
Training/qf2_loss                 15987.50176  1642.66587  18166.30859  13215.87988
Training/pf_norm                  0.15700      0.01849     0.18342      0.13311
Training/qf1_norm                 582.85018    393.88702   1320.53357   204.99110
Training/qf2_norm                 1873.39832   66.93088    1954.13098   1761.22900
log_std/mean                      -0.13059     0.00012     -0.13041     -0.13077
log_probs/mean                    -2.73242     0.00706     -2.72068     -2.74098
mean/mean                         -0.00547     0.00009     -0.00532     -0.00555
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018088579177856445
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71278
epoch first part time 2.384185791015625e-06
replay_buffer._size: [135150]
collect time 0.0008697509765625
inner_dict_sum {'sac_diff0': 0.00020194053649902344, 'sac_diff1': 0.006465911865234375, 'sac_diff2': 0.007805585861206055, 'sac_diff3': 0.00992441177368164, 'sac_diff4': 0.006766557693481445, 'sac_diff5': 0.032366275787353516, 'sac_diff6': 0.0003807544708251953, 'all': 0.06391143798828125}
diff5_list [0.006403684616088867, 0.00619959831237793, 0.006293773651123047, 0.0065991878509521484, 0.0068700313568115234]
time3 0
time4 0.06465530395507812
time5 0.06469941139221191
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8468)
policy weight change tensor(36.5901, grad_fn=<SumBackward0>)
time8 0.0019273757934570312
train_time 0.07568788528442383
eval time 0.14994287490844727
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:42,910 MainThread INFO: EPOCH:894
2024-01-23 01:04:42,910 MainThread INFO: Time Consumed:0.22876214981079102s
2024-01-23 01:04:42,910 MainThread INFO: Total Frames:135000s
  9%|▉         | 895/10000 [06:10<37:44,  4.02it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18343.56571
Train_Epoch_Reward                11226.31022
Running_Training_Average_Rewards  15325.92186
Explore_Time                      0.00087
Train___Time                      0.07569
Eval____Time                      0.14994
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17658.65151
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.56515     1.69316     93.03314     88.62516
alpha_0                           0.63922      0.00009     0.63935      0.63909
Alpha_loss                        -3.01092     0.00184     -3.00742     -3.01276
Training/policy_loss              -4.89800     0.00358     -4.89099     -4.90096
Training/qf1_loss                 7386.77148   1063.86366  8927.93359   5866.72314
Training/qf2_loss                 16323.81602  1307.15646  18176.49414  14184.43457
Training/pf_norm                  0.18290      0.02945     0.22367      0.14676
Training/qf1_norm                 609.84506    368.62602   1283.28845   240.95303
Training/qf2_norm                 1763.10554   33.04089    1789.37415   1708.24536
log_std/mean                      -0.13463     0.00013     -0.13442     -0.13478
log_probs/mean                    -2.72969     0.00280     -2.72487     -2.73229
mean/mean                         -0.00878     0.00002     -0.00875     -0.00881
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018107175827026367
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71278
epoch first part time 2.86102294921875e-06
replay_buffer._size: [135300]
collect time 0.0009438991546630859
inside mustsac before update, task 0, sumup 71278
inside mustsac after update, task 0, sumup 70241
inner_dict_sum {'sac_diff0': 0.00020313262939453125, 'sac_diff1': 0.006452083587646484, 'sac_diff2': 0.007784605026245117, 'sac_diff3': 0.009602546691894531, 'sac_diff4': 0.0067059993743896484, 'sac_diff5': 0.048584699630737305, 'sac_diff6': 0.00039076805114746094, 'all': 0.07972383499145508}
diff5_list [0.009984016418457031, 0.009447813034057617, 0.009907722473144531, 0.009753704071044922, 0.009491443634033203]
time3 0.0008606910705566406
time4 0.0805213451385498
time5 0.08056855201721191
time7 0.008927345275878906
gen_weight_change tensor(-17.8072)
policy weight change tensor(36.5250, grad_fn=<SumBackward0>)
time8 0.001857757568359375
train_time 0.10895538330078125
eval time 0.11126399040222168
epoch last part time 4.291534423828125e-06
2024-01-23 01:04:43,155 MainThread INFO: EPOCH:895
2024-01-23 01:04:43,155 MainThread INFO: Time Consumed:0.22331929206848145s
2024-01-23 01:04:43,155 MainThread INFO: Total Frames:135150s
  9%|▉         | 896/10000 [06:10<37:35,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18261.44174
Train_Epoch_Reward                13622.27584
Running_Training_Average_Rewards  15364.59670
Explore_Time                      0.00094
Train___Time                      0.10896
Eval____Time                      0.11126
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17746.40793
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.44608     2.37835     96.97835     90.47367
alpha_0                           0.63890      0.00009     0.63903      0.63877
Alpha_loss                        -3.01526     0.00338     -3.01111     -3.01948
Training/policy_loss              -5.18613     0.14267     -4.97869     -5.39804
Training/qf1_loss                 7076.43203   1298.36656  9464.60742   5759.65723
Training/qf2_loss                 16175.96191  1770.55044  19457.45117  14452.57520
Training/pf_norm                  0.14454      0.02393     0.17550      0.10993
Training/qf1_norm                 508.07976    310.76524   1002.10669   139.75291
Training/qf2_norm                 1906.02661   78.97259    2019.05835   1822.01465
log_std/mean                      -0.13130     0.01027     -0.11740     -0.14808
log_probs/mean                    -2.73186     0.00574     -2.72489     -2.73978
mean/mean                         -0.00582     0.00190     -0.00294     -0.00858
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018385887145996094
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70241
epoch first part time 2.384185791015625e-06
replay_buffer._size: [135450]
collect time 0.0008990764617919922
inner_dict_sum {'sac_diff0': 0.00019860267639160156, 'sac_diff1': 0.006426334381103516, 'sac_diff2': 0.007766008377075195, 'sac_diff3': 0.010229349136352539, 'sac_diff4': 0.006657600402832031, 'sac_diff5': 0.031200885772705078, 'sac_diff6': 0.0003795623779296875, 'all': 0.06285834312438965}
diff5_list [0.006476402282714844, 0.006156444549560547, 0.005914211273193359, 0.006560087203979492, 0.006093740463256836]
time3 0
time4 0.06360459327697754
time5 0.06364822387695312
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8072)
policy weight change tensor(36.4513, grad_fn=<SumBackward0>)
time8 0.001764535903930664
train_time 0.07433056831359863
eval time 0.1458737850189209
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:43,400 MainThread INFO: EPOCH:896
2024-01-23 01:04:43,400 MainThread INFO: Time Consumed:0.2233743667602539s
2024-01-23 01:04:43,400 MainThread INFO: Total Frames:135300s
  9%|▉         | 897/10000 [06:11<37:27,  4.05it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18206.02789
Train_Epoch_Reward                13588.43557
Running_Training_Average_Rewards  15176.29488
Explore_Time                      0.00089
Train___Time                      0.07433
Eval____Time                      0.14587
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18231.79681
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.94123     1.05077     94.45682     91.36472
alpha_0                           0.63858      0.00009     0.63871      0.63845
Alpha_loss                        -3.01762     0.00068     -3.01630     -3.01825
Training/policy_loss              -5.26876     0.00260     -5.26467     -5.27194
Training/qf1_loss                 8167.56641   1383.03669  10761.57422  6657.76562
Training/qf2_loss                 17383.99414  1529.92054  20322.45703  15860.34570
Training/pf_norm                  0.11638      0.01938     0.14081      0.09115
Training/qf1_norm                 402.94290    159.13931   588.71362    174.63281
Training/qf2_norm                 1922.90723   22.00351    1955.57764   1889.95374
log_std/mean                      -0.13537     0.00016     -0.13514     -0.13557
log_probs/mean                    -2.72963     0.00204     -2.72802     -2.73351
mean/mean                         -0.00169     0.00008     -0.00162     -0.00184
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018069744110107422
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70241
epoch first part time 2.384185791015625e-06
replay_buffer._size: [135600]
collect time 0.0009839534759521484
inner_dict_sum {'sac_diff0': 0.00020837783813476562, 'sac_diff1': 0.007580995559692383, 'sac_diff2': 0.008831501007080078, 'sac_diff3': 0.011512517929077148, 'sac_diff4': 0.007959842681884766, 'sac_diff5': 0.03508925437927246, 'sac_diff6': 0.0004119873046875, 'all': 0.0715944766998291}
diff5_list [0.006877899169921875, 0.007417201995849609, 0.007489204406738281, 0.006942272186279297, 0.0063626766204833984]
time3 0
time4 0.07239627838134766
time5 0.07244443893432617
time7 9.5367431640625e-07
gen_weight_change tensor(-17.8072)
policy weight change tensor(36.4489, grad_fn=<SumBackward0>)
time8 0.0019159317016601562
train_time 0.08365058898925781
eval time 0.14473366737365723
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:43,653 MainThread INFO: EPOCH:897
2024-01-23 01:04:43,653 MainThread INFO: Time Consumed:0.23164153099060059s
2024-01-23 01:04:43,654 MainThread INFO: Total Frames:135450s
  9%|▉         | 898/10000 [06:11<37:44,  4.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           18142.89692
Train_Epoch_Reward                15757.52733
Running_Training_Average_Rewards  15597.98477
Explore_Time                      0.00098
Train___Time                      0.08365
Eval____Time                      0.14473
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18329.00494
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.19324     0.96667    93.88954     91.40144
alpha_0                           0.63826      0.00009    0.63839      0.63813
Alpha_loss                        -3.02134     0.00274    -3.01763     -3.02501
Training/policy_loss              -4.89565     0.00458    -4.88966     -4.90184
Training/qf1_loss                 6575.52490   796.88770  8116.07959   5951.20557
Training/qf2_loss                 15634.73047  990.41068  17539.66797  14896.61426
Training/pf_norm                  0.12464      0.01587    0.14948      0.10730
Training/qf1_norm                 358.61039    128.40836  536.80157    190.58766
Training/qf2_norm                 1757.69485   19.14097   1789.71936   1741.32678
log_std/mean                      -0.12590     0.00002    -0.12588     -0.12594
log_probs/mean                    -2.73042     0.00635    -2.72420     -2.74009
mean/mean                         -0.00658     0.00018    -0.00635     -0.00686
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018202543258666992
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70241
epoch first part time 2.86102294921875e-06
replay_buffer._size: [135750]
collect time 0.0008254051208496094
inner_dict_sum {'sac_diff0': 0.0002205371856689453, 'sac_diff1': 0.0066070556640625, 'sac_diff2': 0.0078105926513671875, 'sac_diff3': 0.009910821914672852, 'sac_diff4': 0.0066356658935546875, 'sac_diff5': 0.030533313751220703, 'sac_diff6': 0.00038361549377441406, 'all': 0.06210160255432129}
diff5_list [0.006234884262084961, 0.005979299545288086, 0.0060100555419921875, 0.006331682205200195, 0.0059773921966552734]
time3 0
time4 0.06282258033752441
time5 0.0628657341003418
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8072)
policy weight change tensor(36.4458, grad_fn=<SumBackward0>)
time8 0.0018146038055419922
train_time 0.07364106178283691
eval time 0.14567255973815918
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:43,897 MainThread INFO: EPOCH:898
2024-01-23 01:04:43,898 MainThread INFO: Time Consumed:0.22237133979797363s
2024-01-23 01:04:43,898 MainThread INFO: Total Frames:135600s
  9%|▉         | 899/10000 [06:11<37:33,  4.04it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18078.02612
Train_Epoch_Reward                13904.30600
Running_Training_Average_Rewards  14941.30881
Explore_Time                      0.00082
Train___Time                      0.07364
Eval____Time                      0.14567
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18096.09446
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.52795     1.91455     97.34066     92.77937
alpha_0                           0.63794      0.00009     0.63807      0.63781
Alpha_loss                        -3.02766     0.00139     -3.02496     -3.02876
Training/policy_loss              -5.09996     0.00223     -5.09782     -5.10375
Training/qf1_loss                 7734.88564   1084.69342  9384.16895   6213.16895
Training/qf2_loss                 17262.28691  1333.15701  19444.03516  15458.25293
Training/pf_norm                  0.11686      0.02398     0.14820      0.09107
Training/qf1_norm                 912.44835    352.99416   1458.20056   595.77777
Training/qf2_norm                 1887.78369   38.56172    1943.55310   1849.88159
log_std/mean                      -0.14177     0.00009     -0.14161     -0.14186
log_probs/mean                    -2.73698     0.00483     -2.72798     -2.74078
mean/mean                         -0.00779     0.00009     -0.00764     -0.00787
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018736600875854492
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70241
epoch first part time 2.384185791015625e-06
replay_buffer._size: [135900]
collect time 0.0008995532989501953
inner_dict_sum {'sac_diff0': 0.00021028518676757812, 'sac_diff1': 0.0063970088958740234, 'sac_diff2': 0.007546663284301758, 'sac_diff3': 0.009720325469970703, 'sac_diff4': 0.006333351135253906, 'sac_diff5': 0.031010866165161133, 'sac_diff6': 0.0003840923309326172, 'all': 0.06160259246826172}
diff5_list [0.006311178207397461, 0.006094932556152344, 0.006079673767089844, 0.006340742111206055, 0.00618433952331543]
time3 0
time4 0.0623469352722168
time5 0.06238961219787598
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8072)
policy weight change tensor(36.4734, grad_fn=<SumBackward0>)
time8 0.0018208026885986328
train_time 0.07327914237976074
eval time 0.16221094131469727
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:44,158 MainThread INFO: EPOCH:899
2024-01-23 01:04:44,158 MainThread INFO: Time Consumed:0.23862838745117188s
2024-01-23 01:04:44,159 MainThread INFO: Total Frames:135750s
  9%|▉         | 900/10000 [06:11<38:09,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18008.13594
Train_Epoch_Reward                9923.66449
Running_Training_Average_Rewards  15090.55620
Explore_Time                      0.00090
Train___Time                      0.07328
Eval____Time                      0.16221
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17928.46514
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.42502     1.86284     94.00455     88.25310
alpha_0                           0.63762      0.00009     0.63775      0.63750
Alpha_loss                        -3.02759     0.00131     -3.02559     -3.02928
Training/policy_loss              -5.17236     0.00376     -5.16981     -5.17955
Training/qf1_loss                 6570.37705   1240.34724  8138.67432   4904.88428
Training/qf2_loss                 15415.86680  1454.65067  17136.77930  13097.73926
Training/pf_norm                  0.13211      0.03039     0.15943      0.07898
Training/qf1_norm                 674.42072    378.41366   1353.22168   223.11646
Training/qf2_norm                 1919.92288   39.66093    1972.59424   1851.67395
log_std/mean                      -0.12987     0.00003     -0.12983     -0.12993
log_probs/mean                    -2.72935     0.00448     -2.72470     -2.73474
mean/mean                         -0.00764     0.00015     -0.00744     -0.00788
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0188748836517334
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70241
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [136050]
collect time 0.0008151531219482422
inside mustsac before update, task 0, sumup 70241
inside mustsac after update, task 0, sumup 71031
inner_dict_sum {'sac_diff0': 0.00020313262939453125, 'sac_diff1': 0.006508588790893555, 'sac_diff2': 0.007817268371582031, 'sac_diff3': 0.00995779037475586, 'sac_diff4': 0.00706171989440918, 'sac_diff5': 0.04934835433959961, 'sac_diff6': 0.0004055500030517578, 'all': 0.08130240440368652}
diff5_list [0.010386466979980469, 0.009805679321289062, 0.009645938873291016, 0.009559154510498047, 0.009951114654541016]
time3 0.0008254051208496094
time4 0.08211565017700195
time5 0.08216714859008789
time7 0.008971691131591797
gen_weight_change tensor(-17.7010)
policy weight change tensor(36.3906, grad_fn=<SumBackward0>)
time8 0.002498626708984375
train_time 0.111602783203125
eval time 0.11849403381347656
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:44,414 MainThread INFO: EPOCH:900
2024-01-23 01:04:44,414 MainThread INFO: Time Consumed:0.2330780029296875s
2024-01-23 01:04:44,414 MainThread INFO: Total Frames:135900s
  9%|▉         | 901/10000 [06:12<38:25,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17935.90365
Train_Epoch_Reward                23609.02399
Running_Training_Average_Rewards  15320.80474
Explore_Time                      0.00081
Train___Time                      0.11160
Eval____Time                      0.11849
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             17791.82229
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.28800     1.78860     95.12627     90.05173
alpha_0                           0.63730      0.00009     0.63743      0.63718
Alpha_loss                        -3.03000     0.00136     -3.02829     -3.03195
Training/policy_loss              -5.32098     0.18926     -5.04554     -5.50962
Training/qf1_loss                 6880.13027   1079.48016  8968.00195   5933.11426
Training/qf2_loss                 15935.70117  1389.15518  18611.60547  14547.40430
Training/pf_norm                  0.15374      0.01694     0.17744      0.12682
Training/qf1_norm                 988.99409    487.03386   1598.50879   197.01399
Training/qf2_norm                 1960.82932   70.02278    2051.53125   1858.47058
log_std/mean                      -0.12942     0.00355     -0.12303     -0.13361
log_probs/mean                    -2.72722     0.00508     -2.72045     -2.73454
mean/mean                         -0.00622     0.00177     -0.00319     -0.00854
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018369197845458984
epoch last part time3 0.0026726722717285156
inside rlalgo, task 0, sumup 71031
epoch first part time 2.86102294921875e-06
replay_buffer._size: [136200]
collect time 0.0008382797241210938
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.006962299346923828, 'sac_diff2': 0.007966279983520508, 'sac_diff3': 0.010610580444335938, 'sac_diff4': 0.006920337677001953, 'sac_diff5': 0.03295469284057617, 'sac_diff6': 0.0004303455352783203, 'all': 0.0660552978515625}
diff5_list [0.006459951400756836, 0.006905317306518555, 0.0074596405029296875, 0.006203889846801758, 0.005925893783569336]
time3 0
time4 0.06681132316589355
time5 0.06686091423034668
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7010)
policy weight change tensor(36.5565, grad_fn=<SumBackward0>)
time8 0.0019311904907226562
train_time 0.07791709899902344
eval time 0.15038824081420898
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:44,669 MainThread INFO: EPOCH:901
2024-01-23 01:04:44,670 MainThread INFO: Time Consumed:0.23143386840820312s
2024-01-23 01:04:44,670 MainThread INFO: Total Frames:136050s
  9%|▉         | 902/10000 [06:12<38:24,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           18260.63388
Train_Epoch_Reward                11892.21017
Running_Training_Average_Rewards  14911.60099
Explore_Time                      0.00083
Train___Time                      0.07792
Eval____Time                      0.15039
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21190.81436
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.06810     1.86915    95.20676     89.69850
alpha_0                           0.63699      0.00009    0.63711      0.63686
Alpha_loss                        -3.03504     0.00227    -3.03172     -3.03854
Training/policy_loss              -5.59210     0.00122    -5.59049     -5.59370
Training/qf1_loss                 7478.39307   609.59368  8423.52051   6795.12891
Training/qf2_loss                 16492.23516  845.30995  18066.49609  15748.17969
Training/pf_norm                  0.10194      0.01582    0.12301      0.08171
Training/qf1_norm                 412.95705    205.46765  757.83832    153.69994
Training/qf2_norm                 2116.66714   42.41205   2187.69922   2061.87622
log_std/mean                      -0.12588     0.00016    -0.12569     -0.12613
log_probs/mean                    -2.73095     0.00385    -2.72508     -2.73573
mean/mean                         -0.00499     0.00020    -0.00472     -0.00530
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01832103729248047
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71031
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [136350]
collect time 0.0008528232574462891
inner_dict_sum {'sac_diff0': 0.00020956993103027344, 'sac_diff1': 0.006515979766845703, 'sac_diff2': 0.007867574691772461, 'sac_diff3': 0.010097503662109375, 'sac_diff4': 0.006651163101196289, 'sac_diff5': 0.030602455139160156, 'sac_diff6': 0.0003745555877685547, 'all': 0.06231880187988281}
diff5_list [0.006195783615112305, 0.006167888641357422, 0.0061190128326416016, 0.0061779022216796875, 0.005941867828369141]
time3 0
time4 0.06305694580078125
time5 0.06310033798217773
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7010)
policy weight change tensor(36.7513, grad_fn=<SumBackward0>)
time8 0.0018057823181152344
train_time 0.07384729385375977
eval time 0.15601634979248047
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:44,924 MainThread INFO: EPOCH:902
2024-01-23 01:04:44,924 MainThread INFO: Time Consumed:0.23294591903686523s
2024-01-23 01:04:44,925 MainThread INFO: Total Frames:136200s
  9%|▉         | 903/10000 [06:12<38:29,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18592.73584
Train_Epoch_Reward                19406.01528
Running_Training_Average_Rewards  15175.34966
Explore_Time                      0.00085
Train___Time                      0.07385
Eval____Time                      0.15602
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21199.46185
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.39541     2.71112     96.50582     88.24269
alpha_0                           0.63667      0.00009     0.63679      0.63654
Alpha_loss                        -3.03980     0.00414     -3.03675     -3.04796
Training/policy_loss              -5.51661     0.00226     -5.51299     -5.52001
Training/qf1_loss                 7176.02158   1033.47671  8527.33105   5396.59863
Training/qf2_loss                 16274.47363  1613.95291  18523.70898  13595.47363
Training/pf_norm                  0.15681      0.03560     0.20963      0.11518
Training/qf1_norm                 1360.02832   526.28003   2176.95239   552.98816
Training/qf2_norm                 1987.06089   56.36620    2071.10059   1899.77710
log_std/mean                      -0.13100     0.00008     -0.13089     -0.13112
log_probs/mean                    -2.73404     0.00836     -2.72877     -2.75062
mean/mean                         -0.00032     0.00015     -0.00012     -0.00053
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018683671951293945
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71031
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [136500]
collect time 0.0008122920989990234
inner_dict_sum {'sac_diff0': 0.0002288818359375, 'sac_diff1': 0.006365537643432617, 'sac_diff2': 0.007525444030761719, 'sac_diff3': 0.009482145309448242, 'sac_diff4': 0.006494045257568359, 'sac_diff5': 0.030296802520751953, 'sac_diff6': 0.0003783702850341797, 'all': 0.06077122688293457}
diff5_list [0.006258726119995117, 0.005957603454589844, 0.005940675735473633, 0.006276845932006836, 0.0058629512786865234]
time3 0
time4 0.06148695945739746
time5 0.06152796745300293
time7 4.76837158203125e-07
gen_weight_change tensor(-17.7010)
policy weight change tensor(36.9201, grad_fn=<SumBackward0>)
time8 0.0017981529235839844
train_time 0.07220578193664551
eval time 0.15667295455932617
epoch last part time 4.5299530029296875e-06
2024-01-23 01:04:45,178 MainThread INFO: EPOCH:903
2024-01-23 01:04:45,178 MainThread INFO: Time Consumed:0.23193120956420898s
2024-01-23 01:04:45,179 MainThread INFO: Total Frames:136350s
  9%|▉         | 904/10000 [06:12<38:27,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           18939.46853
Train_Epoch_Reward                11982.30992
Running_Training_Average_Rewards  15334.58646
Explore_Time                      0.00081
Train___Time                      0.07221
Eval____Time                      0.15667
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21222.16603
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       96.76413     1.29013    98.31278     94.82086
alpha_0                           0.63635      0.00009    0.63648      0.63622
Alpha_loss                        -3.04313     0.00233    -3.03987     -3.04599
Training/policy_loss              -5.21572     0.00553    -5.20791     -5.22143
Training/qf1_loss                 7956.86016   606.02473  8673.26367   7125.13037
Training/qf2_loss                 17924.60703  800.97855  18848.39648  16686.56445
Training/pf_norm                  0.15789      0.02914    0.19118      0.11025
Training/qf1_norm                 745.54777    249.39241  1020.25085   354.57516
Training/qf2_norm                 2012.18479   27.01858   2043.95227   1970.48071
log_std/mean                      -0.12836     0.00009    -0.12824     -0.12849
log_probs/mean                    -2.73397     0.00558    -2.72526     -2.74178
mean/mean                         -0.01129     0.00017    -0.01104     -0.01150
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.017934799194335938
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71031
epoch first part time 3.337860107421875e-06
replay_buffer._size: [136650]
collect time 0.0007593631744384766
inner_dict_sum {'sac_diff0': 0.00022220611572265625, 'sac_diff1': 0.006432771682739258, 'sac_diff2': 0.007531166076660156, 'sac_diff3': 0.009508371353149414, 'sac_diff4': 0.006379127502441406, 'sac_diff5': 0.030721664428710938, 'sac_diff6': 0.0003845691680908203, 'all': 0.06117987632751465}
diff5_list [0.006235837936401367, 0.00672149658203125, 0.005992889404296875, 0.005936861038208008, 0.0058345794677734375]
time3 0
time4 0.06190800666809082
time5 0.061957359313964844
time7 4.76837158203125e-07
gen_weight_change tensor(-17.7010)
policy weight change tensor(37.0002, grad_fn=<SumBackward0>)
time8 0.0019528865814208984
train_time 0.07294917106628418
eval time 0.15040946006774902
epoch last part time 3.814697265625e-06
2024-01-23 01:04:45,426 MainThread INFO: EPOCH:904
2024-01-23 01:04:45,426 MainThread INFO: Time Consumed:0.22635579109191895s
2024-01-23 01:04:45,426 MainThread INFO: Total Frames:136500s
  9%|▉         | 905/10000 [06:13<38:15,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           19281.14886
Train_Epoch_Reward                20392.62470
Running_Training_Average_Rewards  15854.71256
Explore_Time                      0.00076
Train___Time                      0.07295
Eval____Time                      0.15041
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21075.45474
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.98825     1.59191    93.53545     89.58040
alpha_0                           0.63603      0.00009    0.63616      0.63590
Alpha_loss                        -3.04380     0.00117    -3.04231     -3.04531
Training/policy_loss              -5.17985     0.00364    -5.17450     -5.18592
Training/qf1_loss                 6480.53584   220.46377  6824.94824   6209.50488
Training/qf2_loss                 15512.50820  465.50730  16113.78125  14858.20312
Training/pf_norm                  0.14271      0.02531    0.18192      0.11476
Training/qf1_norm                 702.66990    328.89010  1169.56165   391.41739
Training/qf2_norm                 1878.49907   31.98131   1910.31128   1830.57007
log_std/mean                      -0.13662     0.00005    -0.13654     -0.13667
log_probs/mean                    -2.72801     0.00250    -2.72323     -2.73058
mean/mean                         -0.00892     0.00004    -0.00885     -0.00896
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019584178924560547
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71031
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [136800]
collect time 0.0009033679962158203
inside mustsac before update, task 0, sumup 71031
inside mustsac after update, task 0, sumup 71584
inner_dict_sum {'sac_diff0': 0.00020933151245117188, 'sac_diff1': 0.006968975067138672, 'sac_diff2': 0.00824284553527832, 'sac_diff3': 0.011032819747924805, 'sac_diff4': 0.00775909423828125, 'sac_diff5': 0.05399155616760254, 'sac_diff6': 0.0004220008850097656, 'all': 0.08862662315368652}
diff5_list [0.010340690612792969, 0.010764122009277344, 0.012207269668579102, 0.010569572448730469, 0.010109901428222656]
time3 0.0008749961853027344
time4 0.08947992324829102
time5 0.08953022956848145
time7 0.008812189102172852
gen_weight_change tensor(-17.6345)
policy weight change tensor(36.9337, grad_fn=<SumBackward0>)
time8 0.0018548965454101562
train_time 0.11903190612792969
eval time 0.11025357246398926
epoch last part time 5.7220458984375e-06
2024-01-23 01:04:45,681 MainThread INFO: EPOCH:905
2024-01-23 01:04:45,682 MainThread INFO: Time Consumed:0.23253703117370605s
2024-01-23 01:04:45,682 MainThread INFO: Total Frames:136650s
  9%|▉         | 906/10000 [06:13<38:22,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           19614.48955
Train_Epoch_Reward                14936.90761
Running_Training_Average_Rewards  15924.66070
Explore_Time                      0.00090
Train___Time                      0.11903
Eval____Time                      0.11025
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21079.81492
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.50162     2.12793     95.95543     89.80309
alpha_0                           0.63571      0.00009     0.63584      0.63559
Alpha_loss                        -3.04788     0.00498     -3.04270     -3.05655
Training/policy_loss              -5.30851     0.13862     -5.11450     -5.45623
Training/qf1_loss                 7525.22100   850.56418   8767.10547   6141.71777
Training/qf2_loss                 16618.36953  1272.92580  18586.66016  14679.85352
Training/pf_norm                  0.17581      0.03331     0.23327      0.12960
Training/qf1_norm                 1385.71733   799.26970   2691.78809   446.78455
Training/qf2_norm                 1971.75134   114.78061   2079.66260   1800.53125
log_std/mean                      -0.12870     0.00637     -0.11771     -0.13421
log_probs/mean                    -2.72958     0.00970     -2.72034     -2.74725
mean/mean                         -0.00871     0.00268     -0.00589     -0.01261
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019138097763061523
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71584
epoch first part time 3.337860107421875e-06
replay_buffer._size: [136950]
collect time 0.0010466575622558594
inner_dict_sum {'sac_diff0': 0.0002582073211669922, 'sac_diff1': 0.008246660232543945, 'sac_diff2': 0.009968042373657227, 'sac_diff3': 0.012671947479248047, 'sac_diff4': 0.008799552917480469, 'sac_diff5': 0.03949618339538574, 'sac_diff6': 0.0004887580871582031, 'all': 0.07992935180664062}
diff5_list [0.008179426193237305, 0.00777435302734375, 0.008136987686157227, 0.007535696029663086, 0.007869720458984375]
time3 0
time4 0.08070230484008789
time5 0.08074951171875
time7 7.152557373046875e-07
gen_weight_change tensor(-17.6345)
policy weight change tensor(37.0387, grad_fn=<SumBackward0>)
time8 0.0020880699157714844
train_time 0.09286308288574219
eval time 0.14188885688781738
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:45,942 MainThread INFO: EPOCH:906
2024-01-23 01:04:45,943 MainThread INFO: Time Consumed:0.2382204532623291s
2024-01-23 01:04:45,943 MainThread INFO: Total Frames:136800s
  9%|▉         | 907/10000 [06:13<38:42,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           19745.82412
Train_Epoch_Reward                4118.78036
Running_Training_Average_Rewards  15531.20236
Explore_Time                      0.00104
Train___Time                      0.09286
Eval____Time                      0.14189
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             19545.14247
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.70167     3.62301     96.15465     87.34830
alpha_0                           0.63540      0.00009     0.63552      0.63527
Alpha_loss                        -3.04878     0.00275     -3.04371     -3.05114
Training/policy_loss              -5.43676     0.00591     -5.42786     -5.44618
Training/qf1_loss                 7262.31299   1402.91470  8865.67285   5534.64990
Training/qf2_loss                 16224.81758  2089.08355  18661.21094  13648.62500
Training/pf_norm                  0.18643      0.02680     0.21346      0.13972
Training/qf1_norm                 673.31178    344.91464   1082.88062   230.76668
Training/qf2_norm                 1966.43452   76.10714    2061.80884   1875.69336
log_std/mean                      -0.12948     0.00007     -0.12941     -0.12959
log_probs/mean                    -2.72416     0.00763     -2.71149     -2.73192
mean/mean                         -0.01007     0.00020     -0.00981     -0.01037
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01881122589111328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71584
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [137100]
collect time 0.0009677410125732422
inner_dict_sum {'sac_diff0': 0.00026345252990722656, 'sac_diff1': 0.008638858795166016, 'sac_diff2': 0.010314702987670898, 'sac_diff3': 0.013403177261352539, 'sac_diff4': 0.0092315673828125, 'sac_diff5': 0.041391611099243164, 'sac_diff6': 0.0005021095275878906, 'all': 0.08374547958374023}
diff5_list [0.008853912353515625, 0.008620977401733398, 0.008558034896850586, 0.007802248001098633, 0.007556438446044922]
time3 0
time4 0.08453750610351562
time5 0.08458852767944336
time7 7.152557373046875e-07
gen_weight_change tensor(-17.6345)
policy weight change tensor(37.0515, grad_fn=<SumBackward0>)
time8 0.002013683319091797
train_time 0.09676885604858398
eval time 0.14305758476257324
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:46,208 MainThread INFO: EPOCH:907
2024-01-23 01:04:46,209 MainThread INFO: Time Consumed:0.24308991432189941s
2024-01-23 01:04:46,209 MainThread INFO: Total Frames:136950s
  9%|▉         | 908/10000 [06:13<39:10,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           19855.02010
Train_Epoch_Reward                15276.80737
Running_Training_Average_Rewards  15901.92738
Explore_Time                      0.00096
Train___Time                      0.09677
Eval____Time                      0.14306
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             19420.96477
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.48204     2.07861     93.45077     87.52775
alpha_0                           0.63508      0.00009     0.63521      0.63495
Alpha_loss                        -3.05412     0.00272     -3.05090     -3.05736
Training/policy_loss              -5.05982     0.00223     -5.05743     -5.06340
Training/qf1_loss                 6974.23330   904.51587   8247.45020   5488.93066
Training/qf2_loss                 15881.59609  1267.59615  17400.58984  13644.54102
Training/pf_norm                  0.12575      0.01318     0.14626      0.10616
Training/qf1_norm                 684.38054    273.98495   1001.02338   195.77158
Training/qf2_norm                 1833.04233   41.26787    1871.05481   1754.39355
log_std/mean                      -0.12772     0.00007     -0.12761     -0.12780
log_probs/mean                    -2.72852     0.00484     -2.72325     -2.73566
mean/mean                         -0.01413     0.00007     -0.01401     -0.01419
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018641948699951172
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71584
epoch first part time 3.337860107421875e-06
replay_buffer._size: [137250]
collect time 0.0009114742279052734
inner_dict_sum {'sac_diff0': 0.00025653839111328125, 'sac_diff1': 0.008078813552856445, 'sac_diff2': 0.00910043716430664, 'sac_diff3': 0.011967897415161133, 'sac_diff4': 0.008324861526489258, 'sac_diff5': 0.038161516189575195, 'sac_diff6': 0.0004830360412597656, 'all': 0.07637310028076172}
diff5_list [0.00793910026550293, 0.007484912872314453, 0.0077245235443115234, 0.007635593414306641, 0.0073773860931396484]
time3 0
time4 0.07713747024536133
time5 0.07718324661254883
time7 7.152557373046875e-07
gen_weight_change tensor(-17.6345)
policy weight change tensor(36.9661, grad_fn=<SumBackward0>)
time8 0.0020172595977783203
train_time 0.08911943435668945
eval time 0.14002013206481934
epoch last part time 5.7220458984375e-06
2024-01-23 01:04:46,463 MainThread INFO: EPOCH:908
2024-01-23 01:04:46,463 MainThread INFO: Time Consumed:0.2323927879333496s
2024-01-23 01:04:46,464 MainThread INFO: Total Frames:137100s
  9%|▉         | 909/10000 [06:14<40:06,  3.78it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           19970.80617
Train_Epoch_Reward                17023.48973
Running_Training_Average_Rewards  16050.66789
Explore_Time                      0.00091
Train___Time                      0.08912
Eval____Time                      0.14002
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             19253.95515
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.36681     1.52901    94.05661     90.40999
alpha_0                           0.63476      0.00009    0.63489      0.63463
Alpha_loss                        -3.05686     0.00336    -3.05136     -3.06132
Training/policy_loss              -5.49176     0.00312    -5.48720     -5.49601
Training/qf1_loss                 7358.77793   799.34452  8382.72656   6451.47021
Training/qf2_loss                 16412.67793  898.64417  17757.68359  15140.13770
Training/pf_norm                  0.12069      0.00879    0.13303      0.10923
Training/qf1_norm                 1250.54154   288.85307  1570.58887   843.56555
Training/qf2_norm                 2061.41902   33.91161   2100.42871   2017.60376
log_std/mean                      -0.13228     0.00007    -0.13216     -0.13236
log_probs/mean                    -2.72714     0.00674    -2.71652     -2.73474
mean/mean                         -0.01371     0.00007    -0.01359     -0.01377
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.04315805435180664
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71584
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [137400]
collect time 0.0008709430694580078
inner_dict_sum {'sac_diff0': 0.0002677440643310547, 'sac_diff1': 0.008362054824829102, 'sac_diff2': 0.0096435546875, 'sac_diff3': 0.012334585189819336, 'sac_diff4': 0.008433103561401367, 'sac_diff5': 0.04000735282897949, 'sac_diff6': 0.0004928112030029297, 'all': 0.07954120635986328}
diff5_list [0.009550094604492188, 0.008133172988891602, 0.007495880126953125, 0.007335186004638672, 0.007493019104003906]
time3 0
time4 0.08030319213867188
time5 0.08034968376159668
time7 7.152557373046875e-07
gen_weight_change tensor(-17.6345)
policy weight change tensor(36.9503, grad_fn=<SumBackward0>)
time8 0.0019483566284179688
train_time 0.09264278411865234
eval time 0.11150741577148438
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:46,717 MainThread INFO: EPOCH:909
2024-01-23 01:04:46,717 MainThread INFO: Time Consumed:0.20726275444030762s
2024-01-23 01:04:46,718 MainThread INFO: Total Frames:137250s
  9%|▉         | 910/10000 [06:14<38:30,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           20089.65461
Train_Epoch_Reward                18629.84462
Running_Training_Average_Rewards  16270.53103
Explore_Time                      0.00087
Train___Time                      0.09264
Eval____Time                      0.11151
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             19116.94949
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.72799     2.35036     98.79891     92.23705
alpha_0                           0.63444      0.00009     0.63457      0.63432
Alpha_loss                        -3.06059     0.00317     -3.05779     -3.06665
Training/policy_loss              -5.54472     0.00665     -5.53650     -5.55390
Training/qf1_loss                 7169.87236   1369.60029  9031.64062   5586.40723
Training/qf2_loss                 16645.95098  1767.49180  19276.26758  14581.78418
Training/pf_norm                  0.16808      0.01950     0.20242      0.14517
Training/qf1_norm                 1837.32676   459.18220   2627.70435   1391.32947
Training/qf2_norm                 2048.41614   50.46391    2135.36060   1991.28955
log_std/mean                      -0.12054     0.00006     -0.12046     -0.12063
log_probs/mean                    -2.72796     0.00791     -2.71886     -2.74276
mean/mean                         -0.00744     0.00013     -0.00732     -0.00766
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01871514320373535
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71584
epoch first part time 2.86102294921875e-06
replay_buffer._size: [137550]
collect time 0.0008432865142822266
inside mustsac before update, task 0, sumup 71584
inside mustsac after update, task 0, sumup 70445
inner_dict_sum {'sac_diff0': 0.00025391578674316406, 'sac_diff1': 0.007825374603271484, 'sac_diff2': 0.00914621353149414, 'sac_diff3': 0.011989593505859375, 'sac_diff4': 0.00842428207397461, 'sac_diff5': 0.05988454818725586, 'sac_diff6': 0.0004894733428955078, 'all': 0.09801340103149414}
diff5_list [0.012533187866210938, 0.011631965637207031, 0.012017011642456055, 0.01183176040649414, 0.011870622634887695]
time3 0.0011379718780517578
time4 0.09882164001464844
time5 0.09887075424194336
time7 0.009139299392700195
gen_weight_change tensor(-17.5826)
policy weight change tensor(36.9261, grad_fn=<SumBackward0>)
time8 0.0026934146881103516
train_time 0.13108468055725098
eval time 0.09766793251037598
epoch last part time 4.76837158203125e-06
2024-01-23 01:04:46,971 MainThread INFO: EPOCH:910
2024-01-23 01:04:46,972 MainThread INFO: Time Consumed:0.23188567161560059s
2024-01-23 01:04:46,972 MainThread INFO: Total Frames:137400s
  9%|▉         | 911/10000 [06:14<38:38,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           20201.34125
Train_Epoch_Reward                10975.57687
Running_Training_Average_Rewards  16018.74942
Explore_Time                      0.00084
Train___Time                      0.13108
Eval____Time                      0.09767
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18908.68871
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.18776     2.10603     95.89195     89.75740
alpha_0                           0.63413      0.00009     0.63425      0.63400
Alpha_loss                        -3.06198     0.00377     -3.05720     -3.06664
Training/policy_loss              -5.37801     0.16976     -5.08020     -5.60147
Training/qf1_loss                 7113.81006   1106.67713  8823.11914   5719.12695
Training/qf2_loss                 16356.84668  1468.57138  18579.32812  14309.78711
Training/pf_norm                  0.12273      0.03111     0.15192      0.06638
Training/qf1_norm                 598.65313    440.86595   1181.17700   154.75275
Training/qf2_norm                 1995.69966   113.21855   2187.62695   1841.69397
log_std/mean                      -0.12916     0.00475     -0.12335     -0.13441
log_probs/mean                    -2.72363     0.00760     -2.71313     -2.73368
mean/mean                         -0.00940     0.00167     -0.00662     -0.01181
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01873493194580078
epoch last part time3 0.002706289291381836
inside rlalgo, task 0, sumup 70445
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [137700]
collect time 0.0008866786956787109
inner_dict_sum {'sac_diff0': 0.0002727508544921875, 'sac_diff1': 0.008346080780029297, 'sac_diff2': 0.009971857070922852, 'sac_diff3': 0.012542009353637695, 'sac_diff4': 0.009001970291137695, 'sac_diff5': 0.04000258445739746, 'sac_diff6': 0.0005154609680175781, 'all': 0.08065271377563477}
diff5_list [0.008129358291625977, 0.007729768753051758, 0.008831977844238281, 0.007790327072143555, 0.007521152496337891]
time3 0
time4 0.08150792121887207
time5 0.08156085014343262
time7 9.5367431640625e-07
gen_weight_change tensor(-17.5826)
policy weight change tensor(37.1045, grad_fn=<SumBackward0>)
time8 0.0020487308502197266
train_time 0.09363889694213867
eval time 0.13495874404907227
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:47,228 MainThread INFO: EPOCH:911
2024-01-23 01:04:47,228 MainThread INFO: Time Consumed:0.23198771476745605s
2024-01-23 01:04:47,229 MainThread INFO: Total Frames:137550s
  9%|▉         | 912/10000 [06:14<38:37,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           19900.21967
Train_Epoch_Reward                22393.94580
Running_Training_Average_Rewards  16452.46723
Explore_Time                      0.00088
Train___Time                      0.09364
Eval____Time                      0.13496
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18179.59855
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.25669     2.32923     95.43170     89.88469
alpha_0                           0.63381      0.00009     0.63394      0.63368
Alpha_loss                        -3.06513     0.00390     -3.05939     -3.06977
Training/policy_loss              -5.56003     0.00377     -5.55648     -5.56679
Training/qf1_loss                 7022.29785   718.84748   7837.51416   5681.72900
Training/qf2_loss                 16037.45273  1035.27350  17263.99609  14245.79199
Training/pf_norm                  0.19026      0.03073     0.22775      0.13383
Training/qf1_norm                 1393.49387   439.84556   1973.30750   845.76160
Training/qf2_norm                 2049.65894   50.55252    2116.00830   1998.51697
log_std/mean                      -0.12977     0.00019     -0.12952     -0.13006
log_probs/mean                    -2.72318     0.00746     -2.71204     -2.73223
mean/mean                         -0.00667     0.00003     -0.00663     -0.00672
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01952648162841797
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70445
epoch first part time 3.337860107421875e-06
replay_buffer._size: [137850]
collect time 0.0009512901306152344
inner_dict_sum {'sac_diff0': 0.0002548694610595703, 'sac_diff1': 0.008098363876342773, 'sac_diff2': 0.009743690490722656, 'sac_diff3': 0.012556314468383789, 'sac_diff4': 0.008373022079467773, 'sac_diff5': 0.0387415885925293, 'sac_diff6': 0.0004839897155761719, 'all': 0.07825183868408203}
diff5_list [0.008532047271728516, 0.007508993148803711, 0.007695674896240234, 0.007448911666870117, 0.007555961608886719]
time3 0
time4 0.07902240753173828
time5 0.07907414436340332
time7 9.5367431640625e-07
gen_weight_change tensor(-17.5826)
policy weight change tensor(37.3507, grad_fn=<SumBackward0>)
time8 0.001890420913696289
train_time 0.09082198143005371
eval time 0.13696837425231934
epoch last part time 7.62939453125e-06
2024-01-23 01:04:47,483 MainThread INFO: EPOCH:912
2024-01-23 01:04:47,483 MainThread INFO: Time Consumed:0.2311842441558838s
2024-01-23 01:04:47,483 MainThread INFO: Total Frames:137700s
  9%|▉         | 913/10000 [06:15<38:36,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           19588.30735
Train_Epoch_Reward                5748.98101
Running_Training_Average_Rewards  15578.37699
Explore_Time                      0.00095
Train___Time                      0.09082
Eval____Time                      0.13697
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18080.33868
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.17035     2.45948     99.11803     91.61162
alpha_0                           0.63349      0.00009     0.63362      0.63337
Alpha_loss                        -3.07364     0.00146     -3.07143     -3.07554
Training/policy_loss              -5.30163     0.00216     -5.29879     -5.30501
Training/qf1_loss                 8000.79980   989.26313   9341.45312   6464.29883
Training/qf2_loss                 17641.47148  1137.58910  19196.52148  15942.52148
Training/pf_norm                  0.11302      0.03115     0.16409      0.07063
Training/qf1_norm                 477.48507    249.00512   881.47791    184.70229
Training/qf2_norm                 2048.69297   50.65564    2131.55225   1977.15356
log_std/mean                      -0.11848     0.00021     -0.11822     -0.11880
log_probs/mean                    -2.73447     0.00433     -2.72815     -2.74156
mean/mean                         -0.01381     0.00005     -0.01376     -0.01390
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01963067054748535
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70445
epoch first part time 2.86102294921875e-06
replay_buffer._size: [138000]
collect time 0.0009815692901611328
inner_dict_sum {'sac_diff0': 0.0002014636993408203, 'sac_diff1': 0.0073511600494384766, 'sac_diff2': 0.008535623550415039, 'sac_diff3': 0.010751724243164062, 'sac_diff4': 0.0071468353271484375, 'sac_diff5': 0.033487558364868164, 'sac_diff6': 0.00039458274841308594, 'all': 0.06786894798278809}
diff5_list [0.007066965103149414, 0.007681846618652344, 0.0064737796783447266, 0.0061492919921875, 0.00611567497253418]
time3 0
time4 0.06864023208618164
time5 0.06869268417358398
time7 4.76837158203125e-07
gen_weight_change tensor(-17.5826)
policy weight change tensor(37.4114, grad_fn=<SumBackward0>)
time8 0.0018963813781738281
train_time 0.07994604110717773
eval time 0.14647984504699707
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:47,736 MainThread INFO: EPOCH:913
2024-01-23 01:04:47,736 MainThread INFO: Time Consumed:0.22985482215881348s
2024-01-23 01:04:47,736 MainThread INFO: Total Frames:137850s
  9%|▉         | 914/10000 [06:15<38:30,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           19277.24822
Train_Epoch_Reward                18540.55432
Running_Training_Average_Rewards  15716.29051
Explore_Time                      0.00098
Train___Time                      0.07995
Eval____Time                      0.14648
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18111.57470
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.44278     3.57757     97.21868     87.45589
alpha_0                           0.63318      0.00009     0.63330      0.63305
Alpha_loss                        -3.07706     0.00265     -3.07394     -3.08056
Training/policy_loss              -5.80083     0.00582     -5.79445     -5.80976
Training/qf1_loss                 6674.90352   1518.07370  9121.02344   4869.91113
Training/qf2_loss                 15593.29023  2190.07080  19180.75391  13320.96875
Training/pf_norm                  0.12697      0.02470     0.14825      0.08036
Training/qf1_norm                 617.46858    475.38886   1454.60181   155.01717
Training/qf2_norm                 2095.97705   81.58870    2227.44507   2005.12000
log_std/mean                      -0.14124     0.00008     -0.14109     -0.14130
log_probs/mean                    -2.73458     0.00765     -2.72628     -2.74519
mean/mean                         -0.00530     0.00001     -0.00528     -0.00532
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01942610740661621
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70445
epoch first part time 2.86102294921875e-06
replay_buffer._size: [138150]
collect time 0.0009009838104248047
inner_dict_sum {'sac_diff0': 0.00021886825561523438, 'sac_diff1': 0.006619691848754883, 'sac_diff2': 0.007860422134399414, 'sac_diff3': 0.010053873062133789, 'sac_diff4': 0.006952524185180664, 'sac_diff5': 0.031120777130126953, 'sac_diff6': 0.0003733634948730469, 'all': 0.06319952011108398}
diff5_list [0.006396055221557617, 0.00619196891784668, 0.00608062744140625, 0.006320953369140625, 0.006131172180175781]
time3 0
time4 0.06392502784729004
time5 0.06396770477294922
time7 4.76837158203125e-07
gen_weight_change tensor(-17.5826)
policy weight change tensor(37.4322, grad_fn=<SumBackward0>)
time8 0.0017998218536376953
train_time 0.07495403289794922
eval time 0.1475381851196289
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:47,985 MainThread INFO: EPOCH:914
2024-01-23 01:04:47,985 MainThread INFO: Time Consumed:0.22582387924194336s
2024-01-23 01:04:47,985 MainThread INFO: Total Frames:138000s
  9%|▉         | 915/10000 [06:15<38:15,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18987.79905
Train_Epoch_Reward                2948.92371
Running_Training_Average_Rewards  15072.41342
Explore_Time                      0.00090
Train___Time                      0.07495
Eval____Time                      0.14754
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18180.96304
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.28526     2.79330     96.52250     88.78770
alpha_0                           0.63286      0.00009     0.63299      0.63273
Alpha_loss                        -3.07652     0.00317     -3.07179     -3.08108
Training/policy_loss              -5.39291     0.00570     -5.38468     -5.40063
Training/qf1_loss                 6010.76133   1122.83372  7992.36865   5145.66602
Training/qf2_loss                 14856.74414  1561.84001  17842.79297  13661.10156
Training/pf_norm                  0.08165      0.02366     0.11003      0.04314
Training/qf1_norm                 1068.42523   495.15635   2012.19482   696.38861
Training/qf2_norm                 2001.16895   60.54051    2114.47070   1945.46240
log_std/mean                      -0.12847     0.00010     -0.12835     -0.12863
log_probs/mean                    -2.72603     0.00696     -2.71715     -2.73454
mean/mean                         -0.00696     0.00007     -0.00685     -0.00704
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019446611404418945
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70445
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [138300]
collect time 0.0010156631469726562
inside mustsac before update, task 0, sumup 70445
inside mustsac after update, task 0, sumup 70290
inner_dict_sum {'sac_diff0': 0.00023174285888671875, 'sac_diff1': 0.007348775863647461, 'sac_diff2': 0.008807897567749023, 'sac_diff3': 0.011192798614501953, 'sac_diff4': 0.0077817440032958984, 'sac_diff5': 0.05258059501647949, 'sac_diff6': 0.00042176246643066406, 'all': 0.08836531639099121}
diff5_list [0.010605096817016602, 0.010601282119750977, 0.011181116104125977, 0.010117530822753906, 0.010075569152832031]
time3 0.0009317398071289062
time4 0.0892789363861084
time5 0.08933544158935547
time7 0.009496212005615234
gen_weight_change tensor(-17.4998)
policy weight change tensor(37.4371, grad_fn=<SumBackward0>)
time8 0.0019032955169677734
train_time 0.11983156204223633
eval time 0.10418415069580078
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:48,235 MainThread INFO: EPOCH:915
2024-01-23 01:04:48,236 MainThread INFO: Time Consumed:0.2274322509765625s
2024-01-23 01:04:48,236 MainThread INFO: Total Frames:138150s
  9%|▉         | 916/10000 [06:15<38:08,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           18707.35000
Train_Epoch_Reward                26052.63605
Running_Training_Average_Rewards  15329.61011
Explore_Time                      0.00101
Train___Time                      0.11983
Eval____Time                      0.10418
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18275.32443
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       94.55117     1.44496    96.40244     92.30080
alpha_0                           0.63254      0.00009    0.63267      0.63242
Alpha_loss                        -3.07945     0.00278    -3.07627     -3.08336
Training/policy_loss              -5.53779     0.29620    -4.97465     -5.82145
Training/qf1_loss                 7476.75176   590.95482  8469.20801   6711.41504
Training/qf2_loss                 16986.27656  792.65144  18172.02344  16025.58594
Training/pf_norm                  0.16174      0.03590    0.21367      0.10271
Training/qf1_norm                 798.63809    305.32696  1310.06055   402.55228
Training/qf2_norm                 2086.18621   119.73421  2180.67383   1850.25183
log_std/mean                      -0.13035     0.00502    -0.12323     -0.13608
log_probs/mean                    -2.72510     0.00589    -2.71668     -2.73250
mean/mean                         -0.01027     0.00168    -0.00738     -0.01259
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01908397674560547
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70290
epoch first part time 2.86102294921875e-06
replay_buffer._size: [138450]
collect time 0.0008623600006103516
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.007149934768676758, 'sac_diff2': 0.008770942687988281, 'sac_diff3': 0.010808944702148438, 'sac_diff4': 0.007352352142333984, 'sac_diff5': 0.03314399719238281, 'sac_diff6': 0.0004024505615234375, 'all': 0.06784319877624512}
diff5_list [0.006856679916381836, 0.00630640983581543, 0.007387638092041016, 0.006500720977783203, 0.006092548370361328]
time3 0
time4 0.06867241859436035
time5 0.0687255859375
time7 9.5367431640625e-07
gen_weight_change tensor(-17.4998)
policy weight change tensor(37.4710, grad_fn=<SumBackward0>)
time8 0.0018951892852783203
train_time 0.08040380477905273
eval time 0.14613056182861328
epoch last part time 7.62939453125e-06
2024-01-23 01:04:48,488 MainThread INFO: EPOCH:916
2024-01-23 01:04:48,488 MainThread INFO: Time Consumed:0.22980713844299316s
2024-01-23 01:04:48,488 MainThread INFO: Total Frames:138300s
  9%|▉         | 917/10000 [06:16<38:10,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           18432.83988
Train_Epoch_Reward                10354.32215
Running_Training_Average_Rewards  15395.80133
Explore_Time                      0.00086
Train___Time                      0.08040
Eval____Time                      0.14613
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16800.04130
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.02244     2.44971    95.75701     88.65305
alpha_0                           0.63223      0.00009    0.63236      0.63210
Alpha_loss                        -3.08598     0.00275    -3.08323     -3.09073
Training/policy_loss              -5.00184     0.00297    -4.99690     -5.00603
Training/qf1_loss                 7405.00791   346.96120  7845.61084   7006.86475
Training/qf2_loss                 16495.59414  274.46970  16939.03711  16092.90039
Training/pf_norm                  0.12397      0.02444    0.15354      0.09844
Training/qf1_norm                 2083.85820   431.79454  2614.31030   1314.96777
Training/qf2_norm                 1821.36233   47.38757   1874.08386   1736.88525
log_std/mean                      -0.12799     0.00012    -0.12782     -0.12816
log_probs/mean                    -2.73200     0.00481    -2.72747     -2.74089
mean/mean                         -0.01228     0.00004    -0.01225     -0.01235
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019238710403442383
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70290
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [138600]
collect time 0.000985860824584961
inner_dict_sum {'sac_diff0': 0.00021576881408691406, 'sac_diff1': 0.007458686828613281, 'sac_diff2': 0.008817672729492188, 'sac_diff3': 0.011233091354370117, 'sac_diff4': 0.007517814636230469, 'sac_diff5': 0.03444528579711914, 'sac_diff6': 0.0004069805145263672, 'all': 0.07009530067443848}
diff5_list [0.006948709487915039, 0.006674766540527344, 0.007921934127807617, 0.006673097610473633, 0.006226778030395508]
time3 0
time4 0.0709543228149414
time5 0.07100319862365723
time7 7.152557373046875e-07
gen_weight_change tensor(-17.4998)
policy weight change tensor(37.5764, grad_fn=<SumBackward0>)
time8 0.0019304752349853516
train_time 0.08245301246643066
eval time 0.14826464653015137
epoch last part time 7.62939453125e-06
2024-01-23 01:04:48,745 MainThread INFO: EPOCH:917
2024-01-23 01:04:48,745 MainThread INFO: Time Consumed:0.23412132263183594s
2024-01-23 01:04:48,746 MainThread INFO: Total Frames:138450s
  9%|▉         | 918/10000 [06:16<38:22,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18155.90679
Train_Epoch_Reward                27411.53875
Running_Training_Average_Rewards  15685.94828
Explore_Time                      0.00098
Train___Time                      0.08245
Eval____Time                      0.14826
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16651.63389
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.48757     3.60376     95.03743     84.89531
alpha_0                           0.63191      0.00009     0.63204      0.63179
Alpha_loss                        -3.08934     0.00286     -3.08430     -3.09186
Training/policy_loss              -5.41336     0.00588     -5.40536     -5.42056
Training/qf1_loss                 6928.57793   876.49575   7884.31104   5376.41797
Training/qf2_loss                 15806.84785  1634.25824  17520.46289  12857.25977
Training/pf_norm                  0.11870      0.01776     0.15040      0.10096
Training/qf1_norm                 1648.99398   765.09720   3043.86670   875.22296
Training/qf2_norm                 2038.12263   79.38010    2115.72021   1894.12720
log_std/mean                      -0.12651     0.00016     -0.12631     -0.12675
log_probs/mean                    -2.73199     0.00772     -2.71809     -2.73839
mean/mean                         -0.00978     0.00020     -0.00956     -0.01009
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018682003021240234
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70290
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [138750]
collect time 0.0009715557098388672
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.0071887969970703125, 'sac_diff2': 0.008682727813720703, 'sac_diff3': 0.010733604431152344, 'sac_diff4': 0.0074138641357421875, 'sac_diff5': 0.033866167068481445, 'sac_diff6': 0.00041985511779785156, 'all': 0.06851935386657715}
diff5_list [0.006628274917602539, 0.006742000579833984, 0.006577730178833008, 0.0076045989990234375, 0.0063135623931884766]
time3 0
time4 0.06936168670654297
time5 0.06941509246826172
time7 7.152557373046875e-07
gen_weight_change tensor(-17.4998)
policy weight change tensor(37.6740, grad_fn=<SumBackward0>)
time8 0.0019812583923339844
train_time 0.08104920387268066
eval time 0.143141508102417
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:48,995 MainThread INFO: EPOCH:918
2024-01-23 01:04:48,996 MainThread INFO: Time Consumed:0.2276296615600586s
2024-01-23 01:04:48,996 MainThread INFO: Total Frames:138600s
  9%|▉         | 919/10000 [06:16<38:11,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           17879.32785
Train_Epoch_Reward                9700.75808
Running_Training_Average_Rewards  15367.55552
Explore_Time                      0.00097
Train___Time                      0.08105
Eval____Time                      0.14314
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16488.16575
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.78419     2.20268    96.62914     90.08019
alpha_0                           0.63160      0.00009    0.63172      0.63147
Alpha_loss                        -3.09017     0.00292    -3.08687     -3.09452
Training/policy_loss              -5.95354     0.00381    -5.95047     -5.96036
Training/qf1_loss                 6643.56709   477.69875  7411.25293   6074.36084
Training/qf2_loss                 15831.90762  848.53206  17396.78906  15004.80664
Training/pf_norm                  0.15418      0.02178    0.18246      0.13109
Training/qf1_norm                 733.42628    292.43446  1188.96509   298.29459
Training/qf2_norm                 2292.28101   53.90171   2387.39355   2226.59424
log_std/mean                      -0.14258     0.00005    -0.14254     -0.14266
log_probs/mean                    -2.72648     0.00707    -2.71637     -2.73449
mean/mean                         -0.00554     0.00008    -0.00541     -0.00564
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01809382438659668
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70290
epoch first part time 2.86102294921875e-06
replay_buffer._size: [138900]
collect time 0.0009369850158691406
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.007691621780395508, 'sac_diff2': 0.009112358093261719, 'sac_diff3': 0.011519908905029297, 'sac_diff4': 0.00741887092590332, 'sac_diff5': 0.03415083885192871, 'sac_diff6': 0.0004138946533203125, 'all': 0.07053256034851074}
diff5_list [0.008348464965820312, 0.0066759586334228516, 0.006575107574462891, 0.006258964538574219, 0.0062923431396484375]
time3 0
time4 0.07135963439941406
time5 0.07141399383544922
time7 4.76837158203125e-07
gen_weight_change tensor(-17.4998)
policy weight change tensor(37.8089, grad_fn=<SumBackward0>)
time8 0.0018723011016845703
train_time 0.0830233097076416
eval time 0.15940427780151367
epoch last part time 7.62939453125e-06
2024-01-23 01:04:49,263 MainThread INFO: EPOCH:919
2024-01-23 01:04:49,263 MainThread INFO: Time Consumed:0.2458970546722412s
2024-01-23 01:04:49,264 MainThread INFO: Total Frames:138750s
  9%|▉         | 920/10000 [06:16<39:00,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17611.31784
Train_Epoch_Reward                32155.99979
Running_Training_Average_Rewards  16046.02868
Explore_Time                      0.00093
Train___Time                      0.08302
Eval____Time                      0.15940
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16436.84935
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.23646     1.33405     92.82294     89.47976
alpha_0                           0.63128      0.00009     0.63141      0.63116
Alpha_loss                        -3.09221     0.00654     -3.08401     -3.10382
Training/policy_loss              -5.92384     0.00466     -5.91902     -5.93237
Training/qf1_loss                 6656.82930   879.31721   8319.06836   5770.08984
Training/qf2_loss                 15517.89805  1089.58642  17496.65430  14405.12012
Training/pf_norm                  0.12952      0.02882     0.16594      0.09672
Training/qf1_norm                 411.05092    233.97720   727.69775    168.22629
Training/qf2_norm                 2224.89854   33.35788    2263.82642   2181.89307
log_std/mean                      -0.13100     0.00011     -0.13086     -0.13117
log_probs/mean                    -2.72362     0.01322     -2.70868     -2.74738
mean/mean                         -0.00983     0.00010     -0.00968     -0.00997
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020984172821044922
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70290
epoch first part time 1.1444091796875e-05
replay_buffer._size: [139050]
collect time 0.000993490219116211
inside mustsac before update, task 0, sumup 70290
inside mustsac after update, task 0, sumup 70455
inner_dict_sum {'sac_diff0': 0.00022268295288085938, 'sac_diff1': 0.0076024532318115234, 'sac_diff2': 0.009018659591674805, 'sac_diff3': 0.011126041412353516, 'sac_diff4': 0.007756471633911133, 'sac_diff5': 0.053141117095947266, 'sac_diff6': 0.00046825408935546875, 'all': 0.08933568000793457}
diff5_list [0.011100292205810547, 0.011134862899780273, 0.010375499725341797, 0.010127067565917969, 0.01040339469909668]
time3 0.0009253025054931641
time4 0.0903005599975586
time5 0.09036564826965332
time7 0.009132623672485352
gen_weight_change tensor(-17.3545)
policy weight change tensor(37.7820, grad_fn=<SumBackward0>)
time8 0.0027458667755126953
train_time 0.12139749526977539
eval time 0.10090351104736328
epoch last part time 5.7220458984375e-06
2024-01-23 01:04:49,515 MainThread INFO: EPOCH:920
2024-01-23 01:04:49,515 MainThread INFO: Time Consumed:0.22570037841796875s
2024-01-23 01:04:49,515 MainThread INFO: Total Frames:138900s
  9%|▉         | 921/10000 [06:17<38:48,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17374.46123
Train_Epoch_Reward                22755.00641
Running_Training_Average_Rewards  15802.95111
Explore_Time                      0.00098
Train___Time                      0.12140
Eval____Time                      0.10090
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             16540.12255
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.46398     4.21179     98.80190     88.69530
alpha_0                           0.63097      0.00009     0.63109      0.63084
Alpha_loss                        -3.09446     0.00292     -3.09107     -3.09785
Training/policy_loss              -5.69594     0.19866     -5.55532     -6.08771
Training/qf1_loss                 7278.12471   1429.02137  8799.90332   4917.88184
Training/qf2_loss                 16556.27344  2051.10851  18919.28906  13344.34668
Training/pf_norm                  0.16104      0.03057     0.21440      0.12980
Training/qf1_norm                 1035.95093   649.33546   2048.97778   160.22720
Training/qf2_norm                 2134.82144   111.03654   2274.86572   1957.98462
log_std/mean                      -0.13354     0.00444     -0.12675     -0.13907
log_probs/mean                    -2.72121     0.00714     -2.71261     -2.73052
mean/mean                         -0.00947     0.00221     -0.00746     -0.01342
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01958489418029785
epoch last part time3 0.0030760765075683594
inside rlalgo, task 0, sumup 70455
epoch first part time 2.86102294921875e-06
replay_buffer._size: [139200]
collect time 0.000997304916381836
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.00755620002746582, 'sac_diff2': 0.008610963821411133, 'sac_diff3': 0.010904550552368164, 'sac_diff4': 0.007281780242919922, 'sac_diff5': 0.033629655838012695, 'sac_diff6': 0.00040411949157714844, 'all': 0.06860136985778809}
diff5_list [0.0066869258880615234, 0.007268428802490234, 0.007006406784057617, 0.0064775943756103516, 0.006190299987792969]
time3 0
time4 0.06944680213928223
time5 0.06949496269226074
time7 9.5367431640625e-07
gen_weight_change tensor(-17.3545)
policy weight change tensor(37.8727, grad_fn=<SumBackward0>)
time8 0.0019059181213378906
train_time 0.08086943626403809
eval time 0.14616847038269043
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:49,771 MainThread INFO: EPOCH:921
2024-01-23 01:04:49,772 MainThread INFO: Time Consumed:0.23050332069396973s
2024-01-23 01:04:49,772 MainThread INFO: Total Frames:139050s
  9%|▉         | 922/10000 [06:17<38:38,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           17360.46820
Train_Epoch_Reward                1148.58573
Running_Training_Average_Rewards  15033.81111
Explore_Time                      0.00099
Train___Time                      0.08087
Eval____Time                      0.14617
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18039.66826
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.60053     0.51243    91.34725     89.93218
alpha_0                           0.63065      0.00009    0.63078      0.63053
Alpha_loss                        -3.10090     0.00279    -3.09670     -3.10460
Training/policy_loss              -5.95831     0.00275    -5.95414     -5.96167
Training/qf1_loss                 6226.85674   429.32519  6760.44629   5652.84814
Training/qf2_loss                 14970.38711  495.94050  15507.02051  14232.90820
Training/pf_norm                  0.09354      0.01584    0.11710      0.07194
Training/qf1_norm                 719.81218    73.58764   818.32709    614.81061
Training/qf2_norm                 2176.48364   13.51895   2196.41113   2158.02515
log_std/mean                      -0.13445     0.00003    -0.13439     -0.13447
log_probs/mean                    -2.72789     0.00507    -2.72169     -2.73446
mean/mean                         -0.00726     0.00006    -0.00719     -0.00737
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019200563430786133
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70455
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [139350]
collect time 0.0009505748748779297
inner_dict_sum {'sac_diff0': 0.00020575523376464844, 'sac_diff1': 0.00679326057434082, 'sac_diff2': 0.008180379867553711, 'sac_diff3': 0.010583877563476562, 'sac_diff4': 0.007175445556640625, 'sac_diff5': 0.03221535682678223, 'sac_diff6': 0.00043463706970214844, 'all': 0.06558871269226074}
diff5_list [0.006624937057495117, 0.0061893463134765625, 0.006172657012939453, 0.006327629089355469, 0.006900787353515625]
time3 0
time4 0.06642007827758789
time5 0.0664682388305664
time7 1.430511474609375e-06
gen_weight_change tensor(-17.3545)
policy weight change tensor(37.8327, grad_fn=<SumBackward0>)
time8 0.0019085407257080078
train_time 0.07759284973144531
eval time 0.14876937866210938
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:50,024 MainThread INFO: EPOCH:922
2024-01-23 01:04:50,024 MainThread INFO: Time Consumed:0.22972607612609863s
2024-01-23 01:04:50,024 MainThread INFO: Total Frames:139200s
  9%|▉         | 923/10000 [06:17<38:29,  3.93it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           17377.87222
Train_Epoch_Reward                8664.58598
Running_Training_Average_Rewards  14938.06302
Explore_Time                      0.00095
Train___Time                      0.07759
Eval____Time                      0.14877
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18254.37886
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.73096     0.95448    91.79883     89.44820
alpha_0                           0.63034      0.00009    0.63046      0.63021
Alpha_loss                        -3.10177     0.00242    -3.09848     -3.10536
Training/policy_loss              -5.52517     0.00389    -5.51850     -5.53027
Training/qf1_loss                 6310.83105   475.09920  6901.75244   5727.28662
Training/qf2_loss                 15079.83340  611.48372  15886.35449  14225.36914
Training/pf_norm                  0.18823      0.03106    0.23842      0.15465
Training/qf1_norm                 254.83320    141.49352  492.88071    143.01402
Training/qf2_norm                 2018.92212   21.83083   2044.12585   1989.49121
log_std/mean                      -0.13652     0.00020    -0.13622     -0.13677
log_probs/mean                    -2.72250     0.00437    -2.71681     -2.72735
mean/mean                         -0.00667     0.00002    -0.00664     -0.00670
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018540143966674805
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70455
epoch first part time 2.86102294921875e-06
replay_buffer._size: [139500]
collect time 0.0009634494781494141
inner_dict_sum {'sac_diff0': 0.00021958351135253906, 'sac_diff1': 0.0072820186614990234, 'sac_diff2': 0.008685588836669922, 'sac_diff3': 0.011289119720458984, 'sac_diff4': 0.007364988327026367, 'sac_diff5': 0.03298044204711914, 'sac_diff6': 0.00040793418884277344, 'all': 0.06822967529296875}
diff5_list [0.00661468505859375, 0.0068531036376953125, 0.00685882568359375, 0.006364583969116211, 0.006289243698120117]
time3 0
time4 0.06905841827392578
time5 0.06912016868591309
time7 4.76837158203125e-07
gen_weight_change tensor(-17.3545)
policy weight change tensor(37.7187, grad_fn=<SumBackward0>)
time8 0.0018486976623535156
train_time 0.0803992748260498
eval time 0.14229822158813477
epoch last part time 8.106231689453125e-06
2024-01-23 01:04:50,272 MainThread INFO: EPOCH:923
2024-01-23 01:04:50,272 MainThread INFO: Time Consumed:0.22612476348876953s
2024-01-23 01:04:50,273 MainThread INFO: Total Frames:139350s
  9%|▉         | 924/10000 [06:17<38:16,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17416.91022
Train_Epoch_Reward                7465.78865
Running_Training_Average_Rewards  14720.25788
Explore_Time                      0.00096
Train___Time                      0.08040
Eval____Time                      0.14230
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18501.95477
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.41465     1.15710     94.68135     91.52586
alpha_0                           0.63002      0.00009     0.63015      0.62990
Alpha_loss                        -3.10670     0.00385     -3.09972     -3.11148
Training/policy_loss              -5.78475     0.00443     -5.77741     -5.78983
Training/qf1_loss                 6886.04814   1122.15406  8776.57227   5713.79834
Training/qf2_loss                 15973.18516  1331.86577  18337.17578  14682.82520
Training/pf_norm                  0.16580      0.02976     0.19252      0.10926
Training/qf1_norm                 667.28121    231.85360   1124.74756   496.53546
Training/qf2_norm                 2133.44746   26.52511    2185.47632   2112.71826
log_std/mean                      -0.12358     0.00007     -0.12351     -0.12368
log_probs/mean                    -2.72590     0.00803     -2.71078     -2.73333
mean/mean                         -0.00883     0.00030     -0.00847     -0.00932
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020231246948242188
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70455
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [139650]
collect time 0.0009720325469970703
inner_dict_sum {'sac_diff0': 0.00022268295288085938, 'sac_diff1': 0.006697416305541992, 'sac_diff2': 0.008152484893798828, 'sac_diff3': 0.010622501373291016, 'sac_diff4': 0.007025718688964844, 'sac_diff5': 0.03219914436340332, 'sac_diff6': 0.0003974437713623047, 'all': 0.06531739234924316}
diff5_list [0.006860494613647461, 0.006340980529785156, 0.006292104721069336, 0.006278038024902344, 0.0064275264739990234]
time3 0
time4 0.06609344482421875
time5 0.06613826751708984
time7 7.152557373046875e-07
gen_weight_change tensor(-17.3545)
policy weight change tensor(37.6433, grad_fn=<SumBackward0>)
time8 0.0020699501037597656
train_time 0.07759547233581543
eval time 0.14786672592163086
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:50,525 MainThread INFO: EPOCH:924
2024-01-23 01:04:50,525 MainThread INFO: Time Consumed:0.22887563705444336s
2024-01-23 01:04:50,525 MainThread INFO: Total Frames:139500s
  9%|▉         | 925/10000 [06:18<38:11,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           17448.72955
Train_Epoch_Reward                10703.69445
Running_Training_Average_Rewards  14702.83736
Explore_Time                      0.00097
Train___Time                      0.07760
Eval____Time                      0.14787
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18499.15637
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.64842     1.24232    93.65964     90.26671
alpha_0                           0.62971      0.00009    0.62983      0.62958
Alpha_loss                        -3.10922     0.00278    -3.10612     -3.11399
Training/policy_loss              -5.62856     0.00513    -5.62517     -5.63878
Training/qf1_loss                 6501.82754   303.24677  6927.54053   6084.61230
Training/qf2_loss                 15472.77910  433.57505  16298.09082  15119.70117
Training/pf_norm                  0.08630      0.01796    0.10171      0.05513
Training/qf1_norm                 268.78758    97.55580   360.52179    143.95932
Training/qf2_norm                 2093.26812   27.90896   2141.57178   2063.91187
log_std/mean                      -0.13044     0.00005    -0.13038     -0.13051
log_probs/mean                    -2.72409     0.00420    -2.71882     -2.73149
mean/mean                         -0.00662     0.00028    -0.00621     -0.00701
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018360137939453125
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70455
epoch first part time 3.337860107421875e-06
replay_buffer._size: [139800]
collect time 0.0009865760803222656
inside mustsac before update, task 0, sumup 70455
inside mustsac after update, task 0, sumup 70915
inner_dict_sum {'sac_diff0': 0.0002276897430419922, 'sac_diff1': 0.007688045501708984, 'sac_diff2': 0.009176254272460938, 'sac_diff3': 0.011484861373901367, 'sac_diff4': 0.008173942565917969, 'sac_diff5': 0.05641818046569824, 'sac_diff6': 0.0004439353942871094, 'all': 0.0936129093170166}
diff5_list [0.010927677154541016, 0.013371944427490234, 0.01137685775756836, 0.010584354400634766, 0.010157346725463867]
time3 0.0009720325469970703
time4 0.09459376335144043
time5 0.09465360641479492
time7 0.009310007095336914
gen_weight_change tensor(-17.1870)
policy weight change tensor(37.5730, grad_fn=<SumBackward0>)
time8 0.0017933845520019531
train_time 0.12513971328735352
eval time 0.10383009910583496
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:50,780 MainThread INFO: EPOCH:925
2024-01-23 01:04:50,780 MainThread INFO: Time Consumed:0.2323780059814453s
2024-01-23 01:04:50,780 MainThread INFO: Total Frames:139650s
  9%|▉         | 926/10000 [06:18<38:21,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           17470.08466
Train_Epoch_Reward                15358.17240
Running_Training_Average_Rewards  14760.70058
Explore_Time                      0.00098
Train___Time                      0.12514
Eval____Time                      0.10383
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18488.87551
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.80166     0.40173    93.32937     92.28612
alpha_0                           0.62939      0.00009    0.62952      0.62927
Alpha_loss                        -3.11264     0.00298    -3.10966     -3.11648
Training/policy_loss              -5.58273     0.24359    -5.21507     -5.86388
Training/qf1_loss                 7169.96904   273.37427  7556.42969   6713.34521
Training/qf2_loss                 16348.89004  311.17309  16837.61914  15893.70020
Training/pf_norm                  0.15176      0.02868    0.19854      0.11489
Training/qf1_norm                 526.79459    467.60171  1418.83301   132.85838
Training/qf2_norm                 2080.55974   121.14944  2242.04224   1900.84436
log_std/mean                      -0.13365     0.00461    -0.12661     -0.13980
log_probs/mean                    -2.72422     0.00480    -2.71922     -2.73014
mean/mean                         -0.00960     0.00410    -0.00318     -0.01380
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.020128965377807617
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70915
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [139950]
collect time 0.0009756088256835938
inner_dict_sum {'sac_diff0': 0.00020623207092285156, 'sac_diff1': 0.006722688674926758, 'sac_diff2': 0.007895231246948242, 'sac_diff3': 0.010351419448852539, 'sac_diff4': 0.006967782974243164, 'sac_diff5': 0.0316469669342041, 'sac_diff6': 0.00038433074951171875, 'all': 0.06417465209960938}
diff5_list [0.006573915481567383, 0.006298065185546875, 0.006282329559326172, 0.006273746490478516, 0.006218910217285156]
time3 0
time4 0.0649266242980957
time5 0.0649709701538086
time7 4.76837158203125e-07
gen_weight_change tensor(-17.1870)
policy weight change tensor(37.5105, grad_fn=<SumBackward0>)
time8 0.0017862319946289062
train_time 0.07600188255310059
eval time 0.14934277534484863
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:51,032 MainThread INFO: EPOCH:926
2024-01-23 01:04:51,033 MainThread INFO: Time Consumed:0.22875285148620605s
2024-01-23 01:04:51,033 MainThread INFO: Total Frames:139800s
  9%|▉         | 927/10000 [06:18<38:14,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           17831.29773
Train_Epoch_Reward                22582.66954
Running_Training_Average_Rewards  15060.50838
Explore_Time                      0.00097
Train___Time                      0.07600
Eval____Time                      0.14934
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20412.17193
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.58320     1.42528     96.03931     91.70319
alpha_0                           0.62908      0.00009     0.62920      0.62895
Alpha_loss                        -3.11918     0.00271     -3.11563     -3.12343
Training/policy_loss              -5.54722     0.00530     -5.54006     -5.55568
Training/qf1_loss                 6913.80615   1136.23405  9031.31934   5620.09473
Training/qf2_loss                 16246.68887  1367.13243  18866.03516  14850.70117
Training/pf_norm                  0.14113      0.04470     0.20564      0.08454
Training/qf1_norm                 1286.68925   305.47816   1833.18066   919.14056
Training/qf2_norm                 2082.38933   32.58544    2138.10767   2038.95667
log_std/mean                      -0.13569     0.00005     -0.13564     -0.13578
log_probs/mean                    -2.73107     0.00643     -2.72342     -2.74171
mean/mean                         -0.00995     0.00008     -0.00981     -0.01003
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01871514320373535
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70915
epoch first part time 2.86102294921875e-06
replay_buffer._size: [140100]
collect time 0.0008511543273925781
inner_dict_sum {'sac_diff0': 0.0002186298370361328, 'sac_diff1': 0.007077217102050781, 'sac_diff2': 0.008415699005126953, 'sac_diff3': 0.010735750198364258, 'sac_diff4': 0.00738215446472168, 'sac_diff5': 0.03397679328918457, 'sac_diff6': 0.0004048347473144531, 'all': 0.06821107864379883}
diff5_list [0.006847381591796875, 0.006293296813964844, 0.006434202194213867, 0.008044719696044922, 0.0063571929931640625]
time3 0
time4 0.06903505325317383
time5 0.06908702850341797
time7 9.5367431640625e-07
gen_weight_change tensor(-17.1870)
policy weight change tensor(37.3621, grad_fn=<SumBackward0>)
time8 0.0018639564514160156
train_time 0.0803220272064209
eval time 0.1441802978515625
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:51,282 MainThread INFO: EPOCH:927
2024-01-23 01:04:51,283 MainThread INFO: Time Consumed:0.22776532173156738s
2024-01-23 01:04:51,283 MainThread INFO: Total Frames:139950s
  9%|▉         | 928/10000 [06:18<38:06,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18215.27270
Train_Epoch_Reward                18044.27836
Running_Training_Average_Rewards  15136.73341
Explore_Time                      0.00085
Train___Time                      0.08032
Eval____Time                      0.14418
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20491.38364
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.89542     2.54143     97.80321     90.95325
alpha_0                           0.62876      0.00009     0.62889      0.62864
Alpha_loss                        -3.11942     0.00362     -3.11246     -3.12292
Training/policy_loss              -5.67910     0.00503     -5.66951     -5.68384
Training/qf1_loss                 7130.73730   610.41779   7968.55225   6244.70020
Training/qf2_loss                 16492.88965  1019.03406  18182.14648  14967.37988
Training/pf_norm                  0.13638      0.01089     0.14797      0.11724
Training/qf1_norm                 1377.59307   533.19564   2004.18750   551.23169
Training/qf2_norm                 2116.09873   56.42939    2203.27368   2049.64941
log_std/mean                      -0.13182     0.00023     -0.13146     -0.13209
log_probs/mean                    -2.72434     0.00753     -2.70935     -2.72899
mean/mean                         -0.00773     0.00022     -0.00738     -0.00799
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018652677536010742
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70915
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [140250]
collect time 0.0009250640869140625
inner_dict_sum {'sac_diff0': 0.0002243518829345703, 'sac_diff1': 0.0064809322357177734, 'sac_diff2': 0.007664918899536133, 'sac_diff3': 0.009998083114624023, 'sac_diff4': 0.006887197494506836, 'sac_diff5': 0.032160282135009766, 'sac_diff6': 0.00040268898010253906, 'all': 0.06381845474243164}
diff5_list [0.0066411495208740234, 0.006665229797363281, 0.006079912185668945, 0.0061686038970947266, 0.006605386734008789]
time3 0
time4 0.06463027000427246
time5 0.06467795372009277
time7 7.152557373046875e-07
gen_weight_change tensor(-17.1870)
policy weight change tensor(37.1927, grad_fn=<SumBackward0>)
time8 0.0021448135375976562
train_time 0.07604646682739258
eval time 0.14511632919311523
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:51,529 MainThread INFO: EPOCH:928
2024-01-23 01:04:51,529 MainThread INFO: Time Consumed:0.22453570365905762s
2024-01-23 01:04:51,530 MainThread INFO: Total Frames:140100s
  9%|▉         | 929/10000 [06:19<37:51,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           18638.32662
Train_Epoch_Reward                17971.24309
Running_Training_Average_Rewards  15272.29798
Explore_Time                      0.00092
Train___Time                      0.07605
Eval____Time                      0.14512
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20718.70499
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.86431     2.19377     97.60765     91.76955
alpha_0                           0.62845      0.00009     0.62858      0.62832
Alpha_loss                        -3.12478     0.00218     -3.12275     -3.12878
Training/policy_loss              -5.80806     0.00164     -5.80614     -5.81007
Training/qf1_loss                 7865.12393   1308.27690  9424.46191   5571.87207
Training/qf2_loss                 17442.52793  1715.50667  19393.91797  14505.57324
Training/pf_norm                  0.11589      0.00655     0.12775      0.10822
Training/qf1_norm                 753.24049    420.28935   1251.92566   151.37868
Training/qf2_norm                 2153.79561   49.62225    2213.88110   2083.22974
log_std/mean                      -0.14217     0.00021     -0.14187     -0.14248
log_probs/mean                    -2.72866     0.00370     -2.72572     -2.73582
mean/mean                         -0.01656     0.00029     -0.01612     -0.01695
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018610239028930664
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70915
epoch first part time 2.86102294921875e-06
replay_buffer._size: [140400]
collect time 0.0009427070617675781
inner_dict_sum {'sac_diff0': 0.0002281665802001953, 'sac_diff1': 0.0075225830078125, 'sac_diff2': 0.009476423263549805, 'sac_diff3': 0.011617660522460938, 'sac_diff4': 0.007670164108276367, 'sac_diff5': 0.03452157974243164, 'sac_diff6': 0.00044608116149902344, 'all': 0.07148265838623047}
diff5_list [0.006970882415771484, 0.006693840026855469, 0.006328105926513672, 0.007753133773803711, 0.006775617599487305]
time3 0
time4 0.07236266136169434
time5 0.0724184513092041
time7 7.152557373046875e-07
gen_weight_change tensor(-17.1870)
policy weight change tensor(37.0828, grad_fn=<SumBackward0>)
time8 0.0018613338470458984
train_time 0.08388304710388184
eval time 0.14528250694274902
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:51,784 MainThread INFO: EPOCH:929
2024-01-23 01:04:51,784 MainThread INFO: Time Consumed:0.2325119972229004s
2024-01-23 01:04:51,784 MainThread INFO: Total Frames:140250s
  9%|▉         | 930/10000 [06:19<38:03,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           19091.18204
Train_Epoch_Reward                7643.84863
Running_Training_Average_Rewards  15196.30412
Explore_Time                      0.00094
Train___Time                      0.08388
Eval____Time                      0.14528
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20965.40355
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       94.45318     1.66132    96.97890     92.63618
alpha_0                           0.62814      0.00009    0.62826      0.62801
Alpha_loss                        -3.12770     0.00533    -3.12143     -3.13401
Training/policy_loss              -5.89360     0.00635    -5.88546     -5.90257
Training/qf1_loss                 8288.70244   657.42891  9155.34570   7433.12207
Training/qf2_loss                 17783.28281  817.88784  18837.32812  16752.83203
Training/pf_norm                  0.12854      0.02359    0.17354      0.10512
Training/qf1_norm                 693.59214    303.73456  1150.81458   353.80270
Training/qf2_norm                 2284.08882   40.32179   2345.64575   2238.71289
log_std/mean                      -0.12639     0.00008    -0.12632     -0.12652
log_probs/mean                    -2.72770     0.01196    -2.71277     -2.74417
mean/mean                         -0.00527     0.00021    -0.00495     -0.00557
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018738508224487305
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70915
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [140550]
collect time 0.0009496212005615234
inside mustsac before update, task 0, sumup 70915
inside mustsac after update, task 0, sumup 70557
inner_dict_sum {'sac_diff0': 0.0002200603485107422, 'sac_diff1': 0.0076868534088134766, 'sac_diff2': 0.009079694747924805, 'sac_diff3': 0.011778593063354492, 'sac_diff4': 0.007734060287475586, 'sac_diff5': 0.052759647369384766, 'sac_diff6': 0.0004298686981201172, 'all': 0.08968877792358398}
diff5_list [0.010847330093383789, 0.010019540786743164, 0.010730981826782227, 0.010722160339355469, 0.010439634323120117]
time3 0.0009446144104003906
time4 0.09060549736022949
time5 0.09066200256347656
time7 0.00913095474243164
gen_weight_change tensor(-17.0887)
policy weight change tensor(37.0633, grad_fn=<SumBackward0>)
time8 0.0025544166564941406
train_time 0.12163090705871582
eval time 0.10689091682434082
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:52,038 MainThread INFO: EPOCH:930
2024-01-23 01:04:52,038 MainThread INFO: Time Consumed:0.23186564445495605s
2024-01-23 01:04:52,039 MainThread INFO: Total Frames:140400s
  9%|▉         | 931/10000 [06:19<38:17,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           19566.68542
Train_Epoch_Reward                22714.14917
Running_Training_Average_Rewards  15166.47496
Explore_Time                      0.00094
Train___Time                      0.12163
Eval____Time                      0.10689
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21295.15633
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.26933     0.92783    92.10844     89.57635
alpha_0                           0.62782      0.00009    0.62795      0.62770
Alpha_loss                        -3.12868     0.00383    -3.12380     -3.13358
Training/policy_loss              -5.48566     0.25646    -5.05070     -5.73915
Training/qf1_loss                 6917.37490   619.63222  8082.15137   6220.88672
Training/qf2_loss                 15781.33242  756.84891  17047.10352  14695.58398
Training/pf_norm                  0.10679      0.02677    0.15116      0.07074
Training/qf1_norm                 685.49816    324.73610  1149.77161   197.82819
Training/qf2_norm                 1989.41921   136.95871  2143.73535   1759.46204
log_std/mean                      -0.12816     0.00508    -0.12006     -0.13463
log_probs/mean                    -2.72258     0.00681    -2.71355     -2.73022
mean/mean                         -0.01030     0.00184    -0.00764     -0.01331
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018723726272583008
epoch last part time3 0.002849578857421875
inside rlalgo, task 0, sumup 70557
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [140700]
collect time 0.0009529590606689453
inner_dict_sum {'sac_diff0': 0.00021147727966308594, 'sac_diff1': 0.006918668746948242, 'sac_diff2': 0.008448123931884766, 'sac_diff3': 0.010703563690185547, 'sac_diff4': 0.007013559341430664, 'sac_diff5': 0.03233766555786133, 'sac_diff6': 0.0003879070281982422, 'all': 0.06602096557617188}
diff5_list [0.007173299789428711, 0.006309032440185547, 0.0065059661865234375, 0.0062029361724853516, 0.006146430969238281]
time3 0
time4 0.06679964065551758
time5 0.06685090065002441
time7 4.76837158203125e-07
gen_weight_change tensor(-17.0887)
policy weight change tensor(37.0471, grad_fn=<SumBackward0>)
time8 0.001867055892944336
train_time 0.0781240463256836
eval time 0.15094518661499023
epoch last part time 7.62939453125e-06
2024-01-23 01:04:52,295 MainThread INFO: EPOCH:931
2024-01-23 01:04:52,296 MainThread INFO: Time Consumed:0.232452392578125s
2024-01-23 01:04:52,296 MainThread INFO: Total Frames:140550s
  9%|▉         | 932/10000 [06:19<38:20,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           19799.35291
Train_Epoch_Reward                5491.99522
Running_Training_Average_Rewards  14953.13446
Explore_Time                      0.00095
Train___Time                      0.07812
Eval____Time                      0.15095
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20366.34314
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.98759     3.21476     98.30186     89.96385
alpha_0                           0.62751      0.00009     0.62763      0.62738
Alpha_loss                        -3.13206     0.00546     -3.12673     -3.14149
Training/policy_loss              -6.12217     0.00460     -6.11564     -6.13002
Training/qf1_loss                 7262.16309   1370.92616  9694.27734   5843.81787
Training/qf2_loss                 16578.76484  1942.34636  19815.02148  14393.50195
Training/pf_norm                  0.13866      0.03097     0.18064      0.09276
Training/qf1_norm                 1215.45057   617.84219   2087.29102   507.48193
Training/qf2_norm                 2344.55474   82.15663    2454.12329   2238.58960
log_std/mean                      -0.13681     0.00001     -0.13679     -0.13682
log_probs/mean                    -2.72264     0.01192     -2.71120     -2.74432
mean/mean                         -0.00988     0.00020     -0.00958     -0.01013
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018785715103149414
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70557
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [140850]
collect time 0.0009603500366210938
inner_dict_sum {'sac_diff0': 0.0002110004425048828, 'sac_diff1': 0.007193326950073242, 'sac_diff2': 0.008242130279541016, 'sac_diff3': 0.010328531265258789, 'sac_diff4': 0.007511615753173828, 'sac_diff5': 0.033003807067871094, 'sac_diff6': 0.00041174888610839844, 'all': 0.06690216064453125}
diff5_list [0.006581306457519531, 0.006190776824951172, 0.0063741207122802734, 0.0072078704833984375, 0.00664973258972168]
time3 0
time4 0.06773614883422852
time5 0.06778645515441895
time7 7.152557373046875e-07
gen_weight_change tensor(-17.0887)
policy weight change tensor(37.0296, grad_fn=<SumBackward0>)
time8 0.0020225048065185547
train_time 0.07921385765075684
eval time 0.1482701301574707
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:52,549 MainThread INFO: EPOCH:932
2024-01-23 01:04:52,549 MainThread INFO: Time Consumed:0.23087668418884277s
2024-01-23 01:04:52,549 MainThread INFO: Total Frames:140700s
  9%|▉         | 933/10000 [06:20<38:18,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           20050.49207
Train_Epoch_Reward                30059.30128
Running_Training_Average_Rewards  15308.24399
Explore_Time                      0.00096
Train___Time                      0.07921
Eval____Time                      0.14827
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20765.77046
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.59380     1.91096    94.58842     89.05262
alpha_0                           0.62720      0.00009    0.62732      0.62707
Alpha_loss                        -3.13673     0.00361    -3.13093     -3.14016
Training/policy_loss              -5.75217     0.00463    -5.74416     -5.75780
Training/qf1_loss                 7742.60244   725.87471  8610.03613   6586.38916
Training/qf2_loss                 16632.87559  973.50706  17582.37109  14906.44824
Training/pf_norm                  0.17065      0.01683    0.19741      0.15228
Training/qf1_norm                 2869.78193   346.42327  3174.48633   2208.66992
Training/qf2_norm                 2131.38301   43.79691   2177.46362   2050.10107
log_std/mean                      -0.12467     0.00008    -0.12462     -0.12482
log_probs/mean                    -2.72545     0.00721    -2.71301     -2.73425
mean/mean                         -0.00395     0.00030    -0.00348     -0.00434
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0182650089263916
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70557
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [141000]
collect time 0.0009419918060302734
inner_dict_sum {'sac_diff0': 0.00021505355834960938, 'sac_diff1': 0.007630586624145508, 'sac_diff2': 0.009250640869140625, 'sac_diff3': 0.011914730072021484, 'sac_diff4': 0.008011817932128906, 'sac_diff5': 0.03717947006225586, 'sac_diff6': 0.0004596710205078125, 'all': 0.0746619701385498}
diff5_list [0.006735563278198242, 0.0076444149017333984, 0.007315874099731445, 0.007722616195678711, 0.0077610015869140625]
time3 0
time4 0.07553648948669434
time5 0.07559394836425781
time7 7.152557373046875e-07
gen_weight_change tensor(-17.0887)
policy weight change tensor(37.1684, grad_fn=<SumBackward0>)
time8 0.001965045928955078
train_time 0.08720707893371582
eval time 0.1473236083984375
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:52,809 MainThread INFO: EPOCH:933
2024-01-23 01:04:52,809 MainThread INFO: Time Consumed:0.23795056343078613s
2024-01-23 01:04:52,809 MainThread INFO: Total Frames:140850s
  9%|▉         | 934/10000 [06:20<38:39,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           20320.42704
Train_Epoch_Reward                10324.20911
Running_Training_Average_Rewards  15252.97396
Explore_Time                      0.00094
Train___Time                      0.08721
Eval____Time                      0.14732
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21201.30444
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.95468     1.52024     92.84911     88.93822
alpha_0                           0.62688      0.00009     0.62701      0.62676
Alpha_loss                        -3.13968     0.00310     -3.13616     -3.14518
Training/policy_loss              -6.08390     0.00714     -6.07434     -6.09152
Training/qf1_loss                 7100.17715   1026.09524  8282.59863   5257.36816
Training/qf2_loss                 16130.87422  1306.54788  17412.89844  13667.25195
Training/pf_norm                  0.21364      0.02556     0.24587      0.18242
Training/qf1_norm                 383.81796    294.81026   972.91846    215.50722
Training/qf2_norm                 2282.01104   36.36561    2305.79639   2209.96069
log_std/mean                      -0.12597     0.00027     -0.12564     -0.12639
log_probs/mean                    -2.72458     0.00759     -2.71416     -2.73779
mean/mean                         -0.00732     0.00011     -0.00721     -0.00752
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019740581512451172
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70557
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [141150]
collect time 0.0009756088256835938
inner_dict_sum {'sac_diff0': 0.00020360946655273438, 'sac_diff1': 0.00657200813293457, 'sac_diff2': 0.00789952278137207, 'sac_diff3': 0.010131597518920898, 'sac_diff4': 0.006700038909912109, 'sac_diff5': 0.032045841217041016, 'sac_diff6': 0.00038170814514160156, 'all': 0.063934326171875}
diff5_list [0.006559848785400391, 0.00628662109375, 0.006595134735107422, 0.006402015686035156, 0.006202220916748047]
time3 0
time4 0.06468367576599121
time5 0.0647273063659668
time7 9.5367431640625e-07
gen_weight_change tensor(-17.0887)
policy weight change tensor(37.2332, grad_fn=<SumBackward0>)
time8 0.001962423324584961
train_time 0.07592034339904785
eval time 0.14817380905151367
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:53,060 MainThread INFO: EPOCH:934
2024-01-23 01:04:53,060 MainThread INFO: Time Consumed:0.22745656967163086s
2024-01-23 01:04:53,060 MainThread INFO: Total Frames:141000s
  9%|▉         | 935/10000 [06:20<38:21,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           20637.93556
Train_Epoch_Reward                11182.97430
Running_Training_Average_Rewards  14945.98562
Explore_Time                      0.00097
Train___Time                      0.07592
Eval____Time                      0.14817
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21674.24165
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       90.59924     1.44052    92.80086     88.34459
alpha_0                           0.62657      0.00009    0.62669      0.62644
Alpha_loss                        -3.14451     0.00133    -3.14232     -3.14605
Training/policy_loss              -5.47138     0.00175    -5.46972     -5.47363
Training/qf1_loss                 6398.95430   350.56131  6831.36719   5858.60205
Training/qf2_loss                 15151.13613  539.62459  16006.21289  14471.86914
Training/pf_norm                  0.16917      0.02552    0.20182      0.13120
Training/qf1_norm                 488.72963    252.52648  907.57928    137.34415
Training/qf2_norm                 2016.44380   31.34275   2066.57788   1970.55774
log_std/mean                      -0.13929     0.00007    -0.13919     -0.13938
log_probs/mean                    -2.72771     0.00174    -2.72592     -2.73102
mean/mean                         -0.00520     0.00007    -0.00508     -0.00526
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018340349197387695
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70557
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [141300]
collect time 0.0009622573852539062
inside mustsac before update, task 0, sumup 70557
inside mustsac after update, task 0, sumup 70929
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.006943941116333008, 'sac_diff2': 0.00839996337890625, 'sac_diff3': 0.010861396789550781, 'sac_diff4': 0.007580995559692383, 'sac_diff5': 0.05256032943725586, 'sac_diff6': 0.000423431396484375, 'all': 0.08698415756225586}
diff5_list [0.010890007019042969, 0.009990692138671875, 0.011535406112670898, 0.010264873504638672, 0.009879350662231445]
time3 0.0008826255798339844
time4 0.08790111541748047
time5 0.08795332908630371
time7 0.009038925170898438
gen_weight_change tensor(-16.9700)
policy weight change tensor(37.2218, grad_fn=<SumBackward0>)
time8 0.001964092254638672
train_time 0.1178884506225586
eval time 0.10794448852539062
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:53,311 MainThread INFO: EPOCH:935
2024-01-23 01:04:53,311 MainThread INFO: Time Consumed:0.22922873497009277s
2024-01-23 01:04:53,311 MainThread INFO: Total Frames:141150s
  9%|▉         | 936/10000 [06:20<38:15,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           20980.39391
Train_Epoch_Reward                13813.78286
Running_Training_Average_Rewards  14908.54813
Explore_Time                      0.00096
Train___Time                      0.11789
Eval____Time                      0.10794
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21913.45899
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.35516     1.03708    93.45570     90.39252
alpha_0                           0.62626      0.00009    0.62638      0.62613
Alpha_loss                        -3.14706     0.00291    -3.14256     -3.14983
Training/policy_loss              -5.79332     0.26179    -5.38603     -6.19303
Training/qf1_loss                 6207.30918   564.11545  6886.93994   5548.16943
Training/qf2_loss                 15280.19258  664.11125  16053.55762  14267.79492
Training/pf_norm                  0.16464      0.03510    0.22028      0.12882
Training/qf1_norm                 572.72560    245.88904  849.73157    139.73236
Training/qf2_norm                 2118.72839   121.96721  2289.19678   1923.84387
log_std/mean                      -0.13345     0.00457    -0.12708     -0.14084
log_probs/mean                    -2.72598     0.00730    -2.71351     -2.73397
mean/mean                         -0.00485     0.00345    0.00080      -0.00868
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01873302459716797
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70929
epoch first part time 3.337860107421875e-06
replay_buffer._size: [141450]
collect time 0.0010271072387695312
inner_dict_sum {'sac_diff0': 0.0002231597900390625, 'sac_diff1': 0.007615566253662109, 'sac_diff2': 0.008876562118530273, 'sac_diff3': 0.011471033096313477, 'sac_diff4': 0.007563591003417969, 'sac_diff5': 0.034432172775268555, 'sac_diff6': 0.0004229545593261719, 'all': 0.07060503959655762}
diff5_list [0.007834672927856445, 0.006692409515380859, 0.006392478942871094, 0.006503582000732422, 0.007009029388427734]
time3 0
time4 0.07143211364746094
time5 0.07148861885070801
time7 7.152557373046875e-07
gen_weight_change tensor(-16.9700)
policy weight change tensor(37.1403, grad_fn=<SumBackward0>)
time8 0.0019214153289794922
train_time 0.08312821388244629
eval time 0.15881919860839844
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:53,579 MainThread INFO: EPOCH:936
2024-01-23 01:04:53,579 MainThread INFO: Time Consumed:0.24555110931396484s
2024-01-23 01:04:53,579 MainThread INFO: Total Frames:141300s
  9%|▉         | 937/10000 [06:21<38:57,  3.88it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           21159.42713
Train_Epoch_Reward                46590.43988
Running_Training_Average_Rewards  16324.27011
Explore_Time                      0.00102
Train___Time                      0.08313
Eval____Time                      0.15882
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22202.50413
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.29430     1.77523    95.50190     90.50746
alpha_0                           0.62594      0.00009    0.62607      0.62582
Alpha_loss                        -3.15089     0.00321    -3.14702     -3.15434
Training/policy_loss              -5.38289     0.00283    -5.37873     -5.38638
Training/qf1_loss                 6849.05703   613.69665  7576.96777   6005.03125
Training/qf2_loss                 16130.21797  907.81207  17324.33203  14731.28613
Training/pf_norm                  0.10895      0.02483    0.14509      0.06794
Training/qf1_norm                 363.16699    151.97483  556.60828    142.28375
Training/qf2_norm                 2080.46199   40.27627   2133.76147   2019.38562
log_std/mean                      -0.13847     0.00018    -0.13817     -0.13863
log_probs/mean                    -2.72699     0.00646    -2.71800     -2.73579
mean/mean                         -0.00067     0.00003    -0.00063     -0.00071
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01931619644165039
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70929
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [141600]
collect time 0.00095367431640625
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.008368253707885742, 'sac_diff2': 0.009801387786865234, 'sac_diff3': 0.012696266174316406, 'sac_diff4': 0.008631467819213867, 'sac_diff5': 0.038167476654052734, 'sac_diff6': 0.00044918060302734375, 'all': 0.07833504676818848}
diff5_list [0.007747650146484375, 0.007413387298583984, 0.00798797607421875, 0.008112668991088867, 0.006905794143676758]
time3 0
time4 0.07930660247802734
time5 0.07936310768127441
time7 1.430511474609375e-06
gen_weight_change tensor(-16.9700)
policy weight change tensor(36.9263, grad_fn=<SumBackward0>)
time8 0.0019338130950927734
train_time 0.09120559692382812
eval time 0.15515923500061035
epoch last part time 6.9141387939453125e-06
2024-01-23 01:04:53,852 MainThread INFO: EPOCH:937
2024-01-23 01:04:53,852 MainThread INFO: Time Consumed:0.2497107982635498s
2024-01-23 01:04:53,852 MainThread INFO: Total Frames:141450s
  9%|▉         | 938/10000 [06:21<39:36,  3.81it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21345.77548
Train_Epoch_Reward                19979.92643
Running_Training_Average_Rewards  16481.04075
Explore_Time                      0.00095
Train___Time                      0.09121
Eval____Time                      0.15516
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22354.86711
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.51819     1.55099     94.38029     90.58144
alpha_0                           0.62563      0.00009     0.62576      0.62551
Alpha_loss                        -3.15506     0.00346     -3.15051     -3.15919
Training/policy_loss              -6.31764     0.00259     -6.31507     -6.32253
Training/qf1_loss                 7378.87930   1479.08925  9133.89453   5228.42676
Training/qf2_loss                 16483.62910  1784.37409  18608.80859  13958.10938
Training/pf_norm                  0.16345      0.02095     0.19887      0.13624
Training/qf1_norm                 538.33403    313.23885   906.13959    154.53273
Training/qf2_norm                 2349.53027   40.05530    2396.79590   2300.34229
log_std/mean                      -0.13755     0.00029     -0.13708     -0.13789
log_probs/mean                    -2.72871     0.00541     -2.72187     -2.73464
mean/mean                         -0.00800     0.00004     -0.00797     -0.00807
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01889801025390625
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70929
epoch first part time 2.86102294921875e-06
replay_buffer._size: [141750]
collect time 0.0008790493011474609
inner_dict_sum {'sac_diff0': 0.0002295970916748047, 'sac_diff1': 0.0074613094329833984, 'sac_diff2': 0.008800268173217773, 'sac_diff3': 0.011384248733520508, 'sac_diff4': 0.007830142974853516, 'sac_diff5': 0.03441262245178223, 'sac_diff6': 0.0004189014434814453, 'all': 0.07053709030151367}
diff5_list [0.006758689880371094, 0.006323814392089844, 0.006762504577636719, 0.007512092590332031, 0.007055521011352539]
time3 0
time4 0.07138657569885254
time5 0.07143998146057129
time7 9.5367431640625e-07
gen_weight_change tensor(-16.9700)
policy weight change tensor(36.6268, grad_fn=<SumBackward0>)
time8 0.0020325183868408203
train_time 0.08294558525085449
eval time 0.16730046272277832
epoch last part time 7.3909759521484375e-06
2024-01-23 01:04:54,128 MainThread INFO: EPOCH:938
2024-01-23 01:04:54,128 MainThread INFO: Time Consumed:0.253612756729126s
2024-01-23 01:04:54,129 MainThread INFO: Total Frames:141600s
  9%|▉         | 939/10000 [06:21<40:17,  3.75it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21495.17025
Train_Epoch_Reward                3915.63530
Running_Training_Average_Rewards  16044.11226
Explore_Time                      0.00087
Train___Time                      0.08295
Eval____Time                      0.16730
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22212.65268
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.73648     1.96222     96.02409     90.00468
alpha_0                           0.62532      0.00009     0.62544      0.62519
Alpha_loss                        -3.16156     0.00585     -3.15186     -3.16993
Training/policy_loss              -5.91275     0.00641     -5.90306     -5.91943
Training/qf1_loss                 7131.72803   1129.11122  9112.99219   5907.27051
Training/qf2_loss                 16271.25059  1447.12921  18921.95508  14877.91992
Training/pf_norm                  0.13090      0.02028     0.15388      0.09428
Training/qf1_norm                 395.40124    269.92381   891.22009    160.86377
Training/qf2_norm                 2205.01943   46.82495    2284.56738   2141.12476
log_std/mean                      -0.13737     0.00044     -0.13672     -0.13797
log_probs/mean                    -2.73540     0.01146     -2.71759     -2.75323
mean/mean                         0.00058      0.00019     0.00084      0.00029
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02002263069152832
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70929
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [141900]
collect time 0.0009758472442626953
inner_dict_sum {'sac_diff0': 0.00020599365234375, 'sac_diff1': 0.0069811344146728516, 'sac_diff2': 0.008076190948486328, 'sac_diff3': 0.010761022567749023, 'sac_diff4': 0.007197856903076172, 'sac_diff5': 0.03263974189758301, 'sac_diff6': 0.0004239082336425781, 'all': 0.06628584861755371}
diff5_list [0.006578922271728516, 0.006435871124267578, 0.006220340728759766, 0.006190299987792969, 0.00721430778503418]
time3 0
time4 0.06713175773620605
time5 0.06718087196350098
time7 7.152557373046875e-07
gen_weight_change tensor(-16.9700)
policy weight change tensor(36.4069, grad_fn=<SumBackward0>)
time8 0.002151012420654297
train_time 0.07896542549133301
eval time 0.1656632423400879
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:54,400 MainThread INFO: EPOCH:939
2024-01-23 01:04:54,400 MainThread INFO: Time Consumed:0.24811935424804688s
2024-01-23 01:04:54,401 MainThread INFO: Total Frames:141750s
  9%|▉         | 940/10000 [06:22<40:27,  3.73it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           21572.18784
Train_Epoch_Reward                28381.20006
Running_Training_Average_Rewards  16369.15745
Explore_Time                      0.00097
Train___Time                      0.07897
Eval____Time                      0.16566
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21735.57948
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.67312     1.96390    97.01570     91.52998
alpha_0                           0.62501      0.00009    0.62513      0.62488
Alpha_loss                        -3.16337     0.00638    -3.15622     -3.17288
Training/policy_loss              -5.81067     0.00983    -5.79579     -5.82557
Training/qf1_loss                 7409.30537   652.12552  8063.98047   6442.01855
Training/qf2_loss                 16748.28125  898.46116  17921.64453  15733.13477
Training/pf_norm                  0.18421      0.01620    0.19839      0.15486
Training/qf1_norm                 498.74888    331.71705  1058.36084   159.35739
Training/qf2_norm                 2129.42935   44.24655   2204.44263   2082.21704
log_std/mean                      -0.12351     0.00012    -0.12338     -0.12371
log_probs/mean                    -2.73208     0.01311    -2.71545     -2.74946
mean/mean                         -0.00952     0.00005    -0.00943     -0.00958
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018704652786254883
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70929
epoch first part time 3.337860107421875e-06
replay_buffer._size: [142050]
collect time 0.0009942054748535156
inside mustsac before update, task 0, sumup 70929
inside mustsac after update, task 0, sumup 70176
inner_dict_sum {'sac_diff0': 0.0002295970916748047, 'sac_diff1': 0.00808572769165039, 'sac_diff2': 0.009741067886352539, 'sac_diff3': 0.012777328491210938, 'sac_diff4': 0.008647680282592773, 'sac_diff5': 0.057248830795288086, 'sac_diff6': 0.0004532337188720703, 'all': 0.0971834659576416}
diff5_list [0.012174844741821289, 0.012529611587524414, 0.012059926986694336, 0.010658740997314453, 0.009825706481933594]
time3 0.0009746551513671875
time4 0.09813642501831055
time5 0.09819483757019043
time7 0.008919239044189453
gen_weight_change tensor(-16.9914)
policy weight change tensor(36.4603, grad_fn=<SumBackward0>)
time8 0.002817392349243164
train_time 0.13004755973815918
eval time 0.11601758003234863
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:54,672 MainThread INFO: EPOCH:940
2024-01-23 01:04:54,673 MainThread INFO: Time Consumed:0.24964475631713867s
2024-01-23 01:04:54,673 MainThread INFO: Total Frames:141900s
  9%|▉         | 941/10000 [06:22<40:50,  3.70it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21556.62047
Train_Epoch_Reward                10012.55367
Running_Training_Average_Rewards  16337.05667
Explore_Time                      0.00099
Train___Time                      0.13005
Eval____Time                      0.11602
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21139.48264
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.79892     1.82049     95.00926     90.09545
alpha_0                           0.62469      0.00009     0.62482      0.62457
Alpha_loss                        -3.16465     0.00507     -3.15917     -3.17313
Training/policy_loss              -5.86937     0.32625     -5.55254     -6.49736
Training/qf1_loss                 7067.58193   1124.32086  8877.46973   5804.17334
Training/qf2_loss                 16222.75273  1412.48700  18288.54492  14448.85742
Training/pf_norm                  0.15322      0.00711     0.16535      0.14416
Training/qf1_norm                 949.83268    651.02592   2075.70386   211.73051
Training/qf2_norm                 2159.36475   115.05871   2386.26562   2075.83081
log_std/mean                      -0.13014     0.00604     -0.12057     -0.13676
log_probs/mean                    -2.72765     0.01026     -2.71743     -2.74567
mean/mean                         -0.01137     0.00401     -0.00616     -0.01788
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01997208595275879
epoch last part time3 0.002930879592895508
inside rlalgo, task 0, sumup 70176
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [142200]
collect time 0.0009362697601318359
inner_dict_sum {'sac_diff0': 0.0002238750457763672, 'sac_diff1': 0.007711172103881836, 'sac_diff2': 0.008960247039794922, 'sac_diff3': 0.01252603530883789, 'sac_diff4': 0.007825851440429688, 'sac_diff5': 0.03572368621826172, 'sac_diff6': 0.0004303455352783203, 'all': 0.07340121269226074}
diff5_list [0.007412433624267578, 0.006292819976806641, 0.0070056915283203125, 0.007693052291870117, 0.00731968879699707]
time3 0
time4 0.07429265975952148
time5 0.07434511184692383
time7 1.1920928955078125e-06
gen_weight_change tensor(-16.9914)
policy weight change tensor(36.3894, grad_fn=<SumBackward0>)
time8 0.0020148754119873047
train_time 0.08606719970703125
eval time 0.15673375129699707
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:54,945 MainThread INFO: EPOCH:941
2024-01-23 01:04:54,945 MainThread INFO: Time Consumed:0.24616646766662598s
2024-01-23 01:04:54,945 MainThread INFO: Total Frames:142050s
  9%|▉         | 942/10000 [06:22<40:44,  3.71it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21555.57532
Train_Epoch_Reward                16038.67789
Running_Training_Average_Rewards  16125.21441
Explore_Time                      0.00093
Train___Time                      0.08607
Eval____Time                      0.15673
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20355.89165
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.30081     2.54837     98.44030     90.69599
alpha_0                           0.62438      0.00009     0.62451      0.62426
Alpha_loss                        -3.16894     0.00136     -3.16632     -3.17010
Training/policy_loss              -6.10573     0.00415     -6.10080     -6.11261
Training/qf1_loss                 7079.46113   1209.26181  8931.20703   5205.61523
Training/qf2_loss                 16544.68223  1719.83876  19228.06250  13899.59961
Training/pf_norm                  0.11325      0.02468     0.15012      0.07689
Training/qf1_norm                 506.00811    326.73011   1102.29907   149.75108
Training/qf2_norm                 2328.00361   63.82664    2434.14624   2241.07349
log_std/mean                      -0.13174     0.00005     -0.13169     -0.13182
log_probs/mean                    -2.72963     0.00243     -2.72691     -2.73351
mean/mean                         -0.00609     0.00013     -0.00592     -0.00630
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01880049705505371
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70176
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [142350]
collect time 0.0008585453033447266
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.007570743560791016, 'sac_diff2': 0.009282827377319336, 'sac_diff3': 0.012014627456665039, 'sac_diff4': 0.008051156997680664, 'sac_diff5': 0.037014007568359375, 'sac_diff6': 0.0004432201385498047, 'all': 0.07459425926208496}
diff5_list [0.0067098140716552734, 0.008391618728637695, 0.007727384567260742, 0.007235050201416016, 0.0069501399993896484]
time3 0
time4 0.07547903060913086
time5 0.07553601264953613
time7 7.152557373046875e-07
gen_weight_change tensor(-16.9914)
policy weight change tensor(36.4753, grad_fn=<SumBackward0>)
time8 0.00183868408203125
train_time 0.0868539810180664
eval time 0.1572279930114746
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:55,215 MainThread INFO: EPOCH:942
2024-01-23 01:04:55,215 MainThread INFO: Time Consumed:0.24742341041564941s
2024-01-23 01:04:55,215 MainThread INFO: Total Frames:142200s
  9%|▉         | 943/10000 [06:22<40:53,  3.69it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21468.48689
Train_Epoch_Reward                23000.74199
Running_Training_Average_Rewards  16700.27311
Explore_Time                      0.00085
Train___Time                      0.08685
Eval____Time                      0.15723
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             19894.88615
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.58077     1.42628     94.26543     90.37504
alpha_0                           0.62407      0.00009     0.62419      0.62394
Alpha_loss                        -3.17258     0.00452     -3.16437     -3.17688
Training/policy_loss              -5.81541     0.00668     -5.80452     -5.82317
Training/qf1_loss                 7443.51699   863.78071   8231.08984   5962.66699
Training/qf2_loss                 16548.50742  1109.59750  17647.29297  14844.08008
Training/pf_norm                  0.17425      0.01961     0.19903      0.14753
Training/qf1_norm                 343.72341    137.48411   548.55878    149.34637
Training/qf2_norm                 2143.45752   34.14306    2185.16479   2090.87012
log_std/mean                      -0.11844     0.00025     -0.11818     -0.11884
log_probs/mean                    -2.73021     0.00890     -2.71420     -2.73812
mean/mean                         -0.00967     0.00005     -0.00958     -0.00971
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02225017547607422
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70176
epoch first part time 2.86102294921875e-06
replay_buffer._size: [142500]
collect time 0.0011565685272216797
inner_dict_sum {'sac_diff0': 0.0002257823944091797, 'sac_diff1': 0.007513761520385742, 'sac_diff2': 0.008992910385131836, 'sac_diff3': 0.010681867599487305, 'sac_diff4': 0.007146596908569336, 'sac_diff5': 0.032715797424316406, 'sac_diff6': 0.0003876686096191406, 'all': 0.06766438484191895}
diff5_list [0.007581233978271484, 0.006365776062011719, 0.006289482116699219, 0.0062711238861083984, 0.006208181381225586]
time3 0
time4 0.06845235824584961
time5 0.06850194931030273
time7 4.76837158203125e-07
gen_weight_change tensor(-16.9914)
policy weight change tensor(36.5034, grad_fn=<SumBackward0>)
time8 0.0019106864929199219
train_time 0.08049583435058594
eval time 0.1549232006072998
epoch last part time 7.62939453125e-06
2024-01-23 01:04:55,480 MainThread INFO: EPOCH:943
2024-01-23 01:04:55,480 MainThread INFO: Time Consumed:0.2390122413635254s
2024-01-23 01:04:55,481 MainThread INFO: Total Frames:142350s
  9%|▉         | 944/10000 [06:23<40:28,  3.73it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21296.74580
Train_Epoch_Reward                26841.45702
Running_Training_Average_Rewards  16976.96987
Explore_Time                      0.00115
Train___Time                      0.08050
Eval____Time                      0.15492
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             19483.89358
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       91.33083     1.65207     93.50653     88.97829
alpha_0                           0.62376      0.00009     0.62388      0.62363
Alpha_loss                        -3.17473     0.00361     -3.16860     -3.17942
Training/policy_loss              -5.98593     0.00620     -5.97724     -5.99317
Training/qf1_loss                 6736.79795   787.86192   7733.67969   5710.08789
Training/qf2_loss                 15607.86895  1049.10482  16937.69922  14126.34668
Training/pf_norm                  0.21162      0.02709     0.26253      0.18489
Training/qf1_norm                 374.97880    206.51891   648.68323    127.18719
Training/qf2_norm                 2141.21304   40.06320    2193.68677   2085.57544
log_std/mean                      -0.12522     0.00002     -0.12520     -0.12524
log_probs/mean                    -2.72764     0.00904     -2.71322     -2.73900
mean/mean                         -0.01488     0.00029     -0.01443     -0.01521
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01855325698852539
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70176
epoch first part time 2.86102294921875e-06
replay_buffer._size: [142650]
collect time 0.0009496212005615234
inner_dict_sum {'sac_diff0': 0.00022172927856445312, 'sac_diff1': 0.00696110725402832, 'sac_diff2': 0.008603096008300781, 'sac_diff3': 0.010777711868286133, 'sac_diff4': 0.0072498321533203125, 'sac_diff5': 0.03311586380004883, 'sac_diff6': 0.0004024505615234375, 'all': 0.06733179092407227}
diff5_list [0.006647348403930664, 0.006265401840209961, 0.0066683292388916016, 0.0072329044342041016, 0.0063018798828125]
time3 0
time4 0.0681452751159668
time5 0.06819605827331543
time7 7.152557373046875e-07
gen_weight_change tensor(-16.9914)
policy weight change tensor(36.5487, grad_fn=<SumBackward0>)
time8 0.0018253326416015625
train_time 0.07948040962219238
eval time 0.15113186836242676
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:55,736 MainThread INFO: EPOCH:944
2024-01-23 01:04:55,737 MainThread INFO: Time Consumed:0.23407340049743652s
2024-01-23 01:04:55,737 MainThread INFO: Total Frames:142500s
  9%|▉         | 945/10000 [06:23<40:00,  3.77it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           21044.81267
Train_Epoch_Reward                18685.76738
Running_Training_Average_Rewards  17501.53132
Explore_Time                      0.00095
Train___Time                      0.07948
Eval____Time                      0.15113
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             19154.91033
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.20866     2.60901    94.74198     87.46156
alpha_0                           0.62345      0.00009    0.62357      0.62332
Alpha_loss                        -3.17845     0.00510    -3.16925     -3.18439
Training/policy_loss              -5.69071     0.00529    -5.68156     -5.69689
Training/qf1_loss                 6377.53682   546.15791  7350.86084   5824.98975
Training/qf2_loss                 15464.02676  842.35694  16572.22070  13991.47852
Training/pf_norm                  0.12796      0.01730    0.14875      0.10247
Training/qf1_norm                 437.05566    318.62555  1058.08252   147.86057
Training/qf2_norm                 2108.40183   57.83190   2166.66528   2003.44592
log_std/mean                      -0.12991     0.00013    -0.12974     -0.13009
log_probs/mean                    -2.72839     0.01171    -2.70606     -2.73954
mean/mean                         -0.00524     0.00024    -0.00493     -0.00560
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.02005600929260254
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70176
epoch first part time 3.814697265625e-06
replay_buffer._size: [142800]
collect time 0.0009675025939941406
inside mustsac before update, task 0, sumup 70176
inside mustsac after update, task 0, sumup 71067
inner_dict_sum {'sac_diff0': 0.00022125244140625, 'sac_diff1': 0.007210969924926758, 'sac_diff2': 0.00858449935913086, 'sac_diff3': 0.011079072952270508, 'sac_diff4': 0.007998943328857422, 'sac_diff5': 0.054245710372924805, 'sac_diff6': 0.0004210472106933594, 'all': 0.08976149559020996}
diff5_list [0.010929107666015625, 0.010347366333007812, 0.010243892669677734, 0.01056981086730957, 0.012155532836914062]
time3 0.0008840560913085938
time4 0.09065961837768555
time5 0.09071969985961914
time7 0.009324312210083008
gen_weight_change tensor(-17.1190)
policy weight change tensor(36.6208, grad_fn=<SumBackward0>)
time8 0.001920938491821289
train_time 0.12048459053039551
eval time 0.11170196533203125
epoch last part time 6.67572021484375e-06
2024-01-23 01:04:55,996 MainThread INFO: EPOCH:945
2024-01-23 01:04:55,996 MainThread INFO: Time Consumed:0.23565173149108887s
2024-01-23 01:04:55,997 MainThread INFO: Total Frames:142650s
  9%|▉         | 946/10000 [06:23<39:52,  3.79it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           20742.66333
Train_Epoch_Reward                21613.88982
Running_Training_Average_Rewards  17353.57311
Explore_Time                      0.00096
Train___Time                      0.12048
Eval____Time                      0.11170
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             18891.96557
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.10335     2.49504     96.99290     89.73243
alpha_0                           0.62313      0.00009     0.62326      0.62301
Alpha_loss                        -3.18126     0.00324     -3.17595     -3.18541
Training/policy_loss              -6.10640     0.17535     -5.76654     -6.25818
Training/qf1_loss                 7642.54082   1527.66618  10082.35938  6206.25537
Training/qf2_loss                 17051.26738  1951.91571  20116.91406  14676.03711
Training/pf_norm                  0.16220      0.02026     0.19539      0.14004
Training/qf1_norm                 1140.14019   492.55220   1846.88599   439.32190
Training/qf2_norm                 2322.00039   49.91139    2383.21118   2256.01587
log_std/mean                      -0.13213     0.00317     -0.12820     -0.13740
log_probs/mean                    -2.72723     0.00869     -2.71314     -2.73742
mean/mean                         -0.00897     0.00550     0.00070      -0.01585
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02240300178527832
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71067
epoch first part time 2.86102294921875e-06
replay_buffer._size: [142950]
collect time 0.0009806156158447266
inner_dict_sum {'sac_diff0': 0.00022149085998535156, 'sac_diff1': 0.007669687271118164, 'sac_diff2': 0.00873255729675293, 'sac_diff3': 0.010834455490112305, 'sac_diff4': 0.007094860076904297, 'sac_diff5': 0.033176422119140625, 'sac_diff6': 0.0004019737243652344, 'all': 0.0681314468383789}
diff5_list [0.008121252059936523, 0.0063474178314208984, 0.006309032440185547, 0.00620579719543457, 0.006192922592163086]
time3 0
time4 0.06893563270568848
time5 0.0689859390258789
time7 9.5367431640625e-07
gen_weight_change tensor(-17.1190)
policy weight change tensor(36.7881, grad_fn=<SumBackward0>)
time8 0.001863241195678711
train_time 0.0809776782989502
eval time 0.14791536331176758
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:56,255 MainThread INFO: EPOCH:946
2024-01-23 01:04:56,255 MainThread INFO: Time Consumed:0.23214340209960938s
2024-01-23 01:04:56,255 MainThread INFO: Total Frames:142800s
  9%|▉         | 947/10000 [06:23<39:26,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           20978.88390
Train_Epoch_Reward                25108.80905
Running_Training_Average_Rewards  17845.38934
Explore_Time                      0.00097
Train___Time                      0.08098
Eval____Time                      0.14792
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24564.70983
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.91830     1.91954     98.48138     93.14300
alpha_0                           0.62282      0.00009     0.62295      0.62270
Alpha_loss                        -3.18349     0.00318     -3.18004     -3.18775
Training/policy_loss              -6.61377     0.00212     -6.61021     -6.61657
Training/qf1_loss                 8382.22354   1217.52922  10323.36719  6559.84131
Training/qf2_loss                 18388.43438  1587.42053  20683.03125  15735.21289
Training/pf_norm                  0.13821      0.02100     0.16634      0.10554
Training/qf1_norm                 717.36089    422.11801   1551.39832   388.08182
Training/qf2_norm                 2564.48467   49.55303    2607.09253   2467.78979
log_std/mean                      -0.11801     0.00028     -0.11765     -0.11844
log_probs/mean                    -2.72484     0.00786     -2.71611     -2.73667
mean/mean                         -0.00853     0.00027     -0.00815     -0.00890
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.017658472061157227
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71067
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [143100]
collect time 0.000989675521850586
inner_dict_sum {'sac_diff0': 0.0002155303955078125, 'sac_diff1': 0.007375001907348633, 'sac_diff2': 0.008684635162353516, 'sac_diff3': 0.010735750198364258, 'sac_diff4': 0.007129669189453125, 'sac_diff5': 0.03246307373046875, 'sac_diff6': 0.00040221214294433594, 'all': 0.06700587272644043}
diff5_list [0.0072193145751953125, 0.00640416145324707, 0.0064580440521240234, 0.006247758865356445, 0.0061337947845458984]
time3 0
time4 0.06779932975769043
time5 0.06785058975219727
time7 4.76837158203125e-07
gen_weight_change tensor(-17.1190)
policy weight change tensor(37.0090, grad_fn=<SumBackward0>)
time8 0.001827239990234375
train_time 0.07933592796325684
eval time 0.1429126262664795
epoch last part time 5.7220458984375e-06
2024-01-23 01:04:56,503 MainThread INFO: EPOCH:947
2024-01-23 01:04:56,503 MainThread INFO: Time Consumed:0.22557401657104492s
2024-01-23 01:04:56,503 MainThread INFO: Total Frames:142950s
  9%|▉         | 948/10000 [06:24<38:47,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           21209.15903
Train_Epoch_Reward                12686.16153
Running_Training_Average_Rewards  17354.54344
Explore_Time                      0.00098
Train___Time                      0.07934
Eval____Time                      0.14291
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24657.61840
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.47335     1.24871    94.80378     91.44276
alpha_0                           0.62251      0.00009    0.62264      0.62239
Alpha_loss                        -3.18875     0.00223    -3.18445     -3.19051
Training/policy_loss              -5.82641     0.00450    -5.81942     -5.83324
Training/qf1_loss                 6546.81602   509.69798  7065.45801   5660.54980
Training/qf2_loss                 15631.82324  701.66474  16584.13867  14560.18750
Training/pf_norm                  0.13722      0.02767    0.17253      0.09960
Training/qf1_norm                 242.32692    145.35638  527.83966    140.33459
Training/qf2_norm                 2151.60317   29.19856   2206.79834   2128.41455
log_std/mean                      -0.14023     0.00024    -0.13989     -0.14058
log_probs/mean                    -2.72883     0.00396    -2.72119     -2.73245
mean/mean                         -0.00112     0.00037    -0.00058     -0.00161
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018167495727539062
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71067
epoch first part time 2.86102294921875e-06
replay_buffer._size: [143250]
collect time 0.0009503364562988281
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.0066242218017578125, 'sac_diff2': 0.008133172988891602, 'sac_diff3': 0.010281562805175781, 'sac_diff4': 0.0071523189544677734, 'sac_diff5': 0.03171062469482422, 'sac_diff6': 0.0003809928894042969, 'all': 0.06449699401855469}
diff5_list [0.006598234176635742, 0.006221771240234375, 0.006277561187744141, 0.006547212600708008, 0.006065845489501953]
time3 0
time4 0.06524872779846191
time5 0.06530022621154785
time7 4.76837158203125e-07
gen_weight_change tensor(-17.1190)
policy weight change tensor(37.3229, grad_fn=<SumBackward0>)
time8 0.001811981201171875
train_time 0.0764930248260498
eval time 0.15806221961975098
epoch last part time 7.62939453125e-06
2024-01-23 01:04:56,763 MainThread INFO: EPOCH:948
2024-01-23 01:04:56,763 MainThread INFO: Time Consumed:0.2379164695739746s
2024-01-23 01:04:56,763 MainThread INFO: Total Frames:143100s
  9%|▉         | 949/10000 [06:24<38:55,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21520.85999
Train_Epoch_Reward                36107.83131
Running_Training_Average_Rewards  18234.77921
Explore_Time                      0.00095
Train___Time                      0.07649
Eval____Time                      0.15806
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25329.66224
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.97313     3.12623     97.76012     89.47452
alpha_0                           0.62220      0.00009     0.62233      0.62208
Alpha_loss                        -3.18946     0.00292     -3.18619     -3.19475
Training/policy_loss              -6.42508     0.00623     -6.41595     -6.43455
Training/qf1_loss                 7289.48447   1841.67252  10822.82324  5688.92529
Training/qf2_loss                 16675.09121  2311.30069  20980.49023  14790.70312
Training/pf_norm                  0.15036      0.01068     0.16995      0.13872
Training/qf1_norm                 931.75113    585.36072   1721.39490   132.51634
Training/qf2_norm                 2448.33228   81.87582    2547.95288   2332.90308
log_std/mean                      -0.13607     0.00044     -0.13548     -0.13669
log_probs/mean                    -2.72325     0.00603     -2.71617     -2.73297
mean/mean                         -0.00519     0.00018     -0.00496     -0.00546
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018553972244262695
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71067
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [143400]
collect time 0.0009903907775878906
inner_dict_sum {'sac_diff0': 0.00022029876708984375, 'sac_diff1': 0.00708460807800293, 'sac_diff2': 0.008229970932006836, 'sac_diff3': 0.01059269905090332, 'sac_diff4': 0.007126808166503906, 'sac_diff5': 0.032044172286987305, 'sac_diff6': 0.0003914833068847656, 'all': 0.0656900405883789}
diff5_list [0.006628751754760742, 0.006361722946166992, 0.006727933883666992, 0.006227970123291016, 0.0060977935791015625]
time3 0
time4 0.06647706031799316
time5 0.0665278434753418
time7 4.76837158203125e-07
gen_weight_change tensor(-17.1190)
policy weight change tensor(37.4869, grad_fn=<SumBackward0>)
time8 0.0019545555114746094
train_time 0.07795023918151855
eval time 0.14299273490905762
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:57,009 MainThread INFO: EPOCH:949
2024-01-23 01:04:57,009 MainThread INFO: Time Consumed:0.22423672676086426s
2024-01-23 01:04:57,010 MainThread INFO: Total Frames:143250s
 10%|▉         | 950/10000 [06:24<38:24,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21954.77909
Train_Epoch_Reward                36622.06123
Running_Training_Average_Rewards  18383.64793
Explore_Time                      0.00099
Train___Time                      0.07795
Eval____Time                      0.14299
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             26074.77053
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.57639     3.45558     100.69088    91.25822
alpha_0                           0.62189      0.00009     0.62201      0.62177
Alpha_loss                        -3.19432     0.00085     -3.19318     -3.19543
Training/policy_loss              -5.66945     0.00451     -5.66305     -5.67511
Training/qf1_loss                 7173.35293   1245.57846  9259.13965   5925.36865
Training/qf2_loss                 16695.78652  1858.19398  20019.58984  14775.69531
Training/pf_norm                  0.08969      0.00749     0.10053      0.07774
Training/qf1_norm                 621.06549    330.98091   1241.40051   317.19998
Training/qf2_norm                 2126.94893   76.76185    2262.93506   2052.27466
log_std/mean                      -0.12809     0.00003     -0.12804     -0.12813
log_probs/mean                    -2.72641     0.00297     -2.72260     -2.73158
mean/mean                         -0.00787     0.00004     -0.00784     -0.00794
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018567323684692383
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71067
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [143550]
collect time 0.0008516311645507812
inside mustsac before update, task 0, sumup 71067
inside mustsac after update, task 0, sumup 70378
inner_dict_sum {'sac_diff0': 0.00021505355834960938, 'sac_diff1': 0.006974458694458008, 'sac_diff2': 0.008600234985351562, 'sac_diff3': 0.01096200942993164, 'sac_diff4': 0.00755000114440918, 'sac_diff5': 0.052117109298706055, 'sac_diff6': 0.00042629241943359375, 'all': 0.08684515953063965}
diff5_list [0.010602712631225586, 0.010639667510986328, 0.01056218147277832, 0.010028600692749023, 0.010283946990966797]
time3 0.0009007453918457031
time4 0.08773493766784668
time5 0.08778762817382812
time7 0.008936166763305664
gen_weight_change tensor(-17.3050)
policy weight change tensor(37.5339, grad_fn=<SumBackward0>)
time8 0.002624034881591797
train_time 0.11779499053955078
eval time 0.10442042350769043
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:57,257 MainThread INFO: EPOCH:950
2024-01-23 01:04:57,257 MainThread INFO: Time Consumed:0.22536396980285645s
2024-01-23 01:04:57,257 MainThread INFO: Total Frames:143400s
 10%|▉         | 951/10000 [06:24<38:12,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22490.43889
Train_Epoch_Reward                7526.20645
Running_Training_Average_Rewards  17876.02126
Explore_Time                      0.00085
Train___Time                      0.11779
Eval____Time                      0.10442
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             26496.08057
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.90479     2.03142     97.19926     91.59886
alpha_0                           0.62158      0.00009     0.62170      0.62146
Alpha_loss                        -3.19694     0.00233     -3.19440     -3.20067
Training/policy_loss              -6.24234     0.10697     -6.08687     -6.38335
Training/qf1_loss                 7297.53389   637.74767   8530.99707   6808.43506
Training/qf2_loss                 16660.30234  1023.27207  18570.39453  15699.12988
Training/pf_norm                  0.20289      0.04970     0.25490      0.13052
Training/qf1_norm                 977.28781    800.89244   2074.25000   184.91199
Training/qf2_norm                 2340.78540   81.18956    2487.28027   2263.35205
log_std/mean                      -0.12892     0.00377     -0.12174     -0.13285
log_probs/mean                    -2.72484     0.00672     -2.71809     -2.73554
mean/mean                         -0.00714     0.00099     -0.00598     -0.00866
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018330812454223633
epoch last part time3 0.0027747154235839844
inside rlalgo, task 0, sumup 70378
epoch first part time 2.86102294921875e-06
replay_buffer._size: [143700]
collect time 0.0009615421295166016
inner_dict_sum {'sac_diff0': 0.0002155303955078125, 'sac_diff1': 0.007237911224365234, 'sac_diff2': 0.008753299713134766, 'sac_diff3': 0.010947465896606445, 'sac_diff4': 0.007090091705322266, 'sac_diff5': 0.032888174057006836, 'sac_diff6': 0.00040411949157714844, 'all': 0.06753659248352051}
diff5_list [0.0075283050537109375, 0.006476640701293945, 0.006407499313354492, 0.006103515625, 0.006372213363647461]
time3 0
time4 0.06832695007324219
time5 0.06837630271911621
time7 7.152557373046875e-07
gen_weight_change tensor(-17.3050)
policy weight change tensor(37.7056, grad_fn=<SumBackward0>)
time8 0.0018815994262695312
train_time 0.08004975318908691
eval time 0.14151453971862793
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:57,506 MainThread INFO: EPOCH:951
2024-01-23 01:04:57,506 MainThread INFO: Time Consumed:0.22484540939331055s
2024-01-23 01:04:57,507 MainThread INFO: Total Frames:143550s
 10%|▉         | 952/10000 [06:25<37:54,  3.98it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           22469.21636
Train_Epoch_Reward                24065.63735
Running_Training_Average_Rewards  18639.92298
Explore_Time                      0.00096
Train___Time                      0.08005
Eval____Time                      0.14151
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20143.66641
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.59161     2.22464    95.24638     89.19082
alpha_0                           0.62127      0.00009    0.62139      0.62114
Alpha_loss                        -3.20016     0.00270    -3.19710     -3.20488
Training/policy_loss              -6.23391     0.00504    -6.22820     -6.24283
Training/qf1_loss                 7087.75557   615.45619  7700.89746   6015.02979
Training/qf2_loss                 16326.01641  995.04728  17195.39453  14439.18555
Training/pf_norm                  0.13574      0.01294    0.15352      0.12048
Training/qf1_norm                 1351.10994   400.73497  1632.36560   556.28772
Training/qf2_norm                 2425.63013   57.63364   2465.22876   2311.42358
log_std/mean                      -0.12448     0.00012    -0.12436     -0.12469
log_probs/mean                    -2.72457     0.00532    -2.71672     -2.73164
mean/mean                         -0.00990     0.00014    -0.00973     -0.01012
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01870441436767578
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70378
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [143850]
collect time 0.0008556842803955078
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.007091045379638672, 'sac_diff2': 0.00834345817565918, 'sac_diff3': 0.010604619979858398, 'sac_diff4': 0.0070917606353759766, 'sac_diff5': 0.03179574012756348, 'sac_diff6': 0.00038623809814453125, 'all': 0.06553053855895996}
diff5_list [0.006690025329589844, 0.006377458572387695, 0.006348609924316406, 0.006165981292724609, 0.006213665008544922]
time3 0
time4 0.06631183624267578
time5 0.06635928153991699
time7 4.76837158203125e-07
gen_weight_change tensor(-17.3050)
policy weight change tensor(37.8616, grad_fn=<SumBackward0>)
time8 0.001828908920288086
train_time 0.07774567604064941
eval time 0.15072059631347656
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:57,760 MainThread INFO: EPOCH:952
2024-01-23 01:04:57,760 MainThread INFO: Time Consumed:0.23169469833374023s
2024-01-23 01:04:57,760 MainThread INFO: Total Frames:143700s
 10%|▉         | 953/10000 [06:25<38:00,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22521.48088
Train_Epoch_Reward                12991.14848
Running_Training_Average_Rewards  18784.14173
Explore_Time                      0.00085
Train___Time                      0.07775
Eval____Time                      0.15072
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20417.53129
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       92.98800     1.87261     96.10982     91.28693
alpha_0                           0.62096      0.00009     0.62108      0.62083
Alpha_loss                        -3.20692     0.00556     -3.20172     -3.21771
Training/policy_loss              -5.85246     0.00649     -5.84796     -5.86527
Training/qf1_loss                 6819.14668   723.06585   8019.85400   5995.64697
Training/qf2_loss                 16026.54453  1021.79750  17883.71680  14843.07812
Training/pf_norm                  0.11516      0.02307     0.13570      0.08362
Training/qf1_norm                 657.31095    312.28778   991.22589    188.27077
Training/qf2_norm                 2152.95894   42.97917    2225.92285   2115.45776
log_std/mean                      -0.13849     0.00011     -0.13831     -0.13863
log_probs/mean                    -2.73170     0.01141     -2.72361     -2.75434
mean/mean                         -0.00468     0.00014     -0.00448     -0.00489
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01858353614807129
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70378
epoch first part time 2.86102294921875e-06
replay_buffer._size: [144000]
collect time 0.0009336471557617188
inner_dict_sum {'sac_diff0': 0.00020575523376464844, 'sac_diff1': 0.00684046745300293, 'sac_diff2': 0.008037090301513672, 'sac_diff3': 0.010093450546264648, 'sac_diff4': 0.0069904327392578125, 'sac_diff5': 0.03214287757873535, 'sac_diff6': 0.0003800392150878906, 'all': 0.06469011306762695}
diff5_list [0.0065460205078125, 0.0062062740325927734, 0.006554126739501953, 0.006252765655517578, 0.006583690643310547]
time3 0
time4 0.0654289722442627
time5 0.06547403335571289
time7 4.76837158203125e-07
gen_weight_change tensor(-17.3050)
policy weight change tensor(38.0605, grad_fn=<SumBackward0>)
time8 0.001859426498413086
train_time 0.07651066780090332
eval time 0.1438910961151123
epoch last part time 8.106231689453125e-06
2024-01-23 01:04:58,006 MainThread INFO: EPOCH:953
2024-01-23 01:04:58,006 MainThread INFO: Time Consumed:0.22370338439941406s
2024-01-23 01:04:58,006 MainThread INFO: Total Frames:143850s
 10%|▉         | 954/10000 [06:25<37:43,  4.00it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           22630.51482
Train_Epoch_Reward                21078.97521
Running_Training_Average_Rewards  19237.91462
Explore_Time                      0.00093
Train___Time                      0.07651
Eval____Time                      0.14389
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20574.23304
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       91.05009     1.56897    93.04588     89.35239
alpha_0                           0.62065      0.00009    0.62077      0.62052
Alpha_loss                        -3.20692     0.00303    -3.20100     -3.20921
Training/policy_loss              -6.54844     0.00722    -6.53878     -6.55987
Training/qf1_loss                 6805.40820   554.52365  7781.76660   6157.64502
Training/qf2_loss                 15609.53730  730.86328  16545.60547  14584.28613
Training/pf_norm                  0.21718      0.01812    0.25038      0.19637
Training/qf1_norm                 1851.29155   311.11008  2213.94092   1390.01758
Training/qf2_norm                 2488.40840   43.50596   2547.85815   2439.02686
log_std/mean                      -0.12551     0.00016    -0.12530     -0.12574
log_probs/mean                    -2.72464     0.00786    -2.70942     -2.73227
mean/mean                         -0.01108     0.00038    -0.01062     -0.01168
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.0185546875
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70378
epoch first part time 2.86102294921875e-06
replay_buffer._size: [144150]
collect time 0.0009000301361083984
inner_dict_sum {'sac_diff0': 0.00020432472229003906, 'sac_diff1': 0.006654262542724609, 'sac_diff2': 0.007921934127807617, 'sac_diff3': 0.00956583023071289, 'sac_diff4': 0.0066411495208740234, 'sac_diff5': 0.031479597091674805, 'sac_diff6': 0.000377655029296875, 'all': 0.06284475326538086}
diff5_list [0.006296396255493164, 0.0061299800872802734, 0.006158113479614258, 0.006692171096801758, 0.0062029361724853516]
time3 0
time4 0.06360650062561035
time5 0.06365156173706055
time7 4.76837158203125e-07
gen_weight_change tensor(-17.3050)
policy weight change tensor(38.1504, grad_fn=<SumBackward0>)
time8 0.0018458366394042969
train_time 0.07468533515930176
eval time 0.15087246894836426
epoch last part time 7.152557373046875e-06
2024-01-23 01:04:58,256 MainThread INFO: EPOCH:954
2024-01-23 01:04:58,257 MainThread INFO: Time Consumed:0.22872376441955566s
2024-01-23 01:04:58,257 MainThread INFO: Total Frames:144000s
 10%|▉         | 955/10000 [06:25<37:44,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           22762.90904
Train_Epoch_Reward                13794.64767
Running_Training_Average_Rewards  19340.94639
Explore_Time                      0.00090
Train___Time                      0.07469
Eval____Time                      0.15087
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20478.85251
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       94.45113     1.22464    95.85045     92.64079
alpha_0                           0.62034      0.00009    0.62046      0.62021
Alpha_loss                        -3.21162     0.00439    -3.20528     -3.21742
Training/policy_loss              -6.55468     0.00501    -6.54568     -6.55909
Training/qf1_loss                 7283.01592   644.91838  8175.15088   6336.00879
Training/qf2_loss                 16756.18203  793.88200  17886.78516  15761.78906
Training/pf_norm                  0.14739      0.04349    0.23254      0.11473
Training/qf1_norm                 477.19090    210.55788  731.82214    167.19505
Training/qf2_norm                 2466.46372   32.36999   2506.41724   2420.01782
log_std/mean                      -0.13454     0.00005    -0.13446     -0.13460
log_probs/mean                    -2.72744     0.00824    -2.71699     -2.73677
mean/mean                         -0.00471     0.00019    -0.00442     -0.00496
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018568992614746094
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70378
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [144300]
collect time 0.0009374618530273438
inside mustsac before update, task 0, sumup 70378
inside mustsac after update, task 0, sumup 69955
inner_dict_sum {'sac_diff0': 0.0002262592315673828, 'sac_diff1': 0.007432699203491211, 'sac_diff2': 0.008862972259521484, 'sac_diff3': 0.011153221130371094, 'sac_diff4': 0.007658481597900391, 'sac_diff5': 0.05476045608520508, 'sac_diff6': 0.0004296302795410156, 'all': 0.09052371978759766}
diff5_list [0.011165857315063477, 0.011495828628540039, 0.01173257827758789, 0.010298728942871094, 0.010067462921142578]
time3 0.0009367465972900391
time4 0.09143280982971191
time5 0.09148693084716797
time7 0.009108543395996094
gen_weight_change tensor(-17.3682)
policy weight change tensor(38.2156, grad_fn=<SumBackward0>)
time8 0.001911163330078125
train_time 0.12127542495727539
eval time 0.10489082336425781
epoch last part time 5.0067901611328125e-06
2024-01-23 01:04:58,508 MainThread INFO: EPOCH:955
2024-01-23 01:04:58,508 MainThread INFO: Time Consumed:0.2294604778289795s
2024-01-23 01:04:58,509 MainThread INFO: Total Frames:144150s
 10%|▉         | 956/10000 [06:26<37:48,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           22886.56967
Train_Epoch_Reward                19223.43329
Running_Training_Average_Rewards  19469.78842
Explore_Time                      0.00093
Train___Time                      0.12128
Eval____Time                      0.10489
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20128.57190
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.09712     1.07228    94.56047     91.53466
alpha_0                           0.62003      0.00009    0.62015      0.61990
Alpha_loss                        -3.21442     0.00356    -3.20970     -3.22030
Training/policy_loss              -6.16319     0.26945    -5.78424     -6.54842
Training/qf1_loss                 6823.06836   760.15009  7892.03369   5738.64941
Training/qf2_loss                 16054.29570  972.63929  17439.09766  14602.90430
Training/pf_norm                  0.18283      0.04707    0.24353      0.10874
Training/qf1_norm                 662.07714    426.56700  1380.73608   147.67802
Training/qf2_norm                 2307.10703   152.30427  2527.22998   2086.62354
log_std/mean                      -0.13248     0.00578    -0.12403     -0.13872
log_probs/mean                    -2.72626     0.00690    -2.71921     -2.73857
mean/mean                         -0.00891     0.00402    -0.00204     -0.01366
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018703937530517578
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69955
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [144450]
collect time 0.0009987354278564453
inner_dict_sum {'sac_diff0': 0.00021648406982421875, 'sac_diff1': 0.007000446319580078, 'sac_diff2': 0.00823831558227539, 'sac_diff3': 0.010762691497802734, 'sac_diff4': 0.007243633270263672, 'sac_diff5': 0.032518863677978516, 'sac_diff6': 0.00041604042053222656, 'all': 0.06639647483825684}
diff5_list [0.006762266159057617, 0.006354808807373047, 0.006363391876220703, 0.00663447380065918, 0.006403923034667969]
time3 0
time4 0.0672142505645752
time5 0.06726670265197754
time7 4.76837158203125e-07
gen_weight_change tensor(-17.3682)
policy weight change tensor(38.1559, grad_fn=<SumBackward0>)
time8 0.0019779205322265625
train_time 0.07867097854614258
eval time 0.14717721939086914
epoch last part time 6.198883056640625e-06
2024-01-23 01:04:58,760 MainThread INFO: EPOCH:956
2024-01-23 01:04:58,760 MainThread INFO: Time Consumed:0.22923946380615234s
2024-01-23 01:04:58,760 MainThread INFO: Total Frames:144300s
 10%|▉         | 957/10000 [06:26<37:49,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22814.11018
Train_Epoch_Reward                8918.02288
Running_Training_Average_Rewards  19014.30020
Explore_Time                      0.00099
Train___Time                      0.07867
Eval____Time                      0.14718
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23840.11490
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.16506     2.85592     99.59727     91.82691
alpha_0                           0.61972      0.00009     0.61984      0.61959
Alpha_loss                        -3.21485     0.00257     -3.21164     -3.21852
Training/policy_loss              -6.17414     0.00240     -6.17027     -6.17742
Training/qf1_loss                 7615.06152   1488.85294  10571.00391  6597.58691
Training/qf2_loss                 17080.20566  2050.36603  21123.17578  15631.89844
Training/pf_norm                  0.13481      0.02516     0.16533      0.10538
Training/qf1_norm                 564.04489    526.31087   1569.83362   165.41588
Training/qf2_norm                 2347.30342   71.82181    2484.02612   2289.54443
log_std/mean                      -0.12921     0.00005     -0.12915     -0.12927
log_probs/mean                    -2.72016     0.00468     -2.71470     -2.72540
mean/mean                         -0.01275     0.00004     -0.01267     -0.01279
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018415212631225586
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69955
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [144600]
collect time 0.0009641647338867188
inner_dict_sum {'sac_diff0': 0.0002079010009765625, 'sac_diff1': 0.006760358810424805, 'sac_diff2': 0.007871389389038086, 'sac_diff3': 0.010264158248901367, 'sac_diff4': 0.0069179534912109375, 'sac_diff5': 0.031156063079833984, 'sac_diff6': 0.0003879070281982422, 'all': 0.06356573104858398}
diff5_list [0.00646662712097168, 0.005980014801025391, 0.006413698196411133, 0.0062673091888427734, 0.006028413772583008]
time3 0
time4 0.06431245803833008
time5 0.06435585021972656
time7 4.76837158203125e-07
gen_weight_change tensor(-17.3682)
policy weight change tensor(37.9183, grad_fn=<SumBackward0>)
time8 0.0018377304077148438
train_time 0.07571053504943848
eval time 0.15361618995666504
epoch last part time 5.4836273193359375e-06
2024-01-23 01:04:59,014 MainThread INFO: EPOCH:957
2024-01-23 01:04:59,015 MainThread INFO: Time Consumed:0.23254871368408203s
2024-01-23 01:04:59,015 MainThread INFO: Total Frames:144450s
 10%|▉         | 958/10000 [06:26<38:07,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22674.09915
Train_Epoch_Reward                5129.73426
Running_Training_Average_Rewards  18583.81539
Explore_Time                      0.00096
Train___Time                      0.07571
Eval____Time                      0.15362
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23257.50817
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.48504     2.59999     97.56595     89.55572
alpha_0                           0.61941      0.00009     0.61953      0.61929
Alpha_loss                        -3.22217     0.00477     -3.21580     -3.23032
Training/policy_loss              -5.64421     0.00557     -5.63980     -5.65515
Training/qf1_loss                 6669.51475   820.79734   7792.84668   5236.33643
Training/qf2_loss                 15811.37441  1264.89825  17655.42773  13693.94043
Training/pf_norm                  0.12375      0.02100     0.15257      0.09447
Training/qf1_norm                 2380.12185   459.18903   3102.01343   1678.59119
Training/qf2_norm                 2094.97971   58.55620    2186.64648   2006.17468
log_std/mean                      -0.14518     0.00042     -0.14455     -0.14569
log_probs/mean                    -2.72843     0.00942     -2.71512     -2.74404
mean/mean                         -0.01490     0.00009     -0.01477     -0.01501
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021121978759765625
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 69955
epoch first part time 2.86102294921875e-06
replay_buffer._size: [144750]
collect time 0.000997781753540039
inner_dict_sum {'sac_diff0': 0.00021338462829589844, 'sac_diff1': 0.00799107551574707, 'sac_diff2': 0.008863687515258789, 'sac_diff3': 0.011249065399169922, 'sac_diff4': 0.0074846744537353516, 'sac_diff5': 0.03463602066040039, 'sac_diff6': 0.0004303455352783203, 'all': 0.07086825370788574}
diff5_list [0.007238864898681641, 0.0066165924072265625, 0.007676601409912109, 0.006818056106567383, 0.006285905838012695]
time3 0
time4 0.07167530059814453
time5 0.07172393798828125
time7 1.1920928955078125e-06
gen_weight_change tensor(-17.3682)
policy weight change tensor(37.6904, grad_fn=<SumBackward0>)
time8 0.0018787384033203125
train_time 0.08367013931274414
eval time 0.1558380126953125
epoch last part time 5.4836273193359375e-06
2024-01-23 01:04:59,283 MainThread INFO: EPOCH:958
2024-01-23 01:04:59,283 MainThread INFO: Time Consumed:0.2428429126739502s
2024-01-23 01:04:59,283 MainThread INFO: Total Frames:144600s
 10%|▉         | 959/10000 [06:26<38:40,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22417.36624
Train_Epoch_Reward                17487.49282
Running_Training_Average_Rewards  18567.69039
Explore_Time                      0.00099
Train___Time                      0.08367
Eval____Time                      0.15584
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22762.33308
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.79191     4.09277     100.11907    87.55488
alpha_0                           0.61910      0.00009     0.61922      0.61898
Alpha_loss                        -3.22410     0.00165     -3.22215     -3.22693
Training/policy_loss              -6.35831     0.00141     -6.35554     -6.35942
Training/qf1_loss                 7253.02129   1735.10221  10134.25977  5004.66504
Training/qf2_loss                 16831.44453  2510.22253  20797.19141  13156.17188
Training/pf_norm                  0.12984      0.02048     0.16450      0.10639
Training/qf1_norm                 820.53516    414.09877   1540.82703   312.97534
Training/qf2_norm                 2434.73354   104.71087   2570.83447   2249.50830
log_std/mean                      -0.12996     0.00020     -0.12969     -0.13024
log_probs/mean                    -2.72543     0.00496     -2.71996     -2.73415
mean/mean                         -0.01166     0.00015     -0.01142     -0.01186
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018582582473754883
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 69955
epoch first part time 2.1457672119140625e-06
replay_buffer._size: [144900]
collect time 0.0008907318115234375
inner_dict_sum {'sac_diff0': 0.00021648406982421875, 'sac_diff1': 0.006547451019287109, 'sac_diff2': 0.00781559944152832, 'sac_diff3': 0.010198354721069336, 'sac_diff4': 0.006752967834472656, 'sac_diff5': 0.03127241134643555, 'sac_diff6': 0.0003981590270996094, 'all': 0.0632014274597168}
diff5_list [0.0064945220947265625, 0.006218671798706055, 0.006174564361572266, 0.006247520446777344, 0.00613713264465332]
time3 0
time4 0.06394290924072266
time5 0.06398820877075195
time7 7.152557373046875e-07
gen_weight_change tensor(-17.3682)
policy weight change tensor(37.4555, grad_fn=<SumBackward0>)
time8 0.0019423961639404297
train_time 0.07522344589233398
eval time 0.149277925491333
epoch last part time 5.9604644775390625e-06
2024-01-23 01:04:59,533 MainThread INFO: EPOCH:959
2024-01-23 01:04:59,533 MainThread INFO: Time Consumed:0.22780370712280273s
2024-01-23 01:04:59,533 MainThread INFO: Total Frames:144750s
 10%|▉         | 960/10000 [06:27<38:21,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22066.84836
Train_Epoch_Reward                11536.47819
Running_Training_Average_Rewards  18697.44470
Explore_Time                      0.00089
Train___Time                      0.07522
Eval____Time                      0.14928
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22569.59173
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.81333     2.37896     96.68677     89.73544
alpha_0                           0.61879      0.00009     0.61891      0.61867
Alpha_loss                        -3.22940     0.00355     -3.22352     -3.23286
Training/policy_loss              -6.18842     0.00376     -6.18150     -6.19199
Training/qf1_loss                 7511.70547   1557.95834  10608.15625  6557.83447
Training/qf2_loss                 16863.24512  1949.03030  20616.17773  15065.59961
Training/pf_norm                  0.13019      0.01047     0.13989      0.11122
Training/qf1_norm                 990.22812    479.91145   1775.51929   358.42697
Training/qf2_norm                 2312.89639   59.28125    2384.46680   2211.56641
log_std/mean                      -0.14013     0.00030     -0.13963     -0.14047
log_probs/mean                    -2.72947     0.00681     -2.72003     -2.73668
mean/mean                         -0.00509     0.00006     -0.00503     -0.00519
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018494129180908203
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69955
epoch first part time 2.86102294921875e-06
replay_buffer._size: [145050]
collect time 0.0008156299591064453
inside mustsac before update, task 0, sumup 69955
inside mustsac after update, task 0, sumup 71453
inner_dict_sum {'sac_diff0': 0.00020623207092285156, 'sac_diff1': 0.006879329681396484, 'sac_diff2': 0.008020401000976562, 'sac_diff3': 0.009949207305908203, 'sac_diff4': 0.007323265075683594, 'sac_diff5': 0.04993939399719238, 'sac_diff6': 0.00039267539978027344, 'all': 0.08271050453186035}
diff5_list [0.010606050491333008, 0.009812593460083008, 0.010128259658813477, 0.009772777557373047, 0.009619712829589844]
time3 0.0008509159088134766
time4 0.08353853225708008
time5 0.0835881233215332
time7 0.009552478790283203
gen_weight_change tensor(-17.3634)
policy weight change tensor(37.5066, grad_fn=<SumBackward0>)
time8 0.002598285675048828
train_time 0.11368703842163086
eval time 0.11216950416564941
epoch last part time 6.4373016357421875e-06
2024-01-23 01:04:59,783 MainThread INFO: EPOCH:960
2024-01-23 01:04:59,784 MainThread INFO: Time Consumed:0.22899127006530762s
2024-01-23 01:04:59,784 MainThread INFO: Total Frames:144900s
 10%|▉         | 961/10000 [06:27<38:21,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21676.28799
Train_Epoch_Reward                14109.55314
Running_Training_Average_Rewards  18410.62484
Explore_Time                      0.00081
Train___Time                      0.11369
Eval____Time                      0.11217
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22590.47693
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.06315     1.80663     98.06855     92.82587
alpha_0                           0.61848      0.00009     0.61861      0.61836
Alpha_loss                        -3.23239     0.00209     -3.22986     -3.23496
Training/policy_loss              -6.27777     0.21006     -5.89212     -6.48247
Training/qf1_loss                 7459.05625   1189.81366  9654.81934   6157.20410
Training/qf2_loss                 17045.38809  1518.42574  19933.44141  15456.55176
Training/pf_norm                  0.11777      0.02671     0.15437      0.08158
Training/qf1_norm                 961.68301    683.18027   1819.14880   161.45181
Training/qf2_norm                 2409.70044   60.68838    2503.76904   2319.79517
log_std/mean                      -0.13114     0.00287     -0.12699     -0.13580
log_probs/mean                    -2.72871     0.00338     -2.72372     -2.73311
mean/mean                         -0.00980     0.00274     -0.00543     -0.01284
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01911640167236328
epoch last part time3 0.002758026123046875
inside rlalgo, task 0, sumup 71453
epoch first part time 2.86102294921875e-06
replay_buffer._size: [145200]
collect time 0.0008516311645507812
inner_dict_sum {'sac_diff0': 0.0002014636993408203, 'sac_diff1': 0.00677037239074707, 'sac_diff2': 0.007811546325683594, 'sac_diff3': 0.009753704071044922, 'sac_diff4': 0.006638526916503906, 'sac_diff5': 0.031774044036865234, 'sac_diff6': 0.00038123130798339844, 'all': 0.06333088874816895}
diff5_list [0.006574869155883789, 0.006201505661010742, 0.0063169002532958984, 0.0066721439361572266, 0.006008625030517578]
time3 0
time4 0.06408095359802246
time5 0.06412625312805176
time7 7.152557373046875e-07
gen_weight_change tensor(-17.3634)
policy weight change tensor(37.3088, grad_fn=<SumBackward0>)
time8 0.0018787384033203125
train_time 0.07531976699829102
eval time 0.1422712802886963
epoch last part time 4.76837158203125e-06
2024-01-23 01:05:00,029 MainThread INFO: EPOCH:961
2024-01-23 01:05:00,030 MainThread INFO: Time Consumed:0.22071409225463867s
2024-01-23 01:05:00,030 MainThread INFO: Total Frames:145050s
 10%|▉         | 962/10000 [06:27<37:48,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21975.43062
Train_Epoch_Reward                29285.29964
Running_Training_Average_Rewards  19203.73498
Explore_Time                      0.00085
Train___Time                      0.07532
Eval____Time                      0.14227
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23135.09262
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.12543     1.89913     96.17525     90.71919
alpha_0                           0.61817      0.00009     0.61830      0.61805
Alpha_loss                        -3.23546     0.00270     -3.23226     -3.23988
Training/policy_loss              -6.37520     0.00416     -6.36949     -6.38128
Training/qf1_loss                 7083.24307   1475.80308  9565.58105   5550.07129
Training/qf2_loss                 16426.89492  1804.96481  19341.81641  14221.78711
Training/pf_norm                  0.09671      0.01711     0.11881      0.07258
Training/qf1_norm                 795.03145    360.56722   1241.81982   176.94057
Training/qf2_norm                 2411.43843   47.96682    2464.38867   2325.19360
log_std/mean                      -0.12943     0.00024     -0.12914     -0.12979
log_probs/mean                    -2.72810     0.00593     -2.72175     -2.73868
mean/mean                         -0.01469     0.00008     -0.01457     -0.01480
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01891636848449707
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71453
epoch first part time 2.86102294921875e-06
replay_buffer._size: [145350]
collect time 0.0008864402770996094
inner_dict_sum {'sac_diff0': 0.0002200603485107422, 'sac_diff1': 0.007462024688720703, 'sac_diff2': 0.008998870849609375, 'sac_diff3': 0.010735750198364258, 'sac_diff4': 0.007272243499755859, 'sac_diff5': 0.03270864486694336, 'sac_diff6': 0.0004019737243652344, 'all': 0.06779956817626953}
diff5_list [0.0070497989654541016, 0.006869316101074219, 0.0064411163330078125, 0.006234645843505859, 0.006113767623901367]
time3 0
time4 0.06862449645996094
time5 0.0686798095703125
time7 7.152557373046875e-07
gen_weight_change tensor(-17.3634)
policy weight change tensor(37.3568, grad_fn=<SumBackward0>)
time8 0.0019168853759765625
train_time 0.08115506172180176
eval time 0.3751852512359619
epoch last part time 7.867813110351562e-06
2024-01-23 01:05:00,512 MainThread INFO: EPOCH:962
2024-01-23 01:05:00,512 MainThread INFO: Time Consumed:0.45983409881591797s
2024-01-23 01:05:00,512 MainThread INFO: Total Frames:145200s
 10%|▉         | 963/10000 [06:28<48:24,  3.11it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           22243.35486
Train_Epoch_Reward                16151.20076
Running_Training_Average_Rewards  18740.13163
Explore_Time                      0.00088
Train___Time                      0.08116
Eval____Time                      0.37519
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23096.77377
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.44955     1.88846    95.73521     90.04620
alpha_0                           0.61786      0.00009    0.61799      0.61774
Alpha_loss                        -3.23871     0.00473    -3.23366     -3.24754
Training/policy_loss              -6.75861     0.00439    -6.75357     -6.76366
Training/qf1_loss                 6494.27549   663.68470  7551.71387   5860.79980
Training/qf2_loss                 15805.36875  832.73828  16960.13086  14743.60938
Training/pf_norm                  0.17859      0.02392    0.21761      0.14347
Training/qf1_norm                 736.14507    352.96466  1365.74182   364.71115
Training/qf2_norm                 2490.16704   53.43359   2554.97314   2393.07788
log_std/mean                      -0.13453     0.00004    -0.13448     -0.13459
log_probs/mean                    -2.72786     0.00915    -2.71597     -2.74340
mean/mean                         -0.01366     0.00005    -0.01361     -0.01375
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.022232532501220703
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71453
epoch first part time 3.814697265625e-06
replay_buffer._size: [145500]
collect time 0.0009822845458984375
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.007321596145629883, 'sac_diff2': 0.009371519088745117, 'sac_diff3': 0.010531425476074219, 'sac_diff4': 0.0076138973236083984, 'sac_diff5': 0.033174991607666016, 'sac_diff6': 0.0004165172576904297, 'all': 0.06864285469055176}
diff5_list [0.007211446762084961, 0.00641942024230957, 0.006402015686035156, 0.0067331790924072266, 0.0064089298248291016]
time3 0
time4 0.06953096389770508
time5 0.06959652900695801
time7 9.5367431640625e-07
gen_weight_change tensor(-17.3634)
policy weight change tensor(37.6268, grad_fn=<SumBackward0>)
time8 0.0020265579223632812
train_time 0.08205866813659668
eval time 0.20790457725524902
epoch last part time 1.52587890625e-05
2024-01-23 01:05:00,831 MainThread INFO: EPOCH:963
2024-01-23 01:05:00,832 MainThread INFO: Time Consumed:0.29364895820617676s
2024-01-23 01:05:00,832 MainThread INFO: Total Frames:145350s
 10%|▉         | 964/10000 [06:28<48:21,  3.11it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22492.34292
Train_Epoch_Reward                15353.27865
Running_Training_Average_Rewards  18907.76728
Explore_Time                      0.00098
Train___Time                      0.08206
Eval____Time                      0.20790
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23064.11358
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.60056     2.30861     99.42883     93.02470
alpha_0                           0.61755      0.00009     0.61768      0.61743
Alpha_loss                        -3.24086     0.00446     -3.23552     -3.24854
Training/policy_loss              -6.21727     0.00482     -6.21292     -6.22543
Training/qf1_loss                 7726.07803   1027.23604  9167.46094   6785.54150
Training/qf2_loss                 17512.24238  1506.99030  19769.36133  16026.08301
Training/pf_norm                  0.19091      0.01929     0.21955      0.17123
Training/qf1_norm                 536.27078    213.94071   823.96619    180.63667
Training/qf2_norm                 2429.46777   59.04434    2531.16675   2366.28101
log_std/mean                      -0.13199     0.00018     -0.13180     -0.13230
log_probs/mean                    -2.72535     0.01057     -2.71148     -2.74268
mean/mean                         -0.00636     0.00008     -0.00628     -0.00650
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.023096561431884766
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71453
epoch first part time 3.337860107421875e-06
replay_buffer._size: [145525]
collect time 0.0009913444519042969
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.007213115692138672, 'sac_diff2': 0.008519649505615234, 'sac_diff3': 0.010562658309936523, 'sac_diff4': 0.007355928421020508, 'sac_diff5': 0.03234410285949707, 'sac_diff6': 0.0004100799560546875, 'all': 0.06661629676818848}
diff5_list [0.006943464279174805, 0.006433248519897461, 0.006265401840209961, 0.006190299987792969, 0.006511688232421875]
time3 0
time4 0.06740736961364746
time5 0.06745696067810059
time7 7.152557373046875e-07
gen_weight_change tensor(-17.3634)
policy weight change tensor(38.0280, grad_fn=<SumBackward0>)
time8 0.0018916130065917969
train_time 0.0796198844909668
eval time 0.1587212085723877
epoch last part time 5.245208740234375e-06
2024-01-23 01:05:01,100 MainThread INFO: EPOCH:964
2024-01-23 01:05:01,100 MainThread INFO: Time Consumed:0.2416839599609375s
2024-01-23 01:05:01,100 MainThread INFO: Total Frames:145500s
 10%|▉         | 965/10000 [06:28<45:54,  3.28it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22784.30992
Train_Epoch_Reward                66049.24406
Running_Training_Average_Rewards  20736.64294
Explore_Time                      0.00099
Train___Time                      0.07962
Eval____Time                      0.15872
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23398.52249
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       90.82474     2.43960     94.25491     87.63847
alpha_0                           0.61725      0.00009     0.61737      0.61712
Alpha_loss                        -3.24547     0.00147     -3.24344     -3.24801
Training/policy_loss              -6.36923     0.00581     -6.36233     -6.37674
Training/qf1_loss                 6629.55684   740.07727   7669.32178   5685.18848
Training/qf2_loss                 15369.62988  1207.40971  17129.94922  14156.20605
Training/pf_norm                  0.23462      0.02607     0.27291      0.19575
Training/qf1_norm                 1200.38627   473.16860   1766.33960   524.96832
Training/qf2_norm                 2278.06738   60.76415    2363.17456   2200.53345
log_std/mean                      -0.13693     0.00029     -0.13653     -0.13734
log_probs/mean                    -2.72793     0.00471     -2.72233     -2.73599
mean/mean                         -0.00989     0.00014     -0.00971     -0.01011
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0218353271484375
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71453
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [145800]
collect time 0.0009531974792480469
inside mustsac before update, task 0, sumup 71453
inside mustsac after update, task 0, sumup 70636
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.007454872131347656, 'sac_diff2': 0.009570121765136719, 'sac_diff3': 0.011910438537597656, 'sac_diff4': 0.008052349090576172, 'sac_diff5': 0.05448794364929199, 'sac_diff6': 0.00042700767517089844, 'all': 0.0921168327331543}
diff5_list [0.012229204177856445, 0.011026144027709961, 0.010574102401733398, 0.010261297225952148, 0.010397195816040039]
time3 0.0009279251098632812
time4 0.0930328369140625
time5 0.09309196472167969
time7 0.010636091232299805
gen_weight_change tensor(-17.2580)
policy weight change tensor(38.0492, grad_fn=<SumBackward0>)
time8 0.002336263656616211
train_time 0.1255323886871338
eval time 0.11349272727966309
epoch last part time 5.4836273193359375e-06
2024-01-23 01:05:01,367 MainThread INFO: EPOCH:965
2024-01-23 01:05:01,368 MainThread INFO: Time Consumed:0.24228429794311523s
2024-01-23 01:05:01,368 MainThread INFO: Total Frames:145650s
 10%|▉         | 966/10000 [06:28<44:04,  3.42it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23158.42421
Train_Epoch_Reward                19806.28082
Running_Training_Average_Rewards  20936.39288
Explore_Time                      0.00095
Train___Time                      0.12553
Eval____Time                      0.11349
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23869.71486
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.62109     1.56276     97.22859     93.37677
alpha_0                           0.61694      0.00009     0.61706      0.61681
Alpha_loss                        -3.25180     0.00414     -3.24517     -3.25764
Training/policy_loss              -6.05852     0.37116     -5.60360     -6.45763
Training/qf1_loss                 7602.48955   1127.97359  9063.34180   6645.62012
Training/qf2_loss                 17336.09512  1412.83945  19080.29297  15964.72168
Training/pf_norm                  0.15009      0.04931     0.21940      0.08538
Training/qf1_norm                 982.26651    504.27738   1583.48218   322.61050
Training/qf2_norm                 2317.51445   114.84891   2425.52295   2121.15527
log_std/mean                      -0.13056     0.00638     -0.12305     -0.14036
log_probs/mean                    -2.73407     0.00765     -2.72035     -2.74338
mean/mean                         -0.00722     0.00225     -0.00307     -0.00932
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01837301254272461
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70636
epoch first part time 2.86102294921875e-06
replay_buffer._size: [145950]
collect time 0.0009591579437255859
inner_dict_sum {'sac_diff0': 0.00019741058349609375, 'sac_diff1': 0.0065724849700927734, 'sac_diff2': 0.007978200912475586, 'sac_diff3': 0.009827136993408203, 'sac_diff4': 0.006796121597290039, 'sac_diff5': 0.030946969985961914, 'sac_diff6': 0.00035858154296875, 'all': 0.06267690658569336}
diff5_list [0.0070459842681884766, 0.005907773971557617, 0.006334543228149414, 0.005742788314819336, 0.00591588020324707]
time3 0
time4 0.06340384483337402
time5 0.0634469985961914
time7 7.152557373046875e-07
gen_weight_change tensor(-17.2580)
policy weight change tensor(38.4517, grad_fn=<SumBackward0>)
time8 0.0018343925476074219
train_time 0.07498526573181152
eval time 0.16076087951660156
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:01,628 MainThread INFO: EPOCH:966
2024-01-23 01:05:01,629 MainThread INFO: Time Consumed:0.23910164833068848s
2024-01-23 01:05:01,629 MainThread INFO: Total Frames:145800s
 10%|▉         | 967/10000 [06:29<42:44,  3.52it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           23143.47995
Train_Epoch_Reward                7242.13114
Running_Training_Average_Rewards  19624.78258
Explore_Time                      0.00096
Train___Time                      0.07499
Eval____Time                      0.16076
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23690.67229
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.29082     1.76120    95.82540     90.55168
alpha_0                           0.61663      0.00009    0.61675      0.61651
Alpha_loss                        -3.25201     0.00435    -3.24767     -3.26017
Training/policy_loss              -6.11651     0.00623    -6.10979     -6.12832
Training/qf1_loss                 7367.75303   656.08257  8290.04883   6235.13818
Training/qf2_loss                 16610.51562  981.60178  18007.65625  14946.63281
Training/pf_norm                  0.17926      0.02325    0.21807      0.15478
Training/qf1_norm                 1266.12662   333.96940  1744.79639   737.24200
Training/qf2_norm                 2326.53999   46.50136   2391.61646   2251.52417
log_std/mean                      -0.13413     0.00028    -0.13375     -0.13455
log_probs/mean                    -2.72756     0.00771    -2.71997     -2.74164
mean/mean                         -0.01119     0.00006    -0.01108     -0.01125
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.020651817321777344
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70636
epoch first part time 2.86102294921875e-06
replay_buffer._size: [146100]
collect time 0.0009891986846923828
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.007462024688720703, 'sac_diff2': 0.00844120979309082, 'sac_diff3': 0.01105952262878418, 'sac_diff4': 0.0072705745697021484, 'sac_diff5': 0.033086299896240234, 'sac_diff6': 0.0003857612609863281, 'all': 0.06791281700134277}
diff5_list [0.007323265075683594, 0.007021665573120117, 0.006250143051147461, 0.006247997283935547, 0.006243228912353516]
time3 0
time4 0.06867337226867676
time5 0.06871676445007324
time7 4.76837158203125e-07
gen_weight_change tensor(-17.2580)
policy weight change tensor(38.5953, grad_fn=<SumBackward0>)
time8 0.0018928050994873047
train_time 0.08043217658996582
eval time 0.1516251564025879
epoch last part time 5.4836273193359375e-06
2024-01-23 01:05:01,888 MainThread INFO: EPOCH:967
2024-01-23 01:05:01,888 MainThread INFO: Time Consumed:0.23539090156555176s
2024-01-23 01:05:01,888 MainThread INFO: Total Frames:145950s
 10%|▉         | 968/10000 [06:29<41:34,  3.62it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23200.67074
Train_Epoch_Reward                7411.91508
Running_Training_Average_Rewards  19205.84887
Explore_Time                      0.00098
Train___Time                      0.08043
Eval____Time                      0.15163
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23829.41611
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.36879     3.17050     100.04057    91.68578
alpha_0                           0.61632      0.00009     0.61644      0.61620
Alpha_loss                        -3.25274     0.00265     -3.25018     -3.25638
Training/policy_loss              -6.05381     0.00324     -6.04889     -6.05781
Training/qf1_loss                 7639.31299   1742.77641  10758.12012  5787.59619
Training/qf2_loss                 17102.87090  2406.43465  21498.33008  14644.34668
Training/pf_norm                  0.14244      0.03759     0.21239      0.10306
Training/qf1_norm                 1508.02603   703.70463   2123.93774   249.30693
Training/qf2_norm                 2264.87793   76.37773    2401.12085   2198.72070
log_std/mean                      -0.14072     0.00011     -0.14054     -0.14084
log_probs/mean                    -2.72211     0.00666     -2.71433     -2.73054
mean/mean                         -0.01376     0.00015     -0.01355     -0.01398
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019060134887695312
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70636
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [146250]
collect time 0.0009381771087646484
inner_dict_sum {'sac_diff0': 0.00020313262939453125, 'sac_diff1': 0.006705522537231445, 'sac_diff2': 0.00751185417175293, 'sac_diff3': 0.00970458984375, 'sac_diff4': 0.006748676300048828, 'sac_diff5': 0.03086566925048828, 'sac_diff6': 0.00037860870361328125, 'all': 0.0621180534362793}
diff5_list [0.006506204605102539, 0.006115436553955078, 0.005990743637084961, 0.00616002082824707, 0.006093263626098633]
time3 0
time4 0.06287646293640137
time5 0.06291913986206055
time7 4.76837158203125e-07
gen_weight_change tensor(-17.2580)
policy weight change tensor(38.5641, grad_fn=<SumBackward0>)
time8 0.0018203258514404297
train_time 0.07443547248840332
eval time 0.35833001136779785
epoch last part time 5.7220458984375e-06
2024-01-23 01:05:02,347 MainThread INFO: EPOCH:968
2024-01-23 01:05:02,347 MainThread INFO: Time Consumed:0.4360511302947998s
2024-01-23 01:05:02,347 MainThread INFO: Total Frames:146100s
 10%|▉         | 969/10000 [06:29<49:47,  3.02it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           23291.75206
Train_Epoch_Reward                48578.15006
Running_Training_Average_Rewards  20694.59936
Explore_Time                      0.00093
Train___Time                      0.07444
Eval____Time                      0.35833
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23673.14618
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       92.86361     0.48089    93.63524     92.17884
alpha_0                           0.61601      0.00009    0.61614      0.61589
Alpha_loss                        -3.25667     0.00298    -3.25097     -3.25967
Training/policy_loss              -6.37487     0.00432    -6.36696     -6.37996
Training/qf1_loss                 6679.48594   258.95478  6997.88184   6285.58740
Training/qf2_loss                 15861.70977  295.40383  16238.70117  15435.97656
Training/pf_norm                  0.13393      0.02063    0.15385      0.09741
Training/qf1_norm                 194.61456    55.17115   294.63504    132.55330
Training/qf2_norm                 2391.81245   12.27857   2412.32275   2375.72632
log_std/mean                      -0.13281     0.00010    -0.13269     -0.13297
log_probs/mean                    -2.72330     0.00780    -2.70876     -2.73088
mean/mean                         -0.00617     0.00023    -0.00588     -0.00652
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018573999404907227
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70636
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [146400]
collect time 0.0009851455688476562
inner_dict_sum {'sac_diff0': 0.00020813941955566406, 'sac_diff1': 0.006750345230102539, 'sac_diff2': 0.008222579956054688, 'sac_diff3': 0.010546207427978516, 'sac_diff4': 0.0066492557525634766, 'sac_diff5': 0.03233528137207031, 'sac_diff6': 0.0003800392150878906, 'all': 0.06509184837341309}
diff5_list [0.00689387321472168, 0.006276845932006836, 0.006694793701171875, 0.0063097476959228516, 0.00616002082824707]
time3 0
time4 0.06584048271179199
time5 0.06588459014892578
time7 4.76837158203125e-07
gen_weight_change tensor(-17.2580)
policy weight change tensor(38.5901, grad_fn=<SumBackward0>)
time8 0.001865386962890625
train_time 0.07737398147583008
eval time 0.1916332244873047
epoch last part time 4.5299530029296875e-06
2024-01-23 01:05:02,641 MainThread INFO: EPOCH:969
2024-01-23 01:05:02,642 MainThread INFO: Time Consumed:0.27231717109680176s
2024-01-23 01:05:02,642 MainThread INFO: Total Frames:146250s
 10%|▉         | 970/10000 [06:30<48:08,  3.13it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23373.93420
Train_Epoch_Reward                7701.36227
Running_Training_Average_Rewards  20005.27144
Explore_Time                      0.00098
Train___Time                      0.07737
Eval____Time                      0.19163
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23391.41314
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.06935     2.43492     99.22158     92.57137
alpha_0                           0.61571      0.00009     0.61583      0.61558
Alpha_loss                        -3.25912     0.00283     -3.25423     -3.26217
Training/policy_loss              -6.22235     0.00134     -6.22035     -6.22390
Training/qf1_loss                 7827.00820   1274.40087  9961.94336   6240.88672
Training/qf2_loss                 17466.26426  1681.45584  20458.39453  15637.34668
Training/pf_norm                  0.11604      0.01310     0.13220      0.09828
Training/qf1_norm                 480.81388    414.27575   1233.85535   152.20546
Training/qf2_norm                 2438.25601   62.53880    2544.41504   2373.82715
log_std/mean                      -0.12816     0.00010     -0.12803     -0.12833
log_probs/mean                    -2.72141     0.00480     -2.71272     -2.72632
mean/mean                         -0.01526     0.00005     -0.01517     -0.01532
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018324851989746094
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70636
epoch first part time 2.86102294921875e-06
replay_buffer._size: [146419]
collect time 0.0009613037109375
inside mustsac before update, task 0, sumup 70636
inside mustsac after update, task 0, sumup 70191
inner_dict_sum {'sac_diff0': 0.00020694732666015625, 'sac_diff1': 0.0068454742431640625, 'sac_diff2': 0.007887125015258789, 'sac_diff3': 0.010703802108764648, 'sac_diff4': 0.007560014724731445, 'sac_diff5': 0.05041790008544922, 'sac_diff6': 0.00040793418884277344, 'all': 0.0840291976928711}
diff5_list [0.010533571243286133, 0.010148286819458008, 0.010045766830444336, 0.009822607040405273, 0.009867668151855469]
time3 0.0008463859558105469
time4 0.08488225936889648
time5 0.08494853973388672
time7 0.010579586029052734
gen_weight_change tensor(-17.0734)
policy weight change tensor(38.5140, grad_fn=<SumBackward0>)
time8 0.0026803016662597656
train_time 0.11676526069641113
eval time 0.11060166358947754
epoch last part time 4.76837158203125e-06
2024-01-23 01:05:02,894 MainThread INFO: EPOCH:970
2024-01-23 01:05:02,894 MainThread INFO: Time Consumed:0.23057866096496582s
2024-01-23 01:05:02,894 MainThread INFO: Total Frames:146400s
 10%|▉         | 971/10000 [06:30<45:13,  3.33it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23449.79700
Train_Epoch_Reward                71445.73229
Running_Training_Average_Rewards  22053.04406
Explore_Time                      0.00096
Train___Time                      0.11677
Eval____Time                      0.11060
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23349.10499
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.21457     1.40099     98.68057     94.44381
alpha_0                           0.61540      0.00009     0.61552      0.61528
Alpha_loss                        -3.26381     0.00372     -3.25684     -3.26708
Training/policy_loss              -6.36513     0.30180     -6.09210     -6.88142
Training/qf1_loss                 8703.57266   1129.23866  10152.93262  6862.76660
Training/qf2_loss                 18587.40352  1179.96097  19806.68359  16658.69336
Training/pf_norm                  0.15209      0.02628     0.17495      0.10786
Training/qf1_norm                 760.79837    409.72696   1290.49878   176.56880
Training/qf2_norm                 2475.96616   145.77302   2731.34204   2344.57056
log_std/mean                      -0.12983     0.00211     -0.12749     -0.13327
log_probs/mean                    -2.72417     0.00687     -2.71118     -2.73090
mean/mean                         -0.01333     0.00439     -0.00875     -0.02141
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018431663513183594
epoch last part time3 0.0027484893798828125
inside rlalgo, task 0, sumup 70191
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [146700]
collect time 0.0009419918060302734
inner_dict_sum {'sac_diff0': 0.00020170211791992188, 'sac_diff1': 0.006674766540527344, 'sac_diff2': 0.0074503421783447266, 'sac_diff3': 0.010086774826049805, 'sac_diff4': 0.006972074508666992, 'sac_diff5': 0.031572580337524414, 'sac_diff6': 0.00038743019104003906, 'all': 0.06334567070007324}
diff5_list [0.0065648555755615234, 0.006219625473022461, 0.006245851516723633, 0.006314516067504883, 0.006227731704711914]
time3 0
time4 0.0640723705291748
time5 0.06411480903625488
time7 7.152557373046875e-07
gen_weight_change tensor(-17.0734)
policy weight change tensor(38.5744, grad_fn=<SumBackward0>)
time8 0.0018918514251708984
train_time 0.07562518119812012
eval time 0.14722752571105957
epoch last part time 5.0067901611328125e-06
2024-01-23 01:05:03,144 MainThread INFO: EPOCH:971
2024-01-23 01:05:03,145 MainThread INFO: Time Consumed:0.22603058815002441s
2024-01-23 01:05:03,145 MainThread INFO: Total Frames:146550s
 10%|▉         | 972/10000 [06:30<42:50,  3.51it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23615.82827
Train_Epoch_Reward                28948.41899
Running_Training_Average_Rewards  22483.36876
Explore_Time                      0.00094
Train___Time                      0.07563
Eval____Time                      0.14723
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24795.40533
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.72449     3.47544     99.09239     91.31821
alpha_0                           0.61509      0.00009     0.61521      0.61497
Alpha_loss                        -3.26697     0.00145     -3.26473     -3.26875
Training/policy_loss              -5.70797     0.00283     -5.70389     -5.71251
Training/qf1_loss                 7781.86250   1165.25037  9906.95117   6578.98047
Training/qf2_loss                 17392.80215  1808.90194  20411.77539  15535.85742
Training/pf_norm                  0.12162      0.02261     0.15303      0.09087
Training/qf1_norm                 696.94782    189.59655   912.45496    360.70761
Training/qf2_norm                 2121.40029   74.46966    2217.09155   2045.86084
log_std/mean                      -0.13569     0.00006     -0.13560     -0.13578
log_probs/mean                    -2.72376     0.00342     -2.72016     -2.72822
mean/mean                         -0.00806     0.00010     -0.00789     -0.00817
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018368959426879883
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70191
epoch first part time 2.86102294921875e-06
replay_buffer._size: [146850]
collect time 0.0009083747863769531
inner_dict_sum {'sac_diff0': 0.0002009868621826172, 'sac_diff1': 0.0067784786224365234, 'sac_diff2': 0.007733583450317383, 'sac_diff3': 0.010165691375732422, 'sac_diff4': 0.006811857223510742, 'sac_diff5': 0.03196310997009277, 'sac_diff6': 0.00037932395935058594, 'all': 0.06403303146362305}
diff5_list [0.006903171539306641, 0.006319999694824219, 0.006200075149536133, 0.006346225738525391, 0.006193637847900391]
time3 0
time4 0.06476402282714844
time5 0.06480669975280762
time7 7.152557373046875e-07
gen_weight_change tensor(-17.0734)
policy weight change tensor(38.5022, grad_fn=<SumBackward0>)
time8 0.0018765926361083984
train_time 0.07598161697387695
eval time 0.1480088233947754
epoch last part time 4.0531158447265625e-06
2024-01-23 01:05:03,394 MainThread INFO: EPOCH:972
2024-01-23 01:05:03,394 MainThread INFO: Time Consumed:0.22717785835266113s
2024-01-23 01:05:03,394 MainThread INFO: Total Frames:146700s
 10%|▉         | 973/10000 [06:31<41:13,  3.65it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23818.07082
Train_Epoch_Reward                24550.28676
Running_Training_Average_Rewards  22535.02025
Explore_Time                      0.00090
Train___Time                      0.07598
Eval____Time                      0.14801
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25119.19927
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.49114     1.83776     95.57655     90.67915
alpha_0                           0.61478      0.00009     0.61491      0.61466
Alpha_loss                        -3.27078     0.00293     -3.26701     -3.27459
Training/policy_loss              -6.88734     0.00474     -6.88248     -6.89380
Training/qf1_loss                 7851.41523   1165.56887  9248.93555   6438.46973
Training/qf2_loss                 16967.99453  1496.08313  18773.20703  15177.65625
Training/pf_norm                  0.14446      0.02193     0.17030      0.11412
Training/qf1_norm                 2444.88394   320.59901   2832.80664   1965.40723
Training/qf2_norm                 2601.55645   52.41131    2660.37744   2522.21387
log_std/mean                      -0.12671     0.00008     -0.12658     -0.12679
log_probs/mean                    -2.72468     0.00674     -2.71803     -2.73309
mean/mean                         -0.00573     0.00006     -0.00567     -0.00584
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018415451049804688
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70191
epoch first part time 2.86102294921875e-06
replay_buffer._size: [147000]
collect time 0.0008146762847900391
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.006580829620361328, 'sac_diff2': 0.0073947906494140625, 'sac_diff3': 0.009860515594482422, 'sac_diff4': 0.006778717041015625, 'sac_diff5': 0.031497955322265625, 'sac_diff6': 0.000377655029296875, 'all': 0.06270027160644531}
diff5_list [0.006666660308837891, 0.006125688552856445, 0.006365299224853516, 0.006364345550537109, 0.005975961685180664]
time3 0
time4 0.06341719627380371
time5 0.0634610652923584
time7 4.76837158203125e-07
gen_weight_change tensor(-17.0734)
policy weight change tensor(38.5230, grad_fn=<SumBackward0>)
time8 0.0018694400787353516
train_time 0.07483363151550293
eval time 0.1482067108154297
epoch last part time 3.814697265625e-06
2024-01-23 01:05:03,642 MainThread INFO: EPOCH:973
2024-01-23 01:05:03,642 MainThread INFO: Time Consumed:0.22611117362976074s
2024-01-23 01:05:03,642 MainThread INFO: Total Frames:146850s
 10%|▉         | 974/10000 [06:31<40:03,  3.76it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           24032.94750
Train_Epoch_Reward                10822.72825
Running_Training_Average_Rewards  22001.06263
Explore_Time                      0.00081
Train___Time                      0.07483
Eval____Time                      0.14821
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25212.88039
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.58328     2.20302     97.09245     90.74110
alpha_0                           0.61448      0.00009     0.61460      0.61435
Alpha_loss                        -3.27452     0.00237     -3.27134     -3.27862
Training/policy_loss              -6.33408     0.00498     -6.32787     -6.33996
Training/qf1_loss                 7072.27676   1381.96829  9355.01660   5556.10498
Training/qf2_loss                 16580.73633  1770.55505  19217.09766  14282.63281
Training/pf_norm                  0.25986      0.02055     0.28180      0.22092
Training/qf1_norm                 476.46043    224.46189   703.59967    159.66353
Training/qf2_norm                 2410.12983   56.02426    2471.09106   2312.00317
log_std/mean                      -0.12928     0.00015     -0.12910     -0.12950
log_probs/mean                    -2.72546     0.00542     -2.71756     -2.73388
mean/mean                         -0.00977     0.00004     -0.00973     -0.00984
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01846933364868164
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70191
epoch first part time 2.384185791015625e-06
replay_buffer._size: [147150]
collect time 0.0010182857513427734
inner_dict_sum {'sac_diff0': 0.0001964569091796875, 'sac_diff1': 0.006440162658691406, 'sac_diff2': 0.007424116134643555, 'sac_diff3': 0.009586334228515625, 'sac_diff4': 0.006363868713378906, 'sac_diff5': 0.031651973724365234, 'sac_diff6': 0.00038814544677734375, 'all': 0.06205105781555176}
diff5_list [0.006282329559326172, 0.006177186965942383, 0.006205558776855469, 0.006792783737182617, 0.006194114685058594]
time3 0
time4 0.06277990341186523
time5 0.06282210350036621
time7 9.5367431640625e-07
gen_weight_change tensor(-17.0734)
policy weight change tensor(38.5005, grad_fn=<SumBackward0>)
time8 0.0019059181213378906
train_time 0.07396364212036133
eval time 0.15593361854553223
epoch last part time 5.245208740234375e-06
2024-01-23 01:05:03,897 MainThread INFO: EPOCH:974
2024-01-23 01:05:03,897 MainThread INFO: Time Consumed:0.23323345184326172s
2024-01-23 01:05:03,897 MainThread INFO: Total Frames:147000s
 10%|▉         | 975/10000 [06:31<39:33,  3.80it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           24256.23229
Train_Epoch_Reward                25253.67377
Running_Training_Average_Rewards  22219.99284
Explore_Time                      0.00101
Train___Time                      0.07396
Eval____Time                      0.15593
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25631.37036
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.36346     2.54110     98.77421     91.91994
alpha_0                           0.61417      0.00009     0.61429      0.61405
Alpha_loss                        -3.27660     0.00385     -3.27200     -3.28297
Training/policy_loss              -6.83359     0.00616     -6.82233     -6.84000
Training/qf1_loss                 8307.12256   2158.80009  12543.86719  6517.29785
Training/qf2_loss                 17746.75918  2659.42550  22935.33984  15526.09961
Training/pf_norm                  0.23709      0.03631     0.30883      0.21213
Training/qf1_norm                 740.09188    309.72223   1113.53455   307.34842
Training/qf2_norm                 2599.19644   71.73291    2724.17261   2531.51465
log_std/mean                      -0.12817     0.00004     -0.12810     -0.12820
log_probs/mean                    -2.72284     0.00789     -2.71065     -2.73452
mean/mean                         -0.01282     0.00027     -0.01248     -0.01323
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01861858367919922
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70191
epoch first part time 3.337860107421875e-06
replay_buffer._size: [147300]
collect time 0.0009644031524658203
inside mustsac before update, task 0, sumup 70191
inside mustsac after update, task 0, sumup 70466
inner_dict_sum {'sac_diff0': 0.00021719932556152344, 'sac_diff1': 0.006818294525146484, 'sac_diff2': 0.007854938507080078, 'sac_diff3': 0.010355710983276367, 'sac_diff4': 0.007065534591674805, 'sac_diff5': 0.0516972541809082, 'sac_diff6': 0.0004267692565917969, 'all': 0.08443570137023926}
diff5_list [0.01040792465209961, 0.010126590728759766, 0.010690927505493164, 0.010177135467529297, 0.010294675827026367]
time3 0.0008368492126464844
time4 0.08527851104736328
time5 0.08533620834350586
time7 0.010475635528564453
gen_weight_change tensor(-17.0651)
policy weight change tensor(38.4522, grad_fn=<SumBackward0>)
time8 0.002042531967163086
train_time 0.11681675910949707
eval time 0.11432504653930664
epoch last part time 4.291534423828125e-06
2024-01-23 01:05:04,154 MainThread INFO: EPOCH:975
2024-01-23 01:05:04,154 MainThread INFO: Time Consumed:0.23436355590820312s
2024-01-23 01:05:04,154 MainThread INFO: Total Frames:147150s
 10%|▉         | 976/10000 [06:31<39:16,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           24475.64365
Train_Epoch_Reward                54581.92989
Running_Training_Average_Rewards  23318.92751
Explore_Time                      0.00096
Train___Time                      0.11682
Eval____Time                      0.11433
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             26063.82841
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.92730     2.48888     98.08501     91.50269
alpha_0                           0.61386      0.00009     0.61399      0.61374
Alpha_loss                        -3.28043     0.00447     -3.27399     -3.28744
Training/policy_loss              -6.40880     0.21724     -6.10359     -6.71807
Training/qf1_loss                 7658.38535   1869.61578  10197.90430  5587.03564
Training/qf2_loss                 17074.45273  2408.96840  20488.82422  14502.90820
Training/pf_norm                  0.13301      0.05510     0.23235      0.07879
Training/qf1_norm                 629.16341    349.64971   1250.79529   201.78271
Training/qf2_norm                 2419.99253   140.97298   2641.02319   2251.29639
log_std/mean                      -0.13341     0.00356     -0.12831     -0.13902
log_probs/mean                    -2.72380     0.00870     -2.70923     -2.73542
mean/mean                         -0.00964     0.00321     -0.00429     -0.01340
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0185546875
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70466
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [147450]
collect time 0.0009446144104003906
inner_dict_sum {'sac_diff0': 0.00021338462829589844, 'sac_diff1': 0.006566047668457031, 'sac_diff2': 0.007475614547729492, 'sac_diff3': 0.009758234024047852, 'sac_diff4': 0.007134675979614258, 'sac_diff5': 0.032044410705566406, 'sac_diff6': 0.00037789344787597656, 'all': 0.06357026100158691}
diff5_list [0.006519794464111328, 0.006087779998779297, 0.006299734115600586, 0.0069427490234375, 0.006194353103637695]
time3 0
time4 0.06429553031921387
time5 0.06433844566345215
time7 4.76837158203125e-07
gen_weight_change tensor(-17.0651)
policy weight change tensor(38.4596, grad_fn=<SumBackward0>)
time8 0.0019147396087646484
train_time 0.07566475868225098
eval time 0.15193986892700195
epoch last part time 4.5299530029296875e-06
2024-01-23 01:05:04,406 MainThread INFO: EPOCH:976
2024-01-23 01:05:04,407 MainThread INFO: Time Consumed:0.23084735870361328s
2024-01-23 01:05:04,407 MainThread INFO: Total Frames:147300s
 10%|▉         | 977/10000 [06:32<38:53,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           24191.23560
Train_Epoch_Reward                5305.06558
Running_Training_Average_Rewards  22658.80273
Explore_Time                      0.00094
Train___Time                      0.07566
Eval____Time                      0.15194
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20846.59184
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.64812     3.19652     101.94598    93.29812
alpha_0                           0.61356      0.00009     0.61368      0.61343
Alpha_loss                        -3.28456     0.00350     -3.27813     -3.28805
Training/policy_loss              -6.51127     0.00491     -6.50516     -6.51885
Training/qf1_loss                 9092.27402   770.80847   10183.79102  7964.36719
Training/qf2_loss                 19302.26953  1441.59279  21334.04297  17241.06250
Training/pf_norm                  0.07994      0.01819     0.11223      0.05870
Training/qf1_norm                 736.31873    432.64297   1417.81970   191.15663
Training/qf2_norm                 2569.78369   82.65263    2680.73364   2455.71948
log_std/mean                      -0.13633     0.00009     -0.13621     -0.13648
log_probs/mean                    -2.72540     0.00654     -2.71360     -2.73252
mean/mean                         -0.00417     0.00010     -0.00401     -0.00429
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018620729446411133
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70466
epoch first part time 2.86102294921875e-06
replay_buffer._size: [147600]
collect time 0.0009412765502929688
inner_dict_sum {'sac_diff0': 0.00019860267639160156, 'sac_diff1': 0.006630420684814453, 'sac_diff2': 0.007552385330200195, 'sac_diff3': 0.010088443756103516, 'sac_diff4': 0.0070950984954833984, 'sac_diff5': 0.031481266021728516, 'sac_diff6': 0.0003783702850341797, 'all': 0.06342458724975586}
diff5_list [0.0064258575439453125, 0.0062046051025390625, 0.006323337554931641, 0.00635218620300293, 0.00617527961730957]
time3 0
time4 0.06415939331054688
time5 0.06420326232910156
time7 7.152557373046875e-07
gen_weight_change tensor(-17.0651)
policy weight change tensor(38.5001, grad_fn=<SumBackward0>)
time8 0.0018551349639892578
train_time 0.07533073425292969
eval time 0.15054059028625488
epoch last part time 4.291534423828125e-06
2024-01-23 01:05:04,658 MainThread INFO: EPOCH:977
2024-01-23 01:05:04,658 MainThread INFO: Time Consumed:0.22905182838439941s
2024-01-23 01:05:04,658 MainThread INFO: Total Frames:147450s
 10%|▉         | 978/10000 [06:32<38:31,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23918.93081
Train_Epoch_Reward                11776.94319
Running_Training_Average_Rewards  22628.49545
Explore_Time                      0.00094
Train___Time                      0.07533
Eval____Time                      0.15054
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21106.36819
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.18821     2.65489     98.77861     92.00241
alpha_0                           0.61325      0.00009     0.61337      0.61313
Alpha_loss                        -3.28684     0.00214     -3.28500     -3.28977
Training/policy_loss              -6.83802     0.00647     -6.82911     -6.84823
Training/qf1_loss                 8302.01533   1190.93399  10153.67871  7047.78516
Training/qf2_loss                 18001.69766  1720.24087  20608.14453  16075.60352
Training/pf_norm                  0.15555      0.02695     0.19867      0.11924
Training/qf1_norm                 560.31322    236.35237   924.15253    219.05714
Training/qf2_norm                 2707.84678   74.61623    2812.49414   2622.15698
log_std/mean                      -0.13811     0.00010     -0.13795     -0.13824
log_probs/mean                    -2.72318     0.00367     -2.71857     -2.72917
mean/mean                         -0.00262     0.00031     -0.00225     -0.00311
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018233537673950195
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70466
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [147750]
collect time 0.000904083251953125
inner_dict_sum {'sac_diff0': 0.00019598007202148438, 'sac_diff1': 0.0062901973724365234, 'sac_diff2': 0.007266521453857422, 'sac_diff3': 0.009924888610839844, 'sac_diff4': 0.00648045539855957, 'sac_diff5': 0.03161311149597168, 'sac_diff6': 0.00039076805114746094, 'all': 0.062161922454833984}
diff5_list [0.006428241729736328, 0.006150722503662109, 0.0061876773834228516, 0.006497859954833984, 0.006348609924316406]
time3 0
time4 0.06290769577026367
time5 0.06295084953308105
time7 7.152557373046875e-07
gen_weight_change tensor(-17.0651)
policy weight change tensor(38.6491, grad_fn=<SumBackward0>)
time8 0.0017740726470947266
train_time 0.07394051551818848
eval time 0.15548110008239746
epoch last part time 5.245208740234375e-06
2024-01-23 01:05:04,912 MainThread INFO: EPOCH:978
2024-01-23 01:05:04,912 MainThread INFO: Time Consumed:0.23262453079223633s
2024-01-23 01:05:04,913 MainThread INFO: Total Frames:147600s
 10%|▉         | 979/10000 [06:32<38:27,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23664.24211
Train_Epoch_Reward                7492.03936
Running_Training_Average_Rewards  21674.63572
Explore_Time                      0.00090
Train___Time                      0.07394
Eval____Time                      0.15548
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21126.25924
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.09212     3.30509     99.14668     90.80933
alpha_0                           0.61294      0.00009     0.61307      0.61282
Alpha_loss                        -3.28949     0.00375     -3.28285     -3.29330
Training/policy_loss              -5.47477     0.00379     -5.47080     -5.48202
Training/qf1_loss                 8447.73340   1377.49632  10593.99805  6663.97900
Training/qf2_loss                 18074.16816  1805.54712  20237.44141  15727.61230
Training/pf_norm                  0.14187      0.02155     0.16601      0.10773
Training/qf1_norm                 1440.64790   601.41210   2188.33252   671.80103
Training/qf2_norm                 2081.61709   71.24622    2167.00244   1989.97424
log_std/mean                      -0.13096     0.00011     -0.13087     -0.13117
log_probs/mean                    -2.72173     0.00718     -2.70815     -2.72833
mean/mean                         -0.01086     0.00024     -0.01051     -0.01119
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01855015754699707
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70466
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [147900]
collect time 0.0009472370147705078
inner_dict_sum {'sac_diff0': 0.000194549560546875, 'sac_diff1': 0.006259441375732422, 'sac_diff2': 0.007279396057128906, 'sac_diff3': 0.009886503219604492, 'sac_diff4': 0.006505727767944336, 'sac_diff5': 0.03101062774658203, 'sac_diff6': 0.00038051605224609375, 'all': 0.061516761779785156}
diff5_list [0.006491899490356445, 0.006140708923339844, 0.006017208099365234, 0.006311655044555664, 0.006049156188964844]
time3 0
time4 0.062232255935668945
time5 0.062274932861328125
time7 4.76837158203125e-07
gen_weight_change tensor(-17.0651)
policy weight change tensor(38.6231, grad_fn=<SumBackward0>)
time8 0.0018041133880615234
train_time 0.07319211959838867
eval time 0.1455233097076416
epoch last part time 4.76837158203125e-06
2024-01-23 01:05:05,156 MainThread INFO: EPOCH:979
2024-01-23 01:05:05,156 MainThread INFO: Time Consumed:0.22190356254577637s
2024-01-23 01:05:05,157 MainThread INFO: Total Frames:147750s
 10%|▉         | 980/10000 [06:32<37:55,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23434.84776
Train_Epoch_Reward                10703.17968
Running_Training_Average_Rewards  20810.67300
Explore_Time                      0.00094
Train___Time                      0.07319
Eval____Time                      0.14552
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21097.46958
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.66295     1.79589     96.33266     92.39900
alpha_0                           0.61264      0.00009     0.61276      0.61252
Alpha_loss                        -3.29419     0.00400     -3.29044     -3.30176
Training/policy_loss              -6.49714     0.00512     -6.49087     -6.50524
Training/qf1_loss                 7539.60361   1443.26399  9828.93848   6102.55371
Training/qf2_loss                 17095.57695  1660.86419  19651.30664  15384.66406
Training/pf_norm                  0.19435      0.01246     0.20499      0.17097
Training/qf1_norm                 590.93750    324.17689   997.58020    276.43552
Training/qf2_norm                 2568.98433   47.99348    2612.17749   2507.93433
log_std/mean                      -0.13199     0.00020     -0.13163     -0.13216
log_probs/mean                    -2.72448     0.00795     -2.71814     -2.73993
mean/mean                         -0.01328     0.00016     -0.01300     -0.01343
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018550395965576172
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70466
epoch first part time 3.337860107421875e-06
replay_buffer._size: [148050]
collect time 0.0010030269622802734
inside mustsac before update, task 0, sumup 70466
inside mustsac after update, task 0, sumup 70143
inner_dict_sum {'sac_diff0': 0.00021123886108398438, 'sac_diff1': 0.0069580078125, 'sac_diff2': 0.008347272872924805, 'sac_diff3': 0.010561227798461914, 'sac_diff4': 0.0072209835052490234, 'sac_diff5': 0.05166268348693848, 'sac_diff6': 0.00042700767517089844, 'all': 0.0853884220123291}
diff5_list [0.010745525360107422, 0.010286331176757812, 0.010897636413574219, 0.009645700454711914, 0.01008749008178711]
time3 0.0008521080017089844
time4 0.08621978759765625
time5 0.08627104759216309
time7 0.009036064147949219
gen_weight_change tensor(-17.1277)
policy weight change tensor(38.6211, grad_fn=<SumBackward0>)
time8 0.002794981002807617
train_time 0.11690187454223633
eval time 0.10721397399902344
epoch last part time 5.0067901611328125e-06
2024-01-23 01:05:05,406 MainThread INFO: EPOCH:980
2024-01-23 01:05:05,406 MainThread INFO: Time Consumed:0.22735261917114258s
2024-01-23 01:05:05,406 MainThread INFO: Total Frames:147900s
 10%|▉         | 981/10000 [06:33<37:57,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           23211.50136
Train_Epoch_Reward                4845.79218
Running_Training_Average_Rewards  20721.32586
Explore_Time                      0.00100
Train___Time                      0.11690
Eval____Time                      0.10721
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21115.64105
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       95.59834     2.32159    98.62135     92.50896
alpha_0                           0.61233      0.00009    0.61245      0.61221
Alpha_loss                        -3.29654     0.00508    -3.28875     -3.30342
Training/policy_loss              -6.32870     0.43448    -5.49155     -6.72893
Training/qf1_loss                 7656.38457   552.88187  8288.07812   6872.11865
Training/qf2_loss                 17381.53652  966.15691  18658.98828  15993.21582
Training/pf_norm                  0.15802      0.04320    0.21950      0.08971
Training/qf1_norm                 987.82780    737.56434  2391.40112   407.05295
Training/qf2_norm                 2453.62578   188.86774  2602.70752   2082.13135
log_std/mean                      -0.13465     0.00571    -0.12754     -0.14487
log_probs/mean                    -2.72243     0.01027    -2.70927     -2.73783
mean/mean                         -0.01403     0.00344    -0.00943     -0.01880
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018819570541381836
epoch last part time3 0.0027282238006591797
inside rlalgo, task 0, sumup 70143
epoch first part time 3.337860107421875e-06
replay_buffer._size: [148200]
collect time 0.0009810924530029297
inner_dict_sum {'sac_diff0': 0.00020766258239746094, 'sac_diff1': 0.0072479248046875, 'sac_diff2': 0.008933782577514648, 'sac_diff3': 0.011380910873413086, 'sac_diff4': 0.007019758224487305, 'sac_diff5': 0.0331423282623291, 'sac_diff6': 0.0004150867462158203, 'all': 0.06834745407104492}
diff5_list [0.006792545318603516, 0.007032632827758789, 0.00651860237121582, 0.006598472595214844, 0.006200075149536133]
time3 0
time4 0.06911849975585938
time5 0.06917357444763184
time7 7.152557373046875e-07
gen_weight_change tensor(-17.1277)
policy weight change tensor(38.6505, grad_fn=<SumBackward0>)
time8 0.0018954277038574219
train_time 0.08045673370361328
eval time 0.1405327320098877
epoch last part time 5.0067901611328125e-06
2024-01-23 01:05:05,655 MainThread INFO: EPOCH:981
2024-01-23 01:05:05,656 MainThread INFO: Time Consumed:0.22423648834228516s
2024-01-23 01:05:05,656 MainThread INFO: Total Frames:148050s
 10%|▉         | 982/10000 [06:33<37:39,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22837.98261
Train_Epoch_Reward                21140.13614
Running_Training_Average_Rewards  20623.80915
Explore_Time                      0.00098
Train___Time                      0.08046
Eval____Time                      0.14053
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21060.21782
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.73214     1.77935     98.61643     93.17059
alpha_0                           0.61203      0.00009     0.61215      0.61190
Alpha_loss                        -3.29778     0.00506     -3.29366     -3.30675
Training/policy_loss              -6.59029     0.00944     -6.57577     -6.60375
Training/qf1_loss                 8139.63750   894.83869   9669.72852   7175.64941
Training/qf2_loss                 17899.16016  1141.51759  19462.80664  16370.27539
Training/pf_norm                  0.15443      0.02870     0.21101      0.13621
Training/qf1_norm                 1086.04169   410.00271   1670.06750   406.83105
Training/qf2_norm                 2579.41465   49.14844    2659.42725   2509.03906
log_std/mean                      -0.12488     0.00010     -0.12479     -0.12506
log_probs/mean                    -2.71811     0.00906     -2.70835     -2.73364
mean/mean                         -0.00676     0.00007     -0.00670     -0.00688
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018460512161254883
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70143
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [148350]
collect time 0.0008373260498046875
inner_dict_sum {'sac_diff0': 0.00020813941955566406, 'sac_diff1': 0.0064661502838134766, 'sac_diff2': 0.007619619369506836, 'sac_diff3': 0.00969839096069336, 'sac_diff4': 0.0069425106048583984, 'sac_diff5': 0.03130221366882324, 'sac_diff6': 0.0003857612609863281, 'all': 0.0626227855682373}
diff5_list [0.006436824798583984, 0.005984306335449219, 0.00625920295715332, 0.00659489631652832, 0.0060269832611083984]
time3 0
time4 0.06336593627929688
time5 0.06341123580932617
time7 7.152557373046875e-07
gen_weight_change tensor(-17.1277)
policy weight change tensor(38.8591, grad_fn=<SumBackward0>)
time8 0.0017757415771484375
train_time 0.07424426078796387
eval time 0.15604066848754883
epoch last part time 6.198883056640625e-06
2024-01-23 01:05:05,911 MainThread INFO: EPOCH:982
2024-01-23 01:05:05,911 MainThread INFO: Time Consumed:0.23347902297973633s
2024-01-23 01:05:05,911 MainThread INFO: Total Frames:148200s
 10%|▉         | 983/10000 [06:33<37:54,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22430.62309
Train_Epoch_Reward                16661.71583
Running_Training_Average_Rewards  20746.16140
Explore_Time                      0.00083
Train___Time                      0.07424
Eval____Time                      0.15604
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21045.60404
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.21582     2.17055     99.50488     93.47498
alpha_0                           0.61172      0.00009     0.61184      0.61160
Alpha_loss                        -3.30297     0.00191     -3.30027     -3.30554
Training/policy_loss              -6.12452     0.00399     -6.11875     -6.13061
Training/qf1_loss                 8846.83906   1455.32496  11177.31055  7168.48145
Training/qf2_loss                 18947.46094  1778.34391  21394.81055  16499.55273
Training/pf_norm                  0.16686      0.03670     0.21176      0.11651
Training/qf1_norm                 757.22391    371.77980   1168.30554   167.89055
Training/qf2_norm                 2392.53267   53.50390    2446.98975   2300.13379
log_std/mean                      -0.13445     0.00022     -0.13418     -0.13478
log_probs/mean                    -2.72185     0.00322     -2.71739     -2.72605
mean/mean                         -0.01406     0.00014     -0.01383     -0.01420
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01929497718811035
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70143
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [148500]
collect time 0.0009853839874267578
inner_dict_sum {'sac_diff0': 0.00022673606872558594, 'sac_diff1': 0.007210969924926758, 'sac_diff2': 0.008417844772338867, 'sac_diff3': 0.010624408721923828, 'sac_diff4': 0.007350921630859375, 'sac_diff5': 0.0329737663269043, 'sac_diff6': 0.0004017353057861328, 'all': 0.06720638275146484}
diff5_list [0.0070383548736572266, 0.00651097297668457, 0.006849765777587891, 0.006354331970214844, 0.006220340728759766]
time3 0
time4 0.06800031661987305
time5 0.06805086135864258
time7 7.152557373046875e-07
gen_weight_change tensor(-17.1277)
policy weight change tensor(39.1158, grad_fn=<SumBackward0>)
time8 0.0019693374633789062
train_time 0.07974410057067871
eval time 0.14648938179016113
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:06,163 MainThread INFO: EPOCH:983
2024-01-23 01:05:06,164 MainThread INFO: Time Consumed:0.22962570190429688s
2024-01-23 01:05:06,164 MainThread INFO: Total Frames:148350s
 10%|▉         | 984/10000 [06:33<37:54,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22032.31599
Train_Epoch_Reward                18027.30021
Running_Training_Average_Rewards  20644.43890
Explore_Time                      0.00098
Train___Time                      0.07974
Eval____Time                      0.14649
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21229.80937
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.06681     2.02540     99.27653     93.83680
alpha_0                           0.61142      0.00009     0.61154      0.61129
Alpha_loss                        -3.30507     0.00201     -3.30328     -3.30877
Training/policy_loss              -6.95008     0.00439     -6.94549     -6.95591
Training/qf1_loss                 7962.50625   852.41711   9039.76953   6936.96582
Training/qf2_loss                 17784.33789  1146.05633  19499.25781  16419.73828
Training/pf_norm                  0.20095      0.03247     0.23883      0.14746
Training/qf1_norm                 1131.93469   414.47035   1805.10144   715.01318
Training/qf2_norm                 2701.82798   55.37003    2791.15698   2642.14966
log_std/mean                      -0.13127     0.00025     -0.13098     -0.13164
log_probs/mean                    -2.71928     0.00399     -2.71564     -2.72544
mean/mean                         -0.01472     0.00016     -0.01447     -0.01491
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01872396469116211
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70143
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [148650]
collect time 0.0009682178497314453
inner_dict_sum {'sac_diff0': 0.00020766258239746094, 'sac_diff1': 0.007088422775268555, 'sac_diff2': 0.00858449935913086, 'sac_diff3': 0.010568618774414062, 'sac_diff4': 0.007383108139038086, 'sac_diff5': 0.033620357513427734, 'sac_diff6': 0.0004096031188964844, 'all': 0.06786227226257324}
diff5_list [0.0068361759185791016, 0.006819248199462891, 0.0072672367095947266, 0.006584644317626953, 0.0061130523681640625]
time3 0
time4 0.06866121292114258
time5 0.06870913505554199
time7 7.152557373046875e-07
gen_weight_change tensor(-17.1277)
policy weight change tensor(39.3211, grad_fn=<SumBackward0>)
time8 0.0019462108612060547
train_time 0.08024406433105469
eval time 0.1461009979248047
epoch last part time 5.245208740234375e-06
2024-01-23 01:05:06,415 MainThread INFO: EPOCH:984
2024-01-23 01:05:06,416 MainThread INFO: Time Consumed:0.22963809967041016s
2024-01-23 01:05:06,416 MainThread INFO: Total Frames:148500s
 10%|▉         | 985/10000 [06:34<37:52,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21644.11109
Train_Epoch_Reward                3267.27993
Running_Training_Average_Rewards  20293.52664
Explore_Time                      0.00096
Train___Time                      0.08024
Eval____Time                      0.14610
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21749.32133
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.68337     1.48782     98.28622     93.93699
alpha_0                           0.61111      0.00009     0.61123      0.61099
Alpha_loss                        -3.31154     0.00410     -3.30692     -3.31682
Training/policy_loss              -6.41467     0.00376     -6.40929     -6.41786
Training/qf1_loss                 8817.79844   857.87400   10386.97754  7824.91309
Training/qf2_loss                 18785.30352  1092.82819  20661.71094  17255.29297
Training/pf_norm                  0.12558      0.02177     0.15912      0.10347
Training/qf1_norm                 1484.97697   318.03182   1825.25549   920.04266
Training/qf2_norm                 2529.93872   38.99021    2572.93872   2457.73291
log_std/mean                      -0.13672     0.00014     -0.13650     -0.13687
log_probs/mean                    -2.72560     0.00683     -2.71623     -2.73360
mean/mean                         -0.01058     0.00005     -0.01054     -0.01067
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01843547821044922
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70143
epoch first part time 2.86102294921875e-06
replay_buffer._size: [148800]
collect time 0.000942230224609375
inside mustsac before update, task 0, sumup 70143
inside mustsac after update, task 0, sumup 69809
inner_dict_sum {'sac_diff0': 0.00023174285888671875, 'sac_diff1': 0.007666110992431641, 'sac_diff2': 0.008990049362182617, 'sac_diff3': 0.011548280715942383, 'sac_diff4': 0.00757598876953125, 'sac_diff5': 0.05463218688964844, 'sac_diff6': 0.0004184246063232422, 'all': 0.09106278419494629}
diff5_list [0.01157689094543457, 0.011397361755371094, 0.01021575927734375, 0.010225057601928711, 0.011217117309570312]
time3 0.0008761882781982422
time4 0.09194111824035645
time5 0.09199380874633789
time7 0.009153604507446289
gen_weight_change tensor(-17.2097)
policy weight change tensor(39.2916, grad_fn=<SumBackward0>)
time8 0.0019991397857666016
train_time 0.12183046340942383
eval time 0.10712862014770508
epoch last part time 5.0067901611328125e-06
2024-01-23 01:05:06,670 MainThread INFO: EPOCH:985
2024-01-23 01:05:06,670 MainThread INFO: Time Consumed:0.23212599754333496s
2024-01-23 01:05:06,670 MainThread INFO: Total Frames:148650s
 10%|▉         | 986/10000 [06:34<37:59,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21265.00776
Train_Epoch_Reward                15451.85974
Running_Training_Average_Rewards  20167.80752
Explore_Time                      0.00094
Train___Time                      0.12183
Eval____Time                      0.10713
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22272.79517
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.72615     2.63119     98.93602     92.02626
alpha_0                           0.61081      0.00009     0.61093      0.61068
Alpha_loss                        -3.31338     0.00369     -3.30688     -3.31784
Training/policy_loss              -6.62639     0.14754     -6.36749     -6.77241
Training/qf1_loss                 7765.68838   930.14500   9318.03223   6409.79980
Training/qf2_loss                 17527.07480  1401.85766  19773.55078  15686.59668
Training/pf_norm                  0.17455      0.04413     0.23393      0.12965
Training/qf1_norm                 1064.77153   758.72699   2412.75781   204.85031
Training/qf2_norm                 2563.45269   140.07166   2732.96484   2305.95312
log_std/mean                      -0.13272     0.00209     -0.12983     -0.13565
log_probs/mean                    -2.72253     0.00757     -2.70799     -2.72884
mean/mean                         -0.01415     0.00245     -0.01199     -0.01778
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018678665161132812
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69809
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [148950]
collect time 0.0010561943054199219
inner_dict_sum {'sac_diff0': 0.00021529197692871094, 'sac_diff1': 0.007668256759643555, 'sac_diff2': 0.009130001068115234, 'sac_diff3': 0.011606693267822266, 'sac_diff4': 0.00785207748413086, 'sac_diff5': 0.0352025032043457, 'sac_diff6': 0.0004203319549560547, 'all': 0.07209515571594238}
diff5_list [0.0077152252197265625, 0.007019519805908203, 0.007731437683105469, 0.006358146667480469, 0.006378173828125]
time3 0
time4 0.07293105125427246
time5 0.07298588752746582
time7 7.152557373046875e-07
gen_weight_change tensor(-17.2097)
policy weight change tensor(39.4510, grad_fn=<SumBackward0>)
time8 0.0019016265869140625
train_time 0.08471798896789551
eval time 0.15458941459655762
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:06,935 MainThread INFO: EPOCH:986
2024-01-23 01:05:06,936 MainThread INFO: Time Consumed:0.242722749710083s
2024-01-23 01:05:06,936 MainThread INFO: Total Frames:148800s
 10%|▉         | 987/10000 [06:34<38:54,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21386.23715
Train_Epoch_Reward                21945.97477
Running_Training_Average_Rewards  20602.07258
Explore_Time                      0.00105
Train___Time                      0.08472
Eval____Time                      0.15459
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22058.88568
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.84312     2.38795     99.16307     93.17241
alpha_0                           0.61050      0.00009     0.61062      0.61038
Alpha_loss                        -3.31766     0.00455     -3.31030     -3.32439
Training/policy_loss              -6.46127     0.00836     -6.45052     -6.47014
Training/qf1_loss                 7896.61221   1073.04987  9542.35742   6602.57275
Training/qf2_loss                 17712.92148  1524.91035  19791.66602  15866.34570
Training/pf_norm                  0.16837      0.03314     0.21693      0.11885
Training/qf1_norm                 486.76829    170.23092   659.84625    175.15778
Training/qf2_norm                 2499.23389   62.67564    2584.41772   2427.74805
log_std/mean                      -0.13928     0.00003     -0.13924     -0.13931
log_probs/mean                    -2.72440     0.00821     -2.71220     -2.73667
mean/mean                         -0.00677     0.00007     -0.00669     -0.00689
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.026705503463745117
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 69809
epoch first part time 2.86102294921875e-06
replay_buffer._size: [149100]
collect time 0.0009448528289794922
inner_dict_sum {'sac_diff0': 0.00021004676818847656, 'sac_diff1': 0.00691986083984375, 'sac_diff2': 0.008409738540649414, 'sac_diff3': 0.01080465316772461, 'sac_diff4': 0.007224321365356445, 'sac_diff5': 0.03257179260253906, 'sac_diff6': 0.000396728515625, 'all': 0.06653714179992676}
diff5_list [0.006688117980957031, 0.006673574447631836, 0.00647425651550293, 0.00622105598449707, 0.006514787673950195]
time3 0
time4 0.06733417510986328
time5 0.06738615036010742
time7 9.5367431640625e-07
gen_weight_change tensor(-17.2097)
policy weight change tensor(39.6180, grad_fn=<SumBackward0>)
time8 0.001825094223022461
train_time 0.0785367488861084
eval time 0.1410689353942871
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:07,188 MainThread INFO: EPOCH:987
2024-01-23 01:05:07,189 MainThread INFO: Time Consumed:0.22290539741516113s
2024-01-23 01:05:07,189 MainThread INFO: Total Frames:148950s
 10%|▉         | 988/10000 [06:34<38:19,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21537.90927
Train_Epoch_Reward                13748.36415
Running_Training_Average_Rewards  20889.36024
Explore_Time                      0.00094
Train___Time                      0.07854
Eval____Time                      0.14107
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22623.08947
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.69440     2.26909     97.33081     90.46486
alpha_0                           0.61020      0.00009     0.61032      0.61007
Alpha_loss                        -3.31925     0.00340     -3.31632     -3.32536
Training/policy_loss              -6.75509     0.00344     -6.74979     -6.75921
Training/qf1_loss                 7505.08955   1025.99177  9457.60547   6481.73291
Training/qf2_loss                 17049.34160  1269.30843  19178.91016  15181.28418
Training/pf_norm                  0.20472      0.01428     0.22799      0.18838
Training/qf1_norm                 609.41776    181.88738   944.13666    413.15405
Training/qf2_norm                 2614.03911   64.06557    2683.15601   2492.41235
log_std/mean                      -0.13662     0.00002     -0.13660     -0.13665
log_probs/mean                    -2.72082     0.00533     -2.71584     -2.73047
mean/mean                         -0.00645     0.00003     -0.00642     -0.00650
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01969742774963379
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69809
epoch first part time 3.814697265625e-06
replay_buffer._size: [149250]
collect time 0.0008592605590820312
inner_dict_sum {'sac_diff0': 0.00021219253540039062, 'sac_diff1': 0.006887197494506836, 'sac_diff2': 0.008130788803100586, 'sac_diff3': 0.010531187057495117, 'sac_diff4': 0.007112741470336914, 'sac_diff5': 0.03276848793029785, 'sac_diff6': 0.0003895759582519531, 'all': 0.06603217124938965}
diff5_list [0.0069332122802734375, 0.006381988525390625, 0.006256818771362305, 0.006584644317626953, 0.006611824035644531]
time3 0
time4 0.06681084632873535
time5 0.06686735153198242
time7 7.152557373046875e-07
gen_weight_change tensor(-17.2097)
policy weight change tensor(39.7455, grad_fn=<SumBackward0>)
time8 0.001850128173828125
train_time 0.07819008827209473
eval time 0.14468693733215332
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:07,438 MainThread INFO: EPOCH:988
2024-01-23 01:05:07,438 MainThread INFO: Time Consumed:0.22606468200683594s
2024-01-23 01:05:07,438 MainThread INFO: Total Frames:149100s
 10%|▉         | 989/10000 [06:35<38:02,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21760.42074
Train_Epoch_Reward                32018.95606
Running_Training_Average_Rewards  21373.74235
Explore_Time                      0.00085
Train___Time                      0.07819
Eval____Time                      0.14469
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23351.37388
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.40958     3.13362     100.56078    92.02468
alpha_0                           0.60989      0.00009     0.61001      0.60977
Alpha_loss                        -3.32159     0.00401     -3.31370     -3.32426
Training/policy_loss              -6.65612     0.00303     -6.65305     -6.66193
Training/qf1_loss                 8121.39805   1365.44626  10022.35449  6011.62256
Training/qf2_loss                 17785.89375  1839.39989  20438.90234  14895.75977
Training/pf_norm                  0.11869      0.02472     0.15287      0.07904
Training/qf1_norm                 2532.76184   588.45316   3313.24878   1709.67932
Training/qf2_norm                 2492.84873   80.66915    2600.43872   2378.52051
log_std/mean                      -0.13250     0.00006     -0.13241     -0.13255
log_probs/mean                    -2.71877     0.00696     -2.70552     -2.72503
mean/mean                         -0.00419     0.00005     -0.00414     -0.00428
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019164323806762695
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69809
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [149400]
collect time 0.0009031295776367188
inner_dict_sum {'sac_diff0': 0.0002193450927734375, 'sac_diff1': 0.007007122039794922, 'sac_diff2': 0.008314132690429688, 'sac_diff3': 0.010312080383300781, 'sac_diff4': 0.007083415985107422, 'sac_diff5': 0.03226637840270996, 'sac_diff6': 0.00040435791015625, 'all': 0.06560683250427246}
diff5_list [0.006662845611572266, 0.0067064762115478516, 0.00628352165222168, 0.006289005279541016, 0.0063245296478271484]
time3 0
time4 0.06638884544372559
time5 0.06644511222839355
time7 4.76837158203125e-07
gen_weight_change tensor(-17.2097)
policy weight change tensor(39.8579, grad_fn=<SumBackward0>)
time8 0.001859426498413086
train_time 0.07775259017944336
eval time 0.1492776870727539
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:07,691 MainThread INFO: EPOCH:989
2024-01-23 01:05:07,691 MainThread INFO: Time Consumed:0.2302865982055664s
2024-01-23 01:05:07,692 MainThread INFO: Total Frames:149250s
 10%|▉         | 990/10000 [06:35<38:00,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22032.17077
Train_Epoch_Reward                15481.21167
Running_Training_Average_Rewards  21505.23347
Explore_Time                      0.00090
Train___Time                      0.07775
Eval____Time                      0.14928
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23814.96991
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.61969     3.45448     98.88647     89.28078
alpha_0                           0.60959      0.00009     0.60971      0.60946
Alpha_loss                        -3.32546     0.00269     -3.32193     -3.32950
Training/policy_loss              -6.28589     0.00201     -6.28369     -6.28966
Training/qf1_loss                 7328.35586   699.06668   8134.59912   6369.62598
Training/qf2_loss                 16863.94121  1326.61635  18532.76562  14825.56738
Training/pf_norm                  0.17707      0.01426     0.19501      0.16070
Training/qf1_norm                 652.23922    310.66714   1196.34692   340.87402
Training/qf2_norm                 2389.21538   87.42896    2494.88452   2255.03979
log_std/mean                      -0.13203     0.00011     -0.13186     -0.13217
log_probs/mean                    -2.71981     0.00677     -2.71131     -2.73068
mean/mean                         -0.01316     0.00033     -0.01275     -0.01366
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018521547317504883
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69809
epoch first part time 3.337860107421875e-06
replay_buffer._size: [149550]
collect time 0.0010149478912353516
inside mustsac before update, task 0, sumup 69809
inside mustsac after update, task 0, sumup 70909
inner_dict_sum {'sac_diff0': 0.00021505355834960938, 'sac_diff1': 0.0073778629302978516, 'sac_diff2': 0.01331186294555664, 'sac_diff3': 0.012372732162475586, 'sac_diff4': 0.009239912033081055, 'sac_diff5': 0.05609011650085449, 'sac_diff6': 0.0004341602325439453, 'all': 0.09904170036315918}
diff5_list [0.010944843292236328, 0.010397911071777344, 0.010607719421386719, 0.010199308395385742, 0.01394033432006836]
time3 0.0009386539459228516
time4 0.10002851486206055
time5 0.1000978946685791
time7 0.01079249382019043
gen_weight_change tensor(-17.4588)
policy weight change tensor(39.8457, grad_fn=<SumBackward0>)
time8 0.003007650375366211
train_time 0.13309955596923828
eval time 0.10384607315063477
epoch last part time 9.5367431640625e-06
2024-01-23 01:05:07,954 MainThread INFO: EPOCH:990
2024-01-23 01:05:07,954 MainThread INFO: Time Consumed:0.24041104316711426s
2024-01-23 01:05:07,954 MainThread INFO: Total Frames:149400s
 10%|▉         | 991/10000 [06:35<38:35,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22317.53339
Train_Epoch_Reward                38132.50700
Running_Training_Average_Rewards  22305.99860
Explore_Time                      0.00101
Train___Time                      0.13310
Eval____Time                      0.10385
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23969.26727
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.48649     2.39270     99.01746     92.25917
alpha_0                           0.60928      0.00009     0.60940      0.60916
Alpha_loss                        -3.32968     0.00165     -3.32777     -3.33235
Training/policy_loss              -6.56738     0.24212     -6.30162     -6.94888
Training/qf1_loss                 8024.35859   1548.86275  10942.64746  6393.11621
Training/qf2_loss                 17568.01836  2010.53287  21430.07422  15557.07812
Training/pf_norm                  0.18632      0.06158     0.26826      0.10101
Training/qf1_norm                 442.49296    151.22281   649.61804    245.41554
Training/qf2_norm                 2522.49932   49.89334    2589.97266   2467.38403
log_std/mean                      -0.13389     0.00638     -0.12318     -0.14240
log_probs/mean                    -2.72156     0.00394     -2.71749     -2.72830
mean/mean                         -0.01447     0.00342     -0.01111     -0.01908
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019077539443969727
epoch last part time3 0.002971172332763672
inside rlalgo, task 0, sumup 70909
epoch first part time 5.245208740234375e-06
replay_buffer._size: [149700]
collect time 0.0009827613830566406
inner_dict_sum {'sac_diff0': 0.00021910667419433594, 'sac_diff1': 0.007729530334472656, 'sac_diff2': 0.008714675903320312, 'sac_diff3': 0.011402130126953125, 'sac_diff4': 0.008090972900390625, 'sac_diff5': 0.03408956527709961, 'sac_diff6': 0.0004177093505859375, 'all': 0.0706636905670166}
diff5_list [0.0066912174224853516, 0.006831645965576172, 0.006775617599487305, 0.006735086441040039, 0.007055997848510742]
time3 0
time4 0.0715336799621582
time5 0.07158756256103516
time7 7.152557373046875e-07
gen_weight_change tensor(-17.4588)
policy weight change tensor(39.9007, grad_fn=<SumBackward0>)
time8 0.002107381820678711
train_time 0.08356690406799316
eval time 0.14359402656555176
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:08,210 MainThread INFO: EPOCH:991
2024-01-23 01:05:08,210 MainThread INFO: Time Consumed:0.23073315620422363s
2024-01-23 01:05:08,210 MainThread INFO: Total Frames:149550s
 10%|▉         | 992/10000 [06:35<38:23,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22550.87977
Train_Epoch_Reward                28971.42509
Running_Training_Average_Rewards  22295.53611
Explore_Time                      0.00098
Train___Time                      0.08357
Eval____Time                      0.14359
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23393.68160
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.33027     2.74643     100.21712    92.34460
alpha_0                           0.60898      0.00009     0.60910      0.60886
Alpha_loss                        -3.33396     0.00430     -3.32753     -3.33814
Training/policy_loss              -6.83929     0.00647     -6.83196     -6.85088
Training/qf1_loss                 7885.16846   1905.22142  11156.00000  5589.27393
Training/qf2_loss                 17560.75352  2458.85971  21824.03516  14629.97461
Training/pf_norm                  0.16786      0.03468     0.21272      0.11782
Training/qf1_norm                 1341.37063   534.11027   2282.15186   711.55841
Training/qf2_norm                 2627.94546   78.58054    2766.74341   2540.17505
log_std/mean                      -0.13387     0.00009     -0.13375     -0.13400
log_probs/mean                    -2.72342     0.00787     -2.71315     -2.73162
mean/mean                         -0.01126     0.00013     -0.01105     -0.01139
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018531322479248047
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70909
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [149850]
collect time 0.0008647441864013672
inner_dict_sum {'sac_diff0': 0.00020122528076171875, 'sac_diff1': 0.006905078887939453, 'sac_diff2': 0.008160114288330078, 'sac_diff3': 0.010430574417114258, 'sac_diff4': 0.007139921188354492, 'sac_diff5': 0.032770395278930664, 'sac_diff6': 0.0003998279571533203, 'all': 0.06600713729858398}
diff5_list [0.007134437561035156, 0.006261348724365234, 0.006182193756103516, 0.006538867950439453, 0.006653547286987305]
time3 0
time4 0.06683683395385742
time5 0.06688451766967773
time7 7.152557373046875e-07
gen_weight_change tensor(-17.4588)
policy weight change tensor(39.8705, grad_fn=<SumBackward0>)
time8 0.002141237258911133
train_time 0.07848978042602539
eval time 0.14735054969787598
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:08,461 MainThread INFO: EPOCH:992
2024-01-23 01:05:08,461 MainThread INFO: Time Consumed:0.22905898094177246s
2024-01-23 01:05:08,461 MainThread INFO: Total Frames:149700s
 10%|▉         | 993/10000 [06:36<38:11,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22786.67647
Train_Epoch_Reward                16217.24090
Running_Training_Average_Rewards  22297.73745
Explore_Time                      0.00086
Train___Time                      0.07849
Eval____Time                      0.14735
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23403.57104
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.45788     3.14019     100.48042    91.25496
alpha_0                           0.60867      0.00009     0.60880      0.60855
Alpha_loss                        -3.33046     0.00435     -3.32482     -3.33826
Training/policy_loss              -6.83971     0.00392     -6.83603     -6.84644
Training/qf1_loss                 8557.35391   1180.29242  10475.34570  6878.46484
Training/qf2_loss                 18277.99766  1720.69537  21157.83594  15787.67383
Training/pf_norm                  0.21431      0.01435     0.23616      0.19661
Training/qf1_norm                 1280.32150   579.45333   2272.97559   561.83881
Training/qf2_norm                 2678.05850   88.48527    2820.93921   2558.92017
log_std/mean                      -0.12929     0.00002     -0.12926     -0.12931
log_probs/mean                    -2.70959     0.00900     -2.69555     -2.72395
mean/mean                         -0.01666     0.00018     -0.01644     -0.01695
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01895618438720703
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70909
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [150000]
collect time 0.0008776187896728516
inner_dict_sum {'sac_diff0': 0.00021147727966308594, 'sac_diff1': 0.006812334060668945, 'sac_diff2': 0.008216619491577148, 'sac_diff3': 0.010302066802978516, 'sac_diff4': 0.006765842437744141, 'sac_diff5': 0.03240466117858887, 'sac_diff6': 0.0003936290740966797, 'all': 0.06510663032531738}
diff5_list [0.006470918655395508, 0.006406545639038086, 0.005957365036010742, 0.006501197814941406, 0.007068634033203125]
time3 0
time4 0.06587743759155273
time5 0.06592488288879395
time7 7.152557373046875e-07
gen_weight_change tensor(-17.4588)
policy weight change tensor(39.7969, grad_fn=<SumBackward0>)
time8 0.0019443035125732422
train_time 0.0772256851196289
eval time 0.14800095558166504
epoch last part time 5.7220458984375e-06
2024-01-23 01:05:08,712 MainThread INFO: EPOCH:993
2024-01-23 01:05:08,712 MainThread INFO: Time Consumed:0.2285454273223877s
2024-01-23 01:05:08,712 MainThread INFO: Total Frames:149850s
 10%|▉         | 994/10000 [06:36<37:59,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22995.37556
Train_Epoch_Reward                16334.35958
Running_Training_Average_Rewards  22330.44015
Explore_Time                      0.00087
Train___Time                      0.07723
Eval____Time                      0.14800
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23316.80025
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.12618     3.45933     103.68257    94.33495
alpha_0                           0.60837      0.00009     0.60849      0.60825
Alpha_loss                        -3.33580     0.00535     -3.32903     -3.34400
Training/policy_loss              -6.17631     0.00578     -6.16783     -6.18478
Training/qf1_loss                 8059.50059   1924.47825  11870.41602  6684.36621
Training/qf2_loss                 18137.14180  2586.04000  23304.48633  16675.36719
Training/pf_norm                  0.13553      0.03260     0.18654      0.08644
Training/qf1_norm                 1068.76669   711.84487   2420.18457   483.19586
Training/qf2_norm                 2460.25400   88.16459    2628.59595   2393.31421
log_std/mean                      -0.13962     0.00006     -0.13956     -0.13972
log_probs/mean                    -2.71361     0.01207     -2.69998     -2.73282
mean/mean                         -0.01853     0.00009     -0.01839     -0.01865
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01798701286315918
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70909
epoch first part time 2.86102294921875e-06
replay_buffer._size: [150150]
collect time 0.0008347034454345703
inner_dict_sum {'sac_diff0': 0.00022339820861816406, 'sac_diff1': 0.007272243499755859, 'sac_diff2': 0.008681535720825195, 'sac_diff3': 0.010815858840942383, 'sac_diff4': 0.007270336151123047, 'sac_diff5': 0.03420090675354004, 'sac_diff6': 0.0004200935363769531, 'all': 0.06888437271118164}
diff5_list [0.0073506832122802734, 0.00659942626953125, 0.007413625717163086, 0.006504535675048828, 0.0063326358795166016]
time3 0
time4 0.06973528861999512
time5 0.06978893280029297
time7 9.5367431640625e-07
gen_weight_change tensor(-17.4588)
policy weight change tensor(39.9165, grad_fn=<SumBackward0>)
time8 0.001962900161743164
train_time 0.08111286163330078
eval time 0.14906907081604004
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:08,967 MainThread INFO: EPOCH:994
2024-01-23 01:05:08,967 MainThread INFO: Time Consumed:0.23340940475463867s
2024-01-23 01:05:08,967 MainThread INFO: Total Frames:150000s
 10%|▉         | 995/10000 [06:36<38:04,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23162.24305
Train_Epoch_Reward                43071.14302
Running_Training_Average_Rewards  21564.50345
Explore_Time                      0.00083
Train___Time                      0.08111
Eval____Time                      0.14907
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23417.99618
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.53722     1.91334     96.67548     91.90956
alpha_0                           0.60807      0.00009     0.60819      0.60795
Alpha_loss                        -3.34116     0.00333     -3.33766     -3.34609
Training/policy_loss              -6.35698     0.00609     -6.34712     -6.36500
Training/qf1_loss                 6780.87158   1311.20488  8853.44336   5352.42236
Training/qf2_loss                 16351.93145  1644.58489  18648.86328  14353.87500
Training/pf_norm                  0.22220      0.03893     0.27016      0.16763
Training/qf1_norm                 474.15280    352.27853   970.24731    153.42406
Training/qf2_norm                 2367.01113   47.31539    2419.29102   2297.36621
log_std/mean                      -0.13281     0.00010     -0.13274     -0.13299
log_probs/mean                    -2.71765     0.00534     -2.71196     -2.72487
mean/mean                         -0.01298     0.00005     -0.01290     -0.01303
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018165111541748047
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70909
epoch first part time 2.86102294921875e-06
replay_buffer._size: [150300]
collect time 0.0008580684661865234
inside mustsac before update, task 0, sumup 70909
inside mustsac after update, task 0, sumup 71277
inner_dict_sum {'sac_diff0': 0.0002396106719970703, 'sac_diff1': 0.0072896480560302734, 'sac_diff2': 0.008746862411499023, 'sac_diff3': 0.01121068000793457, 'sac_diff4': 0.007979869842529297, 'sac_diff5': 0.05596566200256348, 'sac_diff6': 0.00045680999755859375, 'all': 0.0918891429901123}
diff5_list [0.011678218841552734, 0.012770652770996094, 0.01145029067993164, 0.010267019271850586, 0.009799480438232422]
time3 0.0009212493896484375
time4 0.09277558326721191
time5 0.09282803535461426
time7 0.009305000305175781
gen_weight_change tensor(-17.7045)
policy weight change tensor(40.0273, grad_fn=<SumBackward0>)
time8 0.0022182464599609375
train_time 0.12318110466003418
eval time 0.11473631858825684
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:09,230 MainThread INFO: EPOCH:995
2024-01-23 01:05:09,230 MainThread INFO: Time Consumed:0.24106979370117188s
2024-01-23 01:05:09,230 MainThread INFO: Total Frames:150150s
 10%|▉         | 996/10000 [06:36<38:32,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23303.97817
Train_Epoch_Reward                12956.03813
Running_Training_Average_Rewards  21336.16202
Explore_Time                      0.00085
Train___Time                      0.12318
Eval____Time                      0.11474
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23690.14644
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.69425     1.98008     97.36683     91.77590
alpha_0                           0.60776      0.00009     0.60788      0.60764
Alpha_loss                        -3.34471     0.00409     -3.33978     -3.34918
Training/policy_loss              -6.88385     0.50052     -6.08404     -7.42604
Training/qf1_loss                 7262.61885   745.26721   8468.75195   6425.81104
Training/qf2_loss                 16808.61348  1026.84685  18044.55273  15575.02832
Training/pf_norm                  0.15808      0.03145     0.21785      0.12436
Training/qf1_norm                 807.51721    571.86403   1572.64795   244.13611
Training/qf2_norm                 2620.41714   261.81770   2968.00342   2275.32910
log_std/mean                      -0.13488     0.00367     -0.12927     -0.14067
log_probs/mean                    -2.71804     0.00748     -2.70818     -2.72838
mean/mean                         -0.01791     0.00470     -0.01266     -0.02362
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019511938095092773
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71277
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [150450]
collect time 0.0008590221405029297
inner_dict_sum {'sac_diff0': 0.00021028518676757812, 'sac_diff1': 0.00670623779296875, 'sac_diff2': 0.008083105087280273, 'sac_diff3': 0.010298967361450195, 'sac_diff4': 0.00695490837097168, 'sac_diff5': 0.03224897384643555, 'sac_diff6': 0.0003871917724609375, 'all': 0.06488966941833496}
diff5_list [0.006556987762451172, 0.006514549255371094, 0.006775379180908203, 0.006144046783447266, 0.0062580108642578125]
time3 0
time4 0.0656430721282959
time5 0.06569743156433105
time7 4.76837158203125e-07
gen_weight_change tensor(-17.7045)
policy weight change tensor(40.2257, grad_fn=<SumBackward0>)
time8 0.0019495487213134766
train_time 0.0768885612487793
eval time 0.15074729919433594
epoch last part time 6.198883056640625e-06
2024-01-23 01:05:09,484 MainThread INFO: EPOCH:996
2024-01-23 01:05:09,484 MainThread INFO: Time Consumed:0.23080778121948242s
2024-01-23 01:05:09,484 MainThread INFO: Total Frames:150300s
 10%|▉         | 997/10000 [06:37<38:20,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23118.04849
Train_Epoch_Reward                12445.67552
Running_Training_Average_Rewards  21509.61350
Explore_Time                      0.00085
Train___Time                      0.07689
Eval____Time                      0.15075
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20199.58889
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       93.50216     2.84281     99.01363     90.90172
alpha_0                           0.60746      0.00009     0.60758      0.60734
Alpha_loss                        -3.35011     0.00279     -3.34732     -3.35380
Training/policy_loss              -6.26395     0.00330     -6.25911     -6.26850
Training/qf1_loss                 7602.22744   1595.37680  10569.07422  5805.07764
Training/qf2_loss                 16900.66992  2185.10931  21067.18164  14746.66113
Training/pf_norm                  0.12798      0.03185     0.17991      0.08048
Training/qf1_norm                 1248.37760   557.82522   1733.57324   179.67805
Training/qf2_norm                 2390.02075   72.93238    2531.55884   2323.39185
log_std/mean                      -0.12730     0.00007     -0.12721     -0.12741
log_probs/mean                    -2.72215     0.00415     -2.71679     -2.72820
mean/mean                         -0.00930     0.00008     -0.00918     -0.00940
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018083572387695312
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71277
epoch first part time 2.86102294921875e-06
replay_buffer._size: [150600]
collect time 0.0008628368377685547
inner_dict_sum {'sac_diff0': 0.00023174285888671875, 'sac_diff1': 0.00710296630859375, 'sac_diff2': 0.008675336837768555, 'sac_diff3': 0.010547637939453125, 'sac_diff4': 0.0071849822998046875, 'sac_diff5': 0.03303861618041992, 'sac_diff6': 0.0003962516784667969, 'all': 0.06717753410339355}
diff5_list [0.006794929504394531, 0.006166219711303711, 0.006252288818359375, 0.007533550262451172, 0.006291627883911133]
time3 0
time4 0.06797933578491211
time5 0.06803154945373535
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7045)
policy weight change tensor(40.3962, grad_fn=<SumBackward0>)
time8 0.0020117759704589844
train_time 0.07968544960021973
eval time 0.14258337020874023
epoch last part time 6.198883056640625e-06
2024-01-23 01:05:09,731 MainThread INFO: EPOCH:997
2024-01-23 01:05:09,731 MainThread INFO: Time Consumed:0.2255098819732666s
2024-01-23 01:05:09,731 MainThread INFO: Total Frames:150450s
 10%|▉         | 998/10000 [06:37<37:58,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22898.57536
Train_Epoch_Reward                11707.46878
Running_Training_Average_Rewards  21652.79863
Explore_Time                      0.00086
Train___Time                      0.07969
Eval____Time                      0.14258
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20428.35816
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.34644     2.79433     103.78054    96.24663
alpha_0                           0.60716      0.00009     0.60728      0.60704
Alpha_loss                        -3.35186     0.00333     -3.34554     -3.35416
Training/policy_loss              -7.17754     0.00507     -7.17151     -7.18539
Training/qf1_loss                 8606.81484   1955.84438  11288.43750  6072.73730
Training/qf2_loss                 19084.67246  2505.43977  22672.19141  15901.59668
Training/pf_norm                  0.09699      0.03131     0.13585      0.05652
Training/qf1_norm                 1620.84802   540.52049   2473.60034   1000.44934
Training/qf2_norm                 2855.50420   79.70288    2981.46802   2766.54663
log_std/mean                      -0.14097     0.00007     -0.14085     -0.14105
log_probs/mean                    -2.71893     0.00834     -2.70359     -2.72622
mean/mean                         -0.01652     0.00003     -0.01646     -0.01656
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018426179885864258
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71277
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [150750]
collect time 0.0009124279022216797
inner_dict_sum {'sac_diff0': 0.00021076202392578125, 'sac_diff1': 0.006810188293457031, 'sac_diff2': 0.007866144180297852, 'sac_diff3': 0.010574817657470703, 'sac_diff4': 0.007332563400268555, 'sac_diff5': 0.03273797035217285, 'sac_diff6': 0.0003807544708251953, 'all': 0.06591320037841797}
diff5_list [0.007521867752075195, 0.006207942962646484, 0.006675243377685547, 0.0062596797943115234, 0.0060732364654541016]
time3 0
time4 0.06666374206542969
time5 0.0667121410369873
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7045)
policy weight change tensor(40.6414, grad_fn=<SumBackward0>)
time8 0.0019428730010986328
train_time 0.0781698226928711
eval time 0.14727473258972168
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:09,982 MainThread INFO: EPOCH:998
2024-01-23 01:05:09,982 MainThread INFO: Time Consumed:0.22867846488952637s
2024-01-23 01:05:09,982 MainThread INFO: Total Frames:150600s
 10%|▉         | 999/10000 [06:37<37:54,  3.96it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           22627.94879
Train_Epoch_Reward                23841.54637
Running_Training_Average_Rewards  20828.24517
Explore_Time                      0.00091
Train___Time                      0.07817
Eval____Time                      0.14727
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20645.10820
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       93.88384     1.60117    95.68654     91.01847
alpha_0                           0.60685      0.00009    0.60698      0.60673
Alpha_loss                        -3.35723     0.00214    -3.35516     -3.36100
Training/policy_loss              -6.72170     0.00636    -6.71553     -6.73262
Training/qf1_loss                 7571.41816   646.26418  8458.69238   6456.91260
Training/qf2_loss                 16813.52246  752.47363  18100.44141  15915.32324
Training/pf_norm                  0.21915      0.01899    0.24992      0.19358
Training/qf1_norm                 3003.30342   349.03374  3649.99146   2622.71094
Training/qf2_norm                 2534.05854   42.39034   2579.94165   2458.00757
log_std/mean                      -0.13252     0.00008    -0.13245     -0.13265
log_probs/mean                    -2.72297     0.00349    -2.71748     -2.72784
mean/mean                         -0.01622     0.00037    -0.01564     -0.01666
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01937413215637207
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71277
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [150900]
collect time 0.0009033679962158203
inner_dict_sum {'sac_diff0': 0.00020194053649902344, 'sac_diff1': 0.006752729415893555, 'sac_diff2': 0.008139610290527344, 'sac_diff3': 0.010356664657592773, 'sac_diff4': 0.006943225860595703, 'sac_diff5': 0.03215956687927246, 'sac_diff6': 0.0003788471221923828, 'all': 0.06493258476257324}
diff5_list [0.007488250732421875, 0.006249904632568359, 0.006329536437988281, 0.006094932556152344, 0.0059969425201416016]
time3 0
time4 0.06568408012390137
time5 0.06572866439819336
time7 4.76837158203125e-07
gen_weight_change tensor(-17.7045)
policy weight change tensor(41.0151, grad_fn=<SumBackward0>)
time8 0.0019097328186035156
train_time 0.07692670822143555
eval time 0.14830708503723145
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:10,234 MainThread INFO: EPOCH:999
2024-01-23 01:05:10,234 MainThread INFO: Time Consumed:0.2285449504852295s
2024-01-23 01:05:10,234 MainThread INFO: Total Frames:150750s
 10%|█         | 1000/10000 [06:37<37:51,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22370.92589
Train_Epoch_Reward                9783.29978
Running_Training_Average_Rewards  20897.64309
Explore_Time                      0.00090
Train___Time                      0.07693
Eval____Time                      0.14831
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21244.74091
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.16242     1.64957     96.57348     91.82027
alpha_0                           0.60655      0.00009     0.60667      0.60643
Alpha_loss                        -3.35873     0.00542     -3.34848     -3.36307
Training/policy_loss              -7.49682     0.00255     -7.49239     -7.49979
Training/qf1_loss                 7312.85986   1305.04670  8488.05371   5106.34863
Training/qf2_loss                 16774.38574  1570.24277  18170.65234  14069.60449
Training/pf_norm                  0.19686      0.01545     0.21629      0.17425
Training/qf1_norm                 1186.52319   339.55649   1661.99683   651.05396
Training/qf2_norm                 2806.96777   48.53816    2875.88940   2733.67798
log_std/mean                      -0.13222     0.00027     -0.13192     -0.13268
log_probs/mean                    -2.71926     0.00970     -2.70144     -2.72677
mean/mean                         -0.00697     0.00028     -0.00663     -0.00740
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01920485496520996
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71277
epoch first part time 3.337860107421875e-06
replay_buffer._size: [151050]
collect time 0.0008883476257324219
inside mustsac before update, task 0, sumup 71277
inside mustsac after update, task 0, sumup 71226
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.007284641265869141, 'sac_diff2': 0.00917673110961914, 'sac_diff3': 0.010990142822265625, 'sac_diff4': 0.007659435272216797, 'sac_diff5': 0.05428314208984375, 'sac_diff6': 0.0004134178161621094, 'all': 0.09001803398132324}
diff5_list [0.010894060134887695, 0.010584354400634766, 0.012463092803955078, 0.010458707809448242, 0.009882926940917969]
time3 0.0008664131164550781
time4 0.09088277816772461
time5 0.09093427658081055
time7 0.00876617431640625
gen_weight_change tensor(-17.9886)
policy weight change tensor(40.9340, grad_fn=<SumBackward0>)
time8 0.002566099166870117
train_time 0.12061691284179688
eval time 0.10662150382995605
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:10,487 MainThread INFO: EPOCH:1000
2024-01-23 01:05:10,487 MainThread INFO: Time Consumed:0.23054957389831543s
2024-01-23 01:05:10,487 MainThread INFO: Total Frames:150900s
 10%|█         | 1001/10000 [06:38<38:00,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22118.86260
Train_Epoch_Reward                10862.91198
Running_Training_Average_Rewards  18878.21574
Explore_Time                      0.00088
Train___Time                      0.12062
Eval____Time                      0.10662
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21448.63432
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.05015     1.24688     97.40348     94.20601
alpha_0                           0.60625      0.00009     0.60637      0.60613
Alpha_loss                        -3.36095     0.00341     -3.35560     -3.36586
Training/policy_loss              -6.94478     0.31880     -6.56368     -7.38190
Training/qf1_loss                 8130.41895   1172.15931  9903.41895   6965.78760
Training/qf2_loss                 17978.83555  1347.45513  19901.88867  16461.26172
Training/pf_norm                  0.21529      0.07532     0.34286      0.11650
Training/qf1_norm                 392.29206    197.19151   725.80481    205.55338
Training/qf2_norm                 2707.34019   138.61045   2854.67114   2533.46436
log_std/mean                      -0.13536     0.00508     -0.13032     -0.14276
log_probs/mean                    -2.71698     0.00652     -2.70897     -2.72681
mean/mean                         -0.01394     0.00268     -0.01083     -0.01789
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018837928771972656
epoch last part time3 0.0027594566345214844
inside rlalgo, task 0, sumup 71226
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [151200]
collect time 0.0008528232574462891
inner_dict_sum {'sac_diff0': 0.0002079010009765625, 'sac_diff1': 0.0073795318603515625, 'sac_diff2': 0.008929014205932617, 'sac_diff3': 0.01080942153930664, 'sac_diff4': 0.0069539546966552734, 'sac_diff5': 0.03322768211364746, 'sac_diff6': 0.0004036426544189453, 'all': 0.06791114807128906}
diff5_list [0.006523609161376953, 0.006384849548339844, 0.00777745246887207, 0.0062160491943359375, 0.006325721740722656]
time3 0
time4 0.06871438026428223
time5 0.06876683235168457
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9886)
policy weight change tensor(41.4185, grad_fn=<SumBackward0>)
time8 0.0018129348754882812
train_time 0.0801689624786377
eval time 0.14027070999145508
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:10,735 MainThread INFO: EPOCH:1001
2024-01-23 01:05:10,735 MainThread INFO: Time Consumed:0.22357726097106934s
2024-01-23 01:05:10,736 MainThread INFO: Total Frames:151050s
 10%|█         | 1002/10000 [06:38<37:38,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22703.57707
Train_Epoch_Reward                25957.91257
Running_Training_Average_Rewards  18778.53220
Explore_Time                      0.00085
Train___Time                      0.08017
Eval____Time                      0.14027
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29240.82634
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.93774     2.15242     97.14532     90.93964
alpha_0                           0.60595      0.00009     0.60607      0.60583
Alpha_loss                        -3.36220     0.00551     -3.35517     -3.36721
Training/policy_loss              -6.95865     0.00509     -6.95092     -6.96425
Training/qf1_loss                 8085.31953   1948.65799  11774.43164  6108.50830
Training/qf2_loss                 17618.03047  2335.43891  21855.82422  14703.77930
Training/pf_norm                  0.16301      0.02655     0.21025      0.13266
Training/qf1_norm                 2218.74121   494.64695   3123.43262   1636.08447
Training/qf2_norm                 2622.59902   57.09586    2679.74487   2515.82520
log_std/mean                      -0.13507     0.00036     -0.13457     -0.13558
log_probs/mean                    -2.71280     0.01010     -2.69876     -2.72414
mean/mean                         -0.01138     0.00012     -0.01120     -0.01153
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018593549728393555
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71226
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [151350]
collect time 0.000823974609375
inner_dict_sum {'sac_diff0': 0.0002079010009765625, 'sac_diff1': 0.00711369514465332, 'sac_diff2': 0.008364439010620117, 'sac_diff3': 0.010405302047729492, 'sac_diff4': 0.006978034973144531, 'sac_diff5': 0.03292703628540039, 'sac_diff6': 0.0003895759582519531, 'all': 0.06638598442077637}
diff5_list [0.007621049880981445, 0.006556987762451172, 0.006242275238037109, 0.006140947341918945, 0.006365776062011719]
time3 0
time4 0.06717562675476074
time5 0.06722688674926758
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9886)
policy weight change tensor(41.5609, grad_fn=<SumBackward0>)
time8 0.0018858909606933594
train_time 0.0785059928894043
eval time 0.15604758262634277
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:10,995 MainThread INFO: EPOCH:1002
2024-01-23 01:05:10,995 MainThread INFO: Time Consumed:0.23771071434020996s
2024-01-23 01:05:10,996 MainThread INFO: Total Frames:151200s
 10%|█         | 1003/10000 [06:38<38:03,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23329.82970
Train_Epoch_Reward                18865.61442
Running_Training_Average_Rewards  18589.04312
Explore_Time                      0.00082
Train___Time                      0.07851
Eval____Time                      0.15605
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29666.09733
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.48806     3.37361     100.43492    90.69456
alpha_0                           0.60564      0.00009     0.60576      0.60552
Alpha_loss                        -3.36656     0.00349     -3.36213     -3.37046
Training/policy_loss              -6.53914     0.00538     -6.52964     -6.54424
Training/qf1_loss                 7948.91436   995.16678   9341.77344   6928.67480
Training/qf2_loss                 17479.30156  1481.26468  19654.71094  15692.22363
Training/pf_norm                  0.21644      0.02886     0.26937      0.18714
Training/qf1_norm                 603.97035    378.04149   1249.20422   245.54909
Training/qf2_norm                 2581.04058   92.67290    2745.16895   2476.37842
log_std/mean                      -0.14163     0.00014     -0.14139     -0.14176
log_probs/mean                    -2.71480     0.00786     -2.70419     -2.72527
mean/mean                         -0.01476     0.00015     -0.01461     -0.01502
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019096851348876953
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 71226
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [151500]
collect time 0.0009946823120117188
inner_dict_sum {'sac_diff0': 0.0002186298370361328, 'sac_diff1': 0.007032632827758789, 'sac_diff2': 0.008580207824707031, 'sac_diff3': 0.010863780975341797, 'sac_diff4': 0.007241487503051758, 'sac_diff5': 0.03285837173461914, 'sac_diff6': 0.0003952980041503906, 'all': 0.06719040870666504}
diff5_list [0.006639957427978516, 0.0070726871490478516, 0.006780862808227539, 0.006170034408569336, 0.0061948299407958984]
time3 0
time4 0.06795430183410645
time5 0.06800055503845215
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9886)
policy weight change tensor(41.5485, grad_fn=<SumBackward0>)
time8 0.0018815994262695312
train_time 0.07926106452941895
eval time 0.1439361572265625
epoch last part time 5.4836273193359375e-06
2024-01-23 01:05:11,245 MainThread INFO: EPOCH:1003
2024-01-23 01:05:11,245 MainThread INFO: Time Consumed:0.22658324241638184s
2024-01-23 01:05:11,245 MainThread INFO: Total Frames:151350s
 10%|█         | 1004/10000 [06:38<37:52,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23947.07051
Train_Epoch_Reward                14441.20063
Running_Training_Average_Rewards  18709.65886
Explore_Time                      0.00099
Train___Time                      0.07926
Eval____Time                      0.14394
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29489.20833
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.70010     1.09109     96.70305     93.73899
alpha_0                           0.60534      0.00009     0.60546      0.60522
Alpha_loss                        -3.36865     0.00277     -3.36425     -3.37206
Training/policy_loss              -6.87513     0.00382     -6.87036     -6.88143
Training/qf1_loss                 7915.74502   1522.13762  9210.79492   5093.39014
Training/qf2_loss                 17731.18066  1777.11696  19208.34375  14440.09082
Training/pf_norm                  0.19230      0.03736     0.23308      0.12349
Training/qf1_norm                 1118.61871   252.25532   1346.18054   663.31573
Training/qf2_norm                 2651.22729   30.55061    2680.12305   2595.42017
log_std/mean                      -0.13254     0.00007     -0.13246     -0.13263
log_probs/mean                    -2.71229     0.00616     -2.70086     -2.71776
mean/mean                         -0.00672     0.00023     -0.00633     -0.00694
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01897454261779785
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71226
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [151650]
collect time 0.0009839534759521484
inner_dict_sum {'sac_diff0': 0.0002002716064453125, 'sac_diff1': 0.006688356399536133, 'sac_diff2': 0.008090496063232422, 'sac_diff3': 0.010039091110229492, 'sac_diff4': 0.007135868072509766, 'sac_diff5': 0.032178640365600586, 'sac_diff6': 0.0003783702850341797, 'all': 0.06471109390258789}
diff5_list [0.006532907485961914, 0.006850719451904297, 0.0062067508697509766, 0.006417036056518555, 0.006171226501464844]
time3 0
time4 0.065460205078125
time5 0.06550478935241699
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9886)
policy weight change tensor(41.3745, grad_fn=<SumBackward0>)
time8 0.0019161701202392578
train_time 0.07658743858337402
eval time 0.15132927894592285
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:11,499 MainThread INFO: EPOCH:1004
2024-01-23 01:05:11,499 MainThread INFO: Time Consumed:0.23131299018859863s
2024-01-23 01:05:11,499 MainThread INFO: Total Frames:151500s
 10%|█         | 1005/10000 [06:39<37:55,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           24571.23466
Train_Epoch_Reward                1878.41303
Running_Training_Average_Rewards  17930.48351
Explore_Time                      0.00098
Train___Time                      0.07659
Eval____Time                      0.15133
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29659.63766
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.11402     2.82675    100.30989    92.52912
alpha_0                           0.60504      0.00009    0.60516      0.60492
Alpha_loss                        -3.37527     0.00507    -3.36884     -3.38357
Training/policy_loss              -6.65348     0.00778    -6.64674     -6.66372
Training/qf1_loss                 8827.65664   523.70313  9584.37598   8083.26660
Training/qf2_loss                 18929.58750  930.15818  19802.18359  17305.62695
Training/pf_norm                  0.16987      0.03133    0.20152      0.11331
Training/qf1_norm                 646.43435    317.06941  1081.73352   347.52850
Training/qf2_norm                 2597.15591   74.14961   2682.56177   2476.44604
log_std/mean                      -0.13938     0.00031    -0.13889     -0.13972
log_probs/mean                    -2.71881     0.00861    -2.70868     -2.73264
mean/mean                         -0.00868     0.00039    -0.00809     -0.00919
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01883673667907715
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71226
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [151800]
collect time 0.0008654594421386719
inside mustsac before update, task 0, sumup 71226
inside mustsac after update, task 0, sumup 70040
inner_dict_sum {'sac_diff0': 0.0002257823944091797, 'sac_diff1': 0.007301807403564453, 'sac_diff2': 0.008726358413696289, 'sac_diff3': 0.010792255401611328, 'sac_diff4': 0.007810831069946289, 'sac_diff5': 0.05351710319519043, 'sac_diff6': 0.00042724609375, 'all': 0.08880138397216797}
diff5_list [0.01106882095336914, 0.012453794479370117, 0.01007843017578125, 0.010104894638061523, 0.009811162948608398]
time3 0.0008852481842041016
time4 0.08968496322631836
time5 0.08974051475524902
time7 0.009356975555419922
gen_weight_change tensor(-18.1499)
policy weight change tensor(41.3503, grad_fn=<SumBackward0>)
time8 0.0019707679748535156
train_time 0.11992239952087402
eval time 0.1066279411315918
epoch last part time 7.152557373046875e-06
2024-01-23 01:05:11,751 MainThread INFO: EPOCH:1005
2024-01-23 01:05:11,752 MainThread INFO: Time Consumed:0.22978544235229492s
2024-01-23 01:05:11,752 MainThread INFO: Total Frames:151650s
 10%|█         | 1006/10000 [06:39<37:55,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           25206.19795
Train_Epoch_Reward                21496.44708
Running_Training_Average_Rewards  16827.63408
Explore_Time                      0.00086
Train___Time                      0.11992
Eval____Time                      0.10663
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             30039.77941
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.33600     0.79402     96.14026     93.83876
alpha_0                           0.60474      0.00009     0.60486      0.60462
Alpha_loss                        -3.37721     0.00443     -3.37265     -3.38548
Training/policy_loss              -6.81610     0.11527     -6.71532     -7.01378
Training/qf1_loss                 7223.43711   732.81630   8631.71289   6656.97168
Training/qf2_loss                 16868.96113  843.53254   18422.05078  15920.77832
Training/pf_norm                  0.16692      0.03654     0.20760      0.10158
Training/qf1_norm                 1220.71948   1161.72211  3533.71289   556.69818
Training/qf2_norm                 2616.18901   39.94205    2658.59326   2551.78418
log_std/mean                      -0.13331     0.00417     -0.12651     -0.13741
log_probs/mean                    -2.71600     0.00979     -2.70559     -2.73378
mean/mean                         -0.00518     0.00393     0.00043      -0.00957
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01962113380432129
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70040
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [151950]
collect time 0.0009067058563232422
inner_dict_sum {'sac_diff0': 0.00021338462829589844, 'sac_diff1': 0.006926536560058594, 'sac_diff2': 0.008236169815063477, 'sac_diff3': 0.010433197021484375, 'sac_diff4': 0.0068035125732421875, 'sac_diff5': 0.03152346611022949, 'sac_diff6': 0.00038361549377441406, 'all': 0.06451988220214844}
diff5_list [0.006547689437866211, 0.00641942024230957, 0.006469011306762695, 0.006043672561645508, 0.006043672561645508]
time3 0
time4 0.06526851654052734
time5 0.06531453132629395
time7 4.76837158203125e-07
gen_weight_change tensor(-18.1499)
policy weight change tensor(41.1121, grad_fn=<SumBackward0>)
time8 0.0017850399017333984
train_time 0.07643413543701172
eval time 0.1516861915588379
epoch last part time 7.152557373046875e-06
2024-01-23 01:05:12,007 MainThread INFO: EPOCH:1006
2024-01-23 01:05:12,025 MainThread INFO: Time Consumed:0.2314159870147705s
2024-01-23 01:05:12,026 MainThread INFO: Total Frames:151800s
 10%|█         | 1007/10000 [06:39<38:49,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           26455.75305
Train_Epoch_Reward                11792.65495
Running_Training_Average_Rewards  17043.88706
Explore_Time                      0.00090
Train___Time                      0.07643
Eval____Time                      0.15169
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             32695.13982
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.15217     3.50727     101.92508    92.57751
alpha_0                           0.60444      0.00009     0.60456      0.60432
Alpha_loss                        -3.38007     0.00560     -3.37384     -3.38850
Training/policy_loss              -6.98279     0.00518     -6.97669     -6.99041
Training/qf1_loss                 9235.97881   1036.22991  10572.61914  7534.77490
Training/qf2_loss                 19420.91172  1681.14816  21471.83984  16673.90430
Training/pf_norm                  0.16873      0.02179     0.19350      0.13287
Training/qf1_norm                 2081.76689   614.72681   2721.16870   1090.42249
Training/qf2_norm                 2789.92412   101.47271   2900.25269   2630.59644
log_std/mean                      -0.13962     0.00036     -0.13913     -0.14016
log_probs/mean                    -2.71503     0.01036     -2.70264     -2.72910
mean/mean                         -0.00138     0.00006     -0.00132     -0.00150
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.03740096092224121
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70040
epoch first part time 2.86102294921875e-06
replay_buffer._size: [152100]
collect time 0.0009341239929199219
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.006931304931640625, 'sac_diff2': 0.008510828018188477, 'sac_diff3': 0.010681390762329102, 'sac_diff4': 0.007108926773071289, 'sac_diff5': 0.032952308654785156, 'sac_diff6': 0.00040721893310546875, 'all': 0.06680631637573242}
diff5_list [0.006486415863037109, 0.006651639938354492, 0.007242679595947266, 0.0063779354095458984, 0.006193637847900391]
time3 0
time4 0.06763529777526855
time5 0.0676872730255127
time7 7.152557373046875e-07
gen_weight_change tensor(-18.1499)
policy weight change tensor(40.9158, grad_fn=<SumBackward0>)
time8 0.0020627975463867188
train_time 0.07912087440490723
eval time 0.1251060962677002
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:12,255 MainThread INFO: EPOCH:1007
2024-01-23 01:05:12,255 MainThread INFO: Time Consumed:0.20755910873413086s
2024-01-23 01:05:12,256 MainThread INFO: Total Frames:151950s
 10%|█         | 1008/10000 [06:39<37:30,  4.00it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           27693.10264
Train_Epoch_Reward                17582.48102
Running_Training_Average_Rewards  17237.40499
Explore_Time                      0.00093
Train___Time                      0.07912
Eval____Time                      0.12511
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             32801.85406
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.04438     1.26871    98.84776     94.99679
alpha_0                           0.60413      0.00009    0.60426      0.60401
Alpha_loss                        -3.37915     0.00366    -3.37505     -3.38503
Training/policy_loss              -7.23121     0.00626    -7.22557     -7.24300
Training/qf1_loss                 8668.88193   554.07609  9364.59180   8060.63281
Training/qf2_loss                 18698.96602  720.07338  19515.88672  17704.89648
Training/pf_norm                  0.16072      0.02242    0.19376      0.12788
Training/qf1_norm                 1166.17457   198.54301  1422.84753   855.77948
Training/qf2_norm                 2790.72510   35.94139   2842.07593   2731.46509
log_std/mean                      -0.12478     0.00007    -0.12468     -0.12489
log_probs/mean                    -2.70654     0.00678    -2.69974     -2.71821
mean/mean                         -0.00531     0.00003    -0.00527     -0.00534
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018706083297729492
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70040
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [152250]
collect time 0.0009398460388183594
inner_dict_sum {'sac_diff0': 0.00022554397583007812, 'sac_diff1': 0.007291078567504883, 'sac_diff2': 0.008811712265014648, 'sac_diff3': 0.011695623397827148, 'sac_diff4': 0.007448673248291016, 'sac_diff5': 0.03396463394165039, 'sac_diff6': 0.0004189014434814453, 'all': 0.06985616683959961}
diff5_list [0.007049560546875, 0.006456136703491211, 0.0066335201263427734, 0.006905078887939453, 0.006920337677001953]
time3 0
time4 0.07068562507629395
time5 0.07073473930358887
time7 7.152557373046875e-07
gen_weight_change tensor(-18.1499)
policy weight change tensor(40.6388, grad_fn=<SumBackward0>)
time8 0.0019037723541259766
train_time 0.08203339576721191
eval time 0.14000582695007324
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:12,503 MainThread INFO: EPOCH:1008
2024-01-23 01:05:12,503 MainThread INFO: Time Consumed:0.2253730297088623s
2024-01-23 01:05:12,503 MainThread INFO: Total Frames:152100s
 10%|█         | 1009/10000 [06:40<37:23,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           28921.46218
Train_Epoch_Reward                55659.81520
Running_Training_Average_Rewards  18842.99751
Explore_Time                      0.00094
Train___Time                      0.08203
Eval____Time                      0.14001
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             32928.70361
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.01479     1.73452     97.28419     91.96992
alpha_0                           0.60383      0.00009     0.60395      0.60371
Alpha_loss                        -3.38753     0.00320     -3.38400     -3.39205
Training/policy_loss              -6.94779     0.00711     -6.93973     -6.95913
Training/qf1_loss                 8016.97139   775.83713   8866.97266   6855.89453
Training/qf2_loss                 17538.56582  1018.26474  18545.01953  16186.87793
Training/pf_norm                  0.24116      0.04902     0.31303      0.17986
Training/qf1_norm                 2735.10254   357.70587   3363.21045   2256.14551
Training/qf2_norm                 2628.51562   47.05278    2689.68872   2545.18237
log_std/mean                      -0.13302     0.00023     -0.13268     -0.13331
log_probs/mean                    -2.71653     0.00607     -2.70953     -2.72683
mean/mean                         -0.00740     0.00039     -0.00676     -0.00783
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018767356872558594
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70040
epoch first part time 3.337860107421875e-06
replay_buffer._size: [152400]
collect time 0.0008828639984130859
inner_dict_sum {'sac_diff0': 0.00021839141845703125, 'sac_diff1': 0.007305145263671875, 'sac_diff2': 0.009108543395996094, 'sac_diff3': 0.011037826538085938, 'sac_diff4': 0.007440090179443359, 'sac_diff5': 0.03338432312011719, 'sac_diff6': 0.0004227161407470703, 'all': 0.06891703605651855}
diff5_list [0.006619930267333984, 0.0062198638916015625, 0.007532596588134766, 0.006795167922973633, 0.006216764450073242]
time3 0
time4 0.06977081298828125
time5 0.0698237419128418
time7 4.76837158203125e-07
gen_weight_change tensor(-18.1499)
policy weight change tensor(40.4420, grad_fn=<SumBackward0>)
time8 0.0018575191497802734
train_time 0.08109283447265625
eval time 0.1479027271270752
epoch last part time 6.198883056640625e-06
2024-01-23 01:05:12,757 MainThread INFO: EPOCH:1009
2024-01-23 01:05:12,758 MainThread INFO: Time Consumed:0.23223114013671875s
2024-01-23 01:05:12,758 MainThread INFO: Total Frames:152250s
 10%|█         | 1010/10000 [06:40<37:36,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30145.12833
Train_Epoch_Reward                31504.11174
Running_Training_Average_Rewards  19536.36192
Explore_Time                      0.00088
Train___Time                      0.08109
Eval____Time                      0.14790
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             33481.40241
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.72086     1.09387     98.99387     95.87637
alpha_0                           0.60353      0.00009     0.60365      0.60341
Alpha_loss                        -3.39046     0.00594     -3.38059     -3.39879
Training/policy_loss              -6.97135     0.00413     -6.96508     -6.97754
Training/qf1_loss                 8594.07686   1069.48638  9581.61133   6825.06396
Training/qf2_loss                 18828.65742  1291.34942  20017.58203  16658.38672
Training/pf_norm                  0.20446      0.02543     0.23325      0.16826
Training/qf1_norm                 324.62650    97.92845    441.19983    174.86775
Training/qf2_norm                 2836.65903   32.71240    2874.76025   2782.34229
log_std/mean                      -0.12383     0.00004     -0.12377     -0.12387
log_probs/mean                    -2.71568     0.01105     -2.69879     -2.73219
mean/mean                         -0.00703     0.00027     -0.00667     -0.00744
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018681764602661133
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70040
epoch first part time 3.337860107421875e-06
replay_buffer._size: [152550]
collect time 0.0008363723754882812
inside mustsac before update, task 0, sumup 70040
inside mustsac after update, task 0, sumup 71292
inner_dict_sum {'sac_diff0': 0.00022602081298828125, 'sac_diff1': 0.00733637809753418, 'sac_diff2': 0.008587837219238281, 'sac_diff3': 0.010959148406982422, 'sac_diff4': 0.007710695266723633, 'sac_diff5': 0.05381202697753906, 'sac_diff6': 0.00045680999755859375, 'all': 0.08908891677856445}
diff5_list [0.012295722961425781, 0.011044502258300781, 0.010323047637939453, 0.010081291198730469, 0.010067462921142578]
time3 0.0008745193481445312
time4 0.09000515937805176
time5 0.09005975723266602
time7 0.009449481964111328
gen_weight_change tensor(-18.1401)
policy weight change tensor(40.4279, grad_fn=<SumBackward0>)
time8 0.0028829574584960938
train_time 0.12165260314941406
eval time 0.10523605346679688
epoch last part time 5.0067901611328125e-06
2024-01-23 01:05:13,010 MainThread INFO: EPOCH:1010
2024-01-23 01:05:13,010 MainThread INFO: Time Consumed:0.230088472366333s
2024-01-23 01:05:13,010 MainThread INFO: Total Frames:152400s
 10%|█         | 1011/10000 [06:40<37:50,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31382.75627
Train_Epoch_Reward                45734.63687
Running_Training_Average_Rewards  20899.32341
Explore_Time                      0.00083
Train___Time                      0.12165
Eval____Time                      0.10524
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             33824.91369
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.49474     2.13334     97.86067     92.85869
alpha_0                           0.60323      0.00009     0.60335      0.60311
Alpha_loss                        -3.39530     0.00365     -3.38866     -3.39899
Training/policy_loss              -7.29359     0.25547     -7.02441     -7.70558
Training/qf1_loss                 7914.13447   593.42529   8463.94141   7165.96240
Training/qf2_loss                 17643.73164  1004.10709  18659.92383  16439.44336
Training/pf_norm                  0.20655      0.04830     0.27134      0.12864
Training/qf1_norm                 748.72419    494.91309   1545.12769   195.22627
Training/qf2_norm                 2787.82544   105.73021   2904.81104   2626.31982
log_std/mean                      -0.13054     0.00342     -0.12497     -0.13482
log_probs/mean                    -2.71865     0.00671     -2.70683     -2.72594
mean/mean                         -0.00368     0.00341     -0.00069     -0.01033
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019886493682861328
epoch last part time3 0.0026199817657470703
inside rlalgo, task 0, sumup 71292
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [152700]
collect time 0.0008718967437744141
inner_dict_sum {'sac_diff0': 0.00021886825561523438, 'sac_diff1': 0.006979703903198242, 'sac_diff2': 0.008176088333129883, 'sac_diff3': 0.01040792465209961, 'sac_diff4': 0.006972312927246094, 'sac_diff5': 0.03277921676635742, 'sac_diff6': 0.00038886070251464844, 'all': 0.06592297554016113}
diff5_list [0.007785797119140625, 0.006356716156005859, 0.006506681442260742, 0.006106376647949219, 0.0060236454010009766]
time3 0
time4 0.06670475006103516
time5 0.06675553321838379
time7 7.152557373046875e-07
gen_weight_change tensor(-18.1401)
policy weight change tensor(40.3967, grad_fn=<SumBackward0>)
time8 0.0019028186798095703
train_time 0.0781257152557373
eval time 0.14388275146484375
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:13,261 MainThread INFO: EPOCH:1011
2024-01-23 01:05:13,261 MainThread INFO: Time Consumed:0.22526884078979492s
2024-01-23 01:05:13,261 MainThread INFO: Total Frames:152550s
 10%|█         | 1012/10000 [06:40<37:34,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           31514.70220
Train_Epoch_Reward                12876.05031
Running_Training_Average_Rewards  20623.85388
Explore_Time                      0.00087
Train___Time                      0.07813
Eval____Time                      0.14388
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             30560.28571
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.07226     1.62881    99.21342     94.19508
alpha_0                           0.60293      0.00009    0.60305      0.60281
Alpha_loss                        -3.39575     0.00343    -3.39150     -3.40097
Training/policy_loss              -7.44194     0.00442    -7.43702     -7.44760
Training/qf1_loss                 7698.07363   717.15461  8970.89453   7060.07178
Training/qf2_loss                 17761.03477  984.80543  19517.65039  16820.58203
Training/pf_norm                  0.16130      0.01492    0.18118      0.13811
Training/qf1_norm                 588.47346    341.94308  1225.78894   220.53128
Training/qf2_norm                 2951.37285   48.28130   3010.85083   2863.71924
log_std/mean                      -0.12521     0.00008    -0.12514     -0.12537
log_probs/mean                    -2.71291     0.00691    -2.70658     -2.72322
mean/mean                         -0.01129     0.00022    -0.01097     -0.01161
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018263578414916992
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71292
epoch first part time 2.86102294921875e-06
replay_buffer._size: [152850]
collect time 0.000865936279296875
inner_dict_sum {'sac_diff0': 0.00021195411682128906, 'sac_diff1': 0.007554531097412109, 'sac_diff2': 0.008585929870605469, 'sac_diff3': 0.010651111602783203, 'sac_diff4': 0.0073473453521728516, 'sac_diff5': 0.03465127944946289, 'sac_diff6': 0.0004112720489501953, 'all': 0.06941342353820801}
diff5_list [0.00725102424621582, 0.0062961578369140625, 0.006629467010498047, 0.007585763931274414, 0.006888866424560547]
time3 0
time4 0.07028961181640625
time5 0.0703437328338623
time7 9.5367431640625e-07
gen_weight_change tensor(-18.1401)
policy weight change tensor(40.4852, grad_fn=<SumBackward0>)
time8 0.001987457275390625
train_time 0.08174419403076172
eval time 0.14430904388427734
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:13,512 MainThread INFO: EPOCH:1012
2024-01-23 01:05:13,512 MainThread INFO: Time Consumed:0.2293398380279541s
2024-01-23 01:05:13,512 MainThread INFO: Total Frames:152700s
 10%|█         | 1013/10000 [06:41<37:36,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31576.88923
Train_Epoch_Reward                11221.20224
Running_Training_Average_Rewards  20442.50342
Explore_Time                      0.00086
Train___Time                      0.08174
Eval____Time                      0.14431
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             30287.96761
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.48245     1.11128     97.51076     94.15833
alpha_0                           0.60263      0.00009     0.60275      0.60251
Alpha_loss                        -3.40229     0.00178     -3.39973     -3.40478
Training/policy_loss              -7.67462     0.00265     -7.67162     -7.67796
Training/qf1_loss                 7772.99717   1723.65313  11121.97168  6330.91309
Training/qf2_loss                 17478.63008  1941.40343  21310.85938  16043.39648
Training/pf_norm                  0.12627      0.01613     0.14841      0.09877
Training/qf1_norm                 1135.73474   238.82524   1419.33582   720.01514
Training/qf2_norm                 2932.65981   33.49092    2992.25317   2890.14282
log_std/mean                      -0.14196     0.00004     -0.14189     -0.14199
log_probs/mean                    -2.71920     0.00447     -2.71281     -2.72678
mean/mean                         -0.00532     0.00010     -0.00518     -0.00547
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01884746551513672
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71292
epoch first part time 2.86102294921875e-06
replay_buffer._size: [153000]
collect time 0.0008630752563476562
inner_dict_sum {'sac_diff0': 0.0002186298370361328, 'sac_diff1': 0.007409334182739258, 'sac_diff2': 0.008549690246582031, 'sac_diff3': 0.010390996932983398, 'sac_diff4': 0.006966352462768555, 'sac_diff5': 0.033249855041503906, 'sac_diff6': 0.00040340423583984375, 'all': 0.06718826293945312}
diff5_list [0.007451057434082031, 0.0068204402923583984, 0.006648063659667969, 0.0061626434326171875, 0.00616765022277832]
time3 0
time4 0.06800675392150879
time5 0.06805968284606934
time7 7.152557373046875e-07
gen_weight_change tensor(-18.1401)
policy weight change tensor(40.6566, grad_fn=<SumBackward0>)
time8 0.0019097328186035156
train_time 0.07943844795227051
eval time 0.1426231861114502
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:13,760 MainThread INFO: EPOCH:1013
2024-01-23 01:05:13,760 MainThread INFO: Time Consumed:0.22529125213623047s
2024-01-23 01:05:13,760 MainThread INFO: Total Frames:152850s
 10%|█         | 1014/10000 [06:41<37:29,  3.99it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           31619.84022
Train_Epoch_Reward                17433.75781
Running_Training_Average_Rewards  20422.71868
Explore_Time                      0.00086
Train___Time                      0.07944
Eval____Time                      0.14262
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29918.71825
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       95.59901     0.98442    97.21413     94.54452
alpha_0                           0.60233      0.00009    0.60245      0.60221
Alpha_loss                        -3.40362     0.00351    -3.39968     -3.40939
Training/policy_loss              -8.00595     0.00508    -7.99738     -8.01309
Training/qf1_loss                 8182.52285   939.95229  9598.04102   7032.54004
Training/qf2_loss                 17881.18281  857.42591  19098.93164  16629.98438
Training/pf_norm                  0.27914      0.02955    0.33298      0.24226
Training/qf1_norm                 529.94551    178.88812  827.17151    360.42450
Training/qf2_norm                 3044.97690   29.38218   3093.07446   3011.81250
log_std/mean                      -0.13497     0.00009    -0.13488     -0.13511
log_probs/mean                    -2.71521     0.00748    -2.70480     -2.72528
mean/mean                         -0.01383     0.00034    -0.01329     -0.01424
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01968979835510254
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71292
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [153150]
collect time 0.0008323192596435547
inner_dict_sum {'sac_diff0': 0.00020384788513183594, 'sac_diff1': 0.00690770149230957, 'sac_diff2': 0.007892608642578125, 'sac_diff3': 0.010391712188720703, 'sac_diff4': 0.007101774215698242, 'sac_diff5': 0.03256058692932129, 'sac_diff6': 0.00038814544677734375, 'all': 0.06544637680053711}
diff5_list [0.006745576858520508, 0.006167173385620117, 0.006756782531738281, 0.006693363189697266, 0.006197690963745117]
time3 0
time4 0.06623482704162598
time5 0.06627964973449707
time7 7.152557373046875e-07
gen_weight_change tensor(-18.1401)
policy weight change tensor(40.6833, grad_fn=<SumBackward0>)
time8 0.001982450485229492
train_time 0.07760930061340332
eval time 0.14998388290405273
epoch last part time 5.7220458984375e-06
2024-01-23 01:05:14,014 MainThread INFO: EPOCH:1014
2024-01-23 01:05:14,014 MainThread INFO: Time Consumed:0.23085427284240723s
2024-01-23 01:05:14,014 MainThread INFO: Total Frames:153000s
 10%|█         | 1015/10000 [06:41<37:49,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31712.87309
Train_Epoch_Reward                17917.51773
Running_Training_Average_Rewards  20911.05994
Explore_Time                      0.00083
Train___Time                      0.07761
Eval____Time                      0.14998
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             30589.96635
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.14707     2.98806     100.89156    93.34898
alpha_0                           0.60203      0.00009     0.60215      0.60191
Alpha_loss                        -3.40795     0.00290     -3.40249     -3.41045
Training/policy_loss              -7.02945     0.00536     -7.02014     -7.03404
Training/qf1_loss                 7962.59990   1450.83108  10340.37988  6090.36230
Training/qf2_loss                 18049.83379  1893.25369  20918.48047  15405.22168
Training/pf_norm                  0.17914      0.02247     0.20359      0.14250
Training/qf1_norm                 758.96041    464.23711   1374.10095   224.12621
Training/qf2_norm                 2771.76475   84.95535    2880.23096   2664.34717
log_std/mean                      -0.12848     0.00006     -0.12841     -0.12857
log_probs/mean                    -2.71715     0.00631     -2.70505     -2.72339
mean/mean                         0.00218      0.00023     0.00245      0.00182
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.023337602615356445
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 71292
epoch first part time 3.337860107421875e-06
replay_buffer._size: [153300]
collect time 0.0008947849273681641
inside mustsac before update, task 0, sumup 71292
inside mustsac after update, task 0, sumup 70307
inner_dict_sum {'sac_diff0': 0.0002262592315673828, 'sac_diff1': 0.007626056671142578, 'sac_diff2': 0.009299993515014648, 'sac_diff3': 0.011332273483276367, 'sac_diff4': 0.00800776481628418, 'sac_diff5': 0.053977012634277344, 'sac_diff6': 0.0004253387451171875, 'all': 0.09089469909667969}
diff5_list [0.011100530624389648, 0.010107278823852539, 0.010339736938476562, 0.010392904281616211, 0.012036561965942383]
time3 0.0009224414825439453
time4 0.0917971134185791
time5 0.09185290336608887
time7 0.009335756301879883
gen_weight_change tensor(-18.0904)
policy weight change tensor(40.6327, grad_fn=<SumBackward0>)
time8 0.0019397735595703125
train_time 0.12231779098510742
eval time 0.11144804954528809
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:14,278 MainThread INFO: EPOCH:1015
2024-01-23 01:05:14,279 MainThread INFO: Time Consumed:0.23714613914489746s
2024-01-23 01:05:14,279 MainThread INFO: Total Frames:153150s
 10%|█         | 1016/10000 [06:41<38:10,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31886.70216
Train_Epoch_Reward                43070.00162
Running_Training_Average_Rewards  21831.66467
Explore_Time                      0.00089
Train___Time                      0.12232
Eval____Time                      0.11145
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             31778.07006
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.94072     2.49516     103.23202    95.78369
alpha_0                           0.60173      0.00008     0.60185      0.60161
Alpha_loss                        -3.41028     0.00115     -3.40844     -3.41172
Training/policy_loss              -7.08706     0.34982     -6.52947     -7.44909
Training/qf1_loss                 9242.21426   1001.14386  10591.60254  8144.53125
Training/qf2_loss                 19886.19258  1417.88536  21526.90820  18004.38672
Training/pf_norm                  0.20236      0.05124     0.26880      0.13088
Training/qf1_norm                 1226.84097   802.42965   2375.46777   254.18489
Training/qf2_norm                 2824.65020   171.37095   3042.91724   2610.60156
log_std/mean                      -0.13445     0.00463     -0.12600     -0.13918
log_probs/mean                    -2.71513     0.00386     -2.71018     -2.71959
mean/mean                         -0.00927     0.00669     -0.00006     -0.01971
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01923203468322754
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70307
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [153450]
collect time 0.0008544921875
inner_dict_sum {'sac_diff0': 0.00022172927856445312, 'sac_diff1': 0.007380247116088867, 'sac_diff2': 0.009082794189453125, 'sac_diff3': 0.01168966293334961, 'sac_diff4': 0.008095979690551758, 'sac_diff5': 0.03368353843688965, 'sac_diff6': 0.00044417381286621094, 'all': 0.07059812545776367}
diff5_list [0.006775856018066406, 0.006741523742675781, 0.006487607955932617, 0.007113456726074219, 0.006565093994140625]
time3 0
time4 0.07146239280700684
time5 0.07151603698730469
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0904)
policy weight change tensor(40.5862, grad_fn=<SumBackward0>)
time8 0.0018393993377685547
train_time 0.08297586441040039
eval time 0.14659404754638672
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:14,534 MainThread INFO: EPOCH:1016
2024-01-23 01:05:14,535 MainThread INFO: Time Consumed:0.2328345775604248s
2024-01-23 01:05:14,535 MainThread INFO: Total Frames:153300s
 10%|█         | 1017/10000 [06:42<38:10,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31570.90240
Train_Epoch_Reward                37155.03008
Running_Training_Average_Rewards  22338.63318
Explore_Time                      0.00085
Train___Time                      0.08298
Eval____Time                      0.14659
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29537.14226
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.77574     2.40918     100.91368    93.90400
alpha_0                           0.60143      0.00008     0.60155      0.60131
Alpha_loss                        -3.41623     0.00317     -3.41012     -3.41868
Training/policy_loss              -6.69179     0.00267     -6.68756     -6.69501
Training/qf1_loss                 8492.60469   981.55523   9257.72461   6598.51465
Training/qf2_loss                 18713.27773  1471.51443  20111.03516  15973.33789
Training/pf_norm                  0.11388      0.00927     0.12056      0.09641
Training/qf1_norm                 499.10271    271.75570   1003.71417   247.86186
Training/qf2_norm                 2587.42925   61.54792    2667.16797   2489.90869
log_std/mean                      -0.14172     0.00005     -0.14162     -0.14177
log_probs/mean                    -2.72023     0.00504     -2.71085     -2.72503
mean/mean                         0.00155      0.00013     0.00175      0.00138
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01833343505859375
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70307
epoch first part time 2.86102294921875e-06
replay_buffer._size: [153600]
collect time 0.0010457038879394531
inner_dict_sum {'sac_diff0': 0.0002167224884033203, 'sac_diff1': 0.007606983184814453, 'sac_diff2': 0.008802175521850586, 'sac_diff3': 0.011307001113891602, 'sac_diff4': 0.007288217544555664, 'sac_diff5': 0.03329157829284668, 'sac_diff6': 0.0003962516784667969, 'all': 0.0689089298248291}
diff5_list [0.0074710845947265625, 0.006481170654296875, 0.006646871566772461, 0.006301403045654297, 0.006391048431396484]
time3 0
time4 0.06971192359924316
time5 0.06976580619812012
time7 9.5367431640625e-07
gen_weight_change tensor(-18.0904)
policy weight change tensor(40.6962, grad_fn=<SumBackward0>)
time8 0.0019245147705078125
train_time 0.0814058780670166
eval time 0.1454143524169922
epoch last part time 7.152557373046875e-06
2024-01-23 01:05:14,787 MainThread INFO: EPOCH:1017
2024-01-23 01:05:14,787 MainThread INFO: Time Consumed:0.23028135299682617s
2024-01-23 01:05:14,787 MainThread INFO: Total Frames:153450s
 10%|█         | 1018/10000 [06:42<38:07,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31301.11934
Train_Epoch_Reward                46832.77642
Running_Training_Average_Rewards  23441.44692
Explore_Time                      0.00104
Train___Time                      0.08141
Eval____Time                      0.14541
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             30104.02347
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.74495     3.47994     102.94759    92.34424
alpha_0                           0.60113      0.00008     0.60125      0.60101
Alpha_loss                        -3.41590     0.00183     -3.41414     -3.41925
Training/policy_loss              -6.83883     0.00428     -6.83367     -6.84472
Training/qf1_loss                 8240.15967   1000.08318  9935.87793   6998.27637
Training/qf2_loss                 18265.40352  1534.07797  21222.57422  16984.37695
Training/pf_norm                  0.13780      0.02251     0.16330      0.10710
Training/qf1_norm                 579.51025    457.05266   1332.40430   165.98991
Training/qf2_norm                 2636.37383   94.21638    2803.85815   2517.13745
log_std/mean                      -0.13521     0.00019     -0.13499     -0.13551
log_probs/mean                    -2.71299     0.00263     -2.71034     -2.71693
mean/mean                         0.00299      0.00003     0.00303      0.00294
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019795894622802734
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70307
epoch first part time 2.86102294921875e-06
replay_buffer._size: [153750]
collect time 0.001051187515258789
inner_dict_sum {'sac_diff0': 0.00021147727966308594, 'sac_diff1': 0.007525205612182617, 'sac_diff2': 0.00929713249206543, 'sac_diff3': 0.011734724044799805, 'sac_diff4': 0.007733583450317383, 'sac_diff5': 0.0354459285736084, 'sac_diff6': 0.000423431396484375, 'all': 0.0723714828491211}
diff5_list [0.0074710845947265625, 0.007188081741333008, 0.007358551025390625, 0.00656437873840332, 0.006863832473754883]
time3 0
time4 0.07319259643554688
time5 0.07325434684753418
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0904)
policy weight change tensor(40.7707, grad_fn=<SumBackward0>)
time8 0.0020143985748291016
train_time 0.08505511283874512
eval time 0.14001154899597168
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:15,039 MainThread INFO: EPOCH:1018
2024-01-23 01:05:15,040 MainThread INFO: Time Consumed:0.22861719131469727s
2024-01-23 01:05:15,040 MainThread INFO: Total Frames:153600s
 10%|█         | 1019/10000 [06:42<38:02,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31039.65264
Train_Epoch_Reward                16990.94438
Running_Training_Average_Rewards  22940.51320
Explore_Time                      0.00105
Train___Time                      0.08506
Eval____Time                      0.14001
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             30314.03656
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.17538     0.88672     99.27626     96.56621
alpha_0                           0.60083      0.00008     0.60095      0.60071
Alpha_loss                        -3.41973     0.00321     -3.41371     -3.42265
Training/policy_loss              -7.40504     0.00396     -7.39943     -7.41137
Training/qf1_loss                 8020.29453   1117.15602  9290.70801   6491.55908
Training/qf2_loss                 18279.99551  1286.65394  19802.78516  16382.62988
Training/pf_norm                  0.18840      0.03133     0.22337      0.14663
Training/qf1_norm                 890.84354    149.22097   1166.23718   719.35938
Training/qf2_norm                 2999.00576   26.41014    3033.99609   2952.72168
log_std/mean                      -0.14429     0.00003     -0.14426     -0.14432
log_probs/mean                    -2.71393     0.00764     -2.69950     -2.72097
mean/mean                         -0.00732     0.00020     -0.00706     -0.00764
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020137786865234375
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70307
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [153900]
collect time 0.0010251998901367188
inner_dict_sum {'sac_diff0': 0.00021719932556152344, 'sac_diff1': 0.0076029300689697266, 'sac_diff2': 0.008872509002685547, 'sac_diff3': 0.011129379272460938, 'sac_diff4': 0.0072057247161865234, 'sac_diff5': 0.03234124183654785, 'sac_diff6': 0.0003960132598876953, 'all': 0.0677649974822998}
diff5_list [0.006874561309814453, 0.006278514862060547, 0.006700992584228516, 0.006278514862060547, 0.006208658218383789]
time3 0
time4 0.06857538223266602
time5 0.06862545013427734
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0904)
policy weight change tensor(40.7630, grad_fn=<SumBackward0>)
time8 0.0019788742065429688
train_time 0.08010435104370117
eval time 0.14696669578552246
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:15,294 MainThread INFO: EPOCH:1019
2024-01-23 01:05:15,294 MainThread INFO: Time Consumed:0.2305150032043457s
2024-01-23 01:05:15,294 MainThread INFO: Total Frames:153750s
 10%|█         | 1020/10000 [06:42<38:03,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30721.99730
Train_Epoch_Reward                18395.17740
Running_Training_Average_Rewards  23037.64539
Explore_Time                      0.00102
Train___Time                      0.08010
Eval____Time                      0.14697
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             30304.84907
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.78309     2.86735     100.18583    92.61076
alpha_0                           0.60053      0.00008     0.60065      0.60041
Alpha_loss                        -3.42393     0.00338     -3.41892     -3.42853
Training/policy_loss              -7.10340     0.00316     -7.10011     -7.10741
Training/qf1_loss                 7474.89805   1041.57789  8998.18457   6242.14404
Training/qf2_loss                 17286.46055  1630.26225  19730.09766  15407.43457
Training/pf_norm                  0.17316      0.02505     0.19821      0.12741
Training/qf1_norm                 684.07130    416.80912   1408.67590   245.16626
Training/qf2_norm                 2739.79155   83.04039    2868.94629   2648.64551
log_std/mean                      -0.13903     0.00011     -0.13889     -0.13920
log_probs/mean                    -2.71560     0.00628     -2.70796     -2.72330
mean/mean                         -0.00077     0.00011     -0.00058     -0.00087
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02041339874267578
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70307
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [154050]
collect time 0.0009710788726806641
inside mustsac before update, task 0, sumup 70307
inside mustsac after update, task 0, sumup 71206
inner_dict_sum {'sac_diff0': 0.00022935867309570312, 'sac_diff1': 0.007802486419677734, 'sac_diff2': 0.009096145629882812, 'sac_diff3': 0.011185884475708008, 'sac_diff4': 0.007745027542114258, 'sac_diff5': 0.055108070373535156, 'sac_diff6': 0.0004553794860839844, 'all': 0.09162235260009766}
diff5_list [0.010905981063842773, 0.012385368347167969, 0.010425329208374023, 0.01029658317565918, 0.011094808578491211]
time3 0.0009198188781738281
time4 0.09259724617004395
time5 0.09265947341918945
time7 0.00909566879272461
gen_weight_change tensor(-17.9470)
policy weight change tensor(40.7154, grad_fn=<SumBackward0>)
time8 0.0028028488159179688
train_time 0.1238241195678711
eval time 0.10156798362731934
epoch last part time 7.152557373046875e-06
2024-01-23 01:05:15,547 MainThread INFO: EPOCH:1020
2024-01-23 01:05:15,547 MainThread INFO: Time Consumed:0.22884821891784668s
2024-01-23 01:05:15,548 MainThread INFO: Total Frames:153900s
 10%|█         | 1021/10000 [06:43<38:03,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30377.19174
Train_Epoch_Reward                10111.18742
Running_Training_Average_Rewards  22103.60140
Explore_Time                      0.00097
Train___Time                      0.12382
Eval____Time                      0.10157
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             30376.85809
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.87913     0.85422     99.41505     97.06438
alpha_0                           0.60023      0.00008     0.60035      0.60011
Alpha_loss                        -3.42650     0.00273     -3.42257     -3.42943
Training/policy_loss              -7.48990     0.35350     -6.91539     -7.82737
Training/qf1_loss                 9189.00195   1088.88002  11235.99121  8029.30762
Training/qf2_loss                 19405.64883  1112.42153  21380.92969  18074.90820
Training/pf_norm                  0.20413      0.03653     0.27283      0.16974
Training/qf1_norm                 940.44603    459.56835   1731.83105   323.27057
Training/qf2_norm                 2961.49194   164.00384   3127.91333   2675.64893
log_std/mean                      -0.12953     0.00210     -0.12623     -0.13171
log_probs/mean                    -2.71407     0.00381     -2.70769     -2.71850
mean/mean                         -0.00693     0.00464     -0.00046     -0.01226
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018824338912963867
epoch last part time3 0.0029366016387939453
inside rlalgo, task 0, sumup 71206
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [154200]
collect time 0.0009577274322509766
inner_dict_sum {'sac_diff0': 0.00023031234741210938, 'sac_diff1': 0.007611989974975586, 'sac_diff2': 0.00884556770324707, 'sac_diff3': 0.011419296264648438, 'sac_diff4': 0.0075016021728515625, 'sac_diff5': 0.03323078155517578, 'sac_diff6': 0.0004153251647949219, 'all': 0.06925487518310547}
diff5_list [0.00679469108581543, 0.007126331329345703, 0.006418466567993164, 0.006367683410644531, 0.006523609161376953]
time3 0
time4 0.07006645202636719
time5 0.07011580467224121
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9470)
policy weight change tensor(40.6720, grad_fn=<SumBackward0>)
time8 0.001981496810913086
train_time 0.08167290687561035
eval time 0.14237475395202637
epoch last part time 7.3909759521484375e-06
2024-01-23 01:05:15,800 MainThread INFO: EPOCH:1021
2024-01-23 01:05:15,800 MainThread INFO: Time Consumed:0.22743630409240723s
2024-01-23 01:05:15,800 MainThread INFO: Total Frames:154050s
 10%|█         | 1022/10000 [06:43<37:50,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30164.36641
Train_Epoch_Reward                10666.71776
Running_Training_Average_Rewards  21493.44449
Explore_Time                      0.00095
Train___Time                      0.08167
Eval____Time                      0.14237
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             28432.03242
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.97612     2.71953     101.13869    93.23979
alpha_0                           0.59993      0.00008     0.60005      0.59981
Alpha_loss                        -3.42905     0.00672     -3.41899     -3.43775
Training/policy_loss              -7.12577     0.00759     -7.11854     -7.13979
Training/qf1_loss                 7922.67969   1053.79388  9637.28125   6675.41943
Training/qf2_loss                 17929.13320  1519.34094  19973.54492  15937.58594
Training/pf_norm                  0.23316      0.03925     0.28555      0.17275
Training/qf1_norm                 613.14987    408.83261   1309.63684   164.37604
Training/qf2_norm                 2820.01152   78.04947    2939.34985   2712.38599
log_std/mean                      -0.13807     0.00010     -0.13791     -0.13817
log_probs/mean                    -2.71250     0.01347     -2.69281     -2.73085
mean/mean                         -0.00171     0.00029     -0.00129     -0.00210
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018462657928466797
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71206
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [154350]
collect time 0.0009784698486328125
inner_dict_sum {'sac_diff0': 0.00021910667419433594, 'sac_diff1': 0.007307291030883789, 'sac_diff2': 0.00894618034362793, 'sac_diff3': 0.011295080184936523, 'sac_diff4': 0.007864952087402344, 'sac_diff5': 0.03502941131591797, 'sac_diff6': 0.0004143714904785156, 'all': 0.0710763931274414}
diff5_list [0.006651401519775391, 0.006501436233520508, 0.006707191467285156, 0.007193326950073242, 0.007976055145263672]
time3 0
time4 0.07195472717285156
time5 0.07200884819030762
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9470)
policy weight change tensor(40.7538, grad_fn=<SumBackward0>)
time8 0.0019600391387939453
train_time 0.08363580703735352
eval time 0.14462661743164062
epoch last part time 7.62939453125e-06
2024-01-23 01:05:16,054 MainThread INFO: EPOCH:1022
2024-01-23 01:05:16,054 MainThread INFO: Time Consumed:0.23182463645935059s
2024-01-23 01:05:16,054 MainThread INFO: Total Frames:154200s
 10%|█         | 1023/10000 [06:43<38:00,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30045.36693
Train_Epoch_Reward                8774.83292
Running_Training_Average_Rewards  21245.36423
Explore_Time                      0.00097
Train___Time                      0.08364
Eval____Time                      0.14463
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29097.97273
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.77870     2.86668     99.23343     90.59666
alpha_0                           0.59963      0.00008     0.59975      0.59951
Alpha_loss                        -3.43924     0.00221     -3.43617     -3.44277
Training/policy_loss              -7.34931     0.00463     -7.34424     -7.35589
Training/qf1_loss                 7142.76387   926.75644   8001.00098   5730.42822
Training/qf2_loss                 16913.79531  1478.39935  18477.05859  14437.42383
Training/pf_norm                  0.20405      0.01755     0.22304      0.17284
Training/qf1_norm                 535.57526    325.41497   1038.58887   155.66496
Training/qf2_norm                 2810.46318   83.46676    2911.22314   2659.91870
log_std/mean                      -0.12753     0.00007     -0.12746     -0.12765
log_probs/mean                    -2.72587     0.00573     -2.71988     -2.73542
mean/mean                         -0.00804     0.00010     -0.00793     -0.00820
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.021256208419799805
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71206
epoch first part time 3.814697265625e-06
replay_buffer._size: [154500]
collect time 0.0010616779327392578
inner_dict_sum {'sac_diff0': 0.00023126602172851562, 'sac_diff1': 0.007899045944213867, 'sac_diff2': 0.00875711441040039, 'sac_diff3': 0.010953187942504883, 'sac_diff4': 0.007522106170654297, 'sac_diff5': 0.03458261489868164, 'sac_diff6': 0.00043201446533203125, 'all': 0.07037734985351562}
diff5_list [0.007728099822998047, 0.006786823272705078, 0.0075681209564208984, 0.006285667419433594, 0.0062139034271240234]
time3 0
time4 0.07122206687927246
time5 0.07127690315246582
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9470)
policy weight change tensor(41.0011, grad_fn=<SumBackward0>)
time8 0.0019388198852539062
train_time 0.08317327499389648
eval time 0.15000081062316895
epoch last part time 7.867813110351562e-06
2024-01-23 01:05:16,316 MainThread INFO: EPOCH:1023
2024-01-23 01:05:16,316 MainThread INFO: Time Consumed:0.23670744895935059s
2024-01-23 01:05:16,316 MainThread INFO: Total Frames:154350s
 10%|█         | 1024/10000 [06:43<38:16,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           29999.25913
Train_Epoch_Reward                12863.57585
Running_Training_Average_Rewards  21129.67143
Explore_Time                      0.00106
Train___Time                      0.08317
Eval____Time                      0.15000
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29457.64027
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.64630     0.69377    98.89247     96.75817
alpha_0                           0.59933      0.00008    0.59945      0.59921
Alpha_loss                        -3.43822     0.00593    -3.43103     -3.44589
Training/policy_loss              -7.64289     0.00623    -7.63316     -7.65089
Training/qf1_loss                 8015.24766   604.96577  8821.56738   7348.12109
Training/qf2_loss                 18202.49570  710.86248  19055.43359  17453.70117
Training/pf_norm                  0.16172      0.02701    0.20403      0.12997
Training/qf1_norm                 987.12986    148.72146  1185.02844   765.85077
Training/qf2_norm                 2999.10601   21.19817   3038.20972   2974.89282
log_std/mean                      -0.12993     0.00012    -0.12977     -0.13009
log_probs/mean                    -2.71733     0.01186    -2.70166     -2.73100
mean/mean                         0.00307      0.00006    0.00313      0.00296
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019482135772705078
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71206
epoch first part time 2.86102294921875e-06
replay_buffer._size: [154650]
collect time 0.0010273456573486328
inner_dict_sum {'sac_diff0': 0.00021195411682128906, 'sac_diff1': 0.00697016716003418, 'sac_diff2': 0.008507013320922852, 'sac_diff3': 0.010526895523071289, 'sac_diff4': 0.0068929195404052734, 'sac_diff5': 0.032727718353271484, 'sac_diff6': 0.0003914833068847656, 'all': 0.06622815132141113}
diff5_list [0.006638050079345703, 0.006289005279541016, 0.007050037384033203, 0.00620722770690918, 0.006543397903442383]
time3 0
time4 0.0670003890991211
time5 0.06704902648925781
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9470)
policy weight change tensor(41.2967, grad_fn=<SumBackward0>)
time8 0.0019183158874511719
train_time 0.07844018936157227
eval time 0.1516866683959961
epoch last part time 7.62939453125e-06
2024-01-23 01:05:16,573 MainThread INFO: EPOCH:1024
2024-01-23 01:05:16,573 MainThread INFO: Time Consumed:0.23363399505615234s
2024-01-23 01:05:16,573 MainThread INFO: Total Frames:154500s
 10%|█         | 1025/10000 [06:44<38:16,  3.91it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           29888.75397
Train_Epoch_Reward                48705.66364
Running_Training_Average_Rewards  21317.48879
Explore_Time                      0.00102
Train___Time                      0.07844
Eval____Time                      0.15169
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29484.91473
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       100.23789    1.21176    101.82295    98.79348
alpha_0                           0.59903      0.00008    0.59915      0.59891
Alpha_loss                        -3.44162     0.00431    -3.43576     -3.44797
Training/policy_loss              -7.17529     0.00457    -7.16871     -7.18214
Training/qf1_loss                 9828.77129   844.50985  11026.43066  8847.10352
Training/qf2_loss                 20529.19453  941.88374  22075.48438  19458.72266
Training/pf_norm                  0.13982      0.03206    0.16647      0.07726
Training/qf1_norm                 1442.84697   258.28897  1811.48645   1127.95496
Training/qf2_norm                 2869.45820   32.31526   2909.81079   2828.72852
log_std/mean                      -0.13606     0.00020    -0.13580     -0.13638
log_probs/mean                    -2.71742     0.00683    -2.70729     -2.72719
mean/mean                         -0.00198     0.00008    -0.00186     -0.00209
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018477678298950195
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71206
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [154800]
collect time 0.0009791851043701172
inside mustsac before update, task 0, sumup 71206
inside mustsac after update, task 0, sumup 72176
inner_dict_sum {'sac_diff0': 0.00022029876708984375, 'sac_diff1': 0.007387876510620117, 'sac_diff2': 0.009073495864868164, 'sac_diff3': 0.01114511489868164, 'sac_diff4': 0.007775306701660156, 'sac_diff5': 0.05454111099243164, 'sac_diff6': 0.0004284381866455078, 'all': 0.09057164192199707}
diff5_list [0.010675430297851562, 0.011821508407592773, 0.010059356689453125, 0.011699199676513672, 0.010285615921020508]
time3 0.0008959770202636719
time4 0.09148049354553223
time5 0.0915365219116211
time7 0.009276628494262695
gen_weight_change tensor(-17.8575)
policy weight change tensor(41.3890, grad_fn=<SumBackward0>)
time8 0.0020029544830322266
train_time 0.12170195579528809
eval time 0.1097555160522461
epoch last part time 8.106231689453125e-06
2024-01-23 01:05:16,830 MainThread INFO: EPOCH:1025
2024-01-23 01:05:16,830 MainThread INFO: Time Consumed:0.23487114906311035s
2024-01-23 01:05:16,830 MainThread INFO: Total Frames:154650s
 10%|█         | 1026/10000 [06:44<38:22,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           29644.11503
Train_Epoch_Reward                22241.55448
Running_Training_Average_Rewards  21627.00600
Explore_Time                      0.00097
Train___Time                      0.12170
Eval____Time                      0.10976
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             29331.68074
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.62866     2.31418     99.70643     93.13829
alpha_0                           0.59873      0.00008     0.59885      0.59861
Alpha_loss                        -3.44349     0.00528     -3.43310     -3.44718
Training/policy_loss              -7.40885     0.31676     -7.12070     -8.01135
Training/qf1_loss                 8265.85078   1315.05165  9386.11133   5789.80176
Training/qf2_loss                 18229.66074  1653.03740  19995.90234  15342.73730
Training/pf_norm                  0.21914      0.08251     0.33900      0.13487
Training/qf1_norm                 1079.54861   404.10936   1785.40027   604.46729
Training/qf2_norm                 2889.09648   68.77592    2978.65918   2773.28125
log_std/mean                      -0.13303     0.00439     -0.12615     -0.13888
log_probs/mean                    -2.71452     0.00972     -2.69557     -2.72172
mean/mean                         -0.00484     0.00526     0.00187      -0.01102
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019208908081054688
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 72176
epoch first part time 2.86102294921875e-06
replay_buffer._size: [154950]
collect time 0.0009613037109375
inner_dict_sum {'sac_diff0': 0.0002205371856689453, 'sac_diff1': 0.007350921630859375, 'sac_diff2': 0.008913993835449219, 'sac_diff3': 0.01154184341430664, 'sac_diff4': 0.00786447525024414, 'sac_diff5': 0.03409862518310547, 'sac_diff6': 0.0004189014434814453, 'all': 0.07040929794311523}
diff5_list [0.007775306701660156, 0.007272481918334961, 0.006482124328613281, 0.0063533782958984375, 0.006215333938598633]
time3 0
time4 0.07128715515136719
time5 0.07134604454040527
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8575)
policy weight change tensor(41.3785, grad_fn=<SumBackward0>)
time8 0.002045869827270508
train_time 0.08305931091308594
eval time 0.15223908424377441
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:17,092 MainThread INFO: EPOCH:1026
2024-01-23 01:05:17,092 MainThread INFO: Time Consumed:0.23875832557678223s
2024-01-23 01:05:17,092 MainThread INFO: Total Frames:154800s
 10%|█         | 1027/10000 [06:44<38:40,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30184.91079
Train_Epoch_Reward                35841.66350
Running_Training_Average_Rewards  22406.87227
Explore_Time                      0.00096
Train___Time                      0.08306
Eval____Time                      0.15224
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             34945.09977
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.40711     2.27051     100.97659    94.25755
alpha_0                           0.59843      0.00008     0.59855      0.59831
Alpha_loss                        -3.44667     0.00764     -3.43794     -3.46050
Training/policy_loss              -7.51353     0.00832     -7.50467     -7.52846
Training/qf1_loss                 7952.34434   1187.75750  10074.17383  6917.07031
Training/qf2_loss                 18281.55859  1430.56942  20571.26562  16392.04102
Training/pf_norm                  0.18402      0.04132     0.22969      0.11333
Training/qf1_norm                 552.14145    147.02351   772.96356    314.90622
Training/qf2_norm                 2921.37295   67.49202    2996.26978   2797.60498
log_std/mean                      -0.13356     0.00020     -0.13321     -0.13376
log_probs/mean                    -2.71419     0.01483     -2.69719     -2.73983
mean/mean                         -0.00319     0.00021     -0.00294     -0.00352
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020897865295410156
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 72176
epoch first part time 3.337860107421875e-06
replay_buffer._size: [155100]
collect time 0.001046895980834961
inner_dict_sum {'sac_diff0': 0.00022125244140625, 'sac_diff1': 0.007057905197143555, 'sac_diff2': 0.008569955825805664, 'sac_diff3': 0.010614395141601562, 'sac_diff4': 0.00740361213684082, 'sac_diff5': 0.03365373611450195, 'sac_diff6': 0.0004172325134277344, 'all': 0.06793808937072754}
diff5_list [0.008105039596557617, 0.006714582443237305, 0.006383419036865234, 0.006190061569213867, 0.00626063346862793]
time3 0
time4 0.06872916221618652
time5 0.06877851486206055
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8575)
policy weight change tensor(41.3280, grad_fn=<SumBackward0>)
time8 0.0018322467803955078
train_time 0.08010005950927734
eval time 0.1564328670501709
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:17,357 MainThread INFO: EPOCH:1027
2024-01-23 01:05:17,357 MainThread INFO: Time Consumed:0.23999452590942383s
2024-01-23 01:05:17,357 MainThread INFO: Total Frames:154950s
 10%|█         | 1028/10000 [06:44<38:52,  3.85it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30623.40183
Train_Epoch_Reward                14530.89488
Running_Training_Average_Rewards  22500.98647
Explore_Time                      0.00104
Train___Time                      0.08010
Eval____Time                      0.15643
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             34488.93389
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       94.82167     2.14297     96.81120     91.35429
alpha_0                           0.59813      0.00008     0.59825      0.59801
Alpha_loss                        -3.44909     0.00285     -3.44602     -3.45316
Training/policy_loss              -6.88744     0.00373     -6.88113     -6.89210
Training/qf1_loss                 8038.26797   1755.68728  10750.98242  6204.18359
Training/qf2_loss                 17643.19180  2134.44680  20714.60938  15126.10938
Training/pf_norm                  0.10508      0.02045     0.14338      0.08675
Training/qf1_norm                 1024.05040   389.91930   1367.29968   421.64633
Training/qf2_norm                 2626.06753   61.18376    2684.01392   2529.72095
log_std/mean                      -0.13039     0.00009     -0.13029     -0.13054
log_probs/mean                    -2.71238     0.00611     -2.70510     -2.72160
mean/mean                         -0.00687     0.00016     -0.00663     -0.00709
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01890730857849121
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 72176
epoch first part time 3.337860107421875e-06
replay_buffer._size: [155250]
collect time 0.0009701251983642578
inner_dict_sum {'sac_diff0': 0.0002117156982421875, 'sac_diff1': 0.007404327392578125, 'sac_diff2': 0.009008169174194336, 'sac_diff3': 0.011785030364990234, 'sac_diff4': 0.00785064697265625, 'sac_diff5': 0.03480720520019531, 'sac_diff6': 0.000396728515625, 'all': 0.07146382331848145}
diff5_list [0.006747245788574219, 0.007088184356689453, 0.0073282718658447266, 0.007128715515136719, 0.006514787673950195]
time3 0
time4 0.07229280471801758
time5 0.07234454154968262
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8575)
policy weight change tensor(41.0775, grad_fn=<SumBackward0>)
time8 0.0019872188568115234
train_time 0.08406591415405273
eval time 0.14402151107788086
epoch last part time 4.76837158203125e-06
2024-01-23 01:05:17,611 MainThread INFO: EPOCH:1028
2024-01-23 01:05:17,611 MainThread INFO: Time Consumed:0.23141002655029297s
2024-01-23 01:05:17,611 MainThread INFO: Total Frames:155100s
 10%|█         | 1029/10000 [06:45<38:36,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30979.76995
Train_Epoch_Reward                18106.98725
Running_Training_Average_Rewards  22309.83450
Explore_Time                      0.00096
Train___Time                      0.08407
Eval____Time                      0.14402
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             33877.71783
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.35107     4.06831     102.68410    92.46482
alpha_0                           0.59784      0.00008     0.59796      0.59772
Alpha_loss                        -3.45245     0.00239     -3.44948     -3.45557
Training/policy_loss              -7.54783     0.00423     -7.54139     -7.55277
Training/qf1_loss                 8969.99082   1489.65910  10920.16699  6955.29053
Training/qf2_loss                 19299.78555  2317.59498  22141.39648  16069.88086
Training/pf_norm                  0.24579      0.01369     0.26139      0.22530
Training/qf1_norm                 978.66539    626.90022   1710.38293   218.98996
Training/qf2_norm                 3056.98740   126.39593   3193.79858   2874.33008
log_std/mean                      -0.13426     0.00041     -0.13361     -0.13474
log_probs/mean                    -2.71240     0.00402     -2.70604     -2.71587
mean/mean                         -0.00465     0.00026     -0.00429     -0.00502
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01909661293029785
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 72176
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [155400]
collect time 0.000934600830078125
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.007013797760009766, 'sac_diff2': 0.008289813995361328, 'sac_diff3': 0.01037287712097168, 'sac_diff4': 0.006847858428955078, 'sac_diff5': 0.03240680694580078, 'sac_diff6': 0.0003962516784667969, 'all': 0.06553483009338379}
diff5_list [0.007227420806884766, 0.006425142288208008, 0.006664752960205078, 0.006120204925537109, 0.00596928596496582]
time3 0
time4 0.0663154125213623
time5 0.06636309623718262
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8575)
policy weight change tensor(40.7630, grad_fn=<SumBackward0>)
time8 0.0018427371978759766
train_time 0.0774383544921875
eval time 0.15110254287719727
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:17,866 MainThread INFO: EPOCH:1029
2024-01-23 01:05:17,866 MainThread INFO: Time Consumed:0.23183584213256836s
2024-01-23 01:05:17,866 MainThread INFO: Total Frames:155250s
 10%|█         | 1030/10000 [06:45<38:25,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           31276.22258
Train_Epoch_Reward                15796.78577
Running_Training_Average_Rewards  22510.28403
Explore_Time                      0.00093
Train___Time                      0.07744
Eval____Time                      0.15110
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             33269.37528
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       97.90352     1.24190    100.03667    96.14231
alpha_0                           0.59754      0.00008    0.59766      0.59742
Alpha_loss                        -3.45636     0.00451    -3.45161     -3.46454
Training/policy_loss              -7.24051     0.00436    -7.23419     -7.24595
Training/qf1_loss                 8315.22773   593.09455  9037.52344   7685.29980
Training/qf2_loss                 18606.28437  806.06965  19773.20703  17582.99219
Training/pf_norm                  0.30637      0.01341    0.32371      0.28414
Training/qf1_norm                 448.92425    158.10790  736.26581    256.05954
Training/qf2_norm                 2840.85811   35.24368   2901.95996   2792.28711
log_std/mean                      -0.13684     0.00029    -0.13644     -0.13727
log_probs/mean                    -2.71349     0.00824    -2.70556     -2.72937
mean/mean                         -0.00637     0.00073    -0.00548     -0.00752
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018327951431274414
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 72176
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [155550]
collect time 0.000843048095703125
inside mustsac before update, task 0, sumup 72176
inside mustsac after update, task 0, sumup 69672
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.00681614875793457, 'sac_diff2': 0.008472919464111328, 'sac_diff3': 0.01081705093383789, 'sac_diff4': 0.007584333419799805, 'sac_diff5': 0.051841020584106445, 'sac_diff6': 0.0004277229309082031, 'all': 0.08617162704467773}
diff5_list [0.010931730270385742, 0.010033607482910156, 0.009996175765991211, 0.010706663131713867, 0.010172843933105469]
time3 0.0008485317230224609
time4 0.08701062202453613
time5 0.08706021308898926
time7 0.009390592575073242
gen_weight_change tensor(-17.7001)
policy weight change tensor(40.8617, grad_fn=<SumBackward0>)
time8 0.002711057662963867
train_time 0.1177217960357666
eval time 0.11403989791870117
epoch last part time 5.245208740234375e-06
2024-01-23 01:05:18,122 MainThread INFO: EPOCH:1030
2024-01-23 01:05:18,123 MainThread INFO: Time Consumed:0.23489689826965332s
2024-01-23 01:05:18,123 MainThread INFO: Total Frames:155400s
 10%|█         | 1031/10000 [06:45<38:32,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31471.30114
Train_Epoch_Reward                56332.30361
Running_Training_Average_Rewards  24025.93042
Explore_Time                      0.00084
Train___Time                      0.11772
Eval____Time                      0.11404
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             32327.64377
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.93881     1.94978     101.54796    95.85173
alpha_0                           0.59724      0.00008     0.59736      0.59712
Alpha_loss                        -3.46003     0.00534     -3.45274     -3.46678
Training/policy_loss              -7.63277     0.17194     -7.42870     -7.87025
Training/qf1_loss                 8799.70283   1240.73940  10056.93555  7102.98340
Training/qf2_loss                 18999.38750  1535.19878  20955.71875  17162.11328
Training/pf_norm                  0.24609      0.08919     0.33378      0.11335
Training/qf1_norm                 1313.98596   850.96958   2644.30566   579.58655
Training/qf2_norm                 3020.33838   110.62646   3216.38330   2887.31885
log_std/mean                      -0.13315     0.00294     -0.12832     -0.13698
log_probs/mean                    -2.71411     0.01084     -2.69867     -2.72981
mean/mean                         -0.00608     0.00141     -0.00377     -0.00799
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018468379974365234
epoch last part time3 0.002728700637817383
inside rlalgo, task 0, sumup 69672
epoch first part time 2.86102294921875e-06
replay_buffer._size: [155700]
collect time 0.0009310245513916016
inner_dict_sum {'sac_diff0': 0.0002129077911376953, 'sac_diff1': 0.006836891174316406, 'sac_diff2': 0.008457422256469727, 'sac_diff3': 0.010674238204956055, 'sac_diff4': 0.006992340087890625, 'sac_diff5': 0.031969547271728516, 'sac_diff6': 0.00038361549377441406, 'all': 0.06552696228027344}
diff5_list [0.006675004959106445, 0.0062177181243896484, 0.006457090377807617, 0.006387233734130859, 0.006232500076293945]
time3 0
time4 0.06630063056945801
time5 0.0663449764251709
time7 9.5367431640625e-07
gen_weight_change tensor(-17.7001)
policy weight change tensor(40.7122, grad_fn=<SumBackward0>)
time8 0.001955270767211914
train_time 0.07751870155334473
eval time 0.13807225227355957
epoch last part time 4.5299530029296875e-06
2024-01-23 01:05:18,366 MainThread INFO: EPOCH:1031
2024-01-23 01:05:18,366 MainThread INFO: Time Consumed:0.21878266334533691s
2024-01-23 01:05:18,366 MainThread INFO: Total Frames:155550s
 10%|█         | 1032/10000 [06:45<37:45,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           32170.55392
Train_Epoch_Reward                13564.65574
Running_Training_Average_Rewards  23612.82186
Explore_Time                      0.00093
Train___Time                      0.07752
Eval____Time                      0.13807
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             35424.56017
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       96.62338     3.27139     100.75401    92.41179
alpha_0                           0.59694      0.00008     0.59706      0.59682
Alpha_loss                        -3.46010     0.00420     -3.45435     -3.46610
Training/policy_loss              -7.71565     0.00811     -7.70546     -7.72969
Training/qf1_loss                 7363.23896   869.67522   8522.77246   6288.29980
Training/qf2_loss                 17128.52891  1469.14680  19047.72266  15286.96582
Training/pf_norm                  0.21528      0.05173     0.28701      0.13201
Training/qf1_norm                 2189.81409   591.88501   2949.31421   1412.17920
Training/qf2_norm                 3067.25630   102.62090   3190.06860   2931.87646
log_std/mean                      -0.13054     0.00010     -0.13045     -0.13071
log_probs/mean                    -2.70775     0.00716     -2.69921     -2.71938
mean/mean                         -0.00384     0.00071     -0.00286     -0.00486
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01833200454711914
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 69672
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [155850]
collect time 0.0008172988891601562
inner_dict_sum {'sac_diff0': 0.00023055076599121094, 'sac_diff1': 0.006718158721923828, 'sac_diff2': 0.007852792739868164, 'sac_diff3': 0.010262489318847656, 'sac_diff4': 0.007068157196044922, 'sac_diff5': 0.03186321258544922, 'sac_diff6': 0.0003829002380371094, 'all': 0.06437826156616211}
diff5_list [0.006484031677246094, 0.0062634944915771484, 0.006188631057739258, 0.006488323211669922, 0.006438732147216797]
time3 0
time4 0.06511998176574707
time5 0.06516504287719727
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7001)
policy weight change tensor(40.7999, grad_fn=<SumBackward0>)
time8 0.0018987655639648438
train_time 0.07625818252563477
eval time 0.14570116996765137
epoch last part time 4.76837158203125e-06
2024-01-23 01:05:18,613 MainThread INFO: EPOCH:1032
2024-01-23 01:05:18,613 MainThread INFO: Time Consumed:0.22502422332763672s
2024-01-23 01:05:18,613 MainThread INFO: Total Frames:155700s
 10%|█         | 1033/10000 [06:46<37:30,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           32678.89884
Train_Epoch_Reward                53145.29276
Running_Training_Average_Rewards  24755.47780
Explore_Time                      0.00081
Train___Time                      0.07626
Eval____Time                      0.14570
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             34181.42194
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.66616     3.32338     99.59578     90.65698
alpha_0                           0.59664      0.00008     0.59676      0.59653
Alpha_loss                        -3.46405     0.00525     -3.45599     -3.47033
Training/policy_loss              -7.62616     0.00268     -7.62296     -7.62955
Training/qf1_loss                 6957.10449   996.59473   8581.82227   5650.76074
Training/qf2_loss                 16723.80684  1494.69663  19159.11719  15060.32227
Training/pf_norm                  0.12801      0.02936     0.17686      0.09256
Training/qf1_norm                 649.01009    363.06243   1139.58569   212.50279
Training/qf2_norm                 2983.65479   103.98325   3110.97339   2831.56592
log_std/mean                      -0.13549     0.00024     -0.13522     -0.13589
log_probs/mean                    -2.70893     0.00939     -2.69331     -2.71978
mean/mean                         -0.00966     0.00033     -0.00913     -0.01006
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018720626831054688
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69672
epoch first part time 2.86102294921875e-06
replay_buffer._size: [156000]
collect time 0.0008790493011474609
inner_dict_sum {'sac_diff0': 0.00022935867309570312, 'sac_diff1': 0.006801128387451172, 'sac_diff2': 0.00787353515625, 'sac_diff3': 0.010110139846801758, 'sac_diff4': 0.006824016571044922, 'sac_diff5': 0.031783103942871094, 'sac_diff6': 0.0003826618194580078, 'all': 0.06400394439697266}
diff5_list [0.00655817985534668, 0.0063629150390625, 0.006072521209716797, 0.006585836410522461, 0.006203651428222656]
time3 0
time4 0.06474065780639648
time5 0.06478381156921387
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7001)
policy weight change tensor(41.0473, grad_fn=<SumBackward0>)
time8 0.0018727779388427734
train_time 0.07607412338256836
eval time 0.14146208763122559
epoch last part time 5.0067901611328125e-06
2024-01-23 01:05:18,856 MainThread INFO: EPOCH:1033
2024-01-23 01:05:18,856 MainThread INFO: Time Consumed:0.22067499160766602s
2024-01-23 01:05:18,856 MainThread INFO: Total Frames:155850s
 10%|█         | 1034/10000 [06:46<37:08,  4.02it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           33098.90517
Train_Epoch_Reward                25160.67897
Running_Training_Average_Rewards  25112.79375
Explore_Time                      0.00087
Train___Time                      0.07607
Eval____Time                      0.14146
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             33657.70360
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.03992    1.79547     102.57766    97.67877
alpha_0                           0.59635      0.00008     0.59647      0.59623
Alpha_loss                        -3.47448     0.00316     -3.46967     -3.47896
Training/policy_loss              -7.36214     0.00500     -7.35445     -7.37015
Training/qf1_loss                 8848.95674   818.02954   9674.63672   7453.14307
Training/qf2_loss                 19594.65391  1186.36019  20952.22852  17658.94531
Training/pf_norm                  0.16760      0.01953     0.19854      0.13749
Training/qf1_norm                 451.09181    213.41961   800.70599    216.44516
Training/qf2_norm                 3069.46475   54.09740    3150.24707   3001.21899
log_std/mean                      -0.13013     0.00024     -0.12983     -0.13051
log_probs/mean                    -2.72262     0.00600     -2.71202     -2.72870
mean/mean                         -0.01173     0.00022     -0.01143     -0.01206
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018456220626831055
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69672
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [156150]
collect time 0.0008442401885986328
inner_dict_sum {'sac_diff0': 0.0002048015594482422, 'sac_diff1': 0.006979465484619141, 'sac_diff2': 0.008212089538574219, 'sac_diff3': 0.010796546936035156, 'sac_diff4': 0.006991386413574219, 'sac_diff5': 0.0323641300201416, 'sac_diff6': 0.00040268898010253906, 'all': 0.06595110893249512}
diff5_list [0.006509065628051758, 0.006435871124267578, 0.006623744964599609, 0.006539583206176758, 0.0062558650970458984]
time3 0
time4 0.06670451164245605
time5 0.06674838066101074
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7001)
policy weight change tensor(41.4514, grad_fn=<SumBackward0>)
time8 0.001979351043701172
train_time 0.0779414176940918
eval time 0.15452957153320312
epoch last part time 6.198883056640625e-06
2024-01-23 01:05:19,113 MainThread INFO: EPOCH:1034
2024-01-23 01:05:19,114 MainThread INFO: Time Consumed:0.2357335090637207s
2024-01-23 01:05:19,114 MainThread INFO: Total Frames:156000s
 10%|█         | 1035/10000 [06:46<37:44,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           33477.53039
Train_Epoch_Reward                45386.36538
Running_Training_Average_Rewards  26563.05883
Explore_Time                      0.00084
Train___Time                      0.07794
Eval____Time                      0.15453
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             33271.16693
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       97.95963     3.37074     102.03005    93.54436
alpha_0                           0.59605      0.00008     0.59617      0.59593
Alpha_loss                        -3.47555     0.00259     -3.47239     -3.47858
Training/policy_loss              -7.50885     0.00496     -7.50314     -7.51713
Training/qf1_loss                 8165.57920   1350.95176  10508.65527  6748.02734
Training/qf2_loss                 18443.46836  1980.39313  21513.33203  16125.78320
Training/pf_norm                  0.17242      0.01989     0.19625      0.14630
Training/qf1_norm                 1042.37849   656.09235   1787.70129   228.73640
Training/qf2_norm                 2989.27817   103.70950   3114.79321   2852.65869
log_std/mean                      -0.13441     0.00045     -0.13384     -0.13509
log_probs/mean                    -2.71821     0.00374     -2.71276     -2.72173
mean/mean                         -0.01237     0.00031     -0.01194     -0.01281
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.022815227508544922
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 69672
epoch first part time 3.337860107421875e-06
replay_buffer._size: [156300]
collect time 0.0010263919830322266
inside mustsac before update, task 0, sumup 69672
inside mustsac after update, task 0, sumup 71032
inner_dict_sum {'sac_diff0': 0.000217437744140625, 'sac_diff1': 0.007384538650512695, 'sac_diff2': 0.008800983428955078, 'sac_diff3': 0.011209487915039062, 'sac_diff4': 0.007638216018676758, 'sac_diff5': 0.052179813385009766, 'sac_diff6': 0.00041031837463378906, 'all': 0.08784079551696777}
diff5_list [0.011052370071411133, 0.010024309158325195, 0.010145187377929688, 0.010262250900268555, 0.010695695877075195]
time3 0.0008497238159179688
time4 0.08871078491210938
time5 0.08876395225524902
time7 0.009404182434082031
gen_weight_change tensor(-17.7502)
policy weight change tensor(41.6559, grad_fn=<SumBackward0>)
time8 0.001905202865600586
train_time 0.11911559104919434
eval time 0.2972733974456787
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:19,560 MainThread INFO: EPOCH:1035
2024-01-23 01:05:19,560 MainThread INFO: Time Consumed:0.4198458194732666s
2024-01-23 01:05:19,561 MainThread INFO: Total Frames:156150s
 10%|█         | 1036/10000 [06:47<46:15,  3.23it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           33818.71481
Train_Epoch_Reward                57613.06459
Running_Training_Average_Rewards  27766.94608
Explore_Time                      0.00102
Train___Time                      0.11912
Eval____Time                      0.29727
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             32743.52497
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.12999     1.17112     100.09703    96.82366
alpha_0                           0.59575      0.00008     0.59587      0.59563
Alpha_loss                        -3.47762     0.00722     -3.47008     -3.49125
Training/policy_loss              -7.70522     0.08931     -7.55841     -7.83705
Training/qf1_loss                 8899.79229   1182.58273  9996.45020   6751.47510
Training/qf2_loss                 19145.93164  1114.45199  20296.67188  17060.72266
Training/pf_norm                  0.24896      0.02042     0.28357      0.22533
Training/qf1_norm                 999.13521    1086.24277  3149.13965   236.14413
Training/qf2_norm                 3048.38989   55.66480    3128.82764   2980.51782
log_std/mean                      -0.13400     0.00871     -0.12585     -0.15092
log_probs/mean                    -2.71574     0.01433     -2.70376     -2.74335
mean/mean                         -0.01448     0.00454     -0.00956     -0.02110
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01900768280029297
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71032
epoch first part time 3.337860107421875e-06
replay_buffer._size: [156450]
collect time 0.0008687973022460938
inner_dict_sum {'sac_diff0': 0.00020742416381835938, 'sac_diff1': 0.006939888000488281, 'sac_diff2': 0.008435249328613281, 'sac_diff3': 0.010565757751464844, 'sac_diff4': 0.007027387619018555, 'sac_diff5': 0.0320897102355957, 'sac_diff6': 0.00038170814514160156, 'all': 0.06564712524414062}
diff5_list [0.006756782531738281, 0.00635075569152832, 0.006160259246826172, 0.006546974182128906, 0.0062749385833740234]
time3 0
time4 0.06639647483825684
time5 0.06644010543823242
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7502)
policy weight change tensor(42.1705, grad_fn=<SumBackward0>)
time8 0.0018641948699951172
train_time 0.07765817642211914
eval time 0.14794611930847168
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:19,811 MainThread INFO: EPOCH:1036
2024-01-23 01:05:19,812 MainThread INFO: Time Consumed:0.22883248329162598s
2024-01-23 01:05:19,812 MainThread INFO: Total Frames:156300s
 10%|█         | 1037/10000 [06:47<43:39,  3.42it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           32680.02908
Train_Epoch_Reward                25704.62890
Running_Training_Average_Rewards  28230.67854
Explore_Time                      0.00086
Train___Time                      0.07766
Eval____Time                      0.14795
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23558.24237
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.68999     1.66503     102.55625    97.90172
alpha_0                           0.59545      0.00008     0.59557      0.59534
Alpha_loss                        -3.47908     0.00443     -3.47187     -3.48415
Training/policy_loss              -7.32564     0.00404     -7.31873     -7.33058
Training/qf1_loss                 8644.89678   1066.58736  10440.37598  7414.23291
Training/qf2_loss                 19349.43086  1383.17295  21794.70898  17957.27734
Training/pf_norm                  0.19822      0.03162     0.23832      0.14707
Training/qf1_norm                 386.58227    174.53994   718.24023    223.60942
Training/qf2_norm                 3079.78550   50.97487    3169.24072   3025.16089
log_std/mean                      -0.13059     0.00042     -0.13003     -0.13120
log_probs/mean                    -2.71209     0.00768     -2.69947     -2.72025
mean/mean                         -0.00452     0.00008     -0.00439     -0.00460
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01915764808654785
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71032
epoch first part time 2.86102294921875e-06
replay_buffer._size: [156450]
collect time 0.1715562343597412
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.007422208786010742, 'sac_diff2': 0.008678674697875977, 'sac_diff3': 0.01157069206237793, 'sac_diff4': 0.00728917121887207, 'sac_diff5': 0.03415203094482422, 'sac_diff6': 0.00039696693420410156, 'all': 0.06972408294677734}
diff5_list [0.006642341613769531, 0.007114410400390625, 0.007044076919555664, 0.00700068473815918, 0.006350517272949219]
time3 0
time4 0.07051849365234375
time5 0.07056903839111328
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7502)
policy weight change tensor(42.6911, grad_fn=<SumBackward0>)
time8 0.0019114017486572266
train_time 0.0819239616394043
eval time 0.0005402565002441406
epoch last part time 4.0531158447265625e-06
2024-01-23 01:05:20,090 MainThread INFO: EPOCH:1037
2024-01-23 01:05:20,091 MainThread INFO: Time Consumed:0.2561836242675781s
2024-01-23 01:05:20,091 MainThread INFO: Total Frames:156450s
 10%|█         | 1038/10000 [06:47<43:03,  3.47it/s]--------------------------------  ------------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31579.40225
Train_Epoch_Reward                100920.15545
Running_Training_Average_Rewards  31008.60102
Explore_Time                      0.17155
Train___Time                      0.08192
Eval____Time                      0.00054
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23482.66568
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean          Std         Max          Min
Reward_Mean                       97.72622      3.40881     103.30814    93.26210
alpha_0                           0.59516       0.00008     0.59528      0.59504
Alpha_loss                        -3.48286      0.00361     -3.47727     -3.48717
Training/policy_loss              -8.02394      0.00441     -8.01789     -8.03004
Training/qf1_loss                 9310.62324    2023.10995  11564.81055  6826.40527
Training/qf2_loss                 19493.80371   2675.66093  22855.56641  16108.19043
Training/pf_norm                  0.25831       0.02294     0.29132      0.22060
Training/qf1_norm                 1262.64537    676.15905   2386.09448   385.73221
Training/qf2_norm                 3217.79258    109.21607   3393.82007   3071.37012
log_std/mean                      -0.13140      0.00043     -0.13079     -0.13202
log_probs/mean                    -2.71292      0.00821     -2.70215     -2.72381
mean/mean                         -0.00821      0.00039     -0.00775     -0.00883
--------------------------------  ------------  ----------  -----------  -----------
epoch last part time2 0.019222497940063477
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71032
epoch first part time 2.86102294921875e-06
replay_buffer._size: [156600]
collect time 0.020386934280395508
inner_dict_sum {'sac_diff0': 0.00021719932556152344, 'sac_diff1': 0.007066965103149414, 'sac_diff2': 0.008493185043334961, 'sac_diff3': 0.010911941528320312, 'sac_diff4': 0.0073146820068359375, 'sac_diff5': 0.033194541931152344, 'sac_diff6': 0.00040984153747558594, 'all': 0.06760835647583008}
diff5_list [0.007895469665527344, 0.0064890384674072266, 0.006325960159301758, 0.006548881530761719, 0.005935192108154297]
time3 0
time4 0.06837725639343262
time5 0.06842517852783203
time7 9.5367431640625e-07
gen_weight_change tensor(-17.7502)
policy weight change tensor(42.8886, grad_fn=<SumBackward0>)
time8 0.0019791126251220703
train_time 0.08013343811035156
eval time 0.12417268753051758
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:20,340 MainThread INFO: EPOCH:1038
2024-01-23 01:05:20,341 MainThread INFO: Time Consumed:0.22710800170898438s
2024-01-23 01:05:20,341 MainThread INFO: Total Frames:156600s
 10%|█         | 1039/10000 [06:47<41:19,  3.61it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30539.89704
Train_Epoch_Reward                59156.23454
Running_Training_Average_Rewards  31125.14833
Explore_Time                      0.02038
Train___Time                      0.08013
Eval____Time                      0.12417
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23482.66568
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.29161     2.18215     102.26711    96.39828
alpha_0                           0.59486      0.00008     0.59498      0.59474
Alpha_loss                        -3.48102     0.00485     -3.47429     -3.48602
Training/policy_loss              -7.86824     0.00417     -7.86509     -7.87638
Training/qf1_loss                 8871.93281   917.34969   10054.17871  7422.95312
Training/qf2_loss                 19187.59453  1010.93192  20147.85352  17374.14453
Training/pf_norm                  0.20650      0.02921     0.25469      0.16693
Training/qf1_norm                 812.03492    330.46019   1111.40967   258.14246
Training/qf2_norm                 3158.24966   71.15635    3287.86768   3095.98169
log_std/mean                      -0.13444     0.00016     -0.13417     -0.13460
log_probs/mean                    -2.70292     0.00974     -2.69125     -2.71514
mean/mean                         -0.00742     0.00040     -0.00686     -0.00800
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018874168395996094
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71032
epoch first part time 2.86102294921875e-06
replay_buffer._size: [156873]
collect time 0.0008492469787597656
inner_dict_sum {'sac_diff0': 0.00022292137145996094, 'sac_diff1': 0.007250070571899414, 'sac_diff2': 0.008641242980957031, 'sac_diff3': 0.01162862777709961, 'sac_diff4': 0.0076427459716796875, 'sac_diff5': 0.03442072868347168, 'sac_diff6': 0.0004143714904785156, 'all': 0.0702207088470459}
diff5_list [0.006775856018066406, 0.006502389907836914, 0.007002592086791992, 0.007415771484375, 0.006724119186401367]
time3 0
time4 0.07105827331542969
time5 0.07110738754272461
time7 7.152557373046875e-07
gen_weight_change tensor(-17.7502)
policy weight change tensor(42.7292, grad_fn=<SumBackward0>)
time8 0.001889944076538086
train_time 0.0824744701385498
eval time 0.14182353019714355
epoch last part time 6.198883056640625e-06
2024-01-23 01:05:20,590 MainThread INFO: EPOCH:1039
2024-01-23 01:05:20,591 MainThread INFO: Time Consumed:0.22752594947814941s
2024-01-23 01:05:20,591 MainThread INFO: Total Frames:156750s
 10%|█         | 1040/10000 [06:48<40:05,  3.73it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           29451.00308
Train_Epoch_Reward                14135.96749
Running_Training_Average_Rewards  30546.21019
Explore_Time                      0.00084
Train___Time                      0.08247
Eval____Time                      0.14182
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22380.43565
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       95.68517     2.82296     98.93027     90.56057
alpha_0                           0.59456      0.00008     0.59468      0.59445
Alpha_loss                        -3.48855     0.00338     -3.48551     -3.49513
Training/policy_loss              -7.64649     0.00436     -7.63782     -7.64914
Training/qf1_loss                 7982.81104   1160.24934  9563.61523   5951.81201
Training/qf2_loss                 17758.26777  1705.83063  19722.63672  14601.92676
Training/pf_norm                  0.21483      0.01438     0.23371      0.19439
Training/qf1_norm                 1176.34758   604.26956   2275.26221   490.70230
Training/qf2_norm                 2937.14834   88.51911    3036.99414   2776.22217
log_std/mean                      -0.12885     0.00010     -0.12868     -0.12896
log_probs/mean                    -2.71098     0.00603     -2.70513     -2.72235
mean/mean                         -0.02111     0.00008     -0.02096     -0.02117
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01823592185974121
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71032
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [157050]
collect time 0.0008454322814941406
inside mustsac before update, task 0, sumup 71032
inside mustsac after update, task 0, sumup 71761
inner_dict_sum {'sac_diff0': 0.00022745132446289062, 'sac_diff1': 0.007489681243896484, 'sac_diff2': 0.008997440338134766, 'sac_diff3': 0.011651039123535156, 'sac_diff4': 0.007964849472045898, 'sac_diff5': 0.0528111457824707, 'sac_diff6': 0.0004246234893798828, 'all': 0.08956623077392578}
diff5_list [0.010981321334838867, 0.011194944381713867, 0.010511159896850586, 0.010160207748413086, 0.009963512420654297]
time3 0.0009031295776367188
time4 0.09045648574829102
time5 0.09051036834716797
time7 0.008821964263916016
gen_weight_change tensor(-17.8268)
policy weight change tensor(42.8998, grad_fn=<SumBackward0>)
time8 0.0025768280029296875
train_time 0.12090682983398438
eval time 0.10809922218322754
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:20,844 MainThread INFO: EPOCH:1040
2024-01-23 01:05:20,845 MainThread INFO: Time Consumed:0.23215699195861816s
2024-01-23 01:05:20,845 MainThread INFO: Total Frames:156900s
 10%|█         | 1041/10000 [06:48<39:35,  3.77it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           28410.43840
Train_Epoch_Reward                22319.58601
Running_Training_Average_Rewards  29765.70850
Explore_Time                      0.00084
Train___Time                      0.12091
Eval____Time                      0.10810
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21921.99701
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.63878     2.61634     103.95825    97.45575
alpha_0                           0.59427      0.00008     0.59439      0.59415
Alpha_loss                        -3.49724     0.00299     -3.49250     -3.50161
Training/policy_loss              -7.60165     0.23922     -7.39039     -8.05313
Training/qf1_loss                 9777.34502   2568.17933  14576.14258  7265.49463
Training/qf2_loss                 20318.57539  3062.32192  26050.02930  17399.19727
Training/pf_norm                  0.17617      0.06134     0.27631      0.10465
Training/qf1_norm                 1551.33841   1071.35452  2896.53052   185.50432
Training/qf2_norm                 3038.15811   164.54088   3349.38184   2866.17749
log_std/mean                      -0.14054     0.00298     -0.13555     -0.14442
log_probs/mean                    -2.72124     0.00576     -2.71470     -2.73093
mean/mean                         -0.01863     0.00777     -0.00372     -0.02603
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018539905548095703
epoch last part time3 0.0029828548431396484
inside rlalgo, task 0, sumup 71761
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [157200]
collect time 0.0008471012115478516
inner_dict_sum {'sac_diff0': 0.00022339820861816406, 'sac_diff1': 0.007195472717285156, 'sac_diff2': 0.00844430923461914, 'sac_diff3': 0.011207818984985352, 'sac_diff4': 0.0073430538177490234, 'sac_diff5': 0.03370976448059082, 'sac_diff6': 0.0003838539123535156, 'all': 0.06850767135620117}
diff5_list [0.006721019744873047, 0.006453752517700195, 0.006745576858520508, 0.007135868072509766, 0.006653547286987305]
time3 0
time4 0.06927967071533203
time5 0.06933927536010742
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8268)
policy weight change tensor(42.9524, grad_fn=<SumBackward0>)
time8 0.00189971923828125
train_time 0.08091950416564941
eval time 0.14712047576904297
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:21,101 MainThread INFO: EPOCH:1041
2024-01-23 01:05:21,101 MainThread INFO: Time Consumed:0.23124217987060547s
2024-01-23 01:05:21,101 MainThread INFO: Total Frames:157050s
 10%|█         | 1042/10000 [06:48<39:04,  3.82it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           27392.98255
Train_Epoch_Reward                21374.35477
Running_Training_Average_Rewards  30048.98531
Explore_Time                      0.00084
Train___Time                      0.08092
Eval____Time                      0.14712
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25250.00163
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.48373    2.77866     103.41624    95.75609
alpha_0                           0.59397      0.00008     0.59409      0.59385
Alpha_loss                        -3.49579     0.00432     -3.49091     -3.50271
Training/policy_loss              -7.64664     0.00352     -7.64162     -7.65166
Training/qf1_loss                 9054.41523   1505.91341  11508.98340  6852.28223
Training/qf2_loss                 19748.49102  2024.48227  22807.54102  16618.02148
Training/pf_norm                  0.18584      0.02968     0.23305      0.15424
Training/qf1_norm                 2119.60071   566.38150   2744.28296   1161.83569
Training/qf2_norm                 3073.47261   82.06069    3156.04224   2931.73950
log_std/mean                      -0.13838     0.00015     -0.13824     -0.13865
log_probs/mean                    -2.71203     0.00813     -2.70137     -2.72530
mean/mean                         -0.01852     0.00012     -0.01836     -0.01869
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019437789916992188
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71761
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [157350]
collect time 0.0009086132049560547
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.007551908493041992, 'sac_diff2': 0.008844137191772461, 'sac_diff3': 0.011273622512817383, 'sac_diff4': 0.007384538650512695, 'sac_diff5': 0.03486466407775879, 'sac_diff6': 0.0004086494445800781, 'all': 0.07054519653320312}
diff5_list [0.006494045257568359, 0.00738072395324707, 0.0077648162841796875, 0.0069484710693359375, 0.006276607513427734]
time3 0
time4 0.07134580612182617
time5 0.07139420509338379
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8268)
policy weight change tensor(43.0313, grad_fn=<SumBackward0>)
time8 0.001874685287475586
train_time 0.08276867866516113
eval time 0.15097522735595703
epoch last part time 5.4836273193359375e-06
2024-01-23 01:05:21,360 MainThread INFO: EPOCH:1042
2024-01-23 01:05:21,361 MainThread INFO: Time Consumed:0.23702645301818848s
2024-01-23 01:05:21,361 MainThread INFO: Total Frames:157200s
 10%|█         | 1043/10000 [06:48<38:56,  3.83it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           26468.58066
Train_Epoch_Reward                35970.56780
Running_Training_Average_Rewards  30873.96416
Explore_Time                      0.00090
Train___Time                      0.08277
Eval____Time                      0.15098
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24937.40305
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.22877     0.63380     100.11581    98.58773
alpha_0                           0.59367      0.00008     0.59379      0.59356
Alpha_loss                        -3.49882     0.00536     -3.49220     -3.50826
Training/policy_loss              -7.06081     0.00504     -7.05600     -7.06989
Training/qf1_loss                 9667.33691   1438.43446  11542.40820  7644.97363
Training/qf2_loss                 20258.93164  1426.95530  22044.03125  18314.51172
Training/pf_norm                  0.27108      0.02394     0.30050      0.24105
Training/qf1_norm                 663.35676    60.09187    751.13916    593.99042
Training/qf2_norm                 2925.21406   19.50121    2950.54639   2903.96118
log_std/mean                      -0.13320     0.00003     -0.13315     -0.13324
log_probs/mean                    -2.71141     0.00899     -2.69999     -2.72694
mean/mean                         -0.01936     0.00004     -0.01931     -0.01942
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01821112632751465
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71761
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [157500]
collect time 0.0008540153503417969
inner_dict_sum {'sac_diff0': 0.00021648406982421875, 'sac_diff1': 0.0067980289459228516, 'sac_diff2': 0.00812840461730957, 'sac_diff3': 0.011010169982910156, 'sac_diff4': 0.0074462890625, 'sac_diff5': 0.032958030700683594, 'sac_diff6': 0.0003933906555175781, 'all': 0.06695079803466797}
diff5_list [0.006536245346069336, 0.006228923797607422, 0.006941795349121094, 0.0066335201263427734, 0.006617546081542969]
time3 0
time4 0.06772923469543457
time5 0.06778430938720703
time7 1.1920928955078125e-06
gen_weight_change tensor(-17.8268)
policy weight change tensor(43.1672, grad_fn=<SumBackward0>)
time8 0.001912832260131836
train_time 0.07900404930114746
eval time 0.14789605140686035
epoch last part time 6.198883056640625e-06
2024-01-23 01:05:21,612 MainThread INFO: EPOCH:1043
2024-01-23 01:05:21,613 MainThread INFO: Time Consumed:0.2301032543182373s
2024-01-23 01:05:21,613 MainThread INFO: Total Frames:157350s
 10%|█         | 1044/10000 [06:49<38:34,  3.87it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           25553.18684
Train_Epoch_Reward                14919.58300
Running_Training_Average_Rewards  30790.15834
Explore_Time                      0.00085
Train___Time                      0.07900
Eval____Time                      0.14790
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24503.76539
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       100.60937    1.62035    102.40431    97.63808
alpha_0                           0.59338      0.00008    0.59350      0.59326
Alpha_loss                        -3.50346     0.00330    -3.49987     -3.50919
Training/policy_loss              -6.81089     0.00510    -6.80237     -6.81839
Training/qf1_loss                 9184.05938   533.32561  9917.73730   8653.63965
Training/qf2_loss                 19928.15664  772.99945  21033.69727  18793.44336
Training/pf_norm                  0.16676      0.01733    0.19655      0.14811
Training/qf1_norm                 1794.91953   330.13852  2214.81812   1233.63440
Training/qf2_norm                 2832.55586   43.32559   2883.33301   2755.16284
log_std/mean                      -0.13020     0.00017    -0.13004     -0.13050
log_probs/mean                    -2.71387     0.00755    -2.70610     -2.72744
mean/mean                         -0.02053     0.00005    -0.02045     -0.02057
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01888275146484375
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71761
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [157650]
collect time 0.0009543895721435547
inner_dict_sum {'sac_diff0': 0.00021123886108398438, 'sac_diff1': 0.006992816925048828, 'sac_diff2': 0.008220434188842773, 'sac_diff3': 0.010825872421264648, 'sac_diff4': 0.007610797882080078, 'sac_diff5': 0.03258991241455078, 'sac_diff6': 0.0004146099090576172, 'all': 0.06686568260192871}
diff5_list [0.006685972213745117, 0.006430625915527344, 0.00630497932434082, 0.006264209747314453, 0.006904125213623047]
time3 0
time4 0.06765341758728027
time5 0.06770133972167969
time7 9.5367431640625e-07
gen_weight_change tensor(-17.8268)
policy weight change tensor(43.1603, grad_fn=<SumBackward0>)
time8 0.0019719600677490234
train_time 0.0791013240814209
eval time 0.16894197463989258
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:21,886 MainThread INFO: EPOCH:1044
2024-01-23 01:05:21,887 MainThread INFO: Time Consumed:0.25134825706481934s
2024-01-23 01:05:21,887 MainThread INFO: Total Frames:157500s
 10%|█         | 1045/10000 [06:49<39:15,  3.80it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           24656.98376
Train_Epoch_Reward                11802.42976
Running_Training_Average_Rewards  30586.32207
Explore_Time                      0.00095
Train___Time                      0.07910
Eval____Time                      0.16894
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24309.13613
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.80148     2.74721     102.83982    94.62144
alpha_0                           0.59308      0.00008     0.59320      0.59296
Alpha_loss                        -3.50543     0.00233     -3.50126     -3.50796
Training/policy_loss              -7.72325     0.00175     -7.72059     -7.72582
Training/qf1_loss                 9070.95840   935.95960   10691.20410  8307.36914
Training/qf2_loss                 19722.19883  1264.79199  21581.01758  17795.76172
Training/pf_norm                  0.11673      0.03048     0.15293      0.07276
Training/qf1_norm                 1065.69604   586.02120   2175.82300   441.76886
Training/qf2_norm                 3138.27456   86.40216    3234.71997   2975.75098
log_std/mean                      -0.14332     0.00015     -0.14305     -0.14348
log_probs/mean                    -2.71125     0.00409     -2.70582     -2.71608
mean/mean                         -0.00990     0.00002     -0.00988     -0.00993
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01879262924194336
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71761
epoch first part time 2.86102294921875e-06
replay_buffer._size: [157800]
collect time 0.0009453296661376953
inside mustsac before update, task 0, sumup 71761
inside mustsac after update, task 0, sumup 70142
inner_dict_sum {'sac_diff0': 0.00022125244140625, 'sac_diff1': 0.007371425628662109, 'sac_diff2': 0.008865833282470703, 'sac_diff3': 0.010877609252929688, 'sac_diff4': 0.007485628128051758, 'sac_diff5': 0.051442623138427734, 'sac_diff6': 0.00041103363037109375, 'all': 0.08667540550231934}
diff5_list [0.010646343231201172, 0.010261774063110352, 0.010092973709106445, 0.010155916213989258, 0.010285615921020508]
time3 0.0008864402770996094
time4 0.08757686614990234
time5 0.08763003349304199
time7 0.009061098098754883
gen_weight_change tensor(-17.8809)
policy weight change tensor(43.2703, grad_fn=<SumBackward0>)
time8 0.0018680095672607422
train_time 0.1173086166381836
eval time 0.13099431991577148
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:22,160 MainThread INFO: EPOCH:1045
2024-01-23 01:05:22,161 MainThread INFO: Time Consumed:0.25165557861328125s
2024-01-23 01:05:22,161 MainThread INFO: Total Frames:157650s
 10%|█         | 1046/10000 [06:49<39:45,  3.75it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23803.63824
Train_Epoch_Reward                12196.84907
Running_Training_Average_Rewards  29557.21699
Explore_Time                      0.00094
Train___Time                      0.11731
Eval____Time                      0.13099
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24210.06985
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.06654     3.44380     101.56110    92.34096
alpha_0                           0.59279      0.00008     0.59290      0.59267
Alpha_loss                        -3.50973     0.00308     -3.50613     -3.51339
Training/policy_loss              -7.84397     0.36994     -7.40483     -8.43507
Training/qf1_loss                 9230.81768   967.62113   10220.22754  7693.09619
Training/qf2_loss                 19621.20781  1579.90964  20947.86914  16773.42969
Training/pf_norm                  0.17467      0.03072     0.19826      0.11797
Training/qf1_norm                 1594.77387   1087.74245  3177.77051   207.65849
Training/qf2_norm                 3086.97485   166.44971   3411.31079   2947.84644
log_std/mean                      -0.13783     0.00505     -0.12983     -0.14462
log_probs/mean                    -2.71305     0.00618     -2.70409     -2.72005
mean/mean                         -0.01286     0.00894     0.00431      -0.02175
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018967866897583008
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70142
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [157950]
collect time 0.0008244514465332031
inner_dict_sum {'sac_diff0': 0.00021266937255859375, 'sac_diff1': 0.007448911666870117, 'sac_diff2': 0.008805036544799805, 'sac_diff3': 0.011599540710449219, 'sac_diff4': 0.0076045989990234375, 'sac_diff5': 0.03470802307128906, 'sac_diff6': 0.0004062652587890625, 'all': 0.0707850456237793}
diff5_list [0.007066011428833008, 0.0066568851470947266, 0.0071523189544677734, 0.006833076477050781, 0.0069997310638427734]
time3 0
time4 0.07160139083862305
time5 0.07165193557739258
time7 9.5367431640625e-07
gen_weight_change tensor(-17.8809)
policy weight change tensor(43.2033, grad_fn=<SumBackward0>)
time8 0.0019061565399169922
train_time 0.08302688598632812
eval time 0.16103363037109375
epoch last part time 5.7220458984375e-06
2024-01-23 01:05:22,430 MainThread INFO: EPOCH:1046
2024-01-23 01:05:22,431 MainThread INFO: Time Consumed:0.2473149299621582s
2024-01-23 01:05:22,431 MainThread INFO: Total Frames:157800s
 10%|█         | 1047/10000 [06:50<39:55,  3.74it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           25399.06356
Train_Epoch_Reward                24050.63186
Running_Training_Average_Rewards  29120.40371
Explore_Time                      0.00082
Train___Time                      0.08303
Eval____Time                      0.16103
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             39512.49555
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.67510    1.83429     103.86794    98.16989
alpha_0                           0.59249      0.00008     0.59261      0.59237
Alpha_loss                        -3.51325     0.00477     -3.50763     -3.52004
Training/policy_loss              -7.87668     0.00235     -7.87459     -7.88123
Training/qf1_loss                 9587.22520   849.75520   10601.41504  8236.66992
Training/qf2_loss                 20413.68086  1195.23152  22202.37500  18919.80859
Training/pf_norm                  0.12925      0.01276     0.14909      0.11186
Training/qf1_norm                 1759.28167   408.19418   2195.66675   996.43359
Training/qf2_norm                 3289.76514   57.72122    3388.77783   3208.91772
log_std/mean                      -0.13366     0.00004     -0.13361     -0.13373
log_probs/mean                    -2.71338     0.00893     -2.70138     -2.72380
mean/mean                         -0.01590     0.00014     -0.01566     -0.01605
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01948070526123047
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70142
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [158100]
collect time 0.001010894775390625
inner_dict_sum {'sac_diff0': 0.000217437744140625, 'sac_diff1': 0.00806879997253418, 'sac_diff2': 0.009512901306152344, 'sac_diff3': 0.012074947357177734, 'sac_diff4': 0.008374691009521484, 'sac_diff5': 0.03641176223754883, 'sac_diff6': 0.0004165172576904297, 'all': 0.07507705688476562}
diff5_list [0.007042646408081055, 0.007171630859375, 0.007467985153198242, 0.007329225540161133, 0.0074002742767333984]
time3 0
time4 0.07592320442199707
time5 0.07597470283508301
time7 9.5367431640625e-07
gen_weight_change tensor(-17.8809)
policy weight change tensor(42.8840, grad_fn=<SumBackward0>)
time8 0.002017498016357422
train_time 0.08754372596740723
eval time 0.1695575714111328
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:22,714 MainThread INFO: EPOCH:1047
2024-01-23 01:05:22,714 MainThread INFO: Time Consumed:0.2605102062225342s
2024-01-23 01:05:22,715 MainThread INFO: Total Frames:157950s
 10%|█         | 1048/10000 [06:50<40:38,  3.67it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           26992.32832
Train_Epoch_Reward                26393.89656
Running_Training_Average_Rewards  28439.10772
Explore_Time                      0.00101
Train___Time                      0.08754
Eval____Time                      0.16956
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             39415.31327
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.12510    1.97616     103.14790    98.67210
alpha_0                           0.59220      0.00008     0.59231      0.59208
Alpha_loss                        -3.51162     0.00431     -3.50486     -3.51628
Training/policy_loss              -6.83184     0.00576     -6.82373     -6.84103
Training/qf1_loss                 9198.55098   1041.15727  11083.20703  8211.29980
Training/qf2_loss                 20144.04844  1408.00294  22427.79102  18640.66016
Training/pf_norm                  0.26390      0.03492     0.31847      0.22969
Training/qf1_norm                 539.99210    150.94138   719.70728    369.86069
Training/qf2_norm                 2847.97856   57.84291    2904.58862   2773.81299
log_std/mean                      -0.14615     0.00054     -0.14531     -0.14682
log_probs/mean                    -2.70388     0.00755     -2.69352     -2.71277
mean/mean                         -0.01173     0.00002     -0.01171     -0.01175
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019049882888793945
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70142
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [158250]
collect time 0.000858306884765625
inner_dict_sum {'sac_diff0': 0.0002105236053466797, 'sac_diff1': 0.006798267364501953, 'sac_diff2': 0.007942914962768555, 'sac_diff3': 0.010245084762573242, 'sac_diff4': 0.006964921951293945, 'sac_diff5': 0.032567739486694336, 'sac_diff6': 0.0004067420959472656, 'all': 0.06513619422912598}
diff5_list [0.006807804107666016, 0.006223917007446289, 0.006114959716796875, 0.0069370269775390625, 0.006484031677246094]
time3 0
time4 0.06593608856201172
time5 0.06598258018493652
time7 9.5367431640625e-07
gen_weight_change tensor(-17.8809)
policy weight change tensor(42.5683, grad_fn=<SumBackward0>)
time8 0.0018799304962158203
train_time 0.07710647583007812
eval time 0.15150737762451172
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:22,969 MainThread INFO: EPOCH:1048
2024-01-23 01:05:22,969 MainThread INFO: Time Consumed:0.23188114166259766s
2024-01-23 01:05:22,969 MainThread INFO: Total Frames:158100s
 10%|█         | 1049/10000 [06:50<39:48,  3.75it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           28533.66165
Train_Epoch_Reward                40638.14948
Running_Training_Average_Rewards  29227.34789
Explore_Time                      0.00085
Train___Time                      0.07711
Eval____Time                      0.15151
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             38895.99898
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.59672    1.95053     103.65985    98.34792
alpha_0                           0.59190      0.00008     0.59202      0.59178
Alpha_loss                        -3.51650     0.00385     -3.51119     -3.52305
Training/policy_loss              -7.71479     0.00427     -7.70933     -7.72004
Training/qf1_loss                 10344.28730  1116.33408  11513.45898  8261.18945
Training/qf2_loss                 21213.63320  1457.84995  22676.89648  18460.61914
Training/pf_norm                  0.12500      0.01146     0.13994      0.10503
Training/qf1_norm                 2453.49866   363.71591   2843.30347   1816.61902
Training/qf2_norm                 3142.79932   59.80322    3202.92676   3043.96069
log_std/mean                      -0.12786     0.00020     -0.12760     -0.12818
log_probs/mean                    -2.70681     0.00619     -2.69923     -2.71803
mean/mean                         -0.00810     0.00005     -0.00802     -0.00816
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01849651336669922
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70142
epoch first part time 2.86102294921875e-06
replay_buffer._size: [158400]
collect time 0.0008838176727294922
inner_dict_sum {'sac_diff0': 0.00020837783813476562, 'sac_diff1': 0.006850004196166992, 'sac_diff2': 0.007838249206542969, 'sac_diff3': 0.010283231735229492, 'sac_diff4': 0.006804704666137695, 'sac_diff5': 0.03200817108154297, 'sac_diff6': 0.00038170814514160156, 'all': 0.06437444686889648}
diff5_list [0.0067670345306396484, 0.006357431411743164, 0.006220340728759766, 0.006430625915527344, 0.006232738494873047]
time3 0
time4 0.06513857841491699
time5 0.06518411636352539
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8809)
policy weight change tensor(42.5092, grad_fn=<SumBackward0>)
time8 0.0018723011016845703
train_time 0.07629704475402832
eval time 0.15166330337524414
epoch last part time 5.245208740234375e-06
2024-01-23 01:05:23,222 MainThread INFO: EPOCH:1049
2024-01-23 01:05:23,222 MainThread INFO: Time Consumed:0.23115205764770508s
2024-01-23 01:05:23,222 MainThread INFO: Total Frames:158250s
 10%|█         | 1050/10000 [06:50<39:13,  3.80it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           30121.26590
Train_Epoch_Reward                62865.96036
Running_Training_Average_Rewards  30709.70732
Explore_Time                      0.00088
Train___Time                      0.07630
Eval____Time                      0.15166
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             38256.47810
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       99.10903     1.48302    101.68024    97.45540
alpha_0                           0.59160      0.00008    0.59172      0.59149
Alpha_loss                        -3.52314     0.00174    -3.52048     -3.52581
Training/policy_loss              -8.21046     0.00359    -8.20689     -8.21657
Training/qf1_loss                 9096.38516   464.30019  9683.27539   8260.99219
Training/qf2_loss                 19658.29609  626.22169  20413.48828  18635.85938
Training/pf_norm                  0.34641      0.00737    0.35298      0.33383
Training/qf1_norm                 377.78789    176.11609  697.94250    213.92381
Training/qf2_norm                 3271.60117   46.79155   3352.29761   3225.89795
log_std/mean                      -0.13602     0.00011    -0.13588     -0.13620
log_probs/mean                    -2.71310     0.00295    -2.71039     -2.71818
mean/mean                         -0.02614     0.00017    -0.02595     -0.02643
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019041061401367188
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70142
epoch first part time 2.86102294921875e-06
replay_buffer._size: [158550]
collect time 0.0009124279022216797
inside mustsac before update, task 0, sumup 70142
inside mustsac after update, task 0, sumup 70838
inner_dict_sum {'sac_diff0': 0.0002155303955078125, 'sac_diff1': 0.006966352462768555, 'sac_diff2': 0.008465051651000977, 'sac_diff3': 0.010828733444213867, 'sac_diff4': 0.0075299739837646484, 'sac_diff5': 0.05122113227844238, 'sac_diff6': 0.00041174888610839844, 'all': 0.08563852310180664}
diff5_list [0.010878324508666992, 0.010166406631469727, 0.010339975357055664, 0.009979963302612305, 0.009856462478637695]
time3 0.0008633136749267578
time4 0.08648061752319336
time5 0.0865328311920166
time7 0.009108543395996094
gen_weight_change tensor(-17.8225)
policy weight change tensor(42.6016, grad_fn=<SumBackward0>)
time8 0.0025992393493652344
train_time 0.1165626049041748
eval time 0.10949516296386719
epoch last part time 4.291534423828125e-06
2024-01-23 01:05:23,474 MainThread INFO: EPOCH:1050
2024-01-23 01:05:23,474 MainThread INFO: Time Consumed:0.2291560173034668s
2024-01-23 01:05:23,474 MainThread INFO: Total Frames:158400s
 11%|█         | 1051/10000 [06:51<38:48,  3.84it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31772.00495
Train_Epoch_Reward                34576.83830
Running_Training_Average_Rewards  31525.22901
Explore_Time                      0.00091
Train___Time                      0.11656
Eval____Time                      0.10950
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             38429.38754
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.36703    1.84385     102.43668    97.52035
alpha_0                           0.59131      0.00008     0.59143      0.59119
Alpha_loss                        -3.52176     0.00840     -3.50915     -3.53504
Training/policy_loss              -7.96556     0.08929     -7.83362     -8.07112
Training/qf1_loss                 8651.29453   1450.11161  11026.92188  6504.07666
Training/qf2_loss                 19386.12969  1837.98527  22320.08203  16794.51562
Training/pf_norm                  0.19624      0.04533     0.23573      0.12529
Training/qf1_norm                 1010.78497   412.69903   1820.09619   696.48206
Training/qf2_norm                 3178.40234   60.29218    3222.10498   3059.62524
log_std/mean                      -0.13529     0.00453     -0.12843     -0.14071
log_probs/mean                    -2.70408     0.01539     -2.68009     -2.72809
mean/mean                         -0.01377     0.00131     -0.01210     -0.01612
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018442392349243164
epoch last part time3 0.0025665760040283203
inside rlalgo, task 0, sumup 70838
epoch first part time 2.86102294921875e-06
replay_buffer._size: [158700]
collect time 0.0009388923645019531
inner_dict_sum {'sac_diff0': 0.0002033710479736328, 'sac_diff1': 0.00674128532409668, 'sac_diff2': 0.007895946502685547, 'sac_diff3': 0.01016378402709961, 'sac_diff4': 0.006798267364501953, 'sac_diff5': 0.031148910522460938, 'sac_diff6': 0.00038504600524902344, 'all': 0.06333661079406738}
diff5_list [0.0064198970794677734, 0.0061571598052978516, 0.006275177001953125, 0.006206989288330078, 0.006089687347412109]
time3 0
time4 0.06409621238708496
time5 0.06414103507995605
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8225)
policy weight change tensor(42.6391, grad_fn=<SumBackward0>)
time8 0.0018155574798583984
train_time 0.07514786720275879
eval time 0.14267969131469727
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:23,719 MainThread INFO: EPOCH:1051
2024-01-23 01:05:23,719 MainThread INFO: Time Consumed:0.22109198570251465s
2024-01-23 01:05:23,720 MainThread INFO: Total Frames:158550s
 11%|█         | 1052/10000 [06:51<38:00,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31664.21558
Train_Epoch_Reward                19860.71114
Running_Training_Average_Rewards  31831.69546
Explore_Time                      0.00093
Train___Time                      0.07515
Eval____Time                      0.14268
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24172.10790
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.06194    4.53779     110.00826    97.85927
alpha_0                           0.59102      0.00008     0.59113      0.59090
Alpha_loss                        -3.53139     0.00318     -3.52679     -3.53575
Training/policy_loss              -8.24321     0.00301     -8.23954     -8.24797
Training/qf1_loss                 9683.88262   2357.65167  14141.43164  7113.15137
Training/qf2_loss                 20589.73242  3267.51936  26880.83984  17308.20312
Training/pf_norm                  0.20029      0.02383     0.24149      0.17619
Training/qf1_norm                 1502.29004   940.16449   3345.51392   774.06989
Training/qf2_norm                 3326.33770   145.86292   3614.40625   3223.42041
log_std/mean                      -0.13523     0.00013     -0.13506     -0.13543
log_probs/mean                    -2.71605     0.00707     -2.70603     -2.72561
mean/mean                         -0.01337     0.00011     -0.01317     -0.01346
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01819467544555664
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70838
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [158850]
collect time 0.0008339881896972656
inner_dict_sum {'sac_diff0': 0.000217437744140625, 'sac_diff1': 0.00676727294921875, 'sac_diff2': 0.007967710494995117, 'sac_diff3': 0.010182380676269531, 'sac_diff4': 0.006810426712036133, 'sac_diff5': 0.03223872184753418, 'sac_diff6': 0.0004093647003173828, 'all': 0.06459331512451172}
diff5_list [0.006562471389770508, 0.006192684173583984, 0.00644683837890625, 0.0068187713623046875, 0.00621795654296875]
time3 0
time4 0.0653536319732666
time5 0.0653984546661377
time7 4.76837158203125e-07
gen_weight_change tensor(-17.8225)
policy weight change tensor(42.6315, grad_fn=<SumBackward0>)
time8 0.0019426345825195312
train_time 0.07664966583251953
eval time 0.14890742301940918
epoch last part time 4.5299530029296875e-06
2024-01-23 01:05:23,970 MainThread INFO: EPOCH:1052
2024-01-23 01:05:23,970 MainThread INFO: Time Consumed:0.22863054275512695s
2024-01-23 01:05:23,970 MainThread INFO: Total Frames:158700s
 11%|█         | 1053/10000 [06:51<37:49,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31623.52967
Train_Epoch_Reward                68763.08065
Running_Training_Average_Rewards  33831.30372
Explore_Time                      0.00083
Train___Time                      0.07665
Eval____Time                      0.14891
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24530.54404
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.51234    2.05473     105.37675    99.87925
alpha_0                           0.59072      0.00008     0.59084      0.59060
Alpha_loss                        -3.53106     0.00295     -3.52725     -3.53573
Training/policy_loss              -7.63463     0.00440     -7.62931     -7.64149
Training/qf1_loss                 9821.17773   1522.23188  11229.57520  7199.24512
Training/qf2_loss                 20808.77422  1859.99327  23017.24609  17792.81641
Training/pf_norm                  0.10996      0.01468     0.12979      0.09147
Training/qf1_norm                 909.45548    439.52314   1750.79041   522.77118
Training/qf2_norm                 3225.83506   64.77778    3347.62012   3174.75708
log_std/mean                      -0.13308     0.00019     -0.13281     -0.13333
log_probs/mean                    -2.70904     0.00487     -2.70308     -2.71538
mean/mean                         -0.01222     0.00020     -0.01191     -0.01248
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01864027976989746
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70838
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [159000]
collect time 0.0009038448333740234
inner_dict_sum {'sac_diff0': 0.00023055076599121094, 'sac_diff1': 0.006882667541503906, 'sac_diff2': 0.008324623107910156, 'sac_diff3': 0.010720491409301758, 'sac_diff4': 0.0071468353271484375, 'sac_diff5': 0.032505035400390625, 'sac_diff6': 0.00038361549377441406, 'all': 0.06619381904602051}
diff5_list [0.00648188591003418, 0.0060079097747802734, 0.0070683956146240234, 0.006575345993041992, 0.006371498107910156]
time3 0
time4 0.06693863868713379
time5 0.06698393821716309
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8225)
policy weight change tensor(42.5345, grad_fn=<SumBackward0>)
time8 0.0018181800842285156
train_time 0.07784295082092285
eval time 0.16921663284301758
epoch last part time 5.4836273193359375e-06
2024-01-23 01:05:24,242 MainThread INFO: EPOCH:1053
2024-01-23 01:05:24,242 MainThread INFO: Time Consumed:0.2503213882446289s
2024-01-23 01:05:24,243 MainThread INFO: Total Frames:158850s
 11%|█         | 1054/10000 [06:51<38:39,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31668.65045
Train_Epoch_Reward                16279.37087
Running_Training_Average_Rewards  33945.16355
Explore_Time                      0.00090
Train___Time                      0.07784
Eval____Time                      0.16922
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24954.97317
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.10998    2.63891     103.96580    96.25672
alpha_0                           0.59043      0.00008     0.59054      0.59031
Alpha_loss                        -3.53764     0.00374     -3.53124     -3.54165
Training/policy_loss              -8.32498     0.00508     -8.31837     -8.33144
Training/qf1_loss                 9204.33789   1215.18432  11029.81738  7796.79932
Training/qf2_loss                 20100.64219  1581.17183  21988.78125  17680.69531
Training/pf_norm                  0.12177      0.03017     0.17604      0.08599
Training/qf1_norm                 1675.77900   510.55153   2199.37036   740.69812
Training/qf2_norm                 3433.36299   90.30182    3528.33496   3267.02661
log_std/mean                      -0.13960     0.00019     -0.13931     -0.13984
log_probs/mean                    -2.71519     0.00682     -2.70558     -2.72280
mean/mean                         -0.00026     0.00017     -0.00002     -0.00050
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018274784088134766
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70838
epoch first part time 2.384185791015625e-06
replay_buffer._size: [159150]
collect time 0.0009894371032714844
inner_dict_sum {'sac_diff0': 0.00021958351135253906, 'sac_diff1': 0.007146120071411133, 'sac_diff2': 0.008409976959228516, 'sac_diff3': 0.010923385620117188, 'sac_diff4': 0.0070362091064453125, 'sac_diff5': 0.032402753829956055, 'sac_diff6': 0.0004000663757324219, 'all': 0.06653809547424316}
diff5_list [0.006674051284790039, 0.006770610809326172, 0.006640434265136719, 0.0061206817626953125, 0.0061969757080078125]
time3 0
time4 0.06731700897216797
time5 0.0673666000366211
time7 1.1920928955078125e-06
gen_weight_change tensor(-17.8225)
policy weight change tensor(42.5726, grad_fn=<SumBackward0>)
time8 0.0018732547760009766
train_time 0.07867860794067383
eval time 0.14569354057312012
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:24,492 MainThread INFO: EPOCH:1054
2024-01-23 01:05:24,492 MainThread INFO: Time Consumed:0.22780990600585938s
2024-01-23 01:05:24,492 MainThread INFO: Total Frames:159000s
 11%|█         | 1055/10000 [06:52<38:15,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31760.49847
Train_Epoch_Reward                10215.62920
Running_Training_Average_Rewards  32662.16240
Explore_Time                      0.00098
Train___Time                      0.07868
Eval____Time                      0.14569
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25227.61630
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.26517    3.13257     104.46107    94.85413
alpha_0                           0.59013      0.00008     0.59025      0.59001
Alpha_loss                        -3.54153     0.00429     -3.53795     -3.54867
Training/policy_loss              -8.23392     0.00733     -8.22507     -8.24656
Training/qf1_loss                 8890.56328   1469.17214  11396.68066  6838.56641
Training/qf2_loss                 19648.30078  2144.55790  23125.37305  16362.84961
Training/pf_norm                  0.16195      0.02721     0.19707      0.11353
Training/qf1_norm                 657.14763    469.99152   1564.87073   250.93643
Training/qf2_norm                 3263.42393   105.19181   3407.04370   3083.06494
log_std/mean                      -0.13006     0.00010     -0.12994     -0.13017
log_probs/mean                    -2.71621     0.00744     -2.70888     -2.72721
mean/mean                         -0.00419     0.00010     -0.00405     -0.00434
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019269227981567383
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70838
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [159300]
collect time 0.0009682178497314453
inside mustsac before update, task 0, sumup 70838
inside mustsac after update, task 0, sumup 71097
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.007017850875854492, 'sac_diff2': 0.008635759353637695, 'sac_diff3': 0.010787487030029297, 'sac_diff4': 0.007607221603393555, 'sac_diff5': 0.051722049713134766, 'sac_diff6': 0.00041484832763671875, 'all': 0.08639955520629883}
diff5_list [0.010772228240966797, 0.010486841201782227, 0.010139226913452148, 0.010148286819458008, 0.010175466537475586]
time3 0.0008797645568847656
time4 0.08725905418395996
time5 0.08731198310852051
time7 0.009172201156616211
gen_weight_change tensor(-17.8602)
policy weight change tensor(42.6084, grad_fn=<SumBackward0>)
time8 0.0020716190338134766
train_time 0.11695313453674316
eval time 0.10412216186523438
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:24,739 MainThread INFO: EPOCH:1055
2024-01-23 01:05:24,740 MainThread INFO: Time Consumed:0.22440624237060547s
2024-01-23 01:05:24,740 MainThread INFO: Total Frames:159150s
 11%|█         | 1056/10000 [06:52<37:49,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           31885.56593
Train_Epoch_Reward                29576.16769
Running_Training_Average_Rewards  32906.64951
Explore_Time                      0.00096
Train___Time                      0.11695
Eval____Time                      0.10412
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25460.74448
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.04545    2.25540     104.81738    97.74510
alpha_0                           0.58984      0.00008     0.58995      0.58972
Alpha_loss                        -3.54029     0.00634     -3.52793     -3.54560
Training/policy_loss              -7.79134     0.37412     -7.20927     -8.35557
Training/qf1_loss                 9810.94395   977.38406   11018.30469  8936.92871
Training/qf2_loss                 20761.90000  1390.34109  22901.76562  19286.92578
Training/pf_norm                  0.17859      0.05157     0.21211      0.07640
Training/qf1_norm                 510.11967    282.64788   909.95264    208.00095
Training/qf2_norm                 3162.59385   112.61891   3264.64160   2963.90063
log_std/mean                      -0.13333     0.00263     -0.13100     -0.13750
log_probs/mean                    -2.70751     0.01194     -2.68409     -2.71603
mean/mean                         -0.01217     0.00218     -0.00913     -0.01522
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018794775009155273
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71097
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [159450]
collect time 0.0009520053863525391
inner_dict_sum {'sac_diff0': 0.00021505355834960938, 'sac_diff1': 0.00711369514465332, 'sac_diff2': 0.008510112762451172, 'sac_diff3': 0.010341405868530273, 'sac_diff4': 0.0073354244232177734, 'sac_diff5': 0.03258514404296875, 'sac_diff6': 0.00039005279541015625, 'all': 0.06649088859558105}
diff5_list [0.00671696662902832, 0.006256580352783203, 0.006236553192138672, 0.007001638412475586, 0.006373405456542969]
time3 0
time4 0.06730961799621582
time5 0.06736135482788086
time7 9.5367431640625e-07
gen_weight_change tensor(-17.8602)
policy weight change tensor(42.7341, grad_fn=<SumBackward0>)
time8 0.0018756389617919922
train_time 0.07869672775268555
eval time 0.14628934860229492
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:24,990 MainThread INFO: EPOCH:1056
2024-01-23 01:05:24,990 MainThread INFO: Time Consumed:0.2283635139465332s
2024-01-23 01:05:24,991 MainThread INFO: Total Frames:159300s
 11%|█         | 1057/10000 [06:52<37:39,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           30477.28083
Train_Epoch_Reward                11969.16282
Running_Training_Average_Rewards  32110.89949
Explore_Time                      0.00095
Train___Time                      0.07870
Eval____Time                      0.14629
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25429.64453
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.21015    2.19924     102.25796    96.70947
alpha_0                           0.58954      0.00008     0.58966      0.58942
Alpha_loss                        -3.54425     0.00526     -3.53597     -3.55138
Training/policy_loss              -8.02700     0.00415     -8.02253     -8.03418
Training/qf1_loss                 8223.63213   1211.07323  10042.96777  6622.00684
Training/qf2_loss                 18959.44805  1428.69818  20469.22266  16539.44922
Training/pf_norm                  0.13645      0.03200     0.19282      0.11061
Training/qf1_norm                 518.83007    160.76923   707.85437    266.49411
Training/qf2_norm                 3316.11006   74.93891    3384.08667   3195.10815
log_std/mean                      -0.14335     0.00008     -0.14322     -0.14343
log_probs/mean                    -2.70867     0.00928     -2.69300     -2.72090
mean/mean                         -0.01199     0.00016     -0.01184     -0.01227
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018194198608398438
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71097
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [159600]
collect time 0.0010178089141845703
inner_dict_sum {'sac_diff0': 0.00021767616271972656, 'sac_diff1': 0.007080793380737305, 'sac_diff2': 0.008421659469604492, 'sac_diff3': 0.010954856872558594, 'sac_diff4': 0.007178544998168945, 'sac_diff5': 0.032280921936035156, 'sac_diff6': 0.0003933906555175781, 'all': 0.0665278434753418}
diff5_list [0.0068950653076171875, 0.0063440799713134766, 0.006321907043457031, 0.006496906280517578, 0.006222963333129883]
time3 0
time4 0.06730914115905762
time5 0.06735873222351074
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8602)
policy weight change tensor(42.7886, grad_fn=<SumBackward0>)
time8 0.0018339157104492188
train_time 0.07856225967407227
eval time 0.1498584747314453
epoch last part time 5.7220458984375e-06
2024-01-23 01:05:25,244 MainThread INFO: EPOCH:1057
2024-01-23 01:05:25,244 MainThread INFO: Time Consumed:0.23177528381347656s
2024-01-23 01:05:25,244 MainThread INFO: Total Frames:159450s
 11%|█         | 1058/10000 [06:52<37:43,  3.95it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           29089.19305
Train_Epoch_Reward                14889.05573
Running_Training_Average_Rewards  32122.83818
Explore_Time                      0.00101
Train___Time                      0.07856
Eval____Time                      0.14986
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25534.43548
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       101.70710    0.86519    103.20095    100.55416
alpha_0                           0.58925      0.00008    0.58937      0.58913
Alpha_loss                        -3.54612     0.00301    -3.54144     -3.55054
Training/policy_loss              -8.37504     0.00469    -8.36590     -8.37932
Training/qf1_loss                 10059.44902  921.73851  11137.65234  8384.34570
Training/qf2_loss                 21136.27969  944.18983  22169.71289  19365.09961
Training/pf_norm                  0.24315      0.01720    0.26491      0.22171
Training/qf1_norm                 380.89546    123.06334  590.76636    249.36696
Training/qf2_norm                 3401.94849   28.73043   3452.35571   3363.83423
log_std/mean                      -0.14408     0.00021    -0.14376     -0.14433
log_probs/mean                    -2.70589     0.00493    -2.69703     -2.71171
mean/mean                         -0.01223     0.00048    -0.01168     -0.01301
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01845693588256836
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71097
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [159750]
collect time 0.0009884834289550781
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.00709986686706543, 'sac_diff2': 0.008489847183227539, 'sac_diff3': 0.010865926742553711, 'sac_diff4': 0.007379770278930664, 'sac_diff5': 0.0328369140625, 'sac_diff6': 0.00039958953857421875, 'all': 0.06728649139404297}
diff5_list [0.006952524185180664, 0.00642085075378418, 0.006689786911010742, 0.0064067840576171875, 0.0063669681549072266]
time3 0
time4 0.06810188293457031
time5 0.06815242767333984
time7 9.5367431640625e-07
gen_weight_change tensor(-17.8602)
policy weight change tensor(43.0822, grad_fn=<SumBackward0>)
time8 0.0018885135650634766
train_time 0.07975888252258301
eval time 0.14348435401916504
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:25,493 MainThread INFO: EPOCH:1058
2024-01-23 01:05:25,493 MainThread INFO: Time Consumed:0.22670745849609375s
2024-01-23 01:05:25,493 MainThread INFO: Total Frames:159600s
 11%|█         | 1059/10000 [06:53<37:31,  3.97it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           27733.89931
Train_Epoch_Reward                10188.55594
Running_Training_Average_Rewards  31858.89047
Explore_Time                      0.00098
Train___Time                      0.07976
Eval____Time                      0.14348
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             25343.06154
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       99.42484     0.84344    100.89833    98.56887
alpha_0                           0.58895      0.00008    0.58907      0.58884
Alpha_loss                        -3.54960     0.00328    -3.54584     -3.55467
Training/policy_loss              -7.94768     0.00801    -7.93258     -7.95451
Training/qf1_loss                 9719.37754   478.22258  10565.82227  9261.94531
Training/qf2_loss                 20193.02500  625.17411  21342.63672  19574.60938
Training/pf_norm                  0.40619      0.02878    0.45888      0.37939
Training/qf1_norm                 1957.59622   177.46040  2286.73877   1817.45361
Training/qf2_norm                 3178.15181   24.47918   3215.28076   3150.43481
log_std/mean                      -0.14060     0.00024    -0.14036     -0.14098
log_probs/mean                    -2.70614     0.00542    -2.70030     -2.71572
mean/mean                         -0.02100     0.00090    -0.01982     -0.02236
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018490076065063477
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71097
epoch first part time 2.86102294921875e-06
replay_buffer._size: [159900]
collect time 0.0009920597076416016
inner_dict_sum {'sac_diff0': 0.00021791458129882812, 'sac_diff1': 0.0071392059326171875, 'sac_diff2': 0.008286476135253906, 'sac_diff3': 0.010735750198364258, 'sac_diff4': 0.007472515106201172, 'sac_diff5': 0.0319514274597168, 'sac_diff6': 0.0004105567932128906, 'all': 0.06621384620666504}
diff5_list [0.00654292106628418, 0.006360769271850586, 0.006448984146118164, 0.006288766860961914, 0.006309986114501953]
time3 0
time4 0.06703591346740723
time5 0.06708979606628418
time7 7.152557373046875e-07
gen_weight_change tensor(-17.8602)
policy weight change tensor(43.4823, grad_fn=<SumBackward0>)
time8 0.001987457275390625
train_time 0.0786135196685791
eval time 0.1404891014099121
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:25,738 MainThread INFO: EPOCH:1059
2024-01-23 01:05:25,738 MainThread INFO: Time Consumed:0.2225360870361328s
2024-01-23 01:05:25,738 MainThread INFO: Total Frames:159750s
 11%|█         | 1060/10000 [06:53<37:13,  4.00it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           26389.91537
Train_Epoch_Reward                18638.53692
Running_Training_Average_Rewards  31953.61551
Explore_Time                      0.00099
Train___Time                      0.07861
Eval____Time                      0.14049
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24816.63876
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.47578    3.53649     105.30802    95.81130
alpha_0                           0.58866      0.00008     0.58878      0.58854
Alpha_loss                        -3.55530     0.00344     -3.55136     -3.56076
Training/policy_loss              -8.02585     0.00350     -8.02158     -8.03041
Training/qf1_loss                 9264.93330   1649.84513  11263.50586  7312.49365
Training/qf2_loss                 20062.93086  2379.18816  22960.48242  17147.15430
Training/pf_norm                  0.21255      0.02935     0.25504      0.16749
Training/qf1_norm                 1117.73511   726.00968   2069.07422   199.33798
Training/qf2_norm                 3340.80039   117.65334   3505.48828   3187.28589
log_std/mean                      -0.12838     0.00029     -0.12798     -0.12880
log_probs/mean                    -2.71059     0.00720     -2.70062     -2.72089
mean/mean                         -0.01894     0.00091     -0.01768     -0.02024
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018737077713012695
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71097
epoch first part time 3.337860107421875e-06
replay_buffer._size: [160050]
collect time 0.0010061264038085938
inside mustsac before update, task 0, sumup 71097
inside mustsac after update, task 0, sumup 71125
inner_dict_sum {'sac_diff0': 0.00022840499877929688, 'sac_diff1': 0.007039546966552734, 'sac_diff2': 0.008595705032348633, 'sac_diff3': 0.011130809783935547, 'sac_diff4': 0.007531881332397461, 'sac_diff5': 0.05236935615539551, 'sac_diff6': 0.00041961669921875, 'all': 0.08731532096862793}
diff5_list [0.0109710693359375, 0.010767698287963867, 0.010937929153442383, 0.009886741638183594, 0.009805917739868164]
time3 0.000888824462890625
time4 0.08825469017028809
time5 0.08831620216369629
time7 0.009453058242797852
gen_weight_change tensor(-17.9274)
policy weight change tensor(43.5897, grad_fn=<SumBackward0>)
time8 0.002554655075073242
train_time 0.11887311935424805
eval time 0.10519123077392578
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:25,988 MainThread INFO: EPOCH:1060
2024-01-23 01:05:25,988 MainThread INFO: Time Consumed:0.2274329662322998s
2024-01-23 01:05:25,988 MainThread INFO: Total Frames:159900s
 11%|█         | 1061/10000 [06:53<37:20,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           24969.75553
Train_Epoch_Reward                13725.90567
Running_Training_Average_Rewards  30533.40225
Explore_Time                      0.00100
Train___Time                      0.11887
Eval____Time                      0.10519
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             24227.78906
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       98.43596     1.37244     100.81770    97.02770
alpha_0                           0.58837      0.00008     0.58849      0.58825
Alpha_loss                        -3.55747     0.00663     -3.54590     -3.56659
Training/policy_loss              -8.13082     0.21672     -7.77154     -8.30089
Training/qf1_loss                 8521.98945   2050.55061  12408.89844  6435.16406
Training/qf2_loss                 18819.28457  2383.47948  23293.07422  16302.83105
Training/pf_norm                  0.20887      0.05736     0.28376      0.14290
Training/qf1_norm                 1878.51743   903.35988   2841.53442   373.32321
Training/qf2_norm                 3213.53750   38.13858    3244.65601   3141.71191
log_std/mean                      -0.13521     0.00536     -0.12787     -0.14453
log_probs/mean                    -2.70836     0.01203     -2.68905     -2.72682
mean/mean                         -0.01693     0.00504     -0.00968     -0.02128
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018332719802856445
epoch last part time3 0.002853870391845703
inside rlalgo, task 0, sumup 71125
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [160200]
collect time 0.0009393692016601562
inner_dict_sum {'sac_diff0': 0.0002257823944091797, 'sac_diff1': 0.007041454315185547, 'sac_diff2': 0.009911775588989258, 'sac_diff3': 0.011323213577270508, 'sac_diff4': 0.007311820983886719, 'sac_diff5': 0.03419017791748047, 'sac_diff6': 0.00040602684020996094, 'all': 0.07041025161743164}
diff5_list [0.0066792964935302734, 0.006703376770019531, 0.0065174102783203125, 0.0070858001708984375, 0.007204294204711914]
time3 0
time4 0.07124805450439453
time5 0.07130908966064453
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9274)
policy weight change tensor(43.8683, grad_fn=<SumBackward0>)
time8 0.0018677711486816406
train_time 0.08276557922363281
eval time 0.13620877265930176
epoch last part time 1.0013580322265625e-05
2024-01-23 01:05:26,234 MainThread INFO: EPOCH:1061
2024-01-23 01:05:26,235 MainThread INFO: Time Consumed:0.2222750186920166s
2024-01-23 01:05:26,235 MainThread INFO: Total Frames:160050s
 11%|█         | 1062/10000 [06:53<37:05,  4.02it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           24774.90911
Train_Epoch_Reward                7919.20535
Running_Training_Average_Rewards  30345.22057
Explore_Time                      0.00093
Train___Time                      0.08277
Eval____Time                      0.13621
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22223.64371
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.52646    1.54676     103.41573    99.14048
alpha_0                           0.58807      0.00008     0.58819      0.58796
Alpha_loss                        -3.55691     0.00277     -3.55392     -3.56044
Training/policy_loss              -7.64112     0.00380     -7.63665     -7.64753
Training/qf1_loss                 9943.43965   1400.33547  12182.73145  8462.91016
Training/qf2_loss                 20622.20508  1640.83026  22967.49805  18879.72070
Training/pf_norm                  0.16767      0.01662     0.19025      0.14789
Training/qf1_norm                 1966.07227   281.05480   2452.24316   1674.41370
Training/qf2_norm                 3076.05605   46.90910    3165.11499   3035.12378
log_std/mean                      -0.13291     0.00031     -0.13245     -0.13334
log_probs/mean                    -2.70101     0.00402     -2.69707     -2.70660
mean/mean                         -0.02087     0.00049     -0.02014     -0.02154
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01951313018798828
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 71125
epoch first part time 3.337860107421875e-06
replay_buffer._size: [160350]
collect time 0.0009622573852539062
inner_dict_sum {'sac_diff0': 0.00022721290588378906, 'sac_diff1': 0.00761866569519043, 'sac_diff2': 0.00873708724975586, 'sac_diff3': 0.011152029037475586, 'sac_diff4': 0.007520914077758789, 'sac_diff5': 0.0328059196472168, 'sac_diff6': 0.0004119873046875, 'all': 0.06847381591796875}
diff5_list [0.00644373893737793, 0.006523847579956055, 0.006670236587524414, 0.006708383560180664, 0.006459712982177734]
time3 0
time4 0.06929469108581543
time5 0.0693504810333252
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9274)
policy weight change tensor(43.9739, grad_fn=<SumBackward0>)
time8 0.0019795894622802734
train_time 0.08118224143981934
eval time 0.1440105438232422
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:26,487 MainThread INFO: EPOCH:1062
2024-01-23 01:05:26,487 MainThread INFO: Time Consumed:0.2286381721496582s
2024-01-23 01:05:26,487 MainThread INFO: Total Frames:160200s
 11%|█         | 1063/10000 [06:54<37:12,  4.00it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           24493.18778
Train_Epoch_Reward                23449.58656
Running_Training_Average_Rewards  29355.36369
Explore_Time                      0.00095
Train___Time                      0.08118
Eval____Time                      0.14401
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21713.33080
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       99.82673     0.87914    100.81074    98.74181
alpha_0                           0.58778      0.00008    0.58790      0.58766
Alpha_loss                        -3.55881     0.00426    -3.55373     -3.56556
Training/policy_loss              -8.24254     0.00513    -8.23713     -8.24956
Training/qf1_loss                 8676.31953   429.57696  9098.49023   7943.67676
Training/qf2_loss                 19058.34141  554.68332  19695.71875  18066.33203
Training/pf_norm                  0.26162      0.02451    0.29069      0.23226
Training/qf1_norm                 4033.03242   183.17851  4270.60840   3852.22852
Training/qf2_norm                 3286.04839   30.54658   3326.14624   3250.09106
log_std/mean                      -0.14465     0.00015    -0.14440     -0.14481
log_probs/mean                    -2.69830     0.00721    -2.68998     -2.70974
mean/mean                         -0.02787     0.00023    -0.02751     -0.02815
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019134521484375
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71125
epoch first part time 2.86102294921875e-06
replay_buffer._size: [160500]
collect time 0.0009310245513916016
inner_dict_sum {'sac_diff0': 0.00021266937255859375, 'sac_diff1': 0.006926536560058594, 'sac_diff2': 0.008052587509155273, 'sac_diff3': 0.009998559951782227, 'sac_diff4': 0.006944894790649414, 'sac_diff5': 0.03217720985412598, 'sac_diff6': 0.0003829002380371094, 'all': 0.06469535827636719}
diff5_list [0.006626129150390625, 0.006458759307861328, 0.006017923355102539, 0.006586313247680664, 0.00648808479309082]
time3 0
time4 0.06549406051635742
time5 0.06554150581359863
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9274)
policy weight change tensor(43.8880, grad_fn=<SumBackward0>)
time8 0.0018630027770996094
train_time 0.07670259475708008
eval time 0.14995598793029785
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:26,739 MainThread INFO: EPOCH:1063
2024-01-23 01:05:26,739 MainThread INFO: Time Consumed:0.2298884391784668s
2024-01-23 01:05:26,740 MainThread INFO: Total Frames:160350s
 11%|█         | 1064/10000 [06:54<37:20,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           24128.09910
Train_Epoch_Reward                28122.64768
Running_Training_Average_Rewards  29454.09598
Explore_Time                      0.00093
Train___Time                      0.07670
Eval____Time                      0.14996
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21304.08633
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.75119    2.02902     103.30424    97.51805
alpha_0                           0.58749      0.00008     0.58761      0.58737
Alpha_loss                        -3.56688     0.00508     -3.55849     -3.57417
Training/policy_loss              -8.16118     0.00369     -8.15507     -8.16565
Training/qf1_loss                 10445.56006  1667.19595  12015.72949  7450.02490
Training/qf2_loss                 21352.59258  2003.38583  23281.71484  17649.54297
Training/pf_norm                  0.32427      0.01959     0.34167      0.29501
Training/qf1_norm                 1071.48986   441.34829   1597.22668   364.85696
Training/qf2_norm                 3354.12012   67.33619    3441.13770   3248.76392
log_std/mean                      -0.13660     0.00006     -0.13649     -0.13666
log_probs/mean                    -2.70718     0.00856     -2.69393     -2.72090
mean/mean                         -0.03115     0.00025     -0.03081     -0.03150
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01923847198486328
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71125
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [160650]
collect time 0.0010128021240234375
inner_dict_sum {'sac_diff0': 0.00021338462829589844, 'sac_diff1': 0.007570743560791016, 'sac_diff2': 0.008740901947021484, 'sac_diff3': 0.010565042495727539, 'sac_diff4': 0.007359504699707031, 'sac_diff5': 0.033303022384643555, 'sac_diff6': 0.00041794776916503906, 'all': 0.06817054748535156}
diff5_list [0.006896018981933594, 0.0069141387939453125, 0.006889820098876953, 0.006267547607421875, 0.00633549690246582]
time3 0
time4 0.06897139549255371
time5 0.06902146339416504
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9274)
policy weight change tensor(43.8671, grad_fn=<SumBackward0>)
time8 0.0018839836120605469
train_time 0.08049225807189941
eval time 0.14378738403320312
epoch last part time 5.7220458984375e-06
2024-01-23 01:05:26,990 MainThread INFO: EPOCH:1064
2024-01-23 01:05:26,990 MainThread INFO: Time Consumed:0.2275545597076416s
2024-01-23 01:05:26,990 MainThread INFO: Total Frames:160500s
 11%|█         | 1065/10000 [06:54<37:17,  3.99it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23702.48694
Train_Epoch_Reward                9149.79312
Running_Training_Average_Rewards  28246.21024
Explore_Time                      0.00101
Train___Time                      0.08049
Eval____Time                      0.14379
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20971.49468
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.01978    1.70710     102.42122    97.54741
alpha_0                           0.58720      0.00008     0.58731      0.58708
Alpha_loss                        -3.56868     0.00157     -3.56680     -3.57085
Training/policy_loss              -8.23949     0.00398     -8.23218     -8.24435
Training/qf1_loss                 9322.15059   1138.03157  10840.22070  7317.75879
Training/qf2_loss                 19823.15586  1461.58044  21828.87500  17314.48828
Training/pf_norm                  0.14042      0.01680     0.16846      0.11998
Training/qf1_norm                 2283.70310   310.65616   2716.60840   1793.38684
Training/qf2_norm                 3288.10190   53.99754    3362.59448   3207.28320
log_std/mean                      -0.12655     0.00013     -0.12640     -0.12676
log_probs/mean                    -2.70428     0.00301     -2.70042     -2.70841
mean/mean                         -0.01780     0.00019     -0.01752     -0.01806
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018145084381103516
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71125
epoch first part time 3.337860107421875e-06
replay_buffer._size: [160800]
collect time 0.00096893310546875
inside mustsac before update, task 0, sumup 71125
inside mustsac after update, task 0, sumup 70940
inner_dict_sum {'sac_diff0': 0.0002219676971435547, 'sac_diff1': 0.00706028938293457, 'sac_diff2': 0.008420944213867188, 'sac_diff3': 0.01091146469116211, 'sac_diff4': 0.007983684539794922, 'sac_diff5': 0.05195760726928711, 'sac_diff6': 0.00040221214294433594, 'all': 0.08695816993713379}
diff5_list [0.010643720626831055, 0.009954452514648438, 0.011070728302001953, 0.010337591171264648, 0.009951114654541016]
time3 0.0008530616760253906
time4 0.0877995491027832
time5 0.08785581588745117
time7 0.009227752685546875
gen_weight_change tensor(-17.9426)
policy weight change tensor(44.0512, grad_fn=<SumBackward0>)
time8 0.0020377635955810547
train_time 0.11740350723266602
eval time 0.10365152359008789
epoch last part time 4.5299530029296875e-06
2024-01-23 01:05:27,236 MainThread INFO: EPOCH:1065
2024-01-23 01:05:27,236 MainThread INFO: Time Consumed:0.22422146797180176s
2024-01-23 01:05:27,237 MainThread INFO: Total Frames:160650s
 11%|█         | 1066/10000 [06:54<37:07,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           23239.40664
Train_Epoch_Reward                34957.47451
Running_Training_Average_Rewards  27491.02391
Explore_Time                      0.00096
Train___Time                      0.11740
Eval____Time                      0.10365
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20829.94145
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       102.17634    2.38310     105.46210    98.46645
alpha_0                           0.58690      0.00008     0.58702      0.58679
Alpha_loss                        -3.56956     0.00395     -3.56193     -3.57324
Training/policy_loss              -8.14720     0.42914     -7.66616     -8.87293
Training/qf1_loss                 10457.42637  842.72164   11535.69434  9562.87305
Training/qf2_loss                 21672.00000  1344.23131  23273.73438  19949.82227
Training/pf_norm                  0.27506      0.05698     0.32641      0.20280
Training/qf1_norm                 1166.61000   549.72615   1719.48499   303.07367
Training/qf2_norm                 3320.34092   104.00239   3497.57349   3214.58032
log_std/mean                      -0.13373     0.00288     -0.13026     -0.13720
log_probs/mean                    -2.69967     0.00751     -2.68534     -2.70533
mean/mean                         -0.03042     0.00592     -0.02273     -0.03953
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018907785415649414
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [160950]
collect time 0.0008318424224853516
inner_dict_sum {'sac_diff0': 0.00021600723266601562, 'sac_diff1': 0.0077059268951416016, 'sac_diff2': 0.009103775024414062, 'sac_diff3': 0.011490345001220703, 'sac_diff4': 0.00764012336730957, 'sac_diff5': 0.03444719314575195, 'sac_diff6': 0.0004057884216308594, 'all': 0.07100915908813477}
diff5_list [0.008112907409667969, 0.007259368896484375, 0.0066623687744140625, 0.006201267242431641, 0.006211280822753906]
time3 0
time4 0.07183527946472168
time5 0.07188725471496582
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9426)
policy weight change tensor(44.3986, grad_fn=<SumBackward0>)
time8 0.0019571781158447266
train_time 0.08336830139160156
eval time 0.14177966117858887
epoch last part time 5.4836273193359375e-06
2024-01-23 01:05:27,487 MainThread INFO: EPOCH:1066
2024-01-23 01:05:27,487 MainThread INFO: Time Consumed:0.22835135459899902s
2024-01-23 01:05:27,487 MainThread INFO: Total Frames:160800s
 11%|█         | 1067/10000 [06:55<37:10,  4.01it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22999.33294
Train_Epoch_Reward                28134.20985
Running_Training_Average_Rewards  27572.00994
Explore_Time                      0.00083
Train___Time                      0.08337
Eval____Time                      0.14178
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23028.90762
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.59140    2.34811     102.87306    96.08495
alpha_0                           0.58661      0.00008     0.58673      0.58649
Alpha_loss                        -3.57739     0.00553     -3.57067     -3.58460
Training/policy_loss              -8.28762     0.00904     -8.27664     -8.30315
Training/qf1_loss                 9983.27305   919.71644   11012.09961  8284.73340
Training/qf2_loss                 20866.22305  1467.64024  22467.57031  18115.96484
Training/pf_norm                  0.38933      0.02311     0.41879      0.35347
Training/qf1_norm                 1433.71720   492.29701   2360.17871   906.80914
Training/qf2_norm                 3418.75059   82.36396    3494.00195   3260.04126
log_std/mean                      -0.13723     0.00054     -0.13656     -0.13808
log_probs/mean                    -2.70809     0.00981     -2.69799     -2.72286
mean/mean                         -0.03280     0.00014     -0.03255     -0.03291
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018610477447509766
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [161100]
collect time 0.0009982585906982422
inner_dict_sum {'sac_diff0': 0.0002143383026123047, 'sac_diff1': 0.00725102424621582, 'sac_diff2': 0.008689165115356445, 'sac_diff3': 0.010589838027954102, 'sac_diff4': 0.007355451583862305, 'sac_diff5': 0.03263664245605469, 'sac_diff6': 0.0004038810729980469, 'all': 0.06714034080505371}
diff5_list [0.0066375732421875, 0.006554841995239258, 0.0064923763275146484, 0.006671905517578125, 0.006279945373535156]
time3 0
time4 0.06793379783630371
time5 0.06798338890075684
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9426)
policy weight change tensor(45.0550, grad_fn=<SumBackward0>)
time8 0.0020172595977783203
train_time 0.07944583892822266
eval time 0.15033316612243652
epoch last part time 5.7220458984375e-06
2024-01-23 01:05:27,742 MainThread INFO: EPOCH:1067
2024-01-23 01:05:27,742 MainThread INFO: Time Consumed:0.23317289352416992s
2024-01-23 01:05:27,743 MainThread INFO: Total Frames:160950s
 11%|█         | 1068/10000 [06:55<37:25,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22740.69462
Train_Epoch_Reward                13438.43584
Running_Training_Average_Rewards  24655.95262
Explore_Time                      0.00099
Train___Time                      0.07945
Eval____Time                      0.15033
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22948.05226
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       103.34299    2.78951     106.60583    98.61086
alpha_0                           0.58632      0.00008     0.58643      0.58620
Alpha_loss                        -3.57438     0.00412     -3.57068     -3.58176
Training/policy_loss              -8.60801     0.00215     -8.60608     -8.61218
Training/qf1_loss                 9704.16836   1176.65603  11460.40332  8107.81543
Training/qf2_loss                 21187.31211  1516.29060  22960.09375  18554.78516
Training/pf_norm                  0.27581      0.02744     0.30612      0.23400
Training/qf1_norm                 768.57591    301.57601   1204.66577   456.55087
Training/qf2_norm                 3577.03433   98.47617    3687.54541   3409.51929
log_std/mean                      -0.12892     0.00068     -0.12803     -0.12995
log_probs/mean                    -2.69619     0.00838     -2.68770     -2.71002
mean/mean                         -0.02900     0.00041     -0.02838     -0.02953
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018756628036499023
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 3.337860107421875e-06
replay_buffer._size: [161250]
collect time 0.0009951591491699219
inner_dict_sum {'sac_diff0': 0.00021123886108398438, 'sac_diff1': 0.0073986053466796875, 'sac_diff2': 0.008918046951293945, 'sac_diff3': 0.0107879638671875, 'sac_diff4': 0.007110595703125, 'sac_diff5': 0.03336000442504883, 'sac_diff6': 0.00042057037353515625, 'all': 0.0682070255279541}
diff5_list [0.007041215896606445, 0.0064656734466552734, 0.007300376892089844, 0.0064051151275634766, 0.006147623062133789]
time3 0
time4 0.06899738311767578
time5 0.06904721260070801
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9426)
policy weight change tensor(45.6238, grad_fn=<SumBackward0>)
time8 0.0019419193267822266
train_time 0.08044958114624023
eval time 0.1479341983795166
epoch last part time 5.4836273193359375e-06
2024-01-23 01:05:27,996 MainThread INFO: EPOCH:1068
2024-01-23 01:05:27,997 MainThread INFO: Time Consumed:0.2317190170288086s
2024-01-23 01:05:27,997 MainThread INFO: Total Frames:161100s
 11%|█         | 1069/10000 [06:55<37:33,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22512.52294
Train_Epoch_Reward                22240.06248
Running_Training_Average_Rewards  23425.41355
Explore_Time                      0.00099
Train___Time                      0.08045
Eval____Time                      0.14793
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23061.34469
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.52874    1.67479     102.28291    97.46875
alpha_0                           0.58603      0.00008     0.58614      0.58591
Alpha_loss                        -3.57596     0.00539     -3.56755     -3.58161
Training/policy_loss              -8.00127     0.00361     -7.99536     -8.00507
Training/qf1_loss                 9245.28301   1049.24790  11125.77930  8199.69629
Training/qf2_loss                 19939.54961  1430.29707  22343.83789  18098.62891
Training/pf_norm                  0.10639      0.03060     0.16178      0.08231
Training/qf1_norm                 3330.09941   391.00889   4025.90942   2865.50977
Training/qf2_norm                 3212.38550   56.21281    3270.08374   3107.79077
log_std/mean                      -0.13843     0.00036     -0.13787     -0.13891
log_probs/mean                    -2.69290     0.01063     -2.67840     -2.70598
mean/mean                         -0.03495     0.00013     -0.03480     -0.03518
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018820762634277344
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 2.86102294921875e-06
replay_buffer._size: [161400]
collect time 0.0010528564453125
inner_dict_sum {'sac_diff0': 0.00022125244140625, 'sac_diff1': 0.007309913635253906, 'sac_diff2': 0.008903264999389648, 'sac_diff3': 0.011260509490966797, 'sac_diff4': 0.007261037826538086, 'sac_diff5': 0.034032344818115234, 'sac_diff6': 0.0003972053527832031, 'all': 0.06938552856445312}
diff5_list [0.0068874359130859375, 0.006628513336181641, 0.0070378780364990234, 0.006863832473754883, 0.00661468505859375]
time3 0
time4 0.0701756477355957
time5 0.07022333145141602
time7 9.5367431640625e-07
gen_weight_change tensor(-17.9426)
policy weight change tensor(45.7319, grad_fn=<SumBackward0>)
time8 0.0020477771759033203
train_time 0.08189868927001953
eval time 0.14016079902648926
epoch last part time 4.76837158203125e-06
2024-01-23 01:05:28,245 MainThread INFO: EPOCH:1069
2024-01-23 01:05:28,245 MainThread INFO: Time Consumed:0.22552704811096191s
2024-01-23 01:05:28,245 MainThread INFO: Total Frames:161250s
 11%|█         | 1070/10000 [06:55<37:22,  3.98it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22340.59280
Train_Epoch_Reward                64116.79683
Running_Training_Average_Rewards  25091.44119
Explore_Time                      0.00105
Train___Time                      0.08190
Eval____Time                      0.14016
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23097.33737
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.38155    2.28376     102.72453    96.47324
alpha_0                           0.58573      0.00008     0.58585      0.58562
Alpha_loss                        -3.57754     0.00665     -3.56873     -3.58555
Training/policy_loss              -8.41129     0.00500     -8.40191     -8.41544
Training/qf1_loss                 10358.98301  1387.89352  12405.00586  8298.28320
Training/qf2_loss                 21203.23086  1869.83420  23790.51562  18300.28906
Training/pf_norm                  0.27170      0.01049     0.28465      0.25271
Training/qf1_norm                 778.21457    344.56544   1195.38611   289.49976
Training/qf2_norm                 3432.03047   78.48806    3509.86792   3297.22974
log_std/mean                      -0.14253     0.00015     -0.14228     -0.14268
log_probs/mean                    -2.68962     0.01235     -2.67439     -2.70711
mean/mean                         -0.03005     0.00008     -0.02997     -0.03021
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0186917781829834
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70940
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [161550]
collect time 0.0009748935699462891
inside mustsac before update, task 0, sumup 70940
inside mustsac after update, task 0, sumup 70656
inner_dict_sum {'sac_diff0': 0.00021600723266601562, 'sac_diff1': 0.00784754753112793, 'sac_diff2': 0.009023904800415039, 'sac_diff3': 0.01143503189086914, 'sac_diff4': 0.008089542388916016, 'sac_diff5': 0.053848981857299805, 'sac_diff6': 0.00041222572326660156, 'all': 0.09087324142456055}
diff5_list [0.01273798942565918, 0.010901451110839844, 0.010082006454467773, 0.010319232940673828, 0.00980830192565918]
time3 0.0008947849273681641
time4 0.09175848960876465
time5 0.0918116569519043
time7 0.009021759033203125
gen_weight_change tensor(-18.0054)
policy weight change tensor(45.7245, grad_fn=<SumBackward0>)
time8 0.0026111602783203125
train_time 0.12215995788574219
eval time 0.10942244529724121
epoch last part time 5.7220458984375e-06
2024-01-23 01:05:28,502 MainThread INFO: EPOCH:1070
2024-01-23 01:05:28,503 MainThread INFO: Time Consumed:0.23475003242492676s
2024-01-23 01:05:28,503 MainThread INFO: Total Frames:161400s
 11%|█         | 1071/10000 [06:56<37:45,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22208.98039
Train_Epoch_Reward                26122.67444
Running_Training_Average_Rewards  25218.21081
Explore_Time                      0.00097
Train___Time                      0.12216
Eval____Time                      0.10942
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22911.66501
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.05967    2.35408     103.15092    96.50294
alpha_0                           0.58544      0.00008     0.58556      0.58533
Alpha_loss                        -3.58625     0.00153     -3.58359     -3.58799
Training/policy_loss              -8.42896     0.29162     -7.94747     -8.85667
Training/qf1_loss                 9036.50215   1510.93333  10904.15820  6900.07666
Training/qf2_loss                 19929.00898  1945.87418  22115.44141  16785.88867
Training/pf_norm                  0.19518      0.05149     0.25252      0.10973
Training/qf1_norm                 1130.16317   668.40551   2061.57764   226.19588
Training/qf2_norm                 3407.75166   121.67528   3520.52612   3175.68994
log_std/mean                      -0.13651     0.00204     -0.13381     -0.13986
log_probs/mean                    -2.69966     0.00168     -2.69720     -2.70195
mean/mean                         -0.02120     0.00619     -0.01076     -0.02733
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018247127532958984
epoch last part time3 0.0027725696563720703
inside rlalgo, task 0, sumup 70656
epoch first part time 2.86102294921875e-06
replay_buffer._size: [161700]
collect time 0.00101470947265625
inner_dict_sum {'sac_diff0': 0.0002028942108154297, 'sac_diff1': 0.006728172302246094, 'sac_diff2': 0.008023262023925781, 'sac_diff3': 0.010559320449829102, 'sac_diff4': 0.006905317306518555, 'sac_diff5': 0.03184199333190918, 'sac_diff6': 0.00038170814514160156, 'all': 0.06464266777038574}
diff5_list [0.006705045700073242, 0.006204843521118164, 0.006539106369018555, 0.0062177181243896484, 0.00617527961730957]
time3 0
time4 0.06540942192077637
time5 0.06545448303222656
time7 9.5367431640625e-07
gen_weight_change tensor(-18.0054)
policy weight change tensor(45.4774, grad_fn=<SumBackward0>)
time8 0.0018558502197265625
train_time 0.07656526565551758
eval time 0.14711666107177734
epoch last part time 5.7220458984375e-06
2024-01-23 01:05:28,754 MainThread INFO: EPOCH:1071
2024-01-23 01:05:28,754 MainThread INFO: Time Consumed:0.22699189186096191s
2024-01-23 01:05:28,754 MainThread INFO: Total Frames:161550s
 11%|█         | 1072/10000 [06:56<37:31,  3.97it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22209.76449
Train_Epoch_Reward                19595.46709
Running_Training_Average_Rewards  25158.91455
Explore_Time                      0.00101
Train___Time                      0.07657
Eval____Time                      0.14712
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22231.48473
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.92214     1.67920     101.98894    97.34464
alpha_0                           0.58515      0.00008     0.58527      0.58503
Alpha_loss                        -3.58846     0.00565     -3.58131     -3.59732
Training/policy_loss              -7.85707     0.00371     -7.85394     -7.86418
Training/qf1_loss                 9420.80107   1357.83797  10944.15137  7560.45557
Training/qf2_loss                 20149.50508  1756.47372  22150.32422  17654.90234
Training/pf_norm                  0.22234      0.01981     0.23899      0.18579
Training/qf1_norm                 1725.64775   361.99331   2266.70142   1332.43274
Training/qf2_norm                 3181.09897   51.40469    3244.82471   3100.94653
log_std/mean                      -0.14659     0.00047     -0.14583     -0.14715
log_probs/mean                    -2.69758     0.00983     -2.68548     -2.71410
mean/mean                         -0.02535     0.00026     -0.02500     -0.02575
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01822042465209961
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70656
epoch first part time 2.86102294921875e-06
replay_buffer._size: [161850]
collect time 0.0009074211120605469
inner_dict_sum {'sac_diff0': 0.0002071857452392578, 'sac_diff1': 0.0071430206298828125, 'sac_diff2': 0.008413076400756836, 'sac_diff3': 0.011379003524780273, 'sac_diff4': 0.007349729537963867, 'sac_diff5': 0.03316617012023926, 'sac_diff6': 0.00039887428283691406, 'all': 0.06805706024169922}
diff5_list [0.0068781375885009766, 0.0065212249755859375, 0.006356716156005859, 0.0061490535736083984, 0.007261037826538086]
time3 0
time4 0.06884288787841797
time5 0.06888914108276367
time7 1.1920928955078125e-06
gen_weight_change tensor(-18.0054)
policy weight change tensor(45.0150, grad_fn=<SumBackward0>)
time8 0.0019490718841552734
train_time 0.0802910327911377
eval time 0.14650940895080566
epoch last part time 7.62939453125e-06
2024-01-23 01:05:29,006 MainThread INFO: EPOCH:1072
2024-01-23 01:05:29,006 MainThread INFO: Time Consumed:0.23010563850402832s
2024-01-23 01:05:29,006 MainThread INFO: Total Frames:161700s
 11%|█         | 1073/10000 [06:56<37:32,  3.96it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22235.15377
Train_Epoch_Reward                15331.87447
Running_Training_Average_Rewards  24470.95811
Explore_Time                      0.00090
Train___Time                      0.08029
Eval____Time                      0.14651
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21967.22355
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.77282    3.07687     104.55090    95.73915
alpha_0                           0.58486      0.00008     0.58498      0.58474
Alpha_loss                        -3.59072     0.00520     -3.58114     -3.59648
Training/policy_loss              -9.07198     0.00403     -9.06806     -9.07916
Training/qf1_loss                 9928.23652   1675.58367  11323.58789  6666.70801
Training/qf2_loss                 20631.67441  2134.18717  21909.78516  16383.54199
Training/pf_norm                  0.30063      0.02581     0.33550      0.25745
Training/qf1_norm                 2656.03601   614.03660   3370.86938   1610.76770
Training/qf2_norm                 3721.05356   114.82618   3860.50195   3532.32764
log_std/mean                      -0.14188     0.00068     -0.14088     -0.14278
log_probs/mean                    -2.69556     0.00815     -2.68018     -2.70382
mean/mean                         -0.03267     0.00024     -0.03228     -0.03292
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018532276153564453
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70656
epoch first part time 2.86102294921875e-06
replay_buffer._size: [162000]
collect time 0.0010688304901123047
inner_dict_sum {'sac_diff0': 0.00023436546325683594, 'sac_diff1': 0.00796818733215332, 'sac_diff2': 0.009330511093139648, 'sac_diff3': 0.01137852668762207, 'sac_diff4': 0.007475376129150391, 'sac_diff5': 0.03458070755004883, 'sac_diff6': 0.00043201446533203125, 'all': 0.07139968872070312}
diff5_list [0.008046150207519531, 0.006295442581176758, 0.0072095394134521484, 0.006543397903442383, 0.006486177444458008]
time3 0
time4 0.07227134704589844
time5 0.07232666015625
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0054)
policy weight change tensor(44.8125, grad_fn=<SumBackward0>)
time8 0.001926422119140625
train_time 0.08441972732543945
eval time 0.16084074974060059
epoch last part time 7.152557373046875e-06
2024-01-23 01:05:29,277 MainThread INFO: EPOCH:1073
2024-01-23 01:05:29,278 MainThread INFO: Time Consumed:0.2487180233001709s
2024-01-23 01:05:29,278 MainThread INFO: Total Frames:161850s
 11%|█         | 1074/10000 [06:56<38:25,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22296.59788
Train_Epoch_Reward                42441.52913
Running_Training_Average_Rewards  25388.35631
Explore_Time                      0.00106
Train___Time                      0.08442
Eval____Time                      0.16084
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21918.52745
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       99.72138     4.40421     105.97227    92.70599
alpha_0                           0.58457      0.00008     0.58469      0.58445
Alpha_loss                        -3.59604     0.00714     -3.58301     -3.60376
Training/policy_loss              -9.13384     0.00349     -9.12938     -9.13816
Training/qf1_loss                 9018.39727   1530.79361  10710.33789  7098.23975
Training/qf2_loss                 19624.21172  2210.55470  22225.19141  16296.59961
Training/pf_norm                  0.22119      0.02082     0.24741      0.19333
Training/qf1_norm                 1572.85883   842.29715   2872.76880   296.07736
Training/qf2_norm                 3709.49160   165.34865   3940.45972   3442.85962
log_std/mean                      -0.14356     0.00024     -0.14326     -0.14393
log_probs/mean                    -2.69927     0.01256     -2.67747     -2.71489
mean/mean                         -0.02001     0.00009     -0.01992     -0.02016
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01963639259338379
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70656
epoch first part time 3.337860107421875e-06
replay_buffer._size: [162150]
collect time 0.0009713172912597656
inner_dict_sum {'sac_diff0': 0.0002117156982421875, 'sac_diff1': 0.007735729217529297, 'sac_diff2': 0.008675575256347656, 'sac_diff3': 0.011028528213500977, 'sac_diff4': 0.00746464729309082, 'sac_diff5': 0.03326129913330078, 'sac_diff6': 0.0003943443298339844, 'all': 0.0687718391418457}
diff5_list [0.008404254913330078, 0.006394624710083008, 0.005933046340942383, 0.0062694549560546875, 0.006259918212890625]
time3 0
time4 0.06954097747802734
time5 0.06959939002990723
time7 7.152557373046875e-07
gen_weight_change tensor(-18.0054)
policy weight change tensor(44.7452, grad_fn=<SumBackward0>)
time8 0.0018494129180908203
train_time 0.08113574981689453
eval time 0.1531686782836914
epoch last part time 7.867813110351562e-06
2024-01-23 01:05:29,539 MainThread INFO: EPOCH:1074
2024-01-23 01:05:29,539 MainThread INFO: Time Consumed:0.2377161979675293s
2024-01-23 01:05:29,539 MainThread INFO: Total Frames:162000s
 11%|█         | 1075/10000 [06:57<38:29,  3.86it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22394.72721
Train_Epoch_Reward                14213.47379
Running_Training_Average_Rewards  25468.72445
Explore_Time                      0.00097
Train___Time                      0.08114
Eval____Time                      0.15317
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21952.78801
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.19263    4.50148     105.23862    93.80699
alpha_0                           0.58428      0.00008     0.58439      0.58416
Alpha_loss                        -3.60117     0.00502     -3.59497     -3.60984
Training/policy_loss              -8.59000     0.00129     -8.58854     -8.59157
Training/qf1_loss                 8570.74209   1626.82703  10190.11133  6136.01318
Training/qf2_loss                 19244.13008  2539.73381  21871.52148  15491.42383
Training/pf_norm                  0.19746      0.03697     0.26849      0.16509
Training/qf1_norm                 1483.79404   885.68101   2445.70752   223.60051
Training/qf2_norm                 3432.47983   155.85463   3604.95142   3213.47876
log_std/mean                      -0.14137     0.00033     -0.14085     -0.14177
log_probs/mean                    -2.70261     0.00956     -2.69106     -2.71998
mean/mean                         -0.01821     0.00002     -0.01817     -0.01824
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018058300018310547
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70656
epoch first part time 2.86102294921875e-06
replay_buffer._size: [162300]
collect time 0.0008568763732910156
inside mustsac before update, task 0, sumup 70656
inside mustsac after update, task 0, sumup 70247
inner_dict_sum {'sac_diff0': 0.00022339820861816406, 'sac_diff1': 0.0074291229248046875, 'sac_diff2': 0.008585214614868164, 'sac_diff3': 0.010890722274780273, 'sac_diff4': 0.007546186447143555, 'sac_diff5': 0.05265092849731445, 'sac_diff6': 0.0004220008850097656, 'all': 0.08774757385253906}
diff5_list [0.01074361801147461, 0.010242938995361328, 0.010307788848876953, 0.010764598846435547, 0.010591983795166016]
time3 0.0009131431579589844
time4 0.08870601654052734
time5 0.0887610912322998
time7 0.009119272232055664
gen_weight_change tensor(-17.9082)
policy weight change tensor(44.6547, grad_fn=<SumBackward0>)
time8 0.0018274784088134766
train_time 0.11835312843322754
eval time 0.11283516883850098
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:29,795 MainThread INFO: EPOCH:1075
2024-01-23 01:05:29,795 MainThread INFO: Time Consumed:0.23453640937805176s
2024-01-23 01:05:29,796 MainThread INFO: Total Frames:162150s
 11%|█         | 1076/10000 [06:57<38:23,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22521.91216
Train_Epoch_Reward                48585.07826
Running_Training_Average_Rewards  26681.66542
Explore_Time                      0.00085
Train___Time                      0.11835
Eval____Time                      0.11284
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22101.79096
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.62157    4.30050     109.36241    96.34669
alpha_0                           0.58399      0.00008     0.58410      0.58387
Alpha_loss                        -3.60439     0.00333     -3.59933     -3.60833
Training/policy_loss              -8.27143     0.44768     -7.55934     -8.91407
Training/qf1_loss                 9733.41123   1792.95114  13108.97168  7775.60791
Training/qf2_loss                 20823.86797  2792.44724  25992.56055  17571.50586
Training/pf_norm                  0.21390      0.07843     0.31812      0.09266
Training/qf1_norm                 1001.60646   739.22929   2412.08130   253.35799
Training/qf2_norm                 3386.74468   232.08975   3648.87793   3081.08667
log_std/mean                      -0.14411     0.00609     -0.13584     -0.15092
log_probs/mean                    -2.70238     0.00683     -2.69174     -2.70977
mean/mean                         -0.02120     0.00600     -0.01146     -0.02839
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018323659896850586
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70247
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [162450]
collect time 0.0008819103240966797
inner_dict_sum {'sac_diff0': 0.00022029876708984375, 'sac_diff1': 0.007579326629638672, 'sac_diff2': 0.008770227432250977, 'sac_diff3': 0.011060476303100586, 'sac_diff4': 0.007327079772949219, 'sac_diff5': 0.034357309341430664, 'sac_diff6': 0.00041604042053222656, 'all': 0.06973075866699219}
diff5_list [0.007488250732421875, 0.0064470767974853516, 0.006994009017944336, 0.006923198699951172, 0.00650477409362793]
time3 0
time4 0.07057881355285645
time5 0.0706334114074707
time7 1.1920928955078125e-06
gen_weight_change tensor(-17.9082)
policy weight change tensor(44.4082, grad_fn=<SumBackward0>)
time8 0.0020079612731933594
train_time 0.08226299285888672
eval time 0.1510765552520752
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:30,054 MainThread INFO: EPOCH:1076
2024-01-23 01:05:30,054 MainThread INFO: Time Consumed:0.2366933822631836s
2024-01-23 01:05:30,054 MainThread INFO: Total Frames:162300s
 11%|█         | 1077/10000 [06:57<38:28,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22157.60199
Train_Epoch_Reward                12405.49795
Running_Training_Average_Rewards  26293.49429
Explore_Time                      0.00088
Train___Time                      0.08226
Eval____Time                      0.15108
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             19385.80589
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       102.88845    2.43804     106.11824    99.99223
alpha_0                           0.58370      0.00008     0.58381      0.58358
Alpha_loss                        -3.60446     0.00635     -3.59632     -3.61298
Training/policy_loss              -6.84841     0.00408     -6.84163     -6.85379
Training/qf1_loss                 10424.06152  738.44365   11135.87109  9237.95508
Training/qf2_loss                 21795.47539  1204.29293  23007.34570  19968.44531
Training/pf_norm                  0.21438      0.02027     0.23617      0.18401
Training/qf1_norm                 681.38410    419.67194   1299.04822   235.05348
Training/qf2_norm                 2810.79409   66.19953    2899.57568   2733.93652
log_std/mean                      -0.13718     0.00056     -0.13633     -0.13786
log_probs/mean                    -2.69631     0.01158     -2.68242     -2.71059
mean/mean                         -0.03270     0.00035     -0.03212     -0.03306
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019521236419677734
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70247
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [162600]
collect time 0.0010607242584228516
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.0071337223052978516, 'sac_diff2': 0.00842738151550293, 'sac_diff3': 0.01064920425415039, 'sac_diff4': 0.007315397262573242, 'sac_diff5': 0.03176426887512207, 'sac_diff6': 0.00038313865661621094, 'all': 0.06588554382324219}
diff5_list [0.007012128829956055, 0.006325721740722656, 0.00641632080078125, 0.006036281585693359, 0.00597381591796875]
time3 0
time4 0.0666501522064209
time5 0.0666956901550293
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9082)
policy weight change tensor(44.3212, grad_fn=<SumBackward0>)
time8 0.0018475055694580078
train_time 0.07810020446777344
eval time 0.15095305442810059
epoch last part time 8.106231689453125e-06
2024-01-23 01:05:30,310 MainThread INFO: EPOCH:1077
2024-01-23 01:05:30,311 MainThread INFO: Time Consumed:0.2325456142425537s
2024-01-23 01:05:30,311 MainThread INFO: Total Frames:162450s
 11%|█         | 1078/10000 [06:57<38:25,  3.87it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21839.85979
Train_Epoch_Reward                35356.89527
Running_Training_Average_Rewards  26592.26091
Explore_Time                      0.00105
Train___Time                      0.07810
Eval____Time                      0.15095
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             19770.63025
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       102.10064    2.34071     105.23037    98.07493
alpha_0                           0.58341      0.00008     0.58352      0.58329
Alpha_loss                        -3.61106     0.00485     -3.60336     -3.61779
Training/policy_loss              -8.55678     0.00574     -8.54761     -8.56421
Training/qf1_loss                 9294.94541   1601.23506  11671.29199  7514.69043
Training/qf2_loss                 20459.50664  2103.09700  23707.77734  18176.45703
Training/pf_norm                  0.16112      0.01502     0.18962      0.14706
Training/qf1_norm                 2068.98647   554.84921   2980.22461   1290.47375
Training/qf2_norm                 3540.87539   85.14255    3659.07764   3397.51611
log_std/mean                      -0.13357     0.00008     -0.13348     -0.13371
log_probs/mean                    -2.70237     0.00764     -2.69056     -2.71362
mean/mean                         -0.01599     0.00025     -0.01565     -0.01636
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020841360092163086
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70247
epoch first part time 2.86102294921875e-06
replay_buffer._size: [162750]
collect time 0.0010323524475097656
inner_dict_sum {'sac_diff0': 0.00023508071899414062, 'sac_diff1': 0.0074520111083984375, 'sac_diff2': 0.008563041687011719, 'sac_diff3': 0.011029958724975586, 'sac_diff4': 0.007101774215698242, 'sac_diff5': 0.03288626670837402, 'sac_diff6': 0.0003979206085205078, 'all': 0.06766605377197266}
diff5_list [0.0065631866455078125, 0.006468534469604492, 0.007517337799072266, 0.00626063346862793, 0.0060765743255615234]
time3 0
time4 0.06848669052124023
time5 0.06853961944580078
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9082)
policy weight change tensor(44.3476, grad_fn=<SumBackward0>)
time8 0.001898050308227539
train_time 0.08017349243164062
eval time 0.14946889877319336
epoch last part time 6.198883056640625e-06
2024-01-23 01:05:30,568 MainThread INFO: EPOCH:1078
2024-01-23 01:05:30,569 MainThread INFO: Time Consumed:0.23307347297668457s
2024-01-23 01:05:30,569 MainThread INFO: Total Frames:162600s
 11%|█         | 1079/10000 [06:58<38:20,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21562.17218
Train_Epoch_Reward                5734.90317
Running_Training_Average_Rewards  25428.81937
Explore_Time                      0.00103
Train___Time                      0.08017
Eval____Time                      0.14947
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20284.46858
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       103.10626    3.55230     107.94145    98.76713
alpha_0                           0.58311      0.00008     0.58323      0.58300
Alpha_loss                        -3.61319     0.00553     -3.60667     -3.62333
Training/policy_loss              -8.60182     0.00548     -8.59376     -8.60906
Training/qf1_loss                 9493.73398   1093.80809  11123.00586  7730.60938
Training/qf2_loss                 20796.81328  1793.32890  23558.33984  18009.07227
Training/pf_norm                  0.15268      0.03644     0.20754      0.09266
Training/qf1_norm                 1249.70135   717.51831   2233.76025   309.30945
Training/qf2_norm                 3572.79004   122.37467   3740.46362   3422.18579
log_std/mean                      -0.12850     0.00003     -0.12846     -0.12853
log_probs/mean                    -2.70011     0.01009     -2.68804     -2.71769
mean/mean                         -0.03258     0.00026     -0.03221     -0.03295
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01934528350830078
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70247
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [162900]
collect time 0.0010068416595458984
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.007483720779418945, 'sac_diff2': 0.008672952651977539, 'sac_diff3': 0.010911941528320312, 'sac_diff4': 0.007327079772949219, 'sac_diff5': 0.03372383117675781, 'sac_diff6': 0.0003905296325683594, 'all': 0.06872248649597168}
diff5_list [0.006920337677001953, 0.006992816925048828, 0.007285118103027344, 0.006279706954956055, 0.006245851516723633]
time3 0
time4 0.06952452659606934
time5 0.06957411766052246
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9082)
policy weight change tensor(44.4486, grad_fn=<SumBackward0>)
time8 0.0018906593322753906
train_time 0.08081936836242676
eval time 0.14700865745544434
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:30,823 MainThread INFO: EPOCH:1079
2024-01-23 01:05:30,823 MainThread INFO: Time Consumed:0.23125910758972168s
2024-01-23 01:05:30,823 MainThread INFO: Total Frames:162750s
 11%|█         | 1080/10000 [06:58<38:11,  3.89it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21332.10317
Train_Epoch_Reward                25606.27995
Running_Training_Average_Rewards  24186.83002
Explore_Time                      0.00100
Train___Time                      0.08082
Eval____Time                      0.14701
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             20796.64726
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       102.53233    2.74701     105.54863    97.87772
alpha_0                           0.58282      0.00008     0.58294      0.58271
Alpha_loss                        -3.61959     0.00407     -3.61516     -3.62645
Training/policy_loss              -8.22013     0.00344     -8.21546     -8.22588
Training/qf1_loss                 9354.71973   1256.50459  11654.84668  8113.54688
Training/qf2_loss                 20662.31836  1649.50884  23244.20508  18778.34375
Training/pf_norm                  0.22716      0.02522     0.25544      0.19081
Training/qf1_norm                 604.41732    420.57404   1342.20178   264.79749
Training/qf2_norm                 3347.88887   91.18608    3451.98462   3193.42700
log_std/mean                      -0.14023     0.00004     -0.14017     -0.14028
log_probs/mean                    -2.70580     0.00863     -2.69623     -2.72099
mean/mean                         -0.02994     0.00004     -0.02990     -0.03001
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.019629955291748047
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70247
epoch first part time 3.5762786865234375e-06
replay_buffer._size: [163050]
collect time 0.000904083251953125
inside mustsac before update, task 0, sumup 70247
inside mustsac after update, task 0, sumup 71236
inner_dict_sum {'sac_diff0': 0.00021028518676757812, 'sac_diff1': 0.007213115692138672, 'sac_diff2': 0.008260726928710938, 'sac_diff3': 0.010920524597167969, 'sac_diff4': 0.007424116134643555, 'sac_diff5': 0.05276632308959961, 'sac_diff6': 0.0004169940948486328, 'all': 0.08721208572387695}
diff5_list [0.011358022689819336, 0.01049184799194336, 0.010069131851196289, 0.010545492172241211, 0.010301828384399414]
time3 0.0008609294891357422
time4 0.08814191818237305
time5 0.08820319175720215
time7 0.009693384170532227
gen_weight_change tensor(-17.9410)
policy weight change tensor(44.3450, grad_fn=<SumBackward0>)
time8 0.0028350353240966797
train_time 0.11930966377258301
eval time 0.11108875274658203
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:31,080 MainThread INFO: EPOCH:1080
2024-01-23 01:05:31,080 MainThread INFO: Time Consumed:0.23368453979492188s
2024-01-23 01:05:31,080 MainThread INFO: Total Frames:162900s
 11%|█         | 1081/10000 [06:58<38:16,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21170.70922
Train_Epoch_Reward                11601.01821
Running_Training_Average_Rewards  23420.96935
Explore_Time                      0.00090
Train___Time                      0.11931
Eval____Time                      0.11109
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21297.72554
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.43247    2.69151     106.27876    98.37769
alpha_0                           0.58253      0.00008     0.58265      0.58242
Alpha_loss                        -3.61736     0.00447     -3.61047     -3.62230
Training/policy_loss              -8.28206     0.27895     -7.87902     -8.67009
Training/qf1_loss                 9668.93135   1601.01497  12434.90332  7685.01709
Training/qf2_loss                 20734.41445  2178.87645  24660.27148  18038.96484
Training/pf_norm                  0.23030      0.10861     0.35417      0.08643
Training/qf1_norm                 777.96753    196.48935   1111.49280   504.53986
Training/qf2_norm                 3370.25220   143.58610   3594.82666   3147.28613
log_std/mean                      -0.13596     0.00585     -0.12693     -0.14383
log_probs/mean                    -2.69547     0.00774     -2.68273     -2.70339
mean/mean                         -0.02125     0.00558     -0.01449     -0.02809
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018635988235473633
epoch last part time3 0.002825498580932617
inside rlalgo, task 0, sumup 71236
epoch first part time 2.86102294921875e-06
replay_buffer._size: [163200]
collect time 0.0009558200836181641
inner_dict_sum {'sac_diff0': 0.00021457672119140625, 'sac_diff1': 0.0071828365325927734, 'sac_diff2': 0.008426427841186523, 'sac_diff3': 0.01087808609008789, 'sac_diff4': 0.007448673248291016, 'sac_diff5': 0.03279232978820801, 'sac_diff6': 0.0004062652587890625, 'all': 0.06734919548034668}
diff5_list [0.006651639938354492, 0.006349086761474609, 0.007104158401489258, 0.006595611572265625, 0.0060918331146240234]
time3 0
time4 0.06817841529846191
time5 0.06823134422302246
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9410)
policy weight change tensor(44.5124, grad_fn=<SumBackward0>)
time8 0.0018033981323242188
train_time 0.07932209968566895
eval time 0.14869236946105957
epoch last part time 7.3909759521484375e-06
2024-01-23 01:05:31,336 MainThread INFO: EPOCH:1081
2024-01-23 01:05:31,336 MainThread INFO: Time Consumed:0.23136043548583984s
2024-01-23 01:05:31,336 MainThread INFO: Total Frames:163050s
 11%|█         | 1082/10000 [06:58<38:08,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21113.28268
Train_Epoch_Reward                21907.44090
Running_Training_Average_Rewards  23489.19368
Explore_Time                      0.00095
Train___Time                      0.07932
Eval____Time                      0.14869
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             21657.21931
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.86036    1.61678     103.87788    99.28005
alpha_0                           0.58224      0.00008     0.58236      0.58213
Alpha_loss                        -3.62181     0.00378     -3.61526     -3.62545
Training/policy_loss              -8.40862     0.00278     -8.40468     -8.41275
Training/qf1_loss                 8830.31182   751.55405   9824.68848   8140.21094
Training/qf2_loss                 19701.24766  1069.20076  21195.14453  18710.48047
Training/pf_norm                  0.32493      0.02239     0.36230      0.30123
Training/qf1_norm                 862.47739    301.42237   1160.47217   333.25980
Training/qf2_norm                 3432.37114   60.83621    3544.17065   3371.53687
log_std/mean                      -0.13617     0.00008     -0.13608     -0.13630
log_probs/mean                    -2.69753     0.00815     -2.68295     -2.70550
mean/mean                         -0.03878     0.00008     -0.03864     -0.03886
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01999831199645996
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71236
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [163350]
collect time 0.0009360313415527344
inner_dict_sum {'sac_diff0': 0.00020647048950195312, 'sac_diff1': 0.007190704345703125, 'sac_diff2': 0.008476018905639648, 'sac_diff3': 0.010739326477050781, 'sac_diff4': 0.006936073303222656, 'sac_diff5': 0.03262066841125488, 'sac_diff6': 0.0003840923309326172, 'all': 0.06655335426330566}
diff5_list [0.006989717483520508, 0.006402015686035156, 0.006289958953857422, 0.006211042404174805, 0.006727933883666992]
time3 0
time4 0.06730484962463379
time5 0.06734991073608398
time7 9.5367431640625e-07
gen_weight_change tensor(-17.9410)
policy weight change tensor(44.5658, grad_fn=<SumBackward0>)
time8 0.0018229484558105469
train_time 0.07872200012207031
eval time 0.15114712715148926
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:31,593 MainThread INFO: EPOCH:1082
2024-01-23 01:05:31,593 MainThread INFO: Time Consumed:0.2332141399383545s
2024-01-23 01:05:31,594 MainThread INFO: Total Frames:163200s
 11%|█         | 1083/10000 [06:59<38:07,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21119.98601
Train_Epoch_Reward                24527.20489
Running_Training_Average_Rewards  22014.66449
Explore_Time                      0.00093
Train___Time                      0.07872
Eval____Time                      0.15115
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22034.25682
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       103.00846    2.74387     106.46849    98.65207
alpha_0                           0.58195      0.00008     0.58207      0.58184
Alpha_loss                        -3.62500     0.00883     -3.61214     -3.63480
Training/policy_loss              -8.84102     0.00778     -8.82868     -8.84904
Training/qf1_loss                 10704.54639  2131.42771  14084.59961  7964.18604
Training/qf2_loss                 22121.27187  2702.65015  26070.07227  18288.70703
Training/pf_norm                  0.13130      0.01043     0.14233      0.11608
Training/qf1_norm                 1441.33806   657.69280   2556.64673   678.49261
Training/qf2_norm                 3519.84131   94.47194    3639.07129   3369.45703
log_std/mean                      -0.12656     0.00005     -0.12647     -0.12661
log_probs/mean                    -2.69726     0.01631     -2.67596     -2.71641
mean/mean                         -0.03867     0.00023     -0.03831     -0.03893
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01909017562866211
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71236
epoch first part time 3.337860107421875e-06
replay_buffer._size: [163500]
collect time 0.0009658336639404297
inner_dict_sum {'sac_diff0': 0.00022935867309570312, 'sac_diff1': 0.0073850154876708984, 'sac_diff2': 0.008782386779785156, 'sac_diff3': 0.011476278305053711, 'sac_diff4': 0.007437467575073242, 'sac_diff5': 0.033623456954956055, 'sac_diff6': 0.0004131793975830078, 'all': 0.06934714317321777}
diff5_list [0.0069658756256103516, 0.006615400314331055, 0.006482362747192383, 0.006838321685791016, 0.00672149658203125]
time3 0
time4 0.07019495964050293
time5 0.07024765014648438
time7 9.5367431640625e-07
gen_weight_change tensor(-17.9410)
policy weight change tensor(44.6318, grad_fn=<SumBackward0>)
time8 0.0020055770874023438
train_time 0.08199453353881836
eval time 0.14502263069152832
epoch last part time 6.198883056640625e-06
2024-01-23 01:05:31,847 MainThread INFO: EPOCH:1083
2024-01-23 01:05:31,847 MainThread INFO: Time Consumed:0.23041224479675293s
2024-01-23 01:05:31,847 MainThread INFO: Total Frames:163350s
 11%|█         | 1084/10000 [06:59<37:56,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21181.00380
Train_Epoch_Reward                29755.55799
Running_Training_Average_Rewards  22463.87072
Explore_Time                      0.00096
Train___Time                      0.08199
Eval____Time                      0.14502
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22528.70540
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       104.45766    1.68347     106.33495    102.41012
alpha_0                           0.58166      0.00008     0.58178      0.58155
Alpha_loss                        -3.62930     0.00657     -3.62073     -3.63770
Training/policy_loss              -7.77347     0.00323     -7.76919     -7.77885
Training/qf1_loss                 11135.15137  1419.52960  13296.07520  9418.35352
Training/qf2_loss                 22857.01211  1744.46510  25337.93750  20695.72266
Training/pf_norm                  0.18248      0.01844     0.20773      0.15715
Training/qf1_norm                 1587.72500   371.56389   1983.31958   1107.39282
Training/qf2_norm                 3321.85313   54.47866    3380.77222   3251.23584
log_std/mean                      -0.12050     0.00022     -0.12024     -0.12084
log_probs/mean                    -2.69905     0.01223     -2.68322     -2.71578
mean/mean                         -0.01421     0.00042     -0.01357     -0.01475
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018300533294677734
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71236
epoch first part time 2.6226043701171875e-06
replay_buffer._size: [163650]
collect time 0.0010290145874023438
inner_dict_sum {'sac_diff0': 0.00021719932556152344, 'sac_diff1': 0.007766008377075195, 'sac_diff2': 0.008846044540405273, 'sac_diff3': 0.01089930534362793, 'sac_diff4': 0.0071773529052734375, 'sac_diff5': 0.03307294845581055, 'sac_diff6': 0.00040340423583984375, 'all': 0.06838226318359375}
diff5_list [0.0070285797119140625, 0.006913900375366211, 0.0066509246826171875, 0.006253719329833984, 0.0062258243560791016]
time3 0
time4 0.06923985481262207
time5 0.06929397583007812
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9410)
policy weight change tensor(44.8046, grad_fn=<SumBackward0>)
time8 0.001905679702758789
train_time 0.08068346977233887
eval time 0.15067481994628906
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:32,103 MainThread INFO: EPOCH:1084
2024-01-23 01:05:32,104 MainThread INFO: Time Consumed:0.23480892181396484s
2024-01-23 01:05:32,104 MainThread INFO: Total Frames:163500s
 11%|█         | 1085/10000 [06:59<38:06,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21294.89201
Train_Epoch_Reward                23863.83553
Running_Training_Average_Rewards  22918.81093
Explore_Time                      0.00102
Train___Time                      0.08068
Eval____Time                      0.15067
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23091.67008
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       102.87607    2.78794     108.01233    100.10691
alpha_0                           0.58137      0.00008     0.58149      0.58126
Alpha_loss                        -3.64156     0.00434     -3.63821     -3.64996
Training/policy_loss              -8.97055     0.00623     -8.96288     -8.97697
Training/qf1_loss                 10599.42793  1944.84228  13149.74023  8445.64551
Training/qf2_loss                 21993.65859  2347.74144  25680.91602  19418.51172
Training/pf_norm                  0.17644      0.02163     0.20582      0.15552
Training/qf1_norm                 715.39017    565.55797   1801.24719   266.09113
Training/qf2_norm                 3692.36411   97.94322    3872.34058   3593.76221
log_std/mean                      -0.13577     0.00023     -0.13545     -0.13609
log_probs/mean                    -2.71549     0.00671     -2.70932     -2.72851
mean/mean                         -0.02571     0.00020     -0.02546     -0.02602
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.02051687240600586
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71236
epoch first part time 4.0531158447265625e-06
replay_buffer._size: [163800]
collect time 0.0009548664093017578
inside mustsac before update, task 0, sumup 71236
inside mustsac after update, task 0, sumup 70143
inner_dict_sum {'sac_diff0': 0.000225067138671875, 'sac_diff1': 0.0076122283935546875, 'sac_diff2': 0.008823394775390625, 'sac_diff3': 0.011241912841796875, 'sac_diff4': 0.0076639652252197266, 'sac_diff5': 0.0537419319152832, 'sac_diff6': 0.00042939186096191406, 'all': 0.0897378921508789}
diff5_list [0.010784387588500977, 0.010219812393188477, 0.009968042373657227, 0.010841608047485352, 0.011928081512451172]
time3 0.0009012222290039062
time4 0.09069466590881348
time5 0.09074950218200684
time7 0.0091705322265625
gen_weight_change tensor(-18.2873)
policy weight change tensor(44.8517, grad_fn=<SumBackward0>)
time8 0.0018477439880371094
train_time 0.12062311172485352
eval time 0.10682535171508789
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:32,359 MainThread INFO: EPOCH:1085
2024-01-23 01:05:32,359 MainThread INFO: Time Consumed:0.2308974266052246s
2024-01-23 01:05:32,359 MainThread INFO: Total Frames:163650s
 11%|█         | 1086/10000 [06:59<38:03,  3.90it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           21440.09227
Train_Epoch_Reward                17221.16140
Running_Training_Average_Rewards  22506.97739
Explore_Time                      0.00095
Train___Time                      0.12062
Eval____Time                      0.10683
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             23553.79361
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       100.82123    2.20136     104.52580    99.00507
alpha_0                           0.58108      0.00008     0.58120      0.58097
Alpha_loss                        -3.64280     0.00529     -3.63458     -3.64895
Training/policy_loss              -8.47890     0.55315     -7.47738     -8.95388
Training/qf1_loss                 9918.70156   2151.81084  13522.02344  7493.29004
Training/qf2_loss                 20693.91211  2384.45613  24737.99023  17953.33008
Training/pf_norm                  0.20822      0.07073     0.32103      0.13512
Training/qf1_norm                 1830.56049   1298.48024  4124.13086   206.99583
Training/qf2_norm                 3385.48950   251.02576   3647.55469   2947.16479
log_std/mean                      -0.13625     0.00615     -0.12867     -0.14623
log_probs/mean                    -2.71161     0.01031     -2.69524     -2.72541
mean/mean                         -0.01371     0.00673     -0.00584     -0.02471
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.020584821701049805
epoch last part time3 9.5367431640625e-07
inside rlalgo, task 0, sumup 70143
epoch first part time 3.337860107421875e-06
replay_buffer._size: [163950]
collect time 0.0009195804595947266
inner_dict_sum {'sac_diff0': 0.00022077560424804688, 'sac_diff1': 0.007354021072387695, 'sac_diff2': 0.009426355361938477, 'sac_diff3': 0.011605501174926758, 'sac_diff4': 0.007594585418701172, 'sac_diff5': 0.03388047218322754, 'sac_diff6': 0.00042939186096191406, 'all': 0.0705111026763916}
diff5_list [0.006582975387573242, 0.00684046745300293, 0.007453441619873047, 0.0065822601318359375, 0.006421327590942383]
time3 0
time4 0.07134795188903809
time5 0.07140159606933594
time7 9.5367431640625e-07
gen_weight_change tensor(-18.2873)
policy weight change tensor(45.2259, grad_fn=<SumBackward0>)
time8 0.002003192901611328
train_time 0.0833425521850586
eval time 0.15053772926330566
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:32,621 MainThread INFO: EPOCH:1086
2024-01-23 01:05:32,621 MainThread INFO: Time Consumed:0.23723506927490234s
2024-01-23 01:05:32,621 MainThread INFO: Total Frames:163800s
 11%|█         | 1087/10000 [07:00<38:16,  3.88it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           22915.23383
Train_Epoch_Reward                22556.36132
Running_Training_Average_Rewards  22859.88401
Explore_Time                      0.00091
Train___Time                      0.08334
Eval____Time                      0.15054
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             34137.22141
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.03300    2.54036     105.41292    98.12325
alpha_0                           0.58079      0.00008     0.58091      0.58068
Alpha_loss                        -3.63915     0.00298     -3.63520     -3.64269
Training/policy_loss              -8.66408     0.00332     -8.65860     -8.66827
Training/qf1_loss                 8612.20068   911.14164   10187.70117  7526.46338
Training/qf2_loss                 19587.00234  1473.43713  22147.81445  17846.28320
Training/pf_norm                  0.25416      0.02441     0.28298      0.21128
Training/qf1_norm                 622.73663    225.32792   904.03394    253.56589
Training/qf2_norm                 3494.74888   88.63728    3646.99097   3396.88379
log_std/mean                      -0.14140     0.00026     -0.14106     -0.14178
log_probs/mean                    -2.69872     0.00514     -2.69238     -2.70648
mean/mean                         -0.01777     0.00030     -0.01730     -0.01816
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01963067054748535
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70143
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [164100]
collect time 0.0009412765502929688
inner_dict_sum {'sac_diff0': 0.0002124309539794922, 'sac_diff1': 0.006923198699951172, 'sac_diff2': 0.00847005844116211, 'sac_diff3': 0.010552406311035156, 'sac_diff4': 0.006772518157958984, 'sac_diff5': 0.03281044960021973, 'sac_diff6': 0.00038695335388183594, 'all': 0.06612801551818848}
diff5_list [0.006642818450927734, 0.006448984146118164, 0.006707191467285156, 0.006575107574462891, 0.006436347961425781]
time3 0
time4 0.06690430641174316
time5 0.06695222854614258
time7 9.5367431640625e-07
gen_weight_change tensor(-18.2873)
policy weight change tensor(45.6446, grad_fn=<SumBackward0>)
time8 0.0018451213836669922
train_time 0.0782003402709961
eval time 0.14121556282043457
epoch last part time 7.3909759521484375e-06
2024-01-23 01:05:32,867 MainThread INFO: EPOCH:1087
2024-01-23 01:05:32,867 MainThread INFO: Time Consumed:0.22276639938354492s
2024-01-23 01:05:32,868 MainThread INFO: Total Frames:163950s
 11%|█         | 1088/10000 [07:00<37:43,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           24439.77600
Train_Epoch_Reward                10626.37519
Running_Training_Average_Rewards  22717.79466
Explore_Time                      0.00094
Train___Time                      0.07820
Eval____Time                      0.14122
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             35016.05202
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       101.31565    2.64611    104.23983    97.19033
alpha_0                           0.58050      0.00008    0.58062      0.58039
Alpha_loss                        -3.64084     0.00405    -3.63701     -3.64768
Training/policy_loss              -8.24940     0.00348    -8.24538     -8.25580
Training/qf1_loss                 9217.11055   518.90491  9975.54980   8483.36426
Training/qf2_loss                 20193.62734  903.42677  21399.83203  18612.32617
Training/pf_norm                  0.15390      0.02632    0.17335      0.10247
Training/qf1_norm                 1197.41023   499.64885  1722.48621   440.90845
Training/qf2_norm                 3315.24048   87.01685   3416.82764   3179.07837
log_std/mean                      -0.15116     0.00018    -0.15086     -0.15135
log_probs/mean                    -2.69569     0.00596    -2.68922     -2.70581
mean/mean                         -0.02070     0.00006    -0.02066     -0.02081
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.018639564514160156
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70143
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [164250]
collect time 0.0009207725524902344
inner_dict_sum {'sac_diff0': 0.00021839141845703125, 'sac_diff1': 0.007623434066772461, 'sac_diff2': 0.008837699890136719, 'sac_diff3': 0.011226415634155273, 'sac_diff4': 0.007458686828613281, 'sac_diff5': 0.033838748931884766, 'sac_diff6': 0.00042629241943359375, 'all': 0.06962966918945312}
diff5_list [0.006682872772216797, 0.006654977798461914, 0.007425785064697266, 0.006625652313232422, 0.006449460983276367]
time3 0
time4 0.0704801082611084
time5 0.07053494453430176
time7 7.152557373046875e-07
gen_weight_change tensor(-18.2873)
policy weight change tensor(45.7994, grad_fn=<SumBackward0>)
time8 0.001926422119140625
train_time 0.08196687698364258
eval time 0.14397811889648438
epoch last part time 6.67572021484375e-06
2024-01-23 01:05:33,119 MainThread INFO: EPOCH:1088
2024-01-23 01:05:33,119 MainThread INFO: Time Consumed:0.22925567626953125s
2024-01-23 01:05:33,119 MainThread INFO: Total Frames:164100s
 11%|█         | 1089/10000 [07:00<37:38,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           25945.74665
Train_Epoch_Reward                24728.23178
Running_Training_Average_Rewards  23202.45052
Explore_Time                      0.00092
Train___Time                      0.08197
Eval____Time                      0.14398
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             35344.17500
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.93182    4.60813     108.90127    94.46323
alpha_0                           0.58022      0.00008     0.58033      0.58010
Alpha_loss                        -3.64524     0.00462     -3.63803     -3.65186
Training/policy_loss              -8.59044     0.00253     -8.58763     -8.59433
Training/qf1_loss                 9217.23193   1694.32439  11541.64258  6586.86475
Training/qf2_loss                 20204.61797  2842.34615  24315.43164  15670.90039
Training/pf_norm                  0.18378      0.02392     0.22936      0.16068
Training/qf1_norm                 2965.97361   1095.79232  4764.06885   1343.99280
Training/qf2_norm                 3538.56611   161.24647   3780.99414   3277.26880
log_std/mean                      -0.13716     0.00008     -0.13702     -0.13723
log_probs/mean                    -2.69764     0.00778     -2.68563     -2.70859
mean/mean                         -0.01547     0.00006     -0.01541     -0.01556
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01917433738708496
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 70143
epoch first part time 2.86102294921875e-06
replay_buffer._size: [164400]
collect time 0.0008819103240966797
inner_dict_sum {'sac_diff0': 0.00022101402282714844, 'sac_diff1': 0.0071561336517333984, 'sac_diff2': 0.008409261703491211, 'sac_diff3': 0.01102590560913086, 'sac_diff4': 0.007204771041870117, 'sac_diff5': 0.033248186111450195, 'sac_diff6': 0.00040221214294433594, 'all': 0.06766748428344727}
diff5_list [0.0066928863525390625, 0.006006956100463867, 0.0063190460205078125, 0.007837772369384766, 0.0063915252685546875]
time3 0
time4 0.06849932670593262
time5 0.06854987144470215
time7 7.152557373046875e-07
gen_weight_change tensor(-18.2873)
policy weight change tensor(45.6998, grad_fn=<SumBackward0>)
time8 0.0019164085388183594
train_time 0.07995843887329102
eval time 0.1459343433380127
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:33,371 MainThread INFO: EPOCH:1089
2024-01-23 01:05:33,371 MainThread INFO: Time Consumed:0.22927212715148926s
2024-01-23 01:05:33,372 MainThread INFO: Total Frames:164250s
 11%|█         | 1090/10000 [07:00<37:34,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           27379.18755
Train_Epoch_Reward                37311.19245
Running_Training_Average_Rewards  23824.87237
Explore_Time                      0.00088
Train___Time                      0.07996
Eval____Time                      0.14593
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             35131.05627
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       104.52436    3.17989     108.84358    99.55032
alpha_0                           0.57993      0.00008     0.58004      0.57981
Alpha_loss                        -3.64860     0.00584     -3.63915     -3.65478
Training/policy_loss              -8.43001     0.00444     -8.42181     -8.43398
Training/qf1_loss                 10045.72617  1555.87245  11582.58008  7284.91406
Training/qf2_loss                 21750.03437  2165.68115  24099.09766  17906.68750
Training/pf_norm                  0.19911      0.01798     0.21548      0.16422
Training/qf1_norm                 1320.75446   625.01920   2188.78320   350.09851
Training/qf2_norm                 3564.84092   108.86821   3711.85254   3393.25977
log_std/mean                      -0.14144     0.00030     -0.14093     -0.14177
log_probs/mean                    -2.69769     0.00923     -2.68279     -2.70780
mean/mean                         -0.02248     0.00038     -0.02186     -0.02291
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018973827362060547
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70143
epoch first part time 3.337860107421875e-06
replay_buffer._size: [164550]
collect time 0.0009336471557617188
inside mustsac before update, task 0, sumup 70143
inside mustsac after update, task 0, sumup 70653
inner_dict_sum {'sac_diff0': 0.00022459030151367188, 'sac_diff1': 0.007311820983886719, 'sac_diff2': 0.009213685989379883, 'sac_diff3': 0.011110305786132812, 'sac_diff4': 0.0076334476470947266, 'sac_diff5': 0.05488181114196777, 'sac_diff6': 0.00043082237243652344, 'all': 0.09080648422241211}
diff5_list [0.011222362518310547, 0.010810136795043945, 0.011868953704833984, 0.010906457901000977, 0.01007390022277832]
time3 0.0008809566497802734
time4 0.09171605110168457
time5 0.09177136421203613
time7 0.008923530578613281
gen_weight_change tensor(-18.3884)
policy weight change tensor(45.7992, grad_fn=<SumBackward0>)
time8 0.0026068687438964844
train_time 0.1224370002746582
eval time 0.10920381546020508
epoch last part time 7.62939453125e-06
2024-01-23 01:05:33,629 MainThread INFO: EPOCH:1090
2024-01-23 01:05:33,629 MainThread INFO: Time Consumed:0.2349841594696045s
2024-01-23 01:05:33,629 MainThread INFO: Total Frames:164400s
 11%|█         | 1091/10000 [07:01<37:54,  3.92it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           28770.44574
Train_Epoch_Reward                34408.69314
Running_Training_Average_Rewards  24514.29862
Explore_Time                      0.00093
Train___Time                      0.12244
Eval____Time                      0.10920
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             35210.30747
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       103.33868    2.20829     106.57239    100.86938
alpha_0                           0.57964      0.00008     0.57975      0.57952
Alpha_loss                        -3.65266     0.00524     -3.64733     -3.65959
Training/policy_loss              -8.80757     0.28959     -8.29379     -9.16470
Training/qf1_loss                 10425.85898  1215.72921  12274.68945  8834.45605
Training/qf2_loss                 21743.64414  1678.79436  24075.07812  19715.37109
Training/pf_norm                  0.24729      0.11150     0.37185      0.07518
Training/qf1_norm                 2002.13691   1175.05109  3399.99780   676.95349
Training/qf2_norm                 3636.65576   119.01859   3744.19482   3464.88940
log_std/mean                      -0.13347     0.00358     -0.12878     -0.13934
log_probs/mean                    -2.69902     0.01102     -2.68923     -2.71295
mean/mean                         -0.01489     0.01111     -0.00111     -0.03008
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018370389938354492
epoch last part time3 0.0031080245971679688
inside rlalgo, task 0, sumup 70653
epoch first part time 2.86102294921875e-06
replay_buffer._size: [164700]
collect time 0.0010302066802978516
inner_dict_sum {'sac_diff0': 0.00020837783813476562, 'sac_diff1': 0.006947994232177734, 'sac_diff2': 0.008300304412841797, 'sac_diff3': 0.010779142379760742, 'sac_diff4': 0.0070078372955322266, 'sac_diff5': 0.0324549674987793, 'sac_diff6': 0.00038623809814453125, 'all': 0.0660848617553711}
diff5_list [0.006776094436645508, 0.0063059329986572266, 0.0068204402923583984, 0.0061337947845458984, 0.006418704986572266]
time3 0
time4 0.06683731079101562
time5 0.06688308715820312
time7 4.76837158203125e-07
gen_weight_change tensor(-18.3884)
policy weight change tensor(45.4266, grad_fn=<SumBackward0>)
time8 0.001890420913696289
train_time 0.07821226119995117
eval time 0.15014219284057617
epoch last part time 7.3909759521484375e-06
2024-01-23 01:05:33,886 MainThread INFO: EPOCH:1091
2024-01-23 01:05:33,886 MainThread INFO: Time Consumed:0.23180198669433594s
2024-01-23 01:05:33,886 MainThread INFO: Total Frames:164550s
 11%|█         | 1092/10000 [07:01<37:48,  3.93it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           28833.27341
Train_Epoch_Reward                64311.23064
Running_Training_Average_Rewards  26394.03279
Explore_Time                      0.00102
Train___Time                      0.07821
Eval____Time                      0.15014
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22285.49599
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       103.81324    3.20817     108.14038    99.10970
alpha_0                           0.57935      0.00008     0.57946      0.57923
Alpha_loss                        -3.65332     0.00687     -3.64680     -3.66309
Training/policy_loss              -9.13221     0.00631     -9.12291     -9.14113
Training/qf1_loss                 10085.04297  1954.54150  13636.14355  8329.14648
Training/qf2_loss                 21576.21563  2621.07343  26126.65430  18807.84766
Training/pf_norm                  0.27046      0.04033     0.32082      0.21029
Training/qf1_norm                 1068.74454   663.94714   2017.31140   209.95874
Training/qf2_norm                 3757.10488   116.61151   3917.37939   3587.12061
log_std/mean                      -0.12972     0.00041     -0.12910     -0.13027
log_probs/mean                    -2.69410     0.01164     -2.68093     -2.70956
mean/mean                         -0.01735     0.00046     -0.01666     -0.01797
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018160581588745117
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70653
epoch first part time 2.384185791015625e-06
replay_buffer._size: [164850]
collect time 0.0008711814880371094
inner_dict_sum {'sac_diff0': 0.00021409988403320312, 'sac_diff1': 0.0069162845611572266, 'sac_diff2': 0.008369684219360352, 'sac_diff3': 0.010807514190673828, 'sac_diff4': 0.007454395294189453, 'sac_diff5': 0.0328214168548584, 'sac_diff6': 0.0004057884216308594, 'all': 0.06698918342590332}
diff5_list [0.007069587707519531, 0.006264209747314453, 0.006169319152832031, 0.006658077239990234, 0.0066602230072021484]
time3 0
time4 0.0678253173828125
time5 0.06787872314453125
time7 9.5367431640625e-07
gen_weight_change tensor(-18.3884)
policy weight change tensor(44.9318, grad_fn=<SumBackward0>)
time8 0.0018820762634277344
train_time 0.07921028137207031
eval time 0.15778708457946777
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:34,148 MainThread INFO: EPOCH:1092
2024-01-23 01:05:34,148 MainThread INFO: Time Consumed:0.24024415016174316s
2024-01-23 01:05:34,148 MainThread INFO: Total Frames:164700s
 11%|█         | 1093/10000 [07:01<38:11,  3.89it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           28898.33481
Train_Epoch_Reward                21641.71609
Running_Training_Average_Rewards  26333.77045
Explore_Time                      0.00087
Train___Time                      0.07921
Eval____Time                      0.15779
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22684.87081
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       102.12759    1.50971    103.66551    99.49401
alpha_0                           0.57906      0.00008    0.57918      0.57894
Alpha_loss                        -3.65017     0.00785    -3.63998     -3.66354
Training/policy_loss              -8.17548     0.00493    -8.17166     -8.18508
Training/qf1_loss                 9327.79795   770.31904  10485.08008  8171.39600
Training/qf2_loss                 20392.78945  795.21987  21825.22266  19505.56445
Training/pf_norm                  0.38659      0.01735    0.41786      0.36723
Training/qf1_norm                 2007.23137   317.34890  2335.50928   1465.91541
Training/qf2_norm                 3324.73379   49.86619   3373.73877   3241.09912
log_std/mean                      -0.13361     0.00041    -0.13301     -0.13419
log_probs/mean                    -2.68223     0.01506    -2.66237     -2.70671
mean/mean                         -0.01439     0.00034    -0.01408     -0.01499
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.019359111785888672
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70653
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [165000]
collect time 0.0009143352508544922
inner_dict_sum {'sac_diff0': 0.00021791458129882812, 'sac_diff1': 0.007074117660522461, 'sac_diff2': 0.008262395858764648, 'sac_diff3': 0.010429620742797852, 'sac_diff4': 0.006872415542602539, 'sac_diff5': 0.03114461898803711, 'sac_diff6': 0.0003807544708251953, 'all': 0.06438183784484863}
diff5_list [0.0066204071044921875, 0.0062177181243896484, 0.006188869476318359, 0.005953311920166016, 0.0061643123626708984]
time3 0
time4 0.06511664390563965
time5 0.06516051292419434
time7 7.152557373046875e-07
gen_weight_change tensor(-18.3884)
policy weight change tensor(44.6254, grad_fn=<SumBackward0>)
time8 0.0019295215606689453
train_time 0.07662415504455566
eval time 0.14902806282043457
epoch last part time 6.9141387939453125e-06
2024-01-23 01:05:34,400 MainThread INFO: EPOCH:1093
2024-01-23 01:05:34,401 MainThread INFO: Time Consumed:0.22911834716796875s
2024-01-23 01:05:34,401 MainThread INFO: Total Frames:164850s
 11%|█         | 1094/10000 [07:02<37:59,  3.91it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           28917.28308
Train_Epoch_Reward                28414.21964
Running_Training_Average_Rewards  26343.48951
Explore_Time                      0.00091
Train___Time                      0.07662
Eval____Time                      0.14903
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22718.18818
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       103.24071    2.49555     105.89040    99.02870
alpha_0                           0.57877      0.00008     0.57889      0.57866
Alpha_loss                        -3.66728     0.00181     -3.66499     -3.66933
Training/policy_loss              -8.96066     0.00575     -8.95083     -8.96798
Training/qf1_loss                 9264.95654   1578.47851  12304.66895  7752.41943
Training/qf2_loss                 20610.04609  1987.42695  24206.10547  18239.96289
Training/pf_norm                  0.09705      0.02063     0.11917      0.06054
Training/qf1_norm                 1539.84674   486.47082   2171.20312   777.89081
Training/qf2_norm                 3689.19824   92.34391    3782.71069   3532.51514
log_std/mean                      -0.13853     0.00018     -0.13835     -0.13881
log_probs/mean                    -2.70745     0.00395     -2.70082     -2.71112
mean/mean                         -0.00560     0.00018     -0.00532     -0.00585
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.0198667049407959
epoch last part time3 1.1920928955078125e-06
inside rlalgo, task 0, sumup 70653
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [165150]
collect time 0.0009400844573974609
inner_dict_sum {'sac_diff0': 0.000209808349609375, 'sac_diff1': 0.006899595260620117, 'sac_diff2': 0.008318185806274414, 'sac_diff3': 0.011039495468139648, 'sac_diff4': 0.007203102111816406, 'sac_diff5': 0.0324554443359375, 'sac_diff6': 0.0003845691680908203, 'all': 0.06651020050048828}
diff5_list [0.006888151168823242, 0.006272077560424805, 0.0063321590423583984, 0.006321430206298828, 0.0066416263580322266]
time3 0
time4 0.06727719306945801
time5 0.0673215389251709
time7 7.152557373046875e-07
gen_weight_change tensor(-18.3884)
policy weight change tensor(44.6313, grad_fn=<SumBackward0>)
time8 0.0018961429595947266
train_time 0.07868337631225586
eval time 0.14426732063293457
epoch last part time 8.106231689453125e-06
2024-01-23 01:05:34,650 MainThread INFO: EPOCH:1094
2024-01-23 01:05:34,651 MainThread INFO: Time Consumed:0.22631430625915527s
2024-01-23 01:05:34,651 MainThread INFO: Total Frames:165000s
 11%|█         | 1095/10000 [07:02<37:39,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           28868.45194
Train_Epoch_Reward                23070.84462
Running_Training_Average_Rewards  26807.52456
Explore_Time                      0.00093
Train___Time                      0.07868
Eval____Time                      0.14427
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22603.35862
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       101.21198    1.69482     102.84166    98.74803
alpha_0                           0.57848      0.00008     0.57860      0.57837
Alpha_loss                        -3.66650     0.00356     -3.66274     -3.67316
Training/policy_loss              -8.32445     0.00319     -8.32005     -8.32955
Training/qf1_loss                 9386.45293   1038.77738  10563.18945  7651.32422
Training/qf2_loss                 20187.83516  1318.55895  21760.39453  17914.84180
Training/pf_norm                  0.14427      0.02019     0.15938      0.10516
Training/qf1_norm                 2226.97419   348.30324   2531.94287   1710.92725
Training/qf2_norm                 3325.21836   54.51140    3383.82129   3244.62305
log_std/mean                      -0.12714     0.00010     -0.12706     -0.12734
log_probs/mean                    -2.69992     0.00564     -2.69305     -2.70963
mean/mean                         -0.01452     0.00013     -0.01434     -0.01470
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018256664276123047
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 70653
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [165300]
collect time 0.0008602142333984375
inside mustsac before update, task 0, sumup 70653
inside mustsac after update, task 0, sumup 71354
inner_dict_sum {'sac_diff0': 0.00022220611572265625, 'sac_diff1': 0.00749969482421875, 'sac_diff2': 0.009154796600341797, 'sac_diff3': 0.011918783187866211, 'sac_diff4': 0.007684469223022461, 'sac_diff5': 0.05292820930480957, 'sac_diff6': 0.00041675567626953125, 'all': 0.08982491493225098}
diff5_list [0.010956764221191406, 0.010467767715454102, 0.010462522506713867, 0.011060476303100586, 0.00998067855834961]
time3 0.0008921623229980469
time4 0.09073615074157715
time5 0.0907900333404541
time7 0.009043455123901367
gen_weight_change tensor(-17.9648)
policy weight change tensor(44.5935, grad_fn=<SumBackward0>)
time8 0.001882314682006836
train_time 0.12077951431274414
eval time 0.10869050025939941
epoch last part time 6.4373016357421875e-06
2024-01-23 01:05:34,905 MainThread INFO: EPOCH:1095
2024-01-23 01:05:34,905 MainThread INFO: Time Consumed:0.2327587604522705s
2024-01-23 01:05:34,906 MainThread INFO: Total Frames:165150s
 11%|█         | 1096/10000 [07:02<37:41,  3.94it/s]--------------------------------  -----------  ---------  -----------  -----------
Name                              Value
Running_Average_Rewards           28756.71180
Train_Epoch_Reward                20602.00504
Running_Training_Average_Rewards  26329.00891
Explore_Time                      0.00086
Train___Time                      0.12078
Eval____Time                      0.10869
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             22436.39224
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std        Max          Min
Reward_Mean                       104.48046    1.51397    106.90186    102.40112
alpha_0                           0.57820      0.00008    0.57831      0.57808
Alpha_loss                        -3.66932     0.00597    -3.66106     -3.67831
Training/policy_loss              -8.79752     0.61352    -7.70977     -9.40975
Training/qf1_loss                 9364.49512   319.33500  9894.64355   8979.80469
Training/qf2_loss                 21040.03555  341.96326  21395.18164  20585.83203
Training/pf_norm                  0.26861      0.10103    0.39564      0.16956
Training/qf1_norm                 1029.45245   563.43757  1865.41150   285.16409
Training/qf2_norm                 3661.85020   239.34131  3901.45410   3208.79614
log_std/mean                      -0.12881     0.00707    -0.11814     -0.13914
log_probs/mean                    -2.69897     0.01097    -2.68511     -2.71537
mean/mean                         -0.01980     0.00667    -0.01255     -0.03182
--------------------------------  -----------  ---------  -----------  -----------
epoch last part time2 0.01797628402709961
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71354
epoch first part time 2.86102294921875e-06
replay_buffer._size: [165450]
collect time 0.0008628368377685547
inner_dict_sum {'sac_diff0': 0.000217437744140625, 'sac_diff1': 0.007176876068115234, 'sac_diff2': 0.008686304092407227, 'sac_diff3': 0.010672569274902344, 'sac_diff4': 0.007048130035400391, 'sac_diff5': 0.0326082706451416, 'sac_diff6': 0.0003933906555175781, 'all': 0.066802978515625}
diff5_list [0.0076100826263427734, 0.006417751312255859, 0.0062253475189208984, 0.0062198638916015625, 0.006135225296020508]
time3 0
time4 0.06758666038513184
time5 0.06763625144958496
time7 4.76837158203125e-07
gen_weight_change tensor(-17.9648)
policy weight change tensor(44.8572, grad_fn=<SumBackward0>)
time8 0.001928091049194336
train_time 0.0789492130279541
eval time 0.14577555656433105
epoch last part time 7.3909759521484375e-06
2024-01-23 01:05:35,155 MainThread INFO: EPOCH:1096
2024-01-23 01:05:35,155 MainThread INFO: Time Consumed:0.22799324989318848s
2024-01-23 01:05:35,155 MainThread INFO: Total Frames:165300s
 11%|█         | 1097/10000 [07:02<37:31,  3.95it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           28099.09428
Train_Epoch_Reward                12109.49807
Running_Training_Average_Rewards  25794.85185
Explore_Time                      0.00086
Train___Time                      0.07895
Eval____Time                      0.14578
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             27561.04616
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       102.30760    3.21636     107.37284    98.33556
alpha_0                           0.57791      0.00008     0.57802      0.57779
Alpha_loss                        -3.66838     0.00571     -3.65825     -3.67433
Training/policy_loss              -9.18738     0.00319     -9.18348     -9.19195
Training/qf1_loss                 9751.95527   1691.50370  12969.37305  8051.19922
Training/qf2_loss                 20939.49531  2366.73578  25332.69922  18638.36914
Training/pf_norm                  0.15557      0.01558     0.17415      0.13537
Training/qf1_norm                 696.26857    296.14974   1004.51556   331.85101
Training/qf2_norm                 3799.47451   119.35520   3988.12793   3649.59009
log_std/mean                      -0.13403     0.00039     -0.13356     -0.13462
log_probs/mean                    -2.69116     0.00962     -2.67511     -2.70201
mean/mean                         -0.03710     0.00006     -0.03700     -0.03716
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.018221139907836914
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71354
epoch first part time 2.86102294921875e-06
replay_buffer._size: [165600]
collect time 0.0008790493011474609
inner_dict_sum {'sac_diff0': 0.00020933151245117188, 'sac_diff1': 0.007205009460449219, 'sac_diff2': 0.008620500564575195, 'sac_diff3': 0.011295557022094727, 'sac_diff4': 0.007343769073486328, 'sac_diff5': 0.033564090728759766, 'sac_diff6': 0.00040531158447265625, 'all': 0.06864356994628906}
diff5_list [0.00690460205078125, 0.00619816780090332, 0.006172657012939453, 0.007877588272094727, 0.006411075592041016]
time3 0
time4 0.06947779655456543
time5 0.06952738761901855
time7 7.152557373046875e-07
gen_weight_change tensor(-17.9648)
policy weight change tensor(45.1413, grad_fn=<SumBackward0>)
time8 0.0020411014556884766
train_time 0.08101272583007812
eval time 0.14724135398864746
epoch last part time 7.152557373046875e-06
2024-01-23 01:05:35,409 MainThread INFO: EPOCH:1097
2024-01-23 01:05:35,409 MainThread INFO: Time Consumed:0.23157572746276855s
2024-01-23 01:05:35,409 MainThread INFO: Total Frames:165450s
 11%|█         | 1098/10000 [07:03<37:37,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           27344.94809
Train_Epoch_Reward                12039.59630
Running_Training_Average_Rewards  25748.22387
Explore_Time                      0.00087
Train___Time                      0.08101
Eval____Time                      0.14724
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             27474.59016
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       103.64587    2.59669     107.73104    100.64565
alpha_0                           0.57762      0.00008     0.57773      0.57750
Alpha_loss                        -3.67708     0.00723     -3.66457     -3.68401
Training/policy_loss              -8.61118     0.00371     -8.60757     -8.61776
Training/qf1_loss                 10301.03359  1863.93868  13317.32324  8404.49316
Training/qf2_loss                 21740.74219  2346.23374  25191.42188  19195.36719
Training/pf_norm                  0.13788      0.01456     0.15159      0.11277
Training/qf1_norm                 969.85168    536.74552   1821.99011   421.35367
Training/qf2_norm                 3585.68315   89.19963    3721.63477   3484.75195
log_std/mean                      -0.13594     0.00023     -0.13561     -0.13625
log_probs/mean                    -2.70095     0.01419     -2.67571     -2.71479
mean/mean                         -0.00403     0.00014     -0.00388     -0.00427
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01976299285888672
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71354
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [165750]
collect time 0.001104593276977539
inner_dict_sum {'sac_diff0': 0.00021839141845703125, 'sac_diff1': 0.007447481155395508, 'sac_diff2': 0.008666515350341797, 'sac_diff3': 0.010752677917480469, 'sac_diff4': 0.0069904327392578125, 'sac_diff5': 0.032434701919555664, 'sac_diff6': 0.000385284423828125, 'all': 0.0668954849243164}
diff5_list [0.0070133209228515625, 0.00630950927734375, 0.0069866180419921875, 0.006140232086181641, 0.0059850215911865234]
time3 0
time4 0.06765055656433105
time5 0.06769967079162598
time7 9.5367431640625e-07
gen_weight_change tensor(-17.9648)
policy weight change tensor(45.3528, grad_fn=<SumBackward0>)
time8 0.0019316673278808594
train_time 0.07993078231811523
eval time 0.1468055248260498
epoch last part time 7.152557373046875e-06
2024-01-23 01:05:35,663 MainThread INFO: EPOCH:1098
2024-01-23 01:05:35,663 MainThread INFO: Time Consumed:0.23029613494873047s
2024-01-23 01:05:35,664 MainThread INFO: Total Frames:165600s
 11%|█         | 1099/10000 [07:03<37:37,  3.94it/s]--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           26534.62441
Train_Epoch_Reward                12759.81480
Running_Training_Average_Rewards  25432.21561
Explore_Time                      0.00110
Train___Time                      0.07993
Eval____Time                      0.14681
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             27240.93816
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       103.50445    2.65879     107.24294    99.68280
alpha_0                           0.57733      0.00008     0.57745      0.57722
Alpha_loss                        -3.68207     0.00699     -3.67056     -3.68952
Training/policy_loss              -8.28108     0.00233     -8.27727     -8.28415
Training/qf1_loss                 9316.81680   1253.46476  10710.92090  7422.53027
Training/qf2_loss                 20763.34687  1786.95314  23012.20312  18383.28711
Training/pf_norm                  0.19396      0.03114     0.24214      0.15462
Training/qf1_norm                 656.20354    284.21592   1142.23291   266.11523
Training/qf2_norm                 3413.50439   86.30970    3535.89209   3294.38330
log_std/mean                      -0.13008     0.00019     -0.12979     -0.13033
log_probs/mean                    -2.70394     0.01246     -2.68299     -2.71685
mean/mean                         -0.02363     0.00017     -0.02333     -0.02378
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01916813850402832
epoch last part time3 7.152557373046875e-07
inside rlalgo, task 0, sumup 71354
epoch first part time 2.86102294921875e-06
replay_buffer._size: [165900]
collect time 0.0009818077087402344
inner_dict_sum {'sac_diff0': 0.00020360946655273438, 'sac_diff1': 0.006689786911010742, 'sac_diff2': 0.00792241096496582, 'sac_diff3': 0.01035165786743164, 'sac_diff4': 0.006886720657348633, 'sac_diff5': 0.031217575073242188, 'sac_diff6': 0.000385284423828125, 'all': 0.06365704536437988}
diff5_list [0.006486177444458008, 0.006140470504760742, 0.00630950927734375, 0.0062618255615234375, 0.00601959228515625]
time3 0
time4 0.064422607421875
time5 0.06446504592895508
time7 9.5367431640625e-07
gen_weight_change tensor(-17.9648)
policy weight change tensor(45.3597, grad_fn=<SumBackward0>)
time8 0.001993417739868164
train_time 0.07575559616088867
eval time 0.14949893951416016
epoch last part time 5.9604644775390625e-06
2024-01-23 01:05:35,915 MainThread INFO: EPOCH:1099
2024-01-23 01:05:35,915 MainThread INFO: Time Consumed:0.22869300842285156s
2024-01-23 01:05:35,915 MainThread INFO: Total Frames:165750s
 11%|█         | 1100/10000 [07:03<37:29,  3.96it/s] 11%|█         | 1100/10000 [07:03<57:08,  2.60it/s]
--------------------------------  -----------  ----------  -----------  -----------
Name                              Value
Running_Average_Rewards           25757.07081
Train_Epoch_Reward                11346.42057
Running_Training_Average_Rewards  23673.20307
Explore_Time                      0.00098
Train___Time                      0.07576
Eval____Time                      0.14950
reach-v1_success_rate             0.00000
reach-v1_eval_rewards             27355.52030
0                                 0.00000
mean_success_rate                 0.00000

Name                              Mean         Std         Max          Min
Reward_Mean                       104.70215    2.91049     108.49929    100.31425
alpha_0                           0.57704      0.00008     0.57716      0.57693
Alpha_loss                        -3.68189     0.00696     -3.67207     -3.69389
Training/policy_loss              -8.31305     0.00521     -8.30330     -8.31873
Training/qf1_loss                 10174.32070  1015.82645  11735.59082  9093.00391
Training/qf2_loss                 21766.49570  1564.69490  24088.90625  19899.44922
Training/pf_norm                  0.28577      0.03173     0.31744      0.24368
Training/qf1_norm                 2181.15203   555.91095   2888.59424   1309.01587
Training/qf2_norm                 3424.50366   96.80774    3547.82739   3278.59717
log_std/mean                      -0.13414     0.00008     -0.13399     -0.13423
log_probs/mean                    -2.69755     0.01320     -2.67967     -2.72059
mean/mean                         -0.01447     0.00067     -0.01343     -0.01528
--------------------------------  -----------  ----------  -----------  -----------
epoch last part time2 0.01846909523010254
epoch last part time3 4.76837158203125e-07
inside rlalgo, task 0, sumup 71354
epoch first part time 3.0994415283203125e-06
replay_buffer._size: [166050]
collect time 0.0009551048278808594
inside mustsac before update, task 0, sumup 71354
inside mustsac after update, task 0, sumup 71235
inner_dict_sum {'sac_diff0': 0.00021219253540039062, 'sac_diff1': 0.007213592529296875, 'sac_diff2': 0.008544445037841797, 'sac_diff3': 0.011475563049316406, 'sac_diff4': 0.007559299468994141, 'sac_diff5': 0.0523991584777832, 'sac_diff6': 0.000423431396484375, 'all': 0.08782768249511719}
diff5_list [0.010948419570922852, 0.010417699813842773, 0.011293649673461914, 0.009852409362792969, 0.009886980056762695]
time3 0.0009009838104248047
time4 0.08872056007385254
time5 0.08877372741699219
time7 0.009050369262695312
gen_weight_change tensor(-18.1919)
policy weight change tensor(45.4030, grad_fn=<SumBackward0>)
time8 0.0026726722717285156
train_time 0.11931705474853516
Process Process-2:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/DST_RL/./torchrl/collector/para/async_mt.py", line 338, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255).
Process Process-3:
wandb: 
wandb: Run history:
wandb:                                    0 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                           Alpha_loss ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:                         Eval____Time ▂▃▅▆█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         Explore_Time ▃▃▃▃▂▄▃▃▃▃▃▂█▃▃▃▂▂▂▃▁▃▂▂▃▂▂▅▂▃▂▂▁▂▅▃▃▄▃▃
wandb:                          Reward_Mean ██▇▆▆▆▂▄▂▁▆▄▅▆▂▃▅▃▃▅▅▄▃▃▃▃▂▃▃▄▅▄▆▅▅▇▅▇▇▇
wandb:              Running_Average_Rewards ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▂▂▂▂▃▃▃▃▄▄▅▅▇█▅
wandb:     Running_Training_Average_Rewards ▂▂▂▂▂▂▁▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▃▂▁▁▂▂▃▂▄▂▂▃▃▅▅▄█▅
wandb:                   Train_Epoch_Reward ▃▁▁▃▄▁▂▆▂▃▃▃▃▅▂▃▆▂▃▆▁▃▆▁▃▁▄▁▂▃▂▄▂▄▄█▅▃▅▂
wandb:                         Train___Time ▂▁▁█▁▁▂▁▁▂▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁
wandb:                     Training/pf_norm █▇▄▂▂▂▂▃▂▂▂▂▂▂▂▃▂▂▂▁▂▂▁▁▃▂▁▂▃▂▂▁▃▂▄▂▂▂▄▄
wandb:                 Training/policy_loss ███████████████████████▇▇▇▇▇▆▆▆▅▅▅▄▄▃▃▁▁
wandb:                    Training/qf1_loss █▆▇▇▇▇▅▆▄▃▅▃▅▄▁▂▂▁▂▂▁▂▂▁▂▁▁▁▁▁▂▁▂▁▂▂▂▂▁▂
wandb:                    Training/qf1_norm ▁▁▁▁▂▂▂▃▄▅▇▇██▆▄▁▃▂▃▄▁▁▁▃▂▃▂▂▂▃▁▂▂▁▄▁▁▃▄
wandb:                    Training/qf2_loss █▅▆▆▆▆▃▅▂▁▅▄▆▆▂▄▆▃▃▅▄▄▃▃▃▃▃▃▂▂▄▃▆▄▄▅▄▅▄▆
wandb:                    Training/qf2_norm ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▃▃▄▃▄▅▅▅▆▆▆▇██
wandb:                              alpha_0 ███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁
wandb:                    gen_weight_change █▆▄▂▁▂▃▂▄▄▄▄▃▃▃▄▃▃▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇███▇▇▇▇
wandb:  inside mustsac after update, task 0 ▃▃▄▇▅▃▂▆▅▆▄▅▆▅▅▅▇▄█▁▅▂▆▄▆▂▄▆▃▅▄▆▃▅▂▄▃▆▅▇
wandb: inside mustsac before update, task 0 ▄▃▂▅▅▇▅▅▅▃█▅▅▄▆▆▅▆▆▅▆▂▃▆▄▆▆▁▄▄▇▅▆▄▄▄▆▃▆▅
wandb:                       log_probs/mean █▆▃▃▂▁▂▂▂▃▂▃▃▂▃▃▁▃▁▂▃▃▂▂▃▂▂▄▂▄▃▄▄▃▄▄▃▄▆▅
wandb:                         log_std/mean █▇▄▂▂▂▂▁▂▁▁▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▁▂▂▂▂▂▁▂▁
wandb:                            mean/mean ▇██▇▇█▇▇██▇▇▇▇█▇▇▇▇▇▇▆▆▆▅▆▆▆▅▅▅▅▆▅▆▄▅▂▁▄
wandb:                    mean_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                reach-v1_eval_rewards ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▂▃▃▃▃▃▃▃▅▅▆▅█
wandb:                reach-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                    save_traj_mod_sum ▁▁▁▁▁▁██████████████████████████████████
wandb:                      task_0_mask_sum ▆▆▇█▆█▆▅▆▄▇▆▆▆▆▇█▆▇▇▆▆▅▇▄▇▄▁▅▄█▆▅▅▄▄▆▇▆▄
wandb: 
wandb: Run summary:
wandb:                                    0 0.0
wandb:                           Alpha_loss -3.69055
wandb:                         Eval____Time 0.1495
wandb:                         Explore_Time 0.00098
wandb:                          Reward_Mean 97.93314
wandb:              Running_Average_Rewards 25757.07081
wandb:     Running_Training_Average_Rewards 23673.20307
wandb:                   Train_Epoch_Reward 11346.42057
wandb:                         Train___Time 0.07576
wandb:                     Training/pf_norm 0.262
wandb:                 Training/policy_loss -9.3071
wandb:                    Training/qf1_loss 8702.76855
wandb:                    Training/qf1_norm 2851.68042
wandb:                    Training/qf2_loss 18780.29883
wandb:                    Training/qf2_norm 3548.75562
wandb:                              alpha_0 0.57664
wandb:                    gen_weight_change -18.1919
wandb:  inside mustsac after update, task 0 71235
wandb: inside mustsac before update, task 0 71354
wandb:                       log_probs/mean -2.70479
wandb:                         log_std/mean -0.14665
wandb:                            mean/mean -0.01331
wandb:                    mean_success_rate 0.0
wandb:                reach-v1_eval_rewards 27355.5203
wandb:                reach-v1_success_rate 0.0
wandb:                    save_traj_mod_sum 1
wandb:                      task_0_mask_sum 71354
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /lustre04/scratch/qianxi/sparse_training/dec_must/DST_RL/wandb/offline-run-20240123_005831-ve7gb4ff
wandb: Find logs at: ./wandb/offline-run-20240123_005831-ve7gb4ff/logs
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/DST_RL/./torchrl/collector/para/async_mt.py", line 434, in eval_worker_process
    eval_ob, r, done, info = env_info.env.step( act )
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/must_env/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/must_env/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/must_env/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_reach_push_pick_place.py", line 153, in step
    self.do_simulation([action[-1], -action[-1]])
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 120, in do_simulation
    self.sim.step()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 163, in print_exc
    print_exception(*sys.exc_info(), limit=limit, file=file, chain=chain)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 103, in print_exception
    for line in TracebackException(
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 508, in __init__
    self.stack = StackSummary.extract(
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 366, in extract
    f.line
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 288, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 16, in getline
    lines = getlines(filename, module_globals)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 47, in getlines
    return updatecache(filename, module_globals)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 394, in open
    encoding, lines = detect_encoding(buffer.readline)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 363, in detect_encoding
    first = read_or_stop()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 321, in read_or_stop
    return readline()
KeyboardInterrupt
Traceback (most recent call last):
  File "starter/mt_must_sac.py", line 359, in <module>
    experiment(args)
  File "starter/mt_must_sac.py", line 355, in experiment
    agent.train(env.num_tasks,params,group_name)
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/DST_RL/./torchrl/algo/rl_algo.py", line 400, in train
    eval_infos = self.collector.eval_one_epoch()
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/DST_RL/./torchrl/collector/para/async_mt.py", line 622, in eval_one_epoch
    worker_rst = self.eval_shared_que.get()
  File "<string>", line 2, in get
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/must_env/lib/python3.8/site-packages/wandb/sdk/wandb_manager.py", line 186, in _teardown
    result = self._service.join()
  File "/lustre04/scratch/qianxi/sparse_training/dec_must/must_env/lib/python3.8/site-packages/wandb/sdk/service/service.py", line 235, in join
    ret = self._internal_proc.wait()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/subprocess.py", line 1083, in wait
    return self._wait(timeout=timeout)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/subprocess.py", line 1806, in _wait
    (pid, sts) = self._try_wait(0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/subprocess.py", line 1764, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
