W&B disabled.
2023-11-03 00:04:20,112 MainThread INFO: Experiment Name:testing_must_mtsac
2023-11-03 00:04:20,112 MainThread INFO: {
  "env_name": "mt10",
  "env": {
    "reward_scale": 1,
    "obs_norm": false
  },
  "meta_env": {
    "obs_type": "with_goal_and_id"
  },
  "replay_buffer": {
    "size": 1000000.0
  },
  "net": {
    "hidden_shapes": [
      40,
      40,
      40
    ]
  },
  "task_embedding": {
    "em_hidden_shapes": [
      20,
      20,
      20
    ]
  },
  "traj_encoder": {
    "latent_size": 256
  },
  "sparse_training": {
    "pruning_ratio": 0.4
  },
  "general_setting": {
    "discount": 0.99,
    "pretrain_epochs": 20,
    "num_epochs": 1000,
    "epoch_frames": 150,
    "max_episode_frames": 150,
    "generator_lr": 0.0001,
    "batch_size": 1280,
    "min_pool": 10000,
    "success_traj_update_only": true,
    "target_hard_update_period": 1000,
    "use_soft_update": true,
    "tau": 0.005,
    "opt_times": 5,
    "update_end_epoch": 1000,
    "mask_update_interval": 25,
    "eval_episodes": 3,
    "recent_traj_window": 10,
    "sl_optim_times": 5,
    "use_trajectory_info": 0,
    "use_sl_loss": 0
  },
  "sac": {
    "plr": 0.0003,
    "qlr": 0.0003,
    "reparameterization": true,
    "automatic_entropy_tuning": true,
    "policy_std_reg_weight": 0,
    "policy_mean_reg_weight": 0
  }
}
finish policy net init
mask generator finish initialization
/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Could not seed environment <MTEnv instance>[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
wandb: Tracking run with wandb version 0.15.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
replay_buffer._size: [300 300 300 300 300 300 300 300 300 300]
2023-11-03 00:05:25,351 MainThread INFO: EPOCH:0
2023-11-03 00:05:25,351 MainThread INFO: Time Consumed:0.004810810089111328s
2023-11-03 00:05:25,351 MainThread INFO: Total Frames:1500s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                18041.77474
Running_Training_Average_Rewards  1804.17747

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [300 300 300 300 300 300 300 300 300 300]
2023-11-03 00:05:25,378 MainThread INFO: EPOCH:1
2023-11-03 00:05:25,379 MainThread INFO: Time Consumed:0.0070781707763671875s
2023-11-03 00:05:25,379 MainThread INFO: Total Frames:3000s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                20327.07535
Running_Training_Average_Rewards  1918.44250

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [450 450 450 450 450 450 450 450 450 450]
2023-11-03 00:05:26,151 MainThread INFO: EPOCH:2
2023-11-03 00:05:26,151 MainThread INFO: Time Consumed:0.7661654949188232s
2023-11-03 00:05:26,152 MainThread INFO: Total Frames:4500s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                2483.27083
Running_Training_Average_Rewards  1361.73736

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [600 600 600 600 600 600 600 600 600 600]
2023-11-03 00:05:26,965 MainThread INFO: EPOCH:3
2023-11-03 00:05:26,966 MainThread INFO: Time Consumed:0.8125479221343994s
2023-11-03 00:05:26,966 MainThread INFO: Total Frames:6000s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                9982.21741
Running_Training_Average_Rewards  1093.08545

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [750 750 750 750 750 750 750 750 750 750]
2023-11-03 00:05:27,651 MainThread INFO: EPOCH:4
2023-11-03 00:05:27,652 MainThread INFO: Time Consumed:0.6839702129364014s
2023-11-03 00:05:27,652 MainThread INFO: Total Frames:7500s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                3951.53022
Running_Training_Average_Rewards  547.23395

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [900 900 900 900 900 900 900 900 900 900]
2023-11-03 00:05:28,289 MainThread INFO: EPOCH:5
2023-11-03 00:05:28,290 MainThread INFO: Time Consumed:0.6358671188354492s
2023-11-03 00:05:28,290 MainThread INFO: Total Frames:9000s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                6234.08797
Running_Training_Average_Rewards  672.26119

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [1050 1050 1050 1050 1050 1050 1050 1050 1050 1050]
2023-11-03 00:05:28,869 MainThread INFO: EPOCH:6
2023-11-03 00:05:28,869 MainThread INFO: Time Consumed:0.576052188873291s
2023-11-03 00:05:28,869 MainThread INFO: Total Frames:10500s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                19100.08085
Running_Training_Average_Rewards  976.18997

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [1200 1200 1200 1200 1200 1200 1200 1200 1200 1200]
2023-11-03 00:05:29,522 MainThread INFO: EPOCH:7
2023-11-03 00:05:29,522 MainThread INFO: Time Consumed:0.6499063968658447s
2023-11-03 00:05:29,522 MainThread INFO: Total Frames:12000s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                10442.02565
Running_Training_Average_Rewards  1192.53982

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [1350 1350 1350 1350 1350 1350 1350 1350 1350 1350]
2023-11-03 00:05:30,190 MainThread INFO: EPOCH:8
2023-11-03 00:05:30,190 MainThread INFO: Time Consumed:0.6664044857025146s
2023-11-03 00:05:30,191 MainThread INFO: Total Frames:13500s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                7764.77268
Running_Training_Average_Rewards  1243.56264

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [1500 1500 1500 1500 1500 1500 1500 1500 1500 1500]
2023-11-03 00:05:30,969 MainThread INFO: EPOCH:9
2023-11-03 00:05:30,969 MainThread INFO: Time Consumed:0.7768831253051758s
2023-11-03 00:05:30,970 MainThread INFO: Total Frames:15000s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                6480.09924
Running_Training_Average_Rewards  822.89659

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [1650 1650 1650 1650 1650 1650 1650 1650 1650 1650]
2023-11-03 00:05:31,676 MainThread INFO: EPOCH:10
2023-11-03 00:05:31,677 MainThread INFO: Time Consumed:0.7053642272949219s
2023-11-03 00:05:31,677 MainThread INFO: Total Frames:16500s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                30732.74715
Running_Training_Average_Rewards  1499.25397

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [1800 1800 1800 1800 1800 1800 1800 1800 1800 1800]
2023-11-03 00:05:32,549 MainThread INFO: EPOCH:11
2023-11-03 00:05:32,550 MainThread INFO: Time Consumed:0.8708987236022949s
2023-11-03 00:05:32,550 MainThread INFO: Total Frames:18000s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                10627.77395
Running_Training_Average_Rewards  1594.68734

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [1950 1950 1950 1950 1950 1950 1950 1950 1950 1950]
2023-11-03 00:05:33,286 MainThread INFO: EPOCH:12
2023-11-03 00:05:33,286 MainThread INFO: Time Consumed:0.7343192100524902s
2023-11-03 00:05:33,287 MainThread INFO: Total Frames:19500s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                40702.62655
Running_Training_Average_Rewards  2735.43825

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [2100 2100 2100 2100 2100 2100 2100 2100 2100 2100]
2023-11-03 00:05:34,222 MainThread INFO: EPOCH:13
2023-11-03 00:05:34,222 MainThread INFO: Time Consumed:0.9341106414794922s
2023-11-03 00:05:34,223 MainThread INFO: Total Frames:21000s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                5607.50552
Running_Training_Average_Rewards  1897.93020

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [2250 2250 2250 2250 2250 2250 2250 2250 2250 2250]
2023-11-03 00:05:35,122 MainThread INFO: EPOCH:14
2023-11-03 00:05:35,122 MainThread INFO: Time Consumed:0.8970568180084229s
2023-11-03 00:05:35,122 MainThread INFO: Total Frames:22500s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                1634.63538
Running_Training_Average_Rewards  1598.15891

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [2400 2400 2400 2400 2400 2400 2400 2400 2400 2400]
2023-11-03 00:05:35,897 MainThread INFO: EPOCH:15
2023-11-03 00:05:35,898 MainThread INFO: Time Consumed:0.773639440536499s
2023-11-03 00:05:35,898 MainThread INFO: Total Frames:24000s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                2019.72824
Running_Training_Average_Rewards  308.72897

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [2550 2550 2550 2550 2550 2550 2550 2550 2550 2550]
2023-11-03 00:05:36,670 MainThread INFO: EPOCH:16
2023-11-03 00:05:36,670 MainThread INFO: Time Consumed:0.7703762054443359s
2023-11-03 00:05:36,670 MainThread INFO: Total Frames:25500s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                10782.96983
Running_Training_Average_Rewards  481.24445

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [2700 2700 2700 2700 2700 2700 2700 2700 2700 2700]
2023-11-03 00:05:37,622 MainThread INFO: EPOCH:17
2023-11-03 00:05:37,622 MainThread INFO: Time Consumed:0.9504492282867432s
2023-11-03 00:05:37,623 MainThread INFO: Total Frames:27000s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                4793.11647
Running_Training_Average_Rewards  586.52715

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
replay_buffer._size: [2850 2850 2850 2850 2850 2850 2850 2850 2850 2850]
2023-11-03 00:05:38,716 MainThread INFO: EPOCH:18
2023-11-03 00:05:38,717 MainThread INFO: Time Consumed:1.0924348831176758s
2023-11-03 00:05:38,717 MainThread INFO: Total Frames:28500s
--------------------------------  -----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                32158.64023
Running_Training_Average_Rewards  1591.15755

Name                              Mean         Std  Max  Min
--------------------------------  -----------  ---  ---  ---
replay_buffer._size: [3000 3000 3000 3000 3000 3000 3000 3000 3000 3000]
2023-11-03 00:05:39,716 MainThread INFO: EPOCH:19
2023-11-03 00:05:39,716 MainThread INFO: Time Consumed:0.9972882270812988s
2023-11-03 00:05:39,716 MainThread INFO: Total Frames:30000s
--------------------------------  ----------  ---  ---  ---
Name                              Value
Train_Epoch_Reward                3181.18398
Running_Training_Average_Rewards  1337.76469

Name                              Mean        Std  Max  Min
--------------------------------  ----------  ---  ---  ---
2023-11-03 00:05:39,718 MainThread INFO: Finished Pretrain
  0%|          | 0/1000 [00:00<?, ?it/s]sample: [2, 8, 9, 4, 6, 7, 0, 1, 5, 3]
replay_buffer._size: [3150 3150 3150 3150 3150 3150 3150 3150 3150 3150]
gen_weight_change tensor(-4.9262, device='cuda:0')
train_time 3.9723775386810303
snapshot at best
2023-11-03 00:05:47,493 MainThread INFO: EPOCH:0
2023-11-03 00:05:47,493 MainThread INFO: Time Consumed:7.486409425735474s
2023-11-03 00:05:47,494 MainThread INFO: Total Frames:31500s
/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/algo/rl_algo.py:401: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  value = torch.sum((self.mask_buffer["Policy"][each_task][0] == 0).nonzero().squeeze()).item()
  0%|          | 1/1000 [00:09<2:39:07,  9.56s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1213.77109
Train_Epoch_Reward                    23606.59132
Running_Training_Average_Rewards      1964.88052
Explore_Time                          0.67685
Train___Time                          3.97238
Eval____Time                          1.59017
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.15514
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.49572
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.03877
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.63677
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16867
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.99602
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.26878
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12388.41259
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.56119
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.38067
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.85340      0.51380    9.62531     8.15912
alpha_0                               0.99910      0.00042    0.99970     0.99850
alpha_1                               0.99910      0.00042    0.99970     0.99850
alpha_2                               0.99910      0.00042    0.99970     0.99850
alpha_3                               0.99910      0.00042    0.99970     0.99850
alpha_4                               0.99910      0.00042    0.99970     0.99850
alpha_5                               0.99910      0.00042    0.99970     0.99850
alpha_6                               0.99910      0.00042    0.99970     0.99850
alpha_7                               0.99910      0.00042    0.99970     0.99850
alpha_8                               0.99910      0.00042    0.99970     0.99850
alpha_9                               0.99910      0.00042    0.99970     0.99850
Alpha_loss                            -0.00401     0.00284    -0.00000    -0.00802
Training/policy_loss                  -2.67580     0.00957    -2.65730    -2.68411
Training/qf1_loss                     1740.25872   218.96255  2011.42908  1495.91833
Training/qf2_loss                     1740.22598   218.96572  2011.38806  1495.88904
Training/pf_norm                      0.26978      0.04120    0.35016     0.23621
Training/qf1_norm                     7.45328      0.53798    8.38827     6.86716
Training/qf2_norm                     8.76461      0.79116    9.33045     7.21493
log_std/mean                          -0.00151     0.00098    -0.00007    -0.00291
log_std/std                           0.00104      0.00018    0.00126     0.00084
log_std/max                           0.00027      0.00095    0.00189     -0.00070
log_std/min                           -0.00434     0.00134    -0.00252    -0.00597
log_probs/mean                        -2.67806     0.00978    -2.65910    -2.68567
log_probs/std                         0.43141      0.01100    0.44138     0.41057
log_probs/max                         -1.31385     0.03807    -1.27809    -1.37643
log_probs/min                         -4.07355     0.42803    -3.63346    -4.79812
mean/mean                             0.00031      0.00021    0.00059     -0.00001
mean/std                              0.00155      0.00013    0.00166     0.00129
mean/max                              0.00338      0.00035    0.00375     0.00290
mean/min                              -0.00259     0.00064    -0.00176    -0.00373
------------------------------------  -----------  ---------  ----------  ----------
snapshot at 0
history save at ./log/testing_must_mtsac/mt10/13/model
sample: [7, 1, 3, 9, 0, 4, 6, 8, 5, 2]
replay_buffer._size: [3450 3450 3450 3450 3450 3450 3450 3450 3450 3450]
gen_weight_change tensor(-4.9673, device='cuda:0')
train_time 0.7377092838287354
2023-11-03 00:05:50,063 MainThread INFO: EPOCH:1
2023-11-03 00:05:50,063 MainThread INFO: Time Consumed:0.7477092742919922s
2023-11-03 00:05:50,064 MainThread INFO: Total Frames:33000s
  0%|          | 2/1000 [00:10<1:14:16,  4.47s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1213.35975
Train_Epoch_Reward                    27099.69444
Running_Training_Average_Rewards      1796.24899
Explore_Time                          0.00347
Train___Time                          0.73771
Eval____Time                          0.00264
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.26847
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.49572
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.03877
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.63677
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16867
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.99602
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.26878
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12388.41259
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.56119
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.38067
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.85763      0.76054    10.95434    8.82600
alpha_0                               0.99760      0.00042    0.99820     0.99701
alpha_1                               0.99760      0.00042    0.99820     0.99701
alpha_2                               0.99760      0.00042    0.99820     0.99700
alpha_3                               0.99760      0.00042    0.99820     0.99700
alpha_4                               0.99760      0.00042    0.99820     0.99700
alpha_5                               0.99760      0.00042    0.99820     0.99700
alpha_6                               0.99760      0.00042    0.99820     0.99700
alpha_7                               0.99760      0.00042    0.99820     0.99700
alpha_8                               0.99760      0.00042    0.99820     0.99700
alpha_9                               0.99760      0.00042    0.99820     0.99700
Alpha_loss                            -0.01405     0.00283    -0.01006    -0.01808
Training/policy_loss                  -2.68777     0.01278    -2.66492    -2.70341
Training/qf1_loss                     1997.09932   204.27507  2296.52368  1771.86023
Training/qf2_loss                     1997.08569   204.27382  2296.50806  1771.84924
Training/pf_norm                      0.27652      0.01951    0.29842     0.24967
Training/qf1_norm                     8.16847      0.51767    8.95021     7.50185
Training/qf2_norm                     7.77979      0.49407    8.53908     7.20009
log_std/mean                          -0.00488     0.00095    -0.00355    -0.00623
log_std/std                           0.00169      0.00022    0.00201     0.00139
log_std/max                           -0.00163     0.00045    -0.00099    -0.00227
log_std/min                           -0.00829     0.00122    -0.00657    -0.01004
log_probs/mean                        -2.69229     0.01246    -2.66983    -2.70707
log_probs/std                         0.42773      0.01249    0.44813     0.41376
log_probs/max                         -1.31735     0.08230    -1.23841    -1.44848
log_probs/min                         -4.18951     0.76999    -3.59459    -5.70178
mean/mean                             0.00069      0.00018    0.00093     0.00045
mean/std                              0.00184      0.00010    0.00193     0.00168
mean/max                              0.00348      0.00029    0.00383     0.00306
mean/min                              -0.00395     0.00015    -0.00370    -0.00415
------------------------------------  -----------  ---------  ----------  ----------
sample: [7, 6, 1, 8, 2, 4, 3, 0, 9, 5]
replay_buffer._size: [3600 3600 3600 3600 3600 3600 3600 3600 3600 3600]
gen_weight_change tensor(-4.9888, device='cuda:0')
train_time 1.4441850185394287
snapshot at best
2023-11-03 00:05:52,411 MainThread INFO: EPOCH:2
2023-11-03 00:05:52,411 MainThread INFO: Time Consumed:2.1527087688446045s
2023-11-03 00:05:52,412 MainThread INFO: Total Frames:34500s
  0%|          | 3/1000 [00:12<57:52,  3.48s/it]  ------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1214.09516
Train_Epoch_Reward                    38550.48365
Running_Training_Average_Rewards      2975.22565
Explore_Time                          0.01516
Train___Time                          1.44419
Eval____Time                          0.22982
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.91436
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.49572
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.03877
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.63677
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16867
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.99602
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.26878
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12388.41259
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.56119
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.38067
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.09079     0.32797    10.59820    9.63667
alpha_0                               0.99611      0.00042    0.99671     0.99551
alpha_1                               0.99611      0.00042    0.99671     0.99551
alpha_2                               0.99611      0.00042    0.99670     0.99551
alpha_3                               0.99611      0.00042    0.99670     0.99551
alpha_4                               0.99611      0.00042    0.99671     0.99551
alpha_5                               0.99611      0.00042    0.99670     0.99551
alpha_6                               0.99611      0.00042    0.99671     0.99551
alpha_7                               0.99611      0.00042    0.99670     0.99551
alpha_8                               0.99611      0.00042    0.99671     0.99551
alpha_9                               0.99611      0.00042    0.99670     0.99551
Alpha_loss                            -0.02407     0.00282    -0.02010    -0.02809
Training/policy_loss                  -2.67911     0.00881    -2.66453    -2.69213
Training/qf1_loss                     2203.29766   121.29113  2380.59912  2049.21265
Training/qf2_loss                     2203.28784   121.29169  2380.59058  2049.20093
Training/pf_norm                      0.27276      0.03864    0.34245     0.23519
Training/qf1_norm                     8.50066      0.30347    8.95086     8.05604
Training/qf2_norm                     8.28506      0.33860    8.80179     7.83384
log_std/mean                          -0.00837     0.00103    -0.00693    -0.00985
log_std/std                           0.00253      0.00025    0.00289     0.00218
log_std/max                           -0.00327     0.00049    -0.00260    -0.00399
log_std/min                           -0.01273     0.00129    -0.01092    -0.01458
log_probs/mean                        -2.68582     0.00844    -2.67162    -2.69801
log_probs/std                         0.42702      0.01044    0.44269     0.41575
log_probs/max                         -1.33091     0.13212    -1.20810    -1.55381
log_probs/min                         -4.42524     0.68401    -3.61577    -5.48384
mean/mean                             0.00110      0.00007    0.00120     0.00101
mean/std                              0.00192      0.00002    0.00195     0.00191
mean/max                              0.00419      0.00028    0.00459     0.00384
mean/min                              -0.00322     0.00018    -0.00301    -0.00346
------------------------------------  -----------  ---------  ----------  ----------
sample: [8, 2, 5, 7, 4, 3, 9, 1, 6, 0]
replay_buffer._size: [3750 3750 3750 3750 3750 3750 3750 3750 3750 3750]
gen_weight_change tensor(-5.0011, device='cuda:0')
train_time 1.643641710281372
snapshot at best
2023-11-03 00:05:54,682 MainThread INFO: EPOCH:3
2023-11-03 00:05:54,682 MainThread INFO: Time Consumed:2.0845415592193604s
2023-11-03 00:05:54,683 MainThread INFO: Total Frames:36000s
  0%|          | 4/1000 [00:15<49:51,  3.00s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1228.74333
Train_Epoch_Reward                    7927.79627
Running_Training_Average_Rewards      2452.59915
Explore_Time                          0.02697
Train___Time                          1.64364
Eval____Time                          0.00786
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.11118
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.39075
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.18275
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.60113
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.39930
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.06732
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.35354
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12537.41279
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.49434
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.37916
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.28301      0.70359    10.07115    8.20176
alpha_0                               0.99462      0.00042    0.99521     0.99402
alpha_1                               0.99462      0.00042    0.99521     0.99402
alpha_2                               0.99461      0.00042    0.99521     0.99402
alpha_3                               0.99461      0.00042    0.99521     0.99402
alpha_4                               0.99461      0.00042    0.99521     0.99402
alpha_5                               0.99461      0.00042    0.99521     0.99402
alpha_6                               0.99461      0.00042    0.99521     0.99402
alpha_7                               0.99461      0.00042    0.99521     0.99401
alpha_8                               0.99461      0.00042    0.99521     0.99402
alpha_9                               0.99461      0.00042    0.99521     0.99402
Alpha_loss                            -0.03415     0.00286    -0.03011    -0.03820
Training/policy_loss                  -2.68515     0.00787    -2.67788    -2.69764
Training/qf1_loss                     1871.64019   429.72039  2423.99561  1185.19360
Training/qf2_loss                     1871.63611   429.71941  2423.98999  1185.19214
Training/pf_norm                      0.25821      0.01709    0.28981     0.23910
Training/qf1_norm                     8.27926      0.47668    8.88732     7.65409
Training/qf2_norm                     8.13661      0.45532    8.77299     7.51973
log_std/mean                          -0.01223     0.00115    -0.01063    -0.01387
log_std/std                           0.00345      0.00027    0.00384     0.00307
log_std/max                           -0.00520     0.00059    -0.00438    -0.00605
log_std/min                           -0.01761     0.00151    -0.01555    -0.01983
log_probs/mean                        -2.69404     0.00814    -2.68635    -2.70661
log_probs/std                         0.40634      0.01101    0.42487     0.39347
log_probs/max                         -1.44417     0.05209    -1.38369    -1.51571
log_probs/min                         -4.44104     0.29968    -4.12996    -4.88223
mean/mean                             0.00127      0.00002    0.00131     0.00124
mean/std                              0.00187      0.00004    0.00193     0.00181
mean/max                              0.00502      0.00016    0.00520     0.00474
mean/min                              -0.00225     0.00034    -0.00184    -0.00276
------------------------------------  -----------  ---------  ----------  ----------
sample: [1, 3, 0, 6, 4, 2, 5, 9, 8, 7]
replay_buffer._size: [3900 3900 3900 3900 3900 3900 3900 3900 3900 3900]
gen_weight_change tensor(-5.0084, device='cuda:0')
train_time 1.4509921073913574
snapshot at best
2023-11-03 00:05:57,005 MainThread INFO: EPOCH:4
2023-11-03 00:05:57,005 MainThread INFO: Time Consumed:1.8589491844177246s
2023-11-03 00:05:57,005 MainThread INFO: Total Frames:37500s
  0%|          | 5/1000 [00:17<45:38,  2.75s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1229.83252
Train_Epoch_Reward                    19352.40244
Running_Training_Average_Rewards      2194.35608
Explore_Time                          0.01193
Train___Time                          1.45099
Eval____Time                          0.00285
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.04972
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.28964
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.27314
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.61790
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.53205
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.11047
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.34238
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12549.39895
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.40937
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.44909
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.29750      0.57653    10.01275    8.50086
alpha_0                               0.99313      0.00042    0.99372     0.99253
alpha_1                               0.99313      0.00042    0.99372     0.99253
alpha_2                               0.99312      0.00042    0.99372     0.99252
alpha_3                               0.99312      0.00042    0.99372     0.99253
alpha_4                               0.99312      0.00042    0.99372     0.99252
alpha_5                               0.99312      0.00042    0.99372     0.99252
alpha_6                               0.99312      0.00042    0.99372     0.99252
alpha_7                               0.99312      0.00042    0.99372     0.99252
alpha_8                               0.99312      0.00042    0.99372     0.99253
alpha_9                               0.99312      0.00042    0.99372     0.99252
Alpha_loss                            -0.04425     0.00284    -0.04012    -0.04821
Training/policy_loss                  -2.69247     0.01361    -2.67625    -2.71617
Training/qf1_loss                     1848.87852   215.72689  2223.13428  1564.69287
Training/qf2_loss                     1848.87776   215.72714  2223.13452  1564.69324
Training/pf_norm                      0.23220      0.03481    0.27710     0.19320
Training/qf1_norm                     8.74009      0.43785    9.27654     8.26409
Training/qf2_norm                     8.74376      0.41811    9.25356     8.24652
log_std/mean                          -0.01646     0.00123    -0.01472    -0.01821
log_std/std                           0.00444      0.00028    0.00484     0.00404
log_std/max                           -0.00746     0.00069    -0.00651    -0.00846
log_std/min                           -0.02343     0.00171    -0.02101    -0.02585
log_probs/mean                        -2.70339     0.01367    -2.68632    -2.72687
log_probs/std                         0.39750      0.00656    0.40889     0.39052
log_probs/max                         -1.39312     0.05985    -1.29044    -1.47096
log_probs/min                         -4.21978     0.41798    -3.79291    -4.97939
mean/mean                             0.00121      0.00005    0.00127     0.00113
mean/std                              0.00161      0.00009    0.00173     0.00149
mean/max                              0.00509      0.00011    0.00521     0.00495
mean/min                              -0.00138     0.00012    -0.00122    -0.00152
------------------------------------  -----------  ---------  ----------  ----------
sample: [0, 2, 3, 4, 8, 1, 5, 6, 7, 9]
replay_buffer._size: [4050 4050 4050 4050 4050 4050 4050 4050 4050 4050]
gen_weight_change tensor(-5.0128, device='cuda:0')
train_time 1.3613224029541016
2023-11-03 00:05:58,913 MainThread INFO: EPOCH:5
2023-11-03 00:05:58,914 MainThread INFO: Time Consumed:1.4599382877349854s
2023-11-03 00:05:58,914 MainThread INFO: Total Frames:39000s
  1%|          | 6/1000 [00:19<41:11,  2.49s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1217.15015
Train_Epoch_Reward                    11248.22512
Running_Training_Average_Rewards      1284.28079
Explore_Time                          0.02232
Train___Time                          1.36132
Eval____Time                          0.07237
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.08502
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.29289
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.29947
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.67473
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.56302
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.09733
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.33576
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12418.80800
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.41343
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.54486
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.84716      0.85750    10.93274    9.00796
alpha_0                               0.99164      0.00042    0.99223     0.99105
alpha_1                               0.99164      0.00042    0.99223     0.99104
alpha_2                               0.99163      0.00042    0.99223     0.99104
alpha_3                               0.99163      0.00042    0.99223     0.99104
alpha_4                               0.99163      0.00042    0.99223     0.99104
alpha_5                               0.99163      0.00042    0.99223     0.99104
alpha_6                               0.99163      0.00042    0.99223     0.99104
alpha_7                               0.99163      0.00042    0.99223     0.99104
alpha_8                               0.99164      0.00042    0.99223     0.99104
alpha_9                               0.99163      0.00042    0.99223     0.99103
Alpha_loss                            -0.05420     0.00287    -0.05008    -0.05825
Training/policy_loss                  -2.67753     0.00760    -2.66453    -2.68646
Training/qf1_loss                     2251.27725   454.84994  2821.99976  1645.46448
Training/qf2_loss                     2251.27881   454.84995  2822.00146  1645.46484
Training/pf_norm                      0.25818      0.01184    0.27912     0.24788
Training/qf1_norm                     9.72700      0.70520    10.63568    9.05242
Training/qf2_norm                     9.88424      0.76243    10.91204    9.09864
log_std/mean                          -0.02104     0.00137    -0.01912    -0.02299
log_std/std                           0.00547      0.00030    0.00590     0.00504
log_std/max                           -0.01011     0.00082    -0.00896    -0.01128
log_std/min                           -0.02978     0.00191    -0.02710    -0.03250
log_probs/mean                        -2.69005     0.00790    -2.67630    -2.69875
log_probs/std                         0.39461      0.00268    0.39770     0.39062
log_probs/max                         -1.41219     0.07605    -1.30617    -1.51834
log_probs/min                         -4.04862     0.36862    -3.69291    -4.73679
mean/mean                             0.00078      0.00022    0.00106     0.00046
mean/std                              0.00133      0.00004    0.00140     0.00129
mean/max                              0.00435      0.00026    0.00477     0.00406
mean/min                              -0.00188     0.00017    -0.00162    -0.00210
------------------------------------  -----------  ---------  ----------  ----------
sample: [6, 7, 0, 1, 8, 4, 5, 3, 2, 9]
replay_buffer._size: [4200 4200 4200 4200 4200 4200 4200 4200 4200 4200]
gen_weight_change tensor(-5.0154, device='cuda:0')
train_time 1.8075826168060303
2023-11-03 00:06:01,113 MainThread INFO: EPOCH:6
2023-11-03 00:06:01,114 MainThread INFO: Time Consumed:1.9642975330352783s
2023-11-03 00:06:01,114 MainThread INFO: Total Frames:40500s
  1%|          | 7/1000 [00:21<39:17,  2.37s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1197.43192
Train_Epoch_Reward                    14150.38611
Running_Training_Average_Rewards      1491.70046
Explore_Time                          0.01511
Train___Time                          1.80758
Eval____Time                          0.13775
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.60188
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.39067
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.23801
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.73614
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.48809
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.04881
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.27001
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12225.24308
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.50830
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.64201
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.11046     0.44078    10.60755    9.42083
alpha_0                               0.99015      0.00042    0.99075     0.98956
alpha_1                               0.99015      0.00042    0.99074     0.98956
alpha_2                               0.99014      0.00042    0.99074     0.98955
alpha_3                               0.99014      0.00042    0.99074     0.98955
alpha_4                               0.99014      0.00042    0.99074     0.98955
alpha_5                               0.99014      0.00042    0.99074     0.98955
alpha_6                               0.99014      0.00042    0.99074     0.98955
alpha_7                               0.99015      0.00042    0.99074     0.98955
alpha_8                               0.99015      0.00042    0.99074     0.98955
alpha_9                               0.99014      0.00042    0.99073     0.98954
Alpha_loss                            -0.06439     0.00274    -0.06052    -0.06819
Training/policy_loss                  -2.69198     0.01313    -2.66854    -2.70878
Training/qf1_loss                     2133.92998   206.45015  2446.24878  1844.56799
Training/qf2_loss                     2133.93408   206.45080  2446.25391  1844.57031
Training/pf_norm                      0.23605      0.03845    0.28037     0.17483
Training/qf1_norm                     10.60948     0.41794    11.23940    10.09171
Training/qf2_norm                     10.84070     0.43673    11.47939    10.20280
log_std/mean                          -0.02611     0.00150    -0.02402    -0.02825
log_std/std                           0.00656      0.00031    0.00700     0.00612
log_std/max                           -0.01312     0.00088    -0.01187    -0.01438
log_std/min                           -0.03679     0.00202    -0.03393    -0.03965
log_probs/mean                        -2.70603     0.01295    -2.68283    -2.72248
log_probs/std                         0.38152      0.01077    0.39729     0.36437
log_probs/max                         -1.49571     0.10326    -1.37713    -1.65180
log_probs/min                         -4.89614     0.65780    -4.09130    -5.95894
mean/mean                             -0.00012     0.00028    0.00027     -0.00050
mean/std                              0.00132      0.00004    0.00139     0.00128
mean/max                              0.00372      0.00020    0.00396     0.00348
mean/min                              -0.00253     0.00023    -0.00222    -0.00282
------------------------------------  -----------  ---------  ----------  ----------
sample: [9, 5, 8, 7, 3, 0, 4, 1, 6, 2]
replay_buffer._size: [4350 4350 4350 4350 4350 4350 4350 4350 4350 4350]
gen_weight_change tensor(-5.0170, device='cuda:0')
train_time 2.160315752029419
2023-11-03 00:06:03,691 MainThread INFO: EPOCH:7
2023-11-03 00:06:03,691 MainThread INFO: Time Consumed:2.2013964653015137s
2023-11-03 00:06:03,692 MainThread INFO: Total Frames:42000s
  1%|          | 8/1000 [00:24<40:48,  2.47s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1182.00733
Train_Epoch_Reward                    25902.70592
Running_Training_Average_Rewards      1710.04391
Explore_Time                          0.03494
Train___Time                          2.16032
Eval____Time                          0.00235
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.77243
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.55401
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.14081
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.79100
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.35405
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.94583
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.12879
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12071.05022
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.63526
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.65479
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.19653     0.60425    10.93566    9.55477
alpha_0                               0.98867      0.00042    0.98926     0.98808
alpha_1                               0.98867      0.00042    0.98926     0.98807
alpha_2                               0.98866      0.00042    0.98925     0.98806
alpha_3                               0.98866      0.00042    0.98925     0.98806
alpha_4                               0.98866      0.00042    0.98925     0.98806
alpha_5                               0.98866      0.00042    0.98925     0.98806
alpha_6                               0.98866      0.00042    0.98925     0.98806
alpha_7                               0.98866      0.00042    0.98926     0.98807
alpha_8                               0.98866      0.00042    0.98926     0.98807
alpha_9                               0.98865      0.00042    0.98925     0.98806
Alpha_loss                            -0.07448     0.00289    -0.07041    -0.07855
Training/policy_loss                  -2.69248     0.00804    -2.67957    -2.70342
Training/qf1_loss                     2276.26511   490.59894  2974.84644  1857.04675
Training/qf2_loss                     2276.26365   490.59876  2974.84229  1857.04968
Training/pf_norm                      0.24760      0.00913    0.26531     0.24030
Training/qf1_norm                     11.34079     0.66677    12.35461    10.69968
Training/qf2_norm                     11.87743     0.72626    13.01921    11.10230
log_std/mean                          -0.03175     0.00169    -0.02939    -0.03418
log_std/std                           0.00769      0.00033    0.00816     0.00723
log_std/max                           -0.01650     0.00102    -0.01509    -0.01794
log_std/min                           -0.04430     0.00220    -0.04122    -0.04746
log_probs/mean                        -2.70763     0.00823    -2.69458    -2.71889
log_probs/std                         0.36783      0.00778    0.37841     0.35836
log_probs/max                         -1.56871     0.07914    -1.47761    -1.71396
log_probs/min                         -4.62698     0.74110    -4.07019    -6.00125
mean/mean                             -0.00109     0.00027    -0.00068    -0.00146
mean/std                              0.00159      0.00010    0.00173     0.00144
mean/max                              0.00337      0.00008    0.00351     0.00329
mean/min                              -0.00368     0.00051    -0.00296    -0.00444
------------------------------------  -----------  ---------  ----------  ----------
sample: [8, 1, 3, 4, 7, 2, 9, 6, 5, 0]
replay_buffer._size: [4500 4500 4500 4500 4500 4500 4500 4500 4500 4500]
gen_weight_change tensor(-5.0180, device='cuda:0')
train_time 2.526323080062866
2023-11-03 00:06:06,495 MainThread INFO: EPOCH:8
2023-11-03 00:06:06,495 MainThread INFO: Time Consumed:2.536421537399292s
2023-11-03 00:06:06,495 MainThread INFO: Total Frames:43500s
  1%|          | 9/1000 [00:26<42:24,  2.57s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1172.75908
Train_Epoch_Reward                    12996.76218
Running_Training_Average_Rewards      1768.32847
Explore_Time                          0.00378
Train___Time                          2.52632
Eval____Time                          0.00240
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.88551
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.71657
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.06078
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.81483
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.22201
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.82204
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.03304
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11976.50573
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.74877
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.61141
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.26489     0.77108    11.26908    9.26283
alpha_0                               0.98719      0.00042    0.98778     0.98659
alpha_1                               0.98718      0.00042    0.98778     0.98659
alpha_2                               0.98717      0.00042    0.98777     0.98658
alpha_3                               0.98717      0.00042    0.98777     0.98658
alpha_4                               0.98717      0.00042    0.98777     0.98658
alpha_5                               0.98717      0.00042    0.98777     0.98658
alpha_6                               0.98717      0.00042    0.98777     0.98658
alpha_7                               0.98718      0.00042    0.98777     0.98659
alpha_8                               0.98718      0.00042    0.98777     0.98659
alpha_9                               0.98717      0.00042    0.98776     0.98657
Alpha_loss                            -0.08449     0.00288    -0.08036    -0.08864
Training/policy_loss                  -2.68669     0.00722    -2.67867    -2.69579
Training/qf1_loss                     2286.39099   479.94230  2810.91479  1538.10901
Training/qf2_loss                     2286.38379   479.94257  2810.90625  1538.10278
Training/pf_norm                      0.21841      0.02239    0.24733     0.18501
Training/qf1_norm                     12.17995     0.76383    13.20312    11.17268
Training/qf2_norm                     12.92910     0.79954    14.03060    11.82574
log_std/mean                          -0.03809     0.00189    -0.03545    -0.04079
log_std/std                           0.00888      0.00034    0.00936     0.00840
log_std/max                           -0.02041     0.00122    -0.01878    -0.02223
log_std/min                           -0.05245     0.00239    -0.04905    -0.05584
log_probs/mean                        -2.70244     0.00736    -2.69408    -2.71181
log_probs/std                         0.35725      0.00490    0.36311     0.35002
log_probs/max                         -1.62103     0.06716    -1.51326    -1.72014
log_probs/min                         -4.10226     0.24993    -3.71475    -4.40654
mean/mean                             -0.00192     0.00017    -0.00164    -0.00212
mean/std                              0.00200      0.00013    0.00218     0.00180
mean/max                              0.00375      0.00015    0.00397     0.00353
mean/min                              -0.00573     0.00064    -0.00483    -0.00664
------------------------------------  -----------  ---------  ----------  ----------
sample: [4, 1, 0, 5, 7, 3, 9, 8, 2, 6]
replay_buffer._size: [4650 4650 4650 4650 4650 4650 4650 4650 4650 4650]
gen_weight_change tensor(-5.0186, device='cuda:0')
train_time 2.6618552207946777
2023-11-03 00:06:09,394 MainThread INFO: EPOCH:9
2023-11-03 00:06:09,395 MainThread INFO: Time Consumed:2.672391653060913s
2023-11-03 00:06:09,395 MainThread INFO: Total Frames:45000s
  1%|          | 10/1000 [00:29<44:11,  2.68s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1171.75963
Train_Epoch_Reward                    5508.59963
Running_Training_Average_Rewards      1480.26892
Explore_Time                          0.00445
Train___Time                          2.66186
Eval____Time                          0.00236
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -53.08408
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.84520
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.01615
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.81628
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.07520
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.66053
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.93224
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11970.33771
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.83015
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.48160
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.90722      0.46354    9.74909     8.45481
alpha_0                               0.98571      0.00042    0.98630     0.98511
alpha_1                               0.98570      0.00042    0.98630     0.98511
alpha_2                               0.98569      0.00042    0.98629     0.98510
alpha_3                               0.98569      0.00042    0.98628     0.98510
alpha_4                               0.98569      0.00042    0.98629     0.98510
alpha_5                               0.98569      0.00042    0.98629     0.98510
alpha_6                               0.98569      0.00042    0.98628     0.98510
alpha_7                               0.98570      0.00042    0.98629     0.98511
alpha_8                               0.98570      0.00042    0.98629     0.98510
alpha_9                               0.98568      0.00042    0.98628     0.98509
Alpha_loss                            -0.09467     0.00296    -0.09048    -0.09891
Training/policy_loss                  -2.69446     0.00926    -2.68270    -2.70849
Training/qf1_loss                     1447.01790   132.38908  1631.38818  1247.79773
Training/qf2_loss                     1447.00168   132.39037  1631.36853  1247.77673
Training/pf_norm                      0.19808      0.00543    0.20696     0.19040
Training/qf1_norm                     11.52255     0.60712    12.60832    10.77886
Training/qf2_norm                     12.37613     0.70748    13.60649    11.43363
log_std/mean                          -0.04509     0.00207    -0.04218    -0.04803
log_std/std                           0.01009      0.00035    0.01058     0.00960
log_std/max                           -0.02504     0.00138    -0.02313    -0.02699
log_std/min                           -0.06125     0.00262    -0.05756    -0.06497
log_probs/mean                        -2.71050     0.00936    -2.69859    -2.72466
log_probs/std                         0.34558      0.00494    0.35292     0.33970
log_probs/max                         -1.62511     0.06494    -1.55032    -1.73880
log_probs/min                         -4.29543     0.39515    -3.78176    -4.87289
mean/mean                             -0.00225     0.00007    -0.00215    -0.00234
mean/std                              0.00263      0.00026    0.00303     0.00229
mean/max                              0.00470      0.00049    0.00550     0.00415
mean/min                              -0.00806     0.00080    -0.00701    -0.00925
------------------------------------  -----------  ---------  ----------  ----------
sample: [7, 8, 6, 9, 5, 1, 3, 4, 0, 2]
replay_buffer._size: [4800 4800 4800 4800 4800 4800 4800 4800 4800 4800]
gen_weight_change tensor(-5.0189, device='cuda:0')
train_time 2.7212023735046387
2023-11-03 00:06:12,417 MainThread INFO: EPOCH:10
2023-11-03 00:06:12,418 MainThread INFO: Time Consumed:2.732489585876465s
2023-11-03 00:06:12,418 MainThread INFO: Total Frames:46500s
  1%|          | 11/1000 [00:32<45:21,  2.75s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1181.91335
Train_Epoch_Reward                    9087.69783
Running_Training_Average_Rewards      919.76865
Explore_Time                          0.00487
Train___Time                          2.72120
Eval____Time                          0.00265
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.45889
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.94054
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.04667
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.73054
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.97389
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.55950
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.85459
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12066.84554
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.88931
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.25814
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.97314      0.92684    11.44919    8.86729
alpha_0                               0.98423      0.00042    0.98482     0.98364
alpha_1                               0.98423      0.00042    0.98482     0.98363
alpha_2                               0.98421      0.00042    0.98481     0.98362
alpha_3                               0.98421      0.00042    0.98480     0.98362
alpha_4                               0.98422      0.00042    0.98481     0.98363
alpha_5                               0.98421      0.00042    0.98480     0.98362
alpha_6                               0.98421      0.00042    0.98480     0.98362
alpha_7                               0.98422      0.00042    0.98481     0.98363
alpha_8                               0.98422      0.00042    0.98481     0.98362
alpha_9                               0.98420      0.00042    0.98480     0.98361
Alpha_loss                            -0.10478     0.00286    -0.10064    -0.10892
Training/policy_loss                  -2.69682     0.01109    -2.68708    -2.71539
Training/qf1_loss                     2034.75122   433.50180  2699.28442  1521.47546
Training/qf2_loss                     2034.71821   433.50479  2699.25586  1521.44080
Training/pf_norm                      0.20699      0.04074    0.26634     0.15633
Training/qf1_norm                     13.58181     0.85183    14.87701    12.50550
Training/qf2_norm                     14.77970     0.87381    16.10993    13.64895
log_std/mean                          -0.05274     0.00228    -0.04956    -0.05601
log_std/std                           0.01137      0.00038    0.01192     0.01084
log_std/max                           -0.03017     0.00157    -0.02801    -0.03248
log_std/min                           -0.07080     0.00283    -0.06685    -0.07481
log_probs/mean                        -2.71252     0.01127    -2.70263    -2.73155
log_probs/std                         0.33381      0.00624    0.34255     0.32448
log_probs/max                         -1.69530     0.04595    -1.62474    -1.75952
log_probs/min                         -4.25785     0.35076    -3.89646    -4.90471
mean/mean                             -0.00242     0.00004    -0.00236    -0.00248
mean/std                              0.00368      0.00028    0.00404     0.00326
mean/max                              0.00649      0.00039    0.00695     0.00591
mean/min                              -0.01098     0.00080    -0.00977    -0.01200
------------------------------------  -----------  ---------  ----------  ----------
sample: [5, 9, 6, 1, 3, 7, 8, 4, 0, 2]
replay_buffer._size: [4950 4950 4950 4950 4950 4950 4950 4950 4950 4950]
gen_weight_change tensor(-5.0191, device='cuda:0')
train_time 2.809957265853882
2023-11-03 00:06:15,502 MainThread INFO: EPOCH:11
2023-11-03 00:06:15,502 MainThread INFO: Time Consumed:2.820880889892578s
2023-11-03 00:06:15,502 MainThread INFO: Total Frames:48000s
  1%|          | 12/1000 [00:35<47:44,  2.90s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1212.67417
Train_Epoch_Reward                    11063.03687
Running_Training_Average_Rewards      855.31114
Explore_Time                          0.00473
Train___Time                          2.80996
Eval____Time                          0.00243
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.76824
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -43.04037
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.22464
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.51778
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.99872
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.54968
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.87760
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12376.61337
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.98870
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.90591
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.02826     0.95455    11.48946    8.76321
alpha_0                               0.98275      0.00042    0.98334     0.98216
alpha_1                               0.98275      0.00042    0.98334     0.98216
alpha_2                               0.98274      0.00042    0.98333     0.98215
alpha_3                               0.98273      0.00042    0.98332     0.98214
alpha_4                               0.98274      0.00042    0.98333     0.98215
alpha_5                               0.98274      0.00042    0.98333     0.98214
alpha_6                               0.98273      0.00042    0.98332     0.98214
alpha_7                               0.98275      0.00042    0.98334     0.98216
alpha_8                               0.98274      0.00042    0.98333     0.98214
alpha_9                               0.98273      0.00042    0.98332     0.98214
Alpha_loss                            -0.11499     0.00286    -0.11082    -0.11912
Training/policy_loss                  -2.70501     0.00889    -2.69526    -2.71904
Training/qf1_loss                     1961.41130   427.60384  2429.01807  1356.57446
Training/qf2_loss                     1961.35422   427.59270  2428.94531  1356.52795
Training/pf_norm                      0.17681      0.02867    0.22210     0.13926
Training/qf1_norm                     14.64879     1.49810    16.94310    12.63558
Training/qf2_norm                     16.18900     1.71445    18.78621    13.90504
log_std/mean                          -0.06113     0.00246    -0.05767    -0.06464
log_std/std                           0.01274      0.00039    0.01330     0.01220
log_std/max                           -0.03606     0.00174    -0.03357    -0.03854
log_std/min                           -0.08153     0.00319    -0.07702    -0.08609
log_probs/mean                        -2.71979     0.00905    -2.70955    -2.73437
log_probs/std                         0.31490      0.00659    0.32255     0.30587
log_probs/max                         -1.78354     0.02553    -1.75867    -1.82390
log_probs/min                         -4.49727     0.53954    -3.67685    -5.00335
mean/mean                             -0.00216     0.00015    -0.00194    -0.00235
mean/std                              0.00442      0.00021    0.00471     0.00413
mean/max                              0.00702      0.00012    0.00724     0.00690
mean/min                              -0.01278     0.00031    -0.01233    -0.01323
------------------------------------  -----------  ---------  ----------  ----------
sample: [2, 9, 1, 8, 7, 0, 5, 4, 6, 3]
replay_buffer._size: [5100 5100 5100 5100 5100 5100 5100 5100 5100 5100]
gen_weight_change tensor(-5.0193, device='cuda:0')
train_time 2.2336349487304688
snapshot at best
2023-11-03 00:06:19,048 MainThread INFO: EPOCH:12
2023-11-03 00:06:19,049 MainThread INFO: Time Consumed:3.137392997741699s
2023-11-03 00:06:19,049 MainThread INFO: Total Frames:49500s
  1%|▏         | 13/1000 [00:39<50:21,  3.06s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1235.44427
Train_Epoch_Reward                    17484.64817
Running_Training_Average_Rewards      1254.51276
Explore_Time                          0.01399
Train___Time                          2.23363
Eval____Time                          0.12172
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.29990
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -43.14354
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.36973
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.41962
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.11454
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.64344
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.01189
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12601.22300
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.13875
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.63890
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.74489      0.89946    10.79106    8.56821
alpha_0                               0.98128      0.00042    0.98187     0.98069
alpha_1                               0.98127      0.00042    0.98186     0.98068
alpha_2                               0.98126      0.00042    0.98185     0.98067
alpha_3                               0.98126      0.00042    0.98185     0.98067
alpha_4                               0.98127      0.00042    0.98186     0.98068
alpha_5                               0.98126      0.00042    0.98185     0.98067
alpha_6                               0.98125      0.00042    0.98184     0.98066
alpha_7                               0.98127      0.00042    0.98186     0.98068
alpha_8                               0.98126      0.00042    0.98185     0.98067
alpha_9                               0.98125      0.00042    0.98184     0.98066
Alpha_loss                            -0.12513     0.00284    -0.12099    -0.12903
Training/policy_loss                  -2.70908     0.00510    -2.70287    -2.71444
Training/qf1_loss                     1943.24524   411.80728  2305.07593  1327.76941
Training/qf2_loss                     1943.15857   411.80685  2304.97729  1327.67676
Training/pf_norm                      0.17461      0.01352    0.19245     0.15631
Training/qf1_norm                     15.25016     1.15372    16.38031    13.42277
Training/qf2_norm                     17.14121     1.26755    18.46046    15.11818
log_std/mean                          -0.07011     0.00262    -0.06642    -0.07383
log_std/std                           0.01414      0.00040    0.01471     0.01357
log_std/max                           -0.04253     0.00192    -0.03985    -0.04530
log_std/min                           -0.09298     0.00325    -0.08838    -0.09751
log_probs/mean                        -2.72211     0.00532    -2.71505    -2.72764
log_probs/std                         0.30639      0.00532    0.31294     0.30107
log_probs/max                         -1.82186     0.04904    -1.73196    -1.87961
log_probs/min                         -4.51417     0.32719    -4.16650    -5.06399
mean/mean                             -0.00163     0.00018    -0.00136    -0.00187
mean/std                              0.00505      0.00018    0.00535     0.00482
mean/max                              0.00657      0.00041    0.00707     0.00611
mean/min                              -0.01363     0.00012    -0.01345    -0.01381
------------------------------------  -----------  ---------  ----------  ----------
sample: [8, 7, 9, 2, 5, 0, 1, 3, 4, 6]
replay_buffer._size: [5250 5250 5250 5250 5250 5250 5250 5250 5250 5250]
gen_weight_change tensor(-5.0193, device='cuda:0')
train_time 0.8240909576416016
snapshot at best
2023-11-03 00:06:21,948 MainThread INFO: EPOCH:13
2023-11-03 00:06:21,948 MainThread INFO: Time Consumed:1.9242610931396484s
2023-11-03 00:06:21,948 MainThread INFO: Total Frames:51000s
  1%|▏         | 14/1000 [00:42<49:21,  3.00s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1246.04069
Train_Epoch_Reward                    17164.67753
Running_Training_Average_Rewards      1523.74542
Explore_Time                          0.35859
Train___Time                          0.82409
Eval____Time                          0.16020
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.86621
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -43.19558
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.43910
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.38158
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.32018
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.77480
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.12893
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12713.30096
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.26040
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.52727
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.66451      0.68608    9.50178     7.74315
alpha_0                               0.97981      0.00042    0.98039     0.97922
alpha_1                               0.97980      0.00042    0.98039     0.97921
alpha_2                               0.97979      0.00042    0.98038     0.97920
alpha_3                               0.97978      0.00042    0.98037     0.97919
alpha_4                               0.97979      0.00042    0.98038     0.97920
alpha_5                               0.97978      0.00042    0.98037     0.97919
alpha_6                               0.97978      0.00042    0.98037     0.97919
alpha_7                               0.97980      0.00042    0.98039     0.97921
alpha_8                               0.97978      0.00042    0.98037     0.97919
alpha_9                               0.97978      0.00042    0.98037     0.97919
Alpha_loss                            -0.13534     0.00285    -0.13128    -0.13938
Training/policy_loss                  -2.71673     0.00288    -2.71262    -2.72109
Training/qf1_loss                     1520.31997   265.74563  1833.38672  1150.44226
Training/qf2_loss                     1520.21414   265.73928  1833.25977  1150.33142
Training/pf_norm                      0.15403      0.01399    0.16368     0.12620
Training/qf1_norm                     14.91987     1.09277    16.70977    13.70623
Training/qf2_norm                     16.87474     1.24929    18.92942    15.64101
log_std/mean                          -0.07967     0.00279    -0.07576    -0.08366
log_std/std                           0.01553      0.00039    0.01608     0.01498
log_std/max                           -0.04974     0.00209    -0.04681    -0.05270
log_std/min                           -0.10557     0.00371    -0.10058    -0.11102
log_probs/mean                        -2.72728     0.00303    -2.72246    -2.73180
log_probs/std                         0.29622      0.00873    0.30882     0.28278
log_probs/max                         -1.85457     0.05078    -1.77014    -1.92654
log_probs/min                         -5.14186     0.56310    -4.65814    -6.22196
mean/mean                             -0.00103     0.00012    -0.00085    -0.00120
mean/std                              0.00580      0.00019    0.00603     0.00553
mean/max                              0.00728      0.00050    0.00794     0.00650
mean/min                              -0.01381     0.00012    -0.01362    -0.01397
------------------------------------  -----------  ---------  ----------  ----------
sample: [2, 9, 5, 1, 4, 7, 6, 3, 0, 8]
replay_buffer._size: [5400 5400 5400 5400 5400 5400 5400 5400 5400 5400]
gen_weight_change tensor(-5.0194, device='cuda:0')
train_time 0.7886979579925537
2023-11-03 00:06:24,482 MainThread INFO: EPOCH:14
2023-11-03 00:06:24,483 MainThread INFO: Time Consumed:1.0301122665405273s
2023-11-03 00:06:24,483 MainThread INFO: Total Frames:52500s
  2%|▏         | 15/1000 [00:44<46:57,  2.86s/it]------------------------------------  -----------  ---------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1235.91636
Train_Epoch_Reward                    3589.65692
Running_Training_Average_Rewards      1274.63275
Explore_Time                          0.00308
Train___Time                          0.78870
Eval____Time                          0.23425
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.30262
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -43.35525
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.38479
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.59045
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.59299
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.94987
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.32807
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12610.65888
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.44718
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.54406
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.67043      0.82138    9.89434     7.44431
alpha_0                               0.97834      0.00042    0.97892     0.97775
alpha_1                               0.97833      0.00042    0.97892     0.97774
alpha_2                               0.97832      0.00042    0.97890     0.97773
alpha_3                               0.97831      0.00042    0.97890     0.97772
alpha_4                               0.97832      0.00042    0.97891     0.97773
alpha_5                               0.97831      0.00042    0.97890     0.97772
alpha_6                               0.97830      0.00042    0.97889     0.97771
alpha_7                               0.97833      0.00042    0.97892     0.97774
alpha_8                               0.97831      0.00042    0.97890     0.97772
alpha_9                               0.97830      0.00042    0.97889     0.97771
Alpha_loss                            -0.14547     0.00270    -0.14174    -0.14932
Training/policy_loss                  -2.72125     0.00698    -2.71400    -2.73450
Training/qf1_loss                     1496.92953   407.14629  2192.11304  951.64667
Training/qf2_loss                     1496.79088   407.13148  2191.94312  951.51642
Training/pf_norm                      0.13676      0.01761    0.16613     0.11545
Training/qf1_norm                     16.13954     1.32744    18.30326    14.66966
Training/qf2_norm                     18.39236     1.56274    20.99200    16.68089
log_std/mean                          -0.08962     0.00278    -0.08568    -0.09354
log_std/std                           0.01693      0.00039    0.01747     0.01638
log_std/max                           -0.05669     0.00186    -0.05403    -0.05927
log_std/min                           -0.11861     0.00356    -0.11371    -0.12389
log_probs/mean                        -2.72811     0.00822    -2.71889    -2.74334
log_probs/std                         0.27948      0.00481    0.28692     0.27247
log_probs/max                         -1.91639     0.01910    -1.89800    -1.94384
log_probs/min                         -4.75003     0.49327    -4.25773    -5.48804
mean/mean                             -0.00048     0.00017    -0.00026    -0.00073
mean/std                              0.00614      0.00003    0.00620     0.00611
mean/max                              0.00879      0.00038    0.00928     0.00818
mean/min                              -0.01318     0.00047    -0.01252    -0.01371
------------------------------------  -----------  ---------  ----------  ---------
sample: [1, 4, 7, 0, 5, 2, 6, 3, 9, 8]
replay_buffer._size: [5550 5550 5550 5550 5550 5550 5550 5550 5550 5550]
gen_weight_change tensor(-5.0194, device='cuda:0')
train_time 2.7034120559692383
2023-11-03 00:06:27,628 MainThread INFO: EPOCH:15
2023-11-03 00:06:27,629 MainThread INFO: Time Consumed:2.8569841384887695s
2023-11-03 00:06:27,629 MainThread INFO: Total Frames:54000s
  2%|▏         | 16/1000 [00:47<48:18,  2.95s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1235.18929
Train_Epoch_Reward                    5396.28654
Running_Training_Average_Rewards      871.68737
Explore_Time                          0.02219
Train___Time                          2.70341
Eval____Time                          0.12752
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.51527
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -43.40981
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.32879
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.73066
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.80538
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.11180
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.49661
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12606.42792
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.55983
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.57690
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.84011      0.70060    9.66354     7.54416
alpha_0                               0.97687      0.00042    0.97745     0.97628
alpha_1                               0.97686      0.00042    0.97745     0.97627
alpha_2                               0.97685      0.00042    0.97743     0.97626
alpha_3                               0.97684      0.00042    0.97742     0.97625
alpha_4                               0.97685      0.00042    0.97744     0.97627
alpha_5                               0.97684      0.00042    0.97743     0.97625
alpha_6                               0.97683      0.00042    0.97742     0.97624
alpha_7                               0.97686      0.00042    0.97744     0.97627
alpha_8                               0.97684      0.00042    0.97743     0.97625
alpha_9                               0.97683      0.00042    0.97742     0.97624
Alpha_loss                            -0.15571     0.00285    -0.15175    -0.15986
Training/policy_loss                  -2.73108     0.00384    -2.72730    -2.73758
Training/qf1_loss                     1592.36111   232.77719  1948.21509  1260.06604
Training/qf2_loss                     1592.17876   232.76024  1947.99841  1259.90759
Training/pf_norm                      0.14260      0.01499    0.16285     0.12288
Training/qf1_norm                     17.75943     1.26217    19.72496    15.89305
Training/qf2_norm                     20.36904     1.48866    22.69523    18.10879
log_std/mean                          -0.09968     0.00291    -0.09558    -0.10380
log_std/std                           0.01827      0.00038    0.01881     0.01774
log_std/max                           -0.06330     0.00188    -0.06063    -0.06594
log_std/min                           -0.13191     0.00377    -0.12671    -0.13728
log_probs/mean                        -2.73314     0.00378    -2.72853    -2.73737
log_probs/std                         0.26756      0.00371    0.27329     0.26308
log_probs/max                         -1.97379     0.03064    -1.92815    -2.02209
log_probs/min                         -4.59859     0.70542    -4.14065    -5.99902
mean/mean                             0.00024      0.00025    0.00061     -0.00010
mean/std                              0.00606      0.00005    0.00613     0.00598
mean/max                              0.01011      0.00031    0.01055     0.00964
mean/min                              -0.01124     0.00036    -0.01077    -0.01180
------------------------------------  -----------  ---------  ----------  ----------
sample: [5, 7, 6, 8, 1, 2, 3, 9, 4, 0]
replay_buffer._size: [5700 5700 5700 5700 5700 5700 5700 5700 5700 5700]
gen_weight_change tensor(-5.0194, device='cuda:0')
train_time 3.2210636138916016
2023-11-03 00:06:31,114 MainThread INFO: EPOCH:16
2023-11-03 00:06:31,115 MainThread INFO: Time Consumed:3.2422754764556885s
2023-11-03 00:06:31,115 MainThread INFO: Total Frames:55500s
  2%|▏         | 17/1000 [00:51<50:57,  3.11s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1233.84852
Train_Epoch_Reward                    15488.29353
Running_Training_Average_Rewards      815.80790
Explore_Time                          0.01443
Train___Time                          3.22106
Eval____Time                          0.00301
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.12691
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -43.30279
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.26986
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.75657
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.90530
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.18624
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.50526
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12595.81019
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.52427
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.74780
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.97426      0.51671    10.85127    9.24236
alpha_0                               0.97540      0.00041    0.97599     0.97481
alpha_1                               0.97539      0.00041    0.97598     0.97480
alpha_2                               0.97538      0.00042    0.97596     0.97479
alpha_3                               0.97537      0.00042    0.97595     0.97478
alpha_4                               0.97539      0.00041    0.97597     0.97480
alpha_5                               0.97537      0.00042    0.97596     0.97478
alpha_6                               0.97536      0.00042    0.97595     0.97477
alpha_7                               0.97539      0.00042    0.97597     0.97480
alpha_8                               0.97537      0.00041    0.97596     0.97479
alpha_9                               0.97536      0.00042    0.97595     0.97478
Alpha_loss                            -0.16577     0.00288    -0.16179    -0.16985
Training/policy_loss                  -2.73465     0.00430    -2.72659    -2.73845
Training/qf1_loss                     2020.50874   363.32470  2425.32715  1602.80334
Training/qf2_loss                     2020.24053   363.31652  2425.06104  1602.51331
Training/pf_norm                      0.13295      0.01735    0.16091     0.11193
Training/qf1_norm                     21.33355     0.84732    22.19651    19.71952
Training/qf2_norm                     24.76272     1.10589    25.77595    22.61481
log_std/mean                          -0.11002     0.00291    -0.10591    -0.11410
log_std/std                           0.01959      0.00036    0.02010     0.01908
log_std/max                           -0.07010     0.00190    -0.06747    -0.07280
log_std/min                           -0.14556     0.00411    -0.13968    -0.15139
log_probs/mean                        -2.73042     0.00353    -2.72359    -2.73346
log_probs/std                         0.25389      0.01051    0.27119     0.24179
log_probs/max                         -2.04624     0.07347    -1.95237    -2.13819
log_probs/min                         -4.94486     0.26662    -4.55304    -5.30060
mean/mean                             0.00073      0.00008    0.00088     0.00064
mean/std                              0.00542      0.00044    0.00602     0.00481
mean/max                              0.00990      0.00032    0.01037     0.00956
mean/min                              -0.00949     0.00076    -0.00836    -0.01049
------------------------------------  -----------  ---------  ----------  ----------
sample: [6, 8, 4, 0, 2, 7, 5, 1, 3, 9]
replay_buffer._size: [5850 5850 5850 5850 5850 5850 5850 5850 5850 5850]
gen_weight_change tensor(-5.0195, device='cuda:0')
train_time 3.4301977157592773
2023-11-03 00:06:34,799 MainThread INFO: EPOCH:17
2023-11-03 00:06:34,800 MainThread INFO: Time Consumed:3.4656240940093994s
2023-11-03 00:06:34,800 MainThread INFO: Total Frames:57000s
  2%|▏         | 18/1000 [00:55<53:41,  3.28s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1229.16902
Train_Epoch_Reward                    21463.50299
Running_Training_Average_Rewards      1411.60277
Explore_Time                          0.02918
Train___Time                          3.43020
Eval____Time                          0.00255
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.57309
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -43.17119
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.20079
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.76311
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.01346
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.27998
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.49461
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12544.61134
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.49947
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.92542
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.28207      0.60602    10.05905    8.25791
alpha_0                               0.97393      0.00041    0.97452     0.97335
alpha_1                               0.97392      0.00041    0.97451     0.97334
alpha_2                               0.97391      0.00041    0.97450     0.97332
alpha_3                               0.97390      0.00041    0.97449     0.97331
alpha_4                               0.97392      0.00041    0.97451     0.97334
alpha_5                               0.97390      0.00041    0.97449     0.97331
alpha_6                               0.97389      0.00041    0.97448     0.97331
alpha_7                               0.97392      0.00042    0.97451     0.97333
alpha_8                               0.97391      0.00041    0.97449     0.97332
alpha_9                               0.97390      0.00041    0.97448     0.97331
Alpha_loss                            -0.17598     0.00287    -0.17190    -0.18014
Training/policy_loss                  -2.74520     0.00425    -2.74096    -2.75263
Training/qf1_loss                     1657.22268   358.90964  2281.79614  1206.75610
Training/qf2_loss                     1656.90146   358.90815  2281.47510  1206.43713
Training/pf_norm                      0.13109      0.02375    0.16397     0.09518
Training/qf1_norm                     21.74511     0.78525    22.59454    20.41702
Training/qf2_norm                     25.42313     0.90759    26.39198    23.90320
log_std/mean                          -0.11950     0.00254    -0.11588    -0.12308
log_std/std                           0.02070      0.00027    0.02108     0.02030
log_std/max                           -0.07653     0.00172    -0.07396    -0.07891
log_std/min                           -0.15717     0.00261    -0.15321    -0.16055
log_probs/mean                        -2.73351     0.00352    -2.72831    -2.73777
log_probs/std                         0.25709      0.02087    0.29708     0.23673
log_probs/max                         -2.08441     0.03240    -2.03101    -2.11982
log_probs/min                         -5.05653     1.29770    -4.01884    -7.58405
mean/mean                             0.00151      0.00029    0.00186     0.00107
mean/std                              0.00389      0.00040    0.00449     0.00338
mean/max                              0.00832      0.00059    0.00919     0.00750
mean/min                              -0.00624     0.00095    -0.00495    -0.00760
------------------------------------  -----------  ---------  ----------  ----------
sample: [4, 0, 6, 1, 3, 9, 7, 5, 2, 8]
replay_buffer._size: [6000 6000 6000 6000 6000 6000 6000 6000 6000 6000]
gen_weight_change tensor(-5.0195, device='cuda:0')
train_time 0.9155135154724121
2023-11-03 00:06:38,101 MainThread INFO: EPOCH:18
2023-11-03 00:06:38,102 MainThread INFO: Time Consumed:1.3049938678741455s
2023-11-03 00:06:38,102 MainThread INFO: Total Frames:58500s
  2%|▏         | 19/1000 [00:58<53:49,  3.29s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1221.62614
Train_Epoch_Reward                    20860.52973
Running_Training_Average_Rewards      1927.07754
Explore_Time                          0.23948
Train___Time                          0.91551
Eval____Time                          0.14603
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -54.52651
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.83427
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.22923
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.53358
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.85523
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.20593
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.29857
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12472.16552
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.25822
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.16255
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.07531      0.33488    9.71484     8.74642
alpha_0                               0.97247      0.00041    0.97306     0.97189
alpha_1                               0.97246      0.00041    0.97305     0.97188
alpha_2                               0.97244      0.00041    0.97303     0.97186
alpha_3                               0.97244      0.00041    0.97302     0.97185
alpha_4                               0.97246      0.00041    0.97305     0.97188
alpha_5                               0.97243      0.00041    0.97302     0.97185
alpha_6                               0.97243      0.00041    0.97301     0.97184
alpha_7                               0.97245      0.00041    0.97304     0.97187
alpha_8                               0.97244      0.00041    0.97303     0.97186
alpha_9                               0.97243      0.00041    0.97302     0.97185
Alpha_loss                            -0.18621     0.00286    -0.18209    -0.19036
Training/policy_loss                  -2.75777     0.00720    -2.74882    -2.76683
Training/qf1_loss                     1653.37551   218.97293  1883.31042  1269.15027
Training/qf2_loss                     1652.99749   218.96837  1882.89673  1268.79236
Training/pf_norm                      0.12622      0.01434    0.14585     0.11007
Training/qf1_norm                     23.29474     0.87569    24.62067    22.01630
Training/qf2_norm                     27.24418     1.08870    28.96199    25.69834
log_std/mean                          -0.12761     0.00204    -0.12464    -0.13046
log_std/std                           0.02146      0.00015    0.02166     0.02124
log_std/max                           -0.08244     0.00160    -0.08024    -0.08472
log_std/min                           -0.16619     0.00234    -0.16283    -0.16963
log_probs/mean                        -2.73714     0.00699    -2.72797    -2.74866
log_probs/std                         0.24745      0.00817    0.25743     0.23853
log_probs/max                         -2.15219     0.03117    -2.11150    -2.19137
log_probs/min                         -5.02357     0.50999    -4.45337    -5.95242
mean/mean                             0.00260      0.00040    0.00309     0.00200
mean/std                              0.00326      0.00007    0.00337     0.00319
mean/max                              0.00814      0.00065    0.00905     0.00713
mean/min                              -0.00428     0.00016    -0.00415    -0.00459
------------------------------------  -----------  ---------  ----------  ----------
sample: [8, 7, 3, 9, 5, 2, 6, 0, 1, 4]
replay_buffer._size: [6150 6150 6150 6150 6150 6150 6150 6150 6150 6150]
gen_weight_change tensor(-5.0195, device='cuda:0')
train_time 3.759730100631714
2023-11-03 00:06:42,247 MainThread INFO: EPOCH:19
2023-11-03 00:06:42,248 MainThread INFO: Time Consumed:3.7910075187683105s
2023-11-03 00:06:42,248 MainThread INFO: Total Frames:60000s
  2%|▏         | 20/1000 [01:02<58:07,  3.56s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1224.86823
Train_Epoch_Reward                    4651.01146
Running_Training_Average_Rewards      1565.83481
Explore_Time                          0.02513
Train___Time                          3.75973
Eval____Time                          0.00243
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -46.32881
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.45156
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.40232
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.36343
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.92552
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.33619
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.35367
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12496.17821
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.94967
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.38476
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.48116      0.56623    9.33069     7.57183
alpha_0                               0.97101      0.00041    0.97159     0.97043
alpha_1                               0.97100      0.00041    0.97158     0.97042
alpha_2                               0.97098      0.00041    0.97157     0.97040
alpha_3                               0.97097      0.00041    0.97156     0.97039
alpha_4                               0.97100      0.00041    0.97158     0.97042
alpha_5                               0.97097      0.00041    0.97156     0.97039
alpha_6                               0.97097      0.00041    0.97155     0.97038
alpha_7                               0.97099      0.00041    0.97158     0.97041
alpha_8                               0.97098      0.00041    0.97157     0.97040
alpha_9                               0.97097      0.00041    0.97156     0.97039
Alpha_loss                            -0.19632     0.00296    -0.19209    -0.20043
Training/policy_loss                  -2.76782     0.00688    -2.75719    -2.77458
Training/qf1_loss                     1540.39683   311.35926  2054.88696  1172.27393
Training/qf2_loss                     1539.97078   311.34670  2054.44995  1171.90759
Training/pf_norm                      0.10500      0.01301    0.13046     0.09388
Training/qf1_norm                     24.08643     1.64449    25.70469    21.05177
Training/qf2_norm                     28.17206     1.87509    29.98449    24.67619
log_std/mean                          -0.13366     0.00132    -0.13168    -0.13540
log_std/std                           0.02182      0.00004    0.02186     0.02176
log_std/max                           -0.08724     0.00097    -0.08581    -0.08859
log_std/min                           -0.17393     0.00180    -0.17118    -0.17613
log_probs/mean                        -2.73609     0.00406    -2.72999    -2.74086
log_probs/std                         0.25159      0.01555    0.27184     0.23181
log_probs/max                         -2.13639     0.02522    -2.10771    -2.17509
log_probs/min                         -5.83155     0.98357    -4.48654    -7.39600
mean/mean                             0.00373      0.00024    0.00401     0.00335
mean/std                              0.00327      0.00006    0.00336     0.00318
mean/max                              0.01021      0.00060    0.01113     0.00942
mean/min                              -0.00338     0.00037    -0.00285    -0.00388
------------------------------------  -----------  ---------  ----------  ----------
sample: [0, 6, 7, 8, 3, 2, 1, 9, 4, 5]
replay_buffer._size: [6300 6300 6300 6300 6300 6300 6300 6300 6300 6300]
gen_weight_change tensor(-5.0195, device='cuda:0')
train_time 3.699448347091675
2023-11-03 00:06:46,260 MainThread INFO: EPOCH:20
2023-11-03 00:06:46,260 MainThread INFO: Time Consumed:3.7100045680999756s
2023-11-03 00:06:46,261 MainThread INFO: Total Frames:61500s
  2%|▏         | 21/1000 [01:06<1:00:36,  3.71s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1239.19809
Train_Epoch_Reward                    5052.15307
Running_Training_Average_Rewards      1018.78981
Explore_Time                          0.00440
Train___Time                          3.69945
Eval____Time                          0.00236
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -46.31574
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.14024
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.71186
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.35869
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.15663
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.60944
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.51566
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12640.04379
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.80797
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.44670
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.22551      0.57788    9.33928     7.67815
alpha_0                               0.96955      0.00041    0.97013     0.96897
alpha_1                               0.96954      0.00041    0.97012     0.96896
alpha_2                               0.96952      0.00041    0.97010     0.96894
alpha_3                               0.96951      0.00041    0.97010     0.96893
alpha_4                               0.96954      0.00041    0.97012     0.96896
alpha_5                               0.96951      0.00041    0.97010     0.96893
alpha_6                               0.96950      0.00041    0.97009     0.96892
alpha_7                               0.96953      0.00041    0.97012     0.96895
alpha_8                               0.96952      0.00041    0.97011     0.96894
alpha_9                               0.96951      0.00041    0.97010     0.96893
Alpha_loss                            -0.20627     0.00269    -0.20246    -0.21020
Training/policy_loss                  -2.77510     0.00406    -2.76822    -2.77975
Training/qf1_loss                     1375.30876   197.73450  1627.29919  1144.21875
Training/qf2_loss                     1374.81138   197.69930  1626.70142  1143.74561
Training/pf_norm                      0.12376      0.02096    0.15640     0.09932
Training/qf1_norm                     25.56380     2.00333    29.47245    23.98975
Training/qf2_norm                     29.94840     2.36252    34.60963    28.37407
log_std/mean                          -0.13669     0.00036    -0.13605    -0.13707
log_std/std                           0.02171      0.00011    0.02184     0.02153
log_std/max                           -0.09015     0.00056    -0.08917    -0.09079
log_std/min                           -0.17717     0.00041    -0.17680    -0.17785
log_probs/mean                        -2.73026     0.00702    -2.72024    -2.73829
log_probs/std                         0.22669      0.01287    0.24103     0.21016
log_probs/max                         -2.17631     0.03019    -2.14362    -2.23138
log_probs/min                         -4.51592     0.43980    -4.07716    -5.26552
mean/mean                             0.00418      0.00008    0.00429     0.00406
mean/std                              0.00334      0.00007    0.00344     0.00323
mean/max                              0.01206      0.00029    0.01248     0.01175
mean/min                              -0.00283     0.00006    -0.00274    -0.00292
------------------------------------  -----------  ---------  ----------  ----------
sample: [2, 6, 0, 4, 3, 1, 5, 7, 9, 8]
replay_buffer._size: [6450 6450 6450 6450 6450 6450 6450 6450 6450 6450]
gen_weight_change tensor(-5.0195, device='cuda:0')
train_time 3.113433837890625
snapshot at best
2023-11-03 00:06:50,430 MainThread INFO: EPOCH:21
2023-11-03 00:06:50,430 MainThread INFO: Time Consumed:3.8682429790496826s
2023-11-03 00:06:50,430 MainThread INFO: Total Frames:63000s
  2%|▏         | 22/1000 [01:10<1:02:17,  3.82s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1252.64259
Train_Epoch_Reward                    4367.24581
Running_Training_Average_Rewards      469.01368
Explore_Time                          0.00485
Train___Time                          3.11343
Eval____Time                          0.34173
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.75200
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.73819
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.01395
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.31518
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.25068
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.77197
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.56880
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12779.01409
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.60255
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.57490
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.75815      0.36532    9.15340     8.18453
alpha_0                               0.96809      0.00041    0.96867     0.96751
alpha_1                               0.96808      0.00041    0.96866     0.96750
alpha_2                               0.96806      0.00041    0.96864     0.96748
alpha_3                               0.96806      0.00041    0.96864     0.96748
alpha_4                               0.96809      0.00041    0.96867     0.96750
alpha_5                               0.96805      0.00041    0.96864     0.96747
alpha_6                               0.96805      0.00041    0.96863     0.96747
alpha_7                               0.96807      0.00041    0.96866     0.96749
alpha_8                               0.96806      0.00041    0.96865     0.96748
alpha_9                               0.96806      0.00041    0.96864     0.96747
Alpha_loss                            -0.21648     0.00315    -0.21189    -0.22089
Training/policy_loss                  -2.79289     0.01364    -2.76990    -2.81020
Training/qf1_loss                     1631.63325   175.06561  1872.72058  1385.06726
Training/qf2_loss                     1630.97761   175.07585  1872.05688  1384.39917
Training/pf_norm                      0.13501      0.02569    0.17403     0.09621
Training/qf1_norm                     29.28942     0.43389    29.98855    28.63463
Training/qf2_norm                     34.65230     0.41906    35.14109    33.89164
log_std/mean                          -0.13704     0.00010    -0.13685    -0.13714
log_std/std                           0.02119      0.00018    0.02144     0.02092
log_std/max                           -0.09170     0.00042    -0.09099    -0.09213
log_std/min                           -0.17651     0.00055    -0.17545    -0.17699
log_probs/mean                        -2.73267     0.00952    -2.71559    -2.74371
log_probs/std                         0.23218      0.01195    0.25311     0.22166
log_probs/max                         -2.18368     0.02583    -2.15933    -2.23029
log_probs/min                         -4.77877     0.40966    -4.36672    -5.53962
mean/mean                             0.00385      0.00010    0.00395     0.00370
mean/std                              0.00363      0.00020    0.00398     0.00345
mean/max                              0.01302      0.00040    0.01370     0.01263
mean/min                              -0.00278     0.00031    -0.00249    -0.00337
------------------------------------  -----------  ---------  ----------  ----------
sample: [6, 8, 5, 9, 4, 3, 7, 0, 1, 2]
replay_buffer._size: [6600 6600 6600 6600 6600 6600 6600 6600 6600 6600]
gen_weight_change tensor(-5.0195, device='cuda:0')
train_time 0.7030282020568848
2023-11-03 00:06:54,265 MainThread INFO: EPOCH:22
2023-11-03 00:06:54,266 MainThread INFO: Time Consumed:1.110612392425537s
2023-11-03 00:06:54,266 MainThread INFO: Total Frames:64500s
  2%|▏         | 23/1000 [01:14<1:02:11,  3.82s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1235.89263
Train_Epoch_Reward                    6570.07873
Running_Training_Average_Rewards      532.98259
Explore_Time                          0.00354
Train___Time                          0.70303
Eval____Time                          0.40012
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -53.80958
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.72671
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.94495
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.49087
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.44064
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.96891
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.67184
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12615.47394
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.68670
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.80738
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.37009      0.38424    9.74462     8.88733
alpha_0                               0.96664      0.00041    0.96722     0.96605
alpha_1                               0.96663      0.00041    0.96721     0.96604
alpha_2                               0.96661      0.00041    0.96719     0.96603
alpha_3                               0.96660      0.00041    0.96719     0.96602
alpha_4                               0.96663      0.00041    0.96721     0.96605
alpha_5                               0.96660      0.00041    0.96718     0.96602
alpha_6                               0.96659      0.00041    0.96717     0.96601
alpha_7                               0.96662      0.00041    0.96720     0.96604
alpha_8                               0.96661      0.00041    0.96719     0.96603
alpha_9                               0.96660      0.00041    0.96718     0.96602
Alpha_loss                            -0.22670     0.00294    -0.22271    -0.23071
Training/policy_loss                  -2.81364     0.00950    -2.80147    -2.82845
Training/qf1_loss                     1725.89229   178.29944  1999.67932  1513.15747
Training/qf2_loss                     1725.06560   178.25712  1998.80273  1512.42615
Training/pf_norm                      0.11991      0.01841    0.15373     0.10035
Training/qf1_norm                     33.94708     1.63218    35.78337    31.19455
Training/qf2_norm                     40.20525     1.85221    42.26000    36.93295
log_std/mean                          -0.13586     0.00050    -0.13514    -0.13652
log_std/std                           0.02046      0.00023    0.02077     0.02012
log_std/max                           -0.09220     0.00009    -0.09214    -0.09237
log_std/min                           -0.17355     0.00131    -0.17154    -0.17523
log_probs/mean                        -2.73564     0.00660    -2.72697    -2.74695
log_probs/std                         0.24991      0.01775    0.27289     0.22191
log_probs/max                         -2.16979     0.03566    -2.10831    -2.20193
log_probs/min                         -5.58840     1.09484    -4.47773    -7.53321
mean/mean                             0.00347      0.00011    0.00367     0.00338
mean/std                              0.00486      0.00043    0.00546     0.00425
mean/max                              0.01530      0.00087    0.01661     0.01425
mean/min                              -0.00498     0.00065    -0.00393    -0.00574
------------------------------------  -----------  ---------  ----------  ----------
sample: [6, 0, 5, 1, 8, 4, 3, 9, 7, 2]
replay_buffer._size: [6750 6750 6750 6750 6750 6750 6750 6750 6750 6750]
gen_weight_change tensor(-5.0195, device='cuda:0')
train_time 4.518772840499878
2023-11-03 00:06:59,156 MainThread INFO: EPOCH:23
2023-11-03 00:06:59,156 MainThread INFO: Time Consumed:4.544998407363892s
2023-11-03 00:06:59,157 MainThread INFO: Total Frames:66000s
  2%|▏         | 24/1000 [01:19<1:07:27,  4.15s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1191.58072
Train_Epoch_Reward                    11024.96621
Running_Training_Average_Rewards      732.07636
Explore_Time                          0.01984
Train___Time                          4.51877
Eval____Time                          0.00262
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.30253
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.81293
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.68184
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.83479
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.60236
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.17662
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.71486
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12175.03655
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.82735
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -27.27607
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.81214      0.31121    9.32539     8.44045
alpha_0                               0.96518      0.00041    0.96576     0.96460
alpha_1                               0.96517      0.00041    0.96575     0.96459
alpha_2                               0.96516      0.00041    0.96574     0.96458
alpha_3                               0.96515      0.00041    0.96573     0.96457
alpha_4                               0.96518      0.00041    0.96576     0.96460
alpha_5                               0.96514      0.00041    0.96573     0.96456
alpha_6                               0.96514      0.00041    0.96572     0.96456
alpha_7                               0.96516      0.00041    0.96575     0.96458
alpha_8                               0.96516      0.00041    0.96574     0.96457
alpha_9                               0.96515      0.00041    0.96573     0.96457
Alpha_loss                            -0.23691     0.00283    -0.23294    -0.24109
Training/policy_loss                  -2.83643     0.00659    -2.82995    -2.84902
Training/qf1_loss                     1543.81926   170.39615  1762.05359  1285.16785
Training/qf2_loss                     1542.86152   170.32735  1761.02856  1284.31006
Training/pf_norm                      0.11439      0.01999    0.14115     0.08980
Training/qf1_norm                     34.96998     1.76061    37.23629    32.54679
Training/qf2_norm                     41.68945     2.12058    44.33987    38.74980
log_std/mean                          -0.13397     0.00061    -0.13308    -0.13479
log_std/std                           0.01962      0.00024    0.01997     0.01928
log_std/max                           -0.09268     0.00014    -0.09246    -0.09285
log_std/min                           -0.16894     0.00118    -0.16736    -0.17055
log_probs/mean                        -2.73792     0.00440    -2.72992    -2.74149
log_probs/std                         0.23150      0.00857    0.24120     0.21578
log_probs/max                         -2.17516     0.04300    -2.11301    -2.22494
log_probs/min                         -4.75571     0.43283    -4.08129    -5.44000
mean/mean                             0.00397      0.00031    0.00447     0.00358
mean/std                              0.00645      0.00043    0.00706     0.00584
mean/max                              0.01917      0.00123    0.02094     0.01762
mean/min                              -0.00629     0.00024    -0.00597    -0.00665
------------------------------------  -----------  ---------  ----------  ----------
sample: [9, 3, 8, 7, 6, 2, 1, 4, 5, 0]
replay_buffer._size: [6900 6900 6900 6900 6900 6900 6900 6900 6900 6900]
gen_weight_change tensor(-5.0195, device='cuda:0')
train_time 3.649820566177368
2023-11-03 00:07:03,737 MainThread INFO: EPOCH:24
2023-11-03 00:07:03,738 MainThread INFO: Time Consumed:4.353674650192261s
2023-11-03 00:07:03,738 MainThread INFO: Total Frames:67500s
  2%|▎         | 25/1000 [01:24<1:09:25,  4.27s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1158.77665
Train_Epoch_Reward                    14677.72474
Running_Training_Average_Rewards      1075.75899
Explore_Time                          0.03753
Train___Time                          3.64982
Eval____Time                          0.66229
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.70633
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.04871
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.46241
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -24.23369
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.96531
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.52986
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -22.04211
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11845.45664
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.10582
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -27.59589
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.76353      0.45031    9.35404     8.06637
alpha_0                               0.96373      0.00041    0.96431     0.96315
alpha_1                               0.96372      0.00041    0.96430     0.96314
alpha_2                               0.96371      0.00041    0.96429     0.96313
alpha_3                               0.96370      0.00041    0.96428     0.96313
alpha_4                               0.96373      0.00041    0.96431     0.96315
alpha_5                               0.96369      0.00041    0.96427     0.96311
alpha_6                               0.96369      0.00041    0.96427     0.96311
alpha_7                               0.96371      0.00041    0.96429     0.96313
alpha_8                               0.96370      0.00041    0.96428     0.96312
alpha_9                               0.96370      0.00041    0.96428     0.96312
Alpha_loss                            -0.24703     0.00287    -0.24277    -0.25092
Training/policy_loss                  -2.85977     0.00942    -2.84417    -2.87128
Training/qf1_loss                     1438.44082   205.07956  1735.49121  1207.06824
Training/qf2_loss                     1437.33452   205.05686  1734.39099  1205.95337
Training/pf_norm                      0.12254      0.00846    0.13854     0.11470
Training/qf1_norm                     37.98765     1.35831    39.55161    35.99030
Training/qf2_norm                     45.22559     1.59466    47.15779    43.00951
log_std/mean                          -0.13172     0.00064    -0.13092    -0.13269
log_std/std                           0.01877      0.00026    0.01914     0.01840
log_std/max                           -0.09299     0.00017    -0.09280    -0.09326
log_std/min                           -0.16460     0.00143    -0.16269    -0.16668
log_probs/mean                        -2.73742     0.00640    -2.73166    -2.74952
log_probs/std                         0.23801      0.01323    0.25585     0.21636
log_probs/max                         -2.14945     0.04379    -2.06462    -2.18946
log_probs/min                         -4.87068     0.62241    -4.10771    -5.74051
mean/mean                             0.00480      0.00015    0.00501     0.00459
mean/std                              0.00808      0.00043    0.00865     0.00745
mean/max                              0.02415      0.00146    0.02615     0.02224
mean/min                              -0.00789     0.00047    -0.00713    -0.00840
------------------------------------  -----------  ---------  ----------  ----------
sample: [2, 7, 4, 1, 3, 8, 0, 6, 9, 5]
replay_buffer._size: [7050 7050 7050 7050 7050 7050 7050 7050 7050 7050]
gen_weight_change tensor(-5.1440, device='cuda:0')
train_time 8.17480182647705
2023-11-03 00:07:12,231 MainThread INFO: EPOCH:25
2023-11-03 00:07:12,232 MainThread INFO: Time Consumed:8.202003479003906s
2023-11-03 00:07:12,232 MainThread INFO: Total Frames:69000s
  3%|▎         | 26/1000 [01:32<1:30:20,  5.57s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1138.28830
Train_Epoch_Reward                    12534.83392
Running_Training_Average_Rewards      1274.58416
Explore_Time                          0.02110
Train___Time                          8.17480
Eval____Time                          0.00232
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.44857
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.13989
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.46859
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -24.48448
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -22.40895
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.90873
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -22.49836
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11643.37458
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.17853
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -27.95551
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           7.98454      0.38824    8.69720     7.60256
alpha_0                               0.96228      0.00041    0.96286     0.96170
alpha_1                               0.96227      0.00041    0.96285     0.96169
alpha_2                               0.96226      0.00041    0.96284     0.96168
alpha_3                               0.96226      0.00041    0.96284     0.96168
alpha_4                               0.96229      0.00041    0.96286     0.96171
alpha_5                               0.96225      0.00041    0.96283     0.96167
alpha_6                               0.96225      0.00041    0.96282     0.96167
alpha_7                               0.96226      0.00041    0.96284     0.96168
alpha_8                               0.96225      0.00041    0.96283     0.96168
alpha_9                               0.96225      0.00041    0.96283     0.96167
Alpha_loss                            -0.25678     0.00275    -0.25279    -0.26028
Training/policy_loss                  -2.84883     0.01610    -2.82668    -2.87435
Training/qf1_loss                     1209.53098   100.25553  1404.25635  1142.21777
Training/qf2_loss                     1208.75283   99.65923   1402.07935  1140.70056
Training/pf_norm                      0.11681      0.01572    0.13696     0.09135
Training/qf1_norm                     31.86279     2.49652    36.10005    29.43243
Training/qf2_norm                     39.23767     3.29194    44.30532    33.93655
log_std/mean                          -0.12062     0.00197    -0.11845    -0.12402
log_std/std                           0.01838      0.00170    0.02058     0.01600
log_std/max                           -0.08123     0.00493    -0.07386    -0.08816
log_std/min                           -0.16448     0.00918    -0.15115    -0.17420
log_probs/mean                        -2.72739     0.00806    -2.71326    -2.73823
log_probs/std                         0.24393      0.01290    0.26833     0.23297
log_probs/max                         -2.08899     0.06606    -1.95901    -2.13068
log_probs/min                         -4.88763     0.72960    -4.31592    -6.23082
mean/mean                             0.00496      0.00046    0.00573     0.00433
mean/std                              0.00786      0.00038    0.00821     0.00711
mean/max                              0.02275      0.00158    0.02423     0.01993
mean/min                              -0.00748     0.00119    -0.00547    -0.00891
------------------------------------  -----------  ---------  ----------  ----------
sample: [8, 2, 6, 1, 3, 9, 5, 0, 7, 4]
replay_buffer._size: [7200 7200 7200 7200 7200 7200 7200 7200 7200 7200]
gen_weight_change tensor(-5.3634, device='cuda:0')
train_time 1.2855165004730225
2023-11-03 00:07:13,950 MainThread INFO: EPOCH:26
2023-11-03 00:07:13,950 MainThread INFO: Time Consumed:1.4264686107635498s
2023-11-03 00:07:13,951 MainThread INFO: Total Frames:70500s
  3%|▎         | 27/1000 [01:34<1:11:07,  4.39s/it]------------------------------------  -----------  --------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1140.54366
Train_Epoch_Reward                    3433.40350
Running_Training_Average_Rewards      1021.53207
Explore_Time                          0.01497
Train___Time                          1.28552
Eval____Time                          0.12196
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.55060
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.25832
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.57745
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -24.67137
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -22.90639
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -23.23685
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -23.02834
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11664.01075
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.30996
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -28.03481
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std       Max         Min
Reward_Mean                           8.43048      0.24746   8.76778     8.08315
alpha_0                               0.96084      0.00041   0.96141     0.96026
alpha_1                               0.96082      0.00041   0.96140     0.96024
alpha_2                               0.96081      0.00041   0.96139     0.96024
alpha_3                               0.96081      0.00041   0.96139     0.96023
alpha_4                               0.96084      0.00041   0.96142     0.96026
alpha_5                               0.96080      0.00041   0.96138     0.96022
alpha_6                               0.96080      0.00041   0.96138     0.96022
alpha_7                               0.96082      0.00041   0.96139     0.96024
alpha_8                               0.96081      0.00041   0.96139     0.96023
alpha_9                               0.96080      0.00041   0.96138     0.96022
Alpha_loss                            -0.26700     0.00288   -0.26294    -0.27132
Training/policy_loss                  -2.88239     0.01080   -2.86976    -2.90165
Training/qf1_loss                     1440.99185   61.59558  1524.51746  1350.02930
Training/qf2_loss                     1441.10952   61.63749  1524.67334  1350.07849
Training/pf_norm                      0.10734      0.01577   0.13546     0.08804
Training/qf1_norm                     39.21766     1.68134   41.98169    36.68785
Training/qf2_norm                     43.56757     2.07758   46.77535    40.24417
log_std/mean                          -0.12448     0.00020   -0.12419    -0.12477
log_std/std                           0.01995      0.00010   0.02007     0.01980
log_std/max                           -0.07435     0.00035   -0.07389    -0.07492
log_std/min                           -0.17534     0.00072   -0.17429    -0.17621
log_probs/mean                        -2.73010     0.00629   -2.71843    -2.73700
log_probs/std                         0.24004      0.01279   0.25404     0.21654
log_probs/max                         -2.07925     0.03695   -2.03204    -2.14323
log_probs/min                         -4.69520     0.33047   -4.07760    -5.07326
mean/mean                             0.00548      0.00014   0.00567     0.00525
mean/std                              0.00753      0.00018   0.00781     0.00729
mean/max                              0.02233      0.00089   0.02344     0.02098
mean/min                              -0.00547     0.00013   -0.00534    -0.00573
------------------------------------  -----------  --------  ----------  ----------
sample: [0, 8, 5, 4, 6, 7, 1, 2, 3, 9]
replay_buffer._size: [7350 7350 7350 7350 7350 7350 7350 7350 7350 7350]
gen_weight_change tensor(-5.4938, device='cuda:0')
train_time 1.40065336227417
2023-11-03 00:07:15,502 MainThread INFO: EPOCH:27
2023-11-03 00:07:15,503 MainThread INFO: Time Consumed:1.428999662399292s
2023-11-03 00:07:15,503 MainThread INFO: Total Frames:72000s
  3%|▎         | 28/1000 [01:35<57:31,  3.55s/it]  ------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1142.90721
Train_Epoch_Reward                    2390.20888
Running_Training_Average_Rewards      611.94821
Explore_Time                          0.01419
Train___Time                          1.40065
Eval____Time                          0.01045
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.02462
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.37162
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.94385
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -24.67892
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -22.13235
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -23.07081
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -22.01144
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11686.68713
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.27083
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -27.11061
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.54389      0.60471    9.48392     7.59882
alpha_0                               0.95939      0.00041    0.95997     0.95882
alpha_1                               0.95938      0.00041    0.95995     0.95880
alpha_2                               0.95937      0.00041    0.95995     0.95879
alpha_3                               0.95937      0.00041    0.95995     0.95879
alpha_4                               0.95940      0.00041    0.95998     0.95882
alpha_5                               0.95936      0.00041    0.95993     0.95878
alpha_6                               0.95936      0.00041    0.95994     0.95878
alpha_7                               0.95937      0.00041    0.95995     0.95880
alpha_8                               0.95937      0.00041    0.95994     0.95879
alpha_9                               0.95936      0.00041    0.95994     0.95878
Alpha_loss                            -0.27731     0.00289    -0.27307    -0.28141
Training/policy_loss                  -2.92069     0.01120    -2.90240    -2.93696
Training/qf1_loss                     1552.98848   183.38522  1700.87146  1209.69128
Training/qf2_loss                     1553.03757   183.37946  1700.91821  1209.76062
Training/pf_norm                      0.11938      0.01913    0.14431     0.09594
Training/qf1_norm                     43.44632     3.47213    48.31169    37.82856
Training/qf2_norm                     48.53009     3.94452    53.96749    42.00525
log_std/mean                          -0.12506     0.00021    -0.12489    -0.12544
log_std/std                           0.01946      0.00018    0.01971     0.01923
log_std/max                           -0.07635     0.00088    -0.07523    -0.07767
log_std/min                           -0.17558     0.00039    -0.17533    -0.17636
log_probs/mean                        -2.73487     0.00338    -2.73011    -2.74008
log_probs/std                         0.24678      0.01746    0.27424     0.22684
log_probs/max                         -2.10230     0.04148    -2.04055    -2.16235
log_probs/min                         -4.81964     0.66337    -4.26313    -6.07295
mean/mean                             0.00502      0.00011    0.00520     0.00490
mean/std                              0.00683      0.00022    0.00717     0.00655
mean/max                              0.01939      0.00063    0.02034     0.01854
mean/min                              -0.00600     0.00013    -0.00583    -0.00620
------------------------------------  -----------  ---------  ----------  ----------
sample: [8, 7, 4, 5, 1, 9, 3, 2, 6, 0]
replay_buffer._size: [7500 7500 7500 7500 7500 7500 7500 7500 7500 7500]
gen_weight_change tensor(-5.5714, device='cuda:0')
train_time 1.2075629234313965
2023-11-03 00:07:17,002 MainThread INFO: EPOCH:28
2023-11-03 00:07:17,003 MainThread INFO: Time Consumed:1.246673583984375s
2023-11-03 00:07:17,003 MainThread INFO: Total Frames:73500s
  3%|▎         | 29/1000 [01:37<47:23,  2.93s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1123.30094
Train_Epoch_Reward                    10649.86344
Running_Training_Average_Rewards      549.11586
Explore_Time                          0.02097
Train___Time                          1.20756
Eval____Time                          0.01443
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.15558
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.28951
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.70866
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -24.64817
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.93081
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.71605
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.80865
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11488.86488
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.29177
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -27.30629
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.70169      0.52935    9.71493     8.17757
alpha_0                               0.95796      0.00041    0.95853     0.95738
alpha_1                               0.95794      0.00041    0.95851     0.95736
alpha_2                               0.95793      0.00041    0.95851     0.95736
alpha_3                               0.95793      0.00041    0.95850     0.95735
alpha_4                               0.95796      0.00041    0.95853     0.95738
alpha_5                               0.95792      0.00041    0.95849     0.95734
alpha_6                               0.95792      0.00041    0.95849     0.95734
alpha_7                               0.95793      0.00041    0.95851     0.95736
alpha_8                               0.95792      0.00041    0.95850     0.95735
alpha_9                               0.95792      0.00041    0.95849     0.95734
Alpha_loss                            -0.28711     0.00269    -0.28340    -0.29110
Training/policy_loss                  -2.94989     0.00775    -2.94231    -2.96436
Training/qf1_loss                     1515.12935   142.96089  1710.42566  1359.25098
Training/qf2_loss                     1515.09756   142.94872  1710.31641  1359.29724
Training/pf_norm                      0.10938      0.01886    0.13489     0.08511
Training/qf1_norm                     48.57656     3.62059    55.63557    45.82135
Training/qf2_norm                     54.51183     3.73999    61.81410    52.06946
log_std/mean                          -0.12708     0.00094    -0.12587    -0.12854
log_std/std                           0.01904      0.00008    0.01916     0.01892
log_std/max                           -0.08023     0.00133    -0.07853    -0.08228
log_std/min                           -0.17726     0.00094    -0.17601    -0.17872
log_probs/mean                        -2.72731     0.00543    -2.71842    -2.73513
log_probs/std                         0.23518      0.00383    0.24176     0.23146
log_probs/max                         -2.08486     0.03991    -2.00700    -2.11999
log_probs/min                         -4.48053     0.21118    -4.19524    -4.74282
mean/mean                             0.00452      0.00009    0.00467     0.00444
mean/std                              0.00643      0.00004    0.00648     0.00638
mean/max                              0.01756      0.00018    0.01789     0.01733
mean/min                              -0.00690     0.00023    -0.00649    -0.00713
------------------------------------  -----------  ---------  ----------  ----------
sample: [0, 7, 6, 1, 9, 5, 2, 4, 8, 3]
replay_buffer._size: [7650 7650 7650 7650 7650 7650 7650 7650 7650 7650]
gen_weight_change tensor(-5.6174, device='cuda:0')
train_time 1.2766387462615967
2023-11-03 00:07:18,670 MainThread INFO: EPOCH:29
2023-11-03 00:07:18,670 MainThread INFO: Time Consumed:1.3176493644714355s
2023-11-03 00:07:18,670 MainThread INFO: Total Frames:75000s
  3%|▎         | 30/1000 [01:39<41:22,  2.56s/it]  3%|▎         | 30/1000 [01:40<54:08,  3.35s/it]
------------------------------------  -----------  ---------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1113.64337
Train_Epoch_Reward                    9228.92964
Running_Training_Average_Rewards      742.30007
Explore_Time                          0.02233
Train___Time                          1.27664
Eval____Time                          0.01490
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.03970
6                                     0.00000
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.27024
3                                     0.00000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.46724
5                                     0.00000
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -24.55295
4                                     0.00000
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.66428
7                                     0.00000
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.33650
2                                     0.00000
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.57342
1                                     0.00000
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 11393.01260
0                                     0.00000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.28794
9                                     0.00000
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -27.38663
8                                     0.00000
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.52810      1.23561    10.70184    6.94369
alpha_0                               0.95652      0.00041    0.95709     0.95595
alpha_1                               0.95650      0.00041    0.95707     0.95592
alpha_2                               0.95649      0.00041    0.95707     0.95592
alpha_3                               0.95649      0.00041    0.95706     0.95591
alpha_4                               0.95652      0.00041    0.95709     0.95594
alpha_5                               0.95648      0.00041    0.95705     0.95590
alpha_6                               0.95648      0.00041    0.95705     0.95590
alpha_7                               0.95650      0.00041    0.95707     0.95592
alpha_8                               0.95649      0.00041    0.95706     0.95591
alpha_9                               0.95648      0.00041    0.95705     0.95590
Alpha_loss                            -0.29744     0.00279    -0.29364    -0.30157
Training/policy_loss                  -2.99405     0.01069    -2.98362    -3.01234
Training/qf1_loss                     1385.41536   418.31436  2150.10156  975.79834
Training/qf2_loss                     1385.26554   418.24972  2149.84570  975.69354
Training/pf_norm                      0.10691      0.01811    0.13195     0.08346
Training/qf1_norm                     52.46475     6.58700    62.50838    42.41322
Training/qf2_norm                     59.09360     7.80803    71.42375    47.48807
log_std/mean                          -0.13023     0.00058    -0.12933    -0.13101
log_std/std                           0.01872      0.00013    0.01889     0.01852
log_std/max                           -0.08463     0.00099    -0.08316    -0.08600
log_std/min                           -0.18028     0.00055    -0.17947    -0.18090
log_probs/mean                        -2.73241     0.00488    -2.72398    -2.73800
log_probs/std                         0.23120      0.00587    0.23966     0.22299
log_probs/max                         -2.12800     0.03085    -2.09038    -2.17935
log_probs/min                         -4.46759     0.26972    -4.05922    -4.73664
mean/mean                             0.00471      0.00008    0.00482     0.00460
mean/std                              0.00638      0.00008    0.00647     0.00626
mean/max                              0.01706      0.00028    0.01739     0.01660
mean/min                              -0.00653     0.00032    -0.00606    -0.00694
------------------------------------  -----------  ---------  ----------  ---------
sample: [3, 0, 7, 2, 9, 5, 4, 1, 6, 8]
replay_buffer._size: [7800 7800 7800 7800 7800 7800 7800 7800 7800 7800]
Process Process-9:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 292, in train_worker_process
    new_value = max(traj_collect_mod[env_info.env_rank],success)
  File "<string>", line 2, in __getitem__
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 834, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
KeyboardInterrupt
Process Process-2:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 268, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 179, in take_actions
    next_ob = env_info.env.reset()
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 300, in reset
    return self.env.reset(**kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 287, in reset
    return self.env.reset(**kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 277, in reset
    observation = self.env.reset(**kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 90, in reset
    ob = self.reset_model()
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_reach_push_pick_place.py", line 268, in reset_model
    self._reset_hand()
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_reach_push_pick_place.py", line 321, in _reset_hand
    self.do_simulation([-1,1], self.frame_skip)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 120, in do_simulation
    self.sim.step()
KeyboardInterrupt
Process Process-17:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 363, in eval_worker_process
    start_barrier.wait()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
first case
[-0.03265199  0.51487863  0.23688568  0.          0.49999998  0.09
  0.          0.7         0.04        0.          0.          0.
  0.          0.          1.          0.          0.          0.
  0.        ]
Process Process-7:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 268, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 149, in take_actions
    out = pf.explore(torch.Tensor( ob ).to(env_info.device).unsqueeze(0),
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/policies/continuous_policy.py", line 90, in explore
    ent = dis.entropy().sum(-1, keepdim=True)
KeyboardInterrupt
Process Process-4:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 268, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 149, in take_actions
    out = pf.explore(torch.Tensor( ob ).to(env_info.device).unsqueeze(0),
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/policies/continuous_policy.py", line 90, in explore
    ent = dis.entropy().sum(-1, keepdim=True)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/policies/distribution.py", line 79, in entropy
    return self.normal.entropy()
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/torch/distributions/normal.py", line 89, in entropy
    return 0.5 + 0.5 * math.log(2 * math.pi) + torch.log(self.scale)
KeyboardInterrupt
Process Process-3:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 268, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 149, in take_actions
    out = pf.explore(torch.Tensor( ob ).to(env_info.device).unsqueeze(0),
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/policies/continuous_policy.py", line 90, in explore
    ent = dis.entropy().sum(-1, keepdim=True)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/policies/distribution.py", line 79, in entropy
    return self.normal.entropy()
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/torch/distributions/normal.py", line 89, in entropy
    return 0.5 + 0.5 * math.log(2 * math.pi) + torch.log(self.scale)
KeyboardInterrupt
Process Process-8:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 313, in train_worker_process
    state_trajectory[env_info.env_rank] += [episode_state_traj]
  File "<string>", line 2, in __getitem__
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-16:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 418, in eval_worker_process
    act = pf.eval_act( torch.Tensor( eval_ob ).to(env_info.device).unsqueeze(0), mask_this_task)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/policies/continuous_policy.py", line 82, in eval_act
    return torch.tanh(mean.squeeze(0)).detach().cpu().numpy()
KeyboardInterrupt
Process Process-14:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 420, in eval_worker_process
    eval_ob, r, done, info = env_info.env.step( act )
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_reach_push_pick_place.py", line 153, in step
    self.do_simulation([action[-1], -action[-1]])
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 120, in do_simulation
    self.sim.step()
KeyboardInterrupt
Process Process-6:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 292, in train_worker_process
    new_value = max(traj_collect_mod[env_info.env_rank],success)
  File "<string>", line 2, in __getitem__
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-10:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 268, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 163, in take_actions
    next_ob, reward, done, info = env_info.env.step(act)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_window_open.py", line 124, in step
    self.set_xyz_action(action[:3])
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/sawyer_xyz/base.py", line 100, in set_xyz_action
    action = np.clip(action, -1, 1)
  File "<__array_function__ internals>", line 200, in clip
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 2180, in clip
    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)
KeyboardInterrupt
Process Process-21:
Process Process-20:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 416, in eval_worker_process
    embedding_input[env_info.env_rank] = 1
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 334, in _exit_function
    _run_finalizers(0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 887, in _decref
    conn = _Client(token.address, authkey=authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 757, in answer_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-11:
Process Process-19:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 460, in eval_worker_process
    state_trajectory[env_info.env_rank] += [episode_state_traj]
  File "<string>", line 2, in __setitem__
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 334, in _exit_function
    _run_finalizers(0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 887, in _decref
    conn = _Client(token.address, authkey=authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-5:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 313, in train_worker_process
    state_trajectory[env_info.env_rank] += [episode_state_traj]
  File "<string>", line 2, in __getitem__
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 334, in _exit_function
    _run_finalizers(0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 887, in _decref
    conn = _Client(token.address, authkey=authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-15:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 420, in eval_worker_process
    eval_ob, r, done, info = env_info.env.step( act )
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 274, in step
    return self.observation(observation), reward, done, info
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/env/continuous_wrapper.py", line 65, in observation
    aug_ob = np.concatenate([self._wrapped_env._state_goal,
  File "<__array_function__ internals>", line 200, in concatenate
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 334, in _exit_function
    _run_finalizers(0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 887, in _decref
    conn = _Client(token.address, authkey=authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-12:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 363, in eval_worker_process
    start_barrier.wait()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 334, in _exit_function
    _run_finalizers(0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 887, in _decref
    conn = _Client(token.address, authkey=authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-13:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 363, in eval_worker_process
    start_barrier.wait()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/threading.py", line 619, in wait
    self._wait(timeout)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/threading.py", line 654, in _wait
    if not self._cond.wait_for(lambda : self._state != 0, timeout):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py", line 313, in wait_for
    self.wait(waittime)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/synchronize.py", line 261, in wait
    return self._wait_semaphore.acquire(True, timeout)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 334, in _exit_function
    _run_finalizers(0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 887, in _decref
    conn = _Client(token.address, authkey=authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 757, in answer_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-18:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 418, in eval_worker_process
    act = pf.eval_act( torch.Tensor( eval_ob ).to(env_info.device).unsqueeze(0), mask_this_task)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/policies/continuous_policy.py", line 81, in eval_act
    mean, _, _ = self.forward(x, neuron_masks)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/policies/continuous_policy.py", line 70, in forward
    x = super().forward(x, neuron_masks,enable_mask)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/networks/nets.py", line 175, in forward
    new_weight = layer_weight*neuron_masks[2*idx]
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 334, in _exit_function
    _run_finalizers(0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 887, in _decref
    conn = _Client(token.address, authkey=authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 420, in eval_worker_process
    eval_ob, r, done, info = env_info.env.step( act )
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_window_close.py", line 128, in step
    self.do_simulation([action[-1], -action[-1]])
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/mujoco_env.py", line 118, in do_simulation
    self.sim.data.ctrl[:] = ctrl
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 163, in print_exc
    print_exception(*sys.exc_info(), limit=limit, file=file, chain=chain)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 103, in print_exception
    for line in TracebackException(
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 508, in __init__
    self.stack = StackSummary.extract(
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 366, in extract
    f.line
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 288, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 16, in getline
    lines = getlines(filename, module_globals)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/torch/_fx/graph_module.py", line 27, in patched_getline
    return _orig_getlines(*args, **kwargs)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 47, in getlines
    return updatecache(filename, module_globals)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 394, in open
    encoding, lines = detect_encoding(buffer.readline)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 363, in detect_encoding
    first = read_or_stop()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 321, in read_or_stop
    return readline()
KeyboardInterrupt
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 268, in train_worker_process
    next_ob, done, reward, info = cls.take_actions(local_funcs, env_info, c_ob, replay_buffer, index_mapping, mask_this_task, enable_mask)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/collector/para/async_mt.py", line 163, in take_actions
    next_ob, reward, done, info = env_info.env.step(act)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 304, in step
    return self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 290, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/gym/core.py", line 273, in step
    observation, reward, done, info = self.env.step(action)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_window_close.py", line 134, in step
    reward, reachDist, pickrew, pullDist = self.compute_reward(action, obs_dict, mode = self.rewMode)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/src/metaworld/metaworld/envs/mujoco/sawyer_xyz/sawyer_window_close.py", line 277, in compute_reward
    pullGoal = self._state_goal
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 163, in print_exc
    print_exception(*sys.exc_info(), limit=limit, file=file, chain=chain)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 103, in print_exception
    for line in TracebackException(
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 508, in __init__
    self.stack = StackSummary.extract(
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 366, in extract
    f.line
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/traceback.py", line 288, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 16, in getline
    lines = getlines(filename, module_globals)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/torch/_fx/graph_module.py", line 27, in patched_getline
    return _orig_getlines(*args, **kwargs)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 47, in getlines
    return updatecache(filename, module_globals)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 394, in open
    encoding, lines = detect_encoding(buffer.readline)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 363, in detect_encoding
    first = read_or_stop()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 321, in read_or_stop
    return readline()
KeyboardInterrupt
Error in sys.excepthook:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 47, in getlines
    return updatecache(filename, module_globals)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/linecache.py", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 394, in open
    encoding, lines = detect_encoding(buffer.readline)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 363, in detect_encoding
    first = read_or_stop()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.8.10/lib/python3.8/tokenize.py", line 321, in read_or_stop
    return readline()
KeyboardInterrupt

Original exception was:
Traceback (most recent call last):
  File "starter/mt_must_sac.py", line 347, in <module>
    experiment(args)
  File "starter/mt_must_sac.py", line 342, in experiment
    agent.train(env.num_tasks,params,group_name)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/algo/rl_algo.py", line 443, in train
    self.update_per_epoch(task_sample_index, task_scheduler, self.mask_buffer, epoch,self.use_trajectory_info)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/algo/off_policy/must_sac.py", line 473, in update_per_epoch
    info, times = self.update(batch, task_sample_index, task_scheduler, 
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/./torchrl/algo/off_policy/must_sac.py", line 331, in update
    self.pf_optimizer.step()
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/torch/optim/adam.py", line 82, in step
    grads.append(p.grad)
  File "/lustre04/scratch/qianxi/sparse_training/sep_t3s/venv/lib/python3.8/site-packages/torch/tensor.py", line 933, in grad
    @property
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255).
wandb: 
wandb: Run history:
wandb:                                    0 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                    1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                    2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                    3 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                    4 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                    5 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                    6 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                    7 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                    8 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                    9 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                           Alpha_loss ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                         Eval____Time █▁▂▁▁▁▂▁▁▁▁▁▂▂▂▂▁▁▂▁▁▂▃▁▄▁▂▁▁▁
wandb:                         Explore_Time █▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁
wandb:                          Reward_Mean ▄▃▆▂▃▄▄▇▅▃▄█▃▅▁▅▅▂▃▃▄▂▅▄▃▂▃▃▅▃
wandb:              Running_Average_Rewards ▆▆▆▇▇▆▅▄▄▄▄▆▇█▇▇▇▇▆▇▇█▇▅▃▂▂▂▁▁
wandb:     Running_Training_Average_Rewards ▅▅█▇▆▃▄▄▅▄▂▂▃▄▃▂▂▄▅▄▃▁▁▂▃▃▃▁▁▂
wandb:                   Train_Epoch_Reward ▅▆█▂▄▃▃▆▃▂▂▃▄▄▁▂▄▅▅▁▂▁▂▃▃▃▁▁▃▂
wandb:                         Train___Time ▄▁▂▂▂▂▂▂▃▃▃▃▂▁▁▃▃▄▁▄▄▃▁▅▄█▂▂▁▂
wandb:                     Training/pf_norm ▇▇█▇█▇█▇▆▅▄▃▅▂▄▃▂▂▃▁▂▃▃▃▂▃▁▃▁▂
wandb:                 Training/policy_loss ███████▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▄▄▄▃▃▂▁
wandb:                    Training/qf1_loss ▅▄▆▂▃▅▄█▅▂▃▆▂▄▁▄▅▂▃▂▃▂▅▄▃▂▃▃▃▂
wandb:                    Training/qf1_norm ▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▆▅▆▇██
wandb:                    Training/qf2_loss ▅▄▆▂▃▅▄█▅▂▃▆▂▄▁▄▅▂▃▂▃▂▅▄▃▂▃▃▃▂
wandb:                    Training/qf2_norm ▁▁▁▁▁▁▁▂▂▂▂▂▂▃▂▃▃▃▄▄▅▄▅▆▆▅▆▇██
wandb:                              alpha_0 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                              alpha_1 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                              alpha_2 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                              alpha_3 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                              alpha_4 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                              alpha_5 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                              alpha_6 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                              alpha_7 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                              alpha_8 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb:                              alpha_9 ███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁
wandb: button-press-topdown-v1_eval_rewards ▄▁▇▅▄▇▅▅▆▃▆▅▇▃▅▃▁▅▂██▅▂▁▄▃▆▄▅▄
wandb: button-press-topdown-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 door-v1_eval_rewards ▅▅▅▅▆▆▅▅▄▃▃▃▂▂▁▁▁▂▃▅▆███▇▆▆▅▆▆
wandb:                 door-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         drawer-close-v1_eval_rewards ███▇▆▆▆▇███▇▆▅▅▆▆▇▇▅▃▁▁▃▅▅▄▁▃▅
wandb:         drawer-close-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          drawer-open-v1_eval_rewards ▆▆▆▇▆▆▆▆▅▅▆▇▇█▇▆▆▆▇███▇▅▃▂▁▁▁▂
wandb:          drawer-open-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                        log_probs/max █▇▇▇▇▇▅▅▅▅▅▄▄▃▃▃▂▂▂▁▁▁▁▂▂▂▂▂▂▁
wandb:                       log_probs/mean █▆▇▆▇▇█▅▅▃▄▃▄▃▄▂▂▂▁▂▃▁▂▁▂▅▂▂▃▂
wandb:                        log_probs/min ███▇▇▆▇▆▇▆▇▆▇▆▅▄▅▇▆▃▇▇▁▇▅▅▅▆▇▇
wandb:                        log_probs/std █▇▇▇▆▇▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▃▁▂▂▂▃▂▁
wandb:                          log_std/max ████▇▇▇▇▆▆▆▅▅▄▄▃▃▂▂▁▁▁▁▁▁▂▂▂▂▂
wandb:                         log_std/mean ███▇▇▇▇▆▆▆▅▅▄▄▃▃▂▂▁▁▁▁▁▁▁▂▂▂▁▁
wandb:                          log_std/min ███▇▇▇▇▆▆▆▅▅▄▄▃▃▂▂▁▁▁▁▁▂▂▁▁▁▁▁
wandb:                          log_std/std ▁▁▂▂▂▃▃▃▄▄▅▅▆▆▇▇▇█████▇▇▇▇▇▇▇▇
wandb:                             mean/max ▁▁▂▂▂▁▁▁▁▂▂▂▂▃▃▃▃▂▃▃▄▄▅▆█▇▆▆▅▅
wandb:                            mean/mean ▃▄▄▄▄▃▃▂▁▁▁▁▂▂▃▄▄▅▆▇▇▆▆▇▇██▇▇▇
wandb:                             mean/min ▇▇▇███▇▆▅▄▂▁▁▁▂▃▄▆▆▇▇▇▆▅▄▆▆▅▅▅
wandb:                             mean/std ▁▂▂▁▁▁▁▁▂▃▄▄▅▆▆▆▄▃▃▃▃▄▅▆█▇▇▆▆▆
wandb:                    mean_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      ped-insert-side-v1_eval_rewards ███▇▇▇▇▇▇████▇▇▆▆▆▆▆▅▅▄▄▃▂▁▃▃▄
wandb:      ped-insert-side-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           pick-place-v1_eval_rewards ▇▇▇▇▇▇▇▇▇████▇▇▇▆▆▆▆▅▅▄▄▃▂▁▁▂▃
wandb:           pick-place-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 push-v1_eval_rewards ▇▇▇▆▆▆▇▇▇███▇▇▆▆▆▆▇▆▆▆▅▅▄▃▁▄▅▆
wandb:                 push-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                reach-v1_eval_rewards ▆▆▆▇▇▆▅▄▄▄▄▆▇█▇▇▇▇▆▇▇█▇▅▃▂▂▂▁▁
wandb:                reach-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                    save_traj_mod_sum ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                   task_policy_mask_0 █▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▁▁▁▁
wandb:                   task_policy_mask_1 █▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▁▁▁▁
wandb:                   task_policy_mask_2 ▁█████████████████████████▂▂▂▂▂
wandb:                   task_policy_mask_3 █▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁
wandb:                   task_policy_mask_4 ▂█████████████████████████▁▁▁▁▁
wandb:                   task_policy_mask_5 ▁█████████████████████████▅▅▅▅▅
wandb:                   task_policy_mask_6 ▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████
wandb:                   task_policy_mask_7 █▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▁▁▁▁
wandb:                   task_policy_mask_8 █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▄▄▄
wandb:                   task_policy_mask_9 ▅█████████████████████████▁▁▁▁▁
wandb:         window-close-v1_eval_rewards ▆▆▆▇▇▇▇▆▅▅▅▄▃▃▂▁▁▁▃▄▅▆▆▅▃▃▂███
wandb:         window-close-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          window-open-v1_eval_rewards ▆▆▆▆▅▅▅▅▅▅▆▇████▇▇▆▆▅▅▄▃▂▁▁▄▃▃
wandb:          window-open-v1_success_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                    0 0.0
wandb:                                    1 0.0
wandb:                                    2 0.0
wandb:                                    3 0.0
wandb:                                    4 0.0
wandb:                                    5 0.0
wandb:                                    6 0.0
wandb:                                    7 0.0
wandb:                                    8 0.0
wandb:                                    9 0.0
wandb:                           Alpha_loss -0.30157
wandb:                         Eval____Time 0.0149
wandb:                         Explore_Time 0.02233
wandb:                          Reward_Mean 8.31942
wandb:              Running_Average_Rewards 1113.64337
wandb:     Running_Training_Average_Rewards 742.30007
wandb:                   Train_Epoch_Reward 9228.92964
wandb:                         Train___Time 1.27664
wandb:                     Training/pf_norm 0.12383
wandb:                 Training/policy_loss -3.01234
wandb:                    Training/qf1_loss 1170.96082
wandb:                    Training/qf1_norm 53.69204
wandb:                    Training/qf2_loss 1170.96655
wandb:                    Training/qf2_norm 59.29476
wandb:                              alpha_0 0.95595
wandb:                              alpha_1 0.95592
wandb:                              alpha_2 0.95592
wandb:                              alpha_3 0.95591
wandb:                              alpha_4 0.95594
wandb:                              alpha_5 0.9559
wandb:                              alpha_6 0.9559
wandb:                              alpha_7 0.95592
wandb:                              alpha_8 0.95591
wandb:                              alpha_9 0.9559
wandb: button-press-topdown-v1_eval_rewards -52.0397
wandb: button-press-topdown-v1_success_rate 0.0
wandb:                 door-v1_eval_rewards -42.27024
wandb:                 door-v1_success_rate 0.0
wandb:         drawer-close-v1_eval_rewards -18.46724
wandb:         drawer-close-v1_success_rate 0.0
wandb:          drawer-open-v1_eval_rewards -24.55295
wandb:          drawer-open-v1_success_rate 0.0
wandb:                        log_probs/max -2.17935
wandb:                       log_probs/mean -2.73418
wandb:                        log_probs/min -4.2401
wandb:                        log_probs/std 0.22299
wandb:                          log_std/max -0.086
wandb:                         log_std/mean -0.13101
wandb:                          log_std/min -0.1809
wandb:                          log_std/std 0.01852
wandb:                             mean/max 0.0166
wandb:                            mean/mean 0.00482
wandb:                             mean/min -0.00606
wandb:                             mean/std 0.00626
wandb:                    mean_success_rate 0.0
wandb:      ped-insert-side-v1_eval_rewards -21.66428
wandb:      ped-insert-side-v1_success_rate 0.0
wandb:           pick-place-v1_eval_rewards -22.3365
wandb:           pick-place-v1_success_rate 0.0
wandb:                 push-v1_eval_rewards -21.57342
wandb:                 push-v1_success_rate 0.0
wandb:                reach-v1_eval_rewards 11393.0126
wandb:                reach-v1_success_rate 0.0
wandb:                    save_traj_mod_sum 1
wandb:                   task_policy_mask_0 8289
wandb:                   task_policy_mask_1 8458
wandb:                   task_policy_mask_2 8633
wandb:                   task_policy_mask_3 8523
wandb:                   task_policy_mask_4 8529
wandb:                   task_policy_mask_5 8599
wandb:                   task_policy_mask_6 8815
wandb:                   task_policy_mask_7 8595
wandb:                   task_policy_mask_8 8725
wandb:                   task_policy_mask_9 8364
wandb:         window-close-v1_eval_rewards -26.28794
wandb:         window-close-v1_success_rate 0.0
wandb:          window-open-v1_eval_rewards -27.38663
wandb:          window-open-v1_success_rate 0.0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /lustre04/scratch/qianxi/sparse_training/sep_t3s/DST_RL/wandb/offline-run-20231103_000524-xjx2kgg1
wandb: Find logs at: ./wandb/offline-run-20231103_000524-xjx2kgg1/logs
