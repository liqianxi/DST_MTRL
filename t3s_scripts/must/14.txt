W&B disabled.
2023-08-12 10:38:41,702 MainThread INFO: Experiment Name:testing_must_mtsac
2023-08-12 10:38:41,702 MainThread INFO: {
  "env_name": "mt10",
  "env": {
    "reward_scale": 1,
    "obs_norm": false
  },
  "meta_env": {
    "obs_type": "with_goal_and_id"
  },
  "replay_buffer": {
    "size": 1000000.0
  },
  "net": {
    "hidden_shapes": [
      10,
      10
    ]
  },
  "task_embedding": {
    "em_hidden_shapes": [
      10,
      10
    ]
  },
  "traj_encoder": {
    "hidden_shapes": [
      10,
      10
    ],
    "latent_size": 64
  },
  "sparse_training": {
    "pruning_ratio": 0.5
  },
  "general_setting": {
    "discount": 0.99,
    "pretrain_epochs": 0,
    "num_epochs": 80,
    "epoch_frames": 150,
    "max_episode_frames": 150,
    "batch_size": 1280,
    "min_pool": 10000,
    "target_hard_update_period": 1000,
    "use_soft_update": true,
    "tau": 0.005,
    "opt_times": 5,
    "mask_update_interval": 2,
    "update_end_epoch": 3750,
    "eval_episodes": 3
  },
  "sac": {
    "plr": 0.0003,
    "qlr": 0.0003,
    "reparameterization": true,
    "automatic_entropy_tuning": true,
    "policy_std_reg_weight": 0,
    "policy_mean_reg_weight": 0
  }
}
finish policy net init
mask generator finish initialization
/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Could not seed environment <MTEnv instance>[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
2023-08-12 10:40:06,545 MainThread INFO: Finished Pretrain
  0%|          | 0/80 [00:00<?, ?it/s]self.update_end_epoch 3750
sample: [9, 5, 3, 2, 8, 0, 6, 1, 7, 4]
replay_buffer._size: [300 300 300 300 300 300 300 300 300 300]
snapshot at best
2023-08-12 10:40:12,701 MainThread INFO: EPOCH:0
2023-08-12 10:40:12,701 MainThread INFO: Time Consumed:6.119117021560669s
2023-08-12 10:40:12,701 MainThread INFO: Total Frames:1500s
/scratch/qianxi/t3s/t3s_code/./torchrl/algo/rl_algo.py:364: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  value = torch.sum((self.mask_buffer["Policy"][each_task][0] == 0).nonzero().squeeze()).item()
  1%|‚ñè         | 1/80 [00:06<09:06,  6.92s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1186.48874
Train_Epoch_Reward                    7943.17692
Running_Training_Average_Rewards      794.31769
Explore_Time                          0.00432
Train___Time                          0.44883
Eval____Time                          5.18416
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.81411
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.58064
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.85047
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.52161
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.01480
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.72233
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.85048
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12112.36416
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.60918
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.51316
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           12.93314     2.27932    17.31365    10.78271
alpha_0                               0.99910      0.00042    0.99970     0.99850
alpha_1                               0.99910      0.00042    0.99970     0.99850
alpha_2                               0.99910      0.00042    0.99970     0.99850
alpha_3                               0.99910      0.00042    0.99970     0.99850
alpha_4                               0.99910      0.00042    0.99970     0.99850
alpha_5                               0.99910      0.00042    0.99970     0.99850
alpha_6                               0.99910      0.00042    0.99970     0.99850
alpha_7                               0.99910      0.00042    0.99970     0.99850
alpha_8                               0.99910      0.00042    0.99970     0.99850
alpha_9                               0.99910      0.00042    0.99970     0.99850
Alpha_loss                            -0.00401     0.00284    -0.00000    -0.00802
Training/policy_loss                  -2.67625     0.01217    -2.66172    -2.69443
Training/qf1_loss                     3985.80356   770.40455  5484.88623  3370.71948
Training/qf2_loss                     3985.67144   770.38825  5484.72119  3370.60205
Training/pf_norm                      0.43305      0.02868    0.48090     0.39995
Training/qf1_norm                     31.71498     4.62657    40.59393    27.30868
Training/qf2_norm                     33.57747     5.07367    43.33053    28.80940
log_std/mean                          -0.00226     0.00065    -0.00134    -0.00318
log_std/std                           0.00121      0.00001    0.00123     0.00121
log_std/max                           0.00022      0.00066    0.00116     -0.00071
log_std/min                           -0.00446     0.00067    -0.00351    -0.00541
log_probs/mean                        -2.67918     0.01245    -2.66427    -2.69774
log_probs/std                         0.43383      0.00852    0.44313     0.42237
log_probs/max                         -1.31072     0.06402    -1.18584    -1.36517
log_probs/min                         -4.57440     0.72450    -3.95482    -5.95515
mean/mean                             -0.00140     0.00005    -0.00133    -0.00147
mean/std                              0.00205      0.00005    0.00213     0.00199
mean/max                              0.00224      0.00032    0.00273     0.00189
mean/min                              -0.00461     0.00073    -0.00356    -0.00563
------------------------------------  -----------  ---------  ----------  ----------
snapshot at 0
history save at ./log/testing_must_mtsac/mt10/14/model
self.update_end_epoch 3750
sample: [6, 0, 8, 5, 4, 9, 2, 1, 7, 3]
replay_buffer._size: [450 450 450 450 450 450 450 450 450 450]
snapshot at best
2023-08-12 10:40:14,385 MainThread INFO: EPOCH:1
2023-08-12 10:40:14,385 MainThread INFO: Time Consumed:0.8918566703796387s
2023-08-12 10:40:14,385 MainThread INFO: Total Frames:3000s
  2%|‚ñé         | 2/80 [00:07<04:28,  3.44s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1186.49511
Train_Epoch_Reward                    42779.74968
Running_Training_Average_Rewards      2536.14633
Explore_Time                          0.00271
Train___Time                          0.34802
Eval____Time                          0.00288
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.75039
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.58064
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.85047
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.52161
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.01480
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.72233
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.85048
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12112.36416
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.60918
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.51316
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.39704     1.53925    12.99855    9.20613
alpha_0                               0.99760      0.00042    0.99820     0.99700
alpha_1                               0.99760      0.00042    0.99820     0.99700
alpha_2                               0.99760      0.00042    0.99820     0.99700
alpha_3                               0.99760      0.00042    0.99820     0.99700
alpha_4                               0.99760      0.00042    0.99820     0.99700
alpha_5                               0.99760      0.00042    0.99820     0.99700
alpha_6                               0.99760      0.00042    0.99820     0.99700
alpha_7                               0.99760      0.00042    0.99820     0.99700
alpha_8                               0.99760      0.00042    0.99820     0.99700
alpha_9                               0.99760      0.00042    0.99820     0.99700
Alpha_loss                            -0.01405     0.00284    -0.01006    -0.01807
Training/policy_loss                  -2.68472     0.01273    -2.66829    -2.70267
Training/qf1_loss                     3313.65537   756.90434  4361.20264  2414.67310
Training/qf2_loss                     3313.52407   756.89083  4361.05859  2414.55225
Training/pf_norm                      0.41263      0.03555    0.45324     0.35245
Training/qf1_norm                     28.57578     3.17133    31.88923    24.07196
Training/qf2_norm                     30.34768     3.38140    33.80357    25.51514
log_std/mean                          -0.00456     0.00065    -0.00364    -0.00549
log_std/std                           0.00125      0.00001    0.00127     0.00124
log_std/max                           -0.00212     0.00066    -0.00118    -0.00306
log_std/min                           -0.00682     0.00066    -0.00589    -0.00775
log_probs/mean                        -2.68943     0.01269    -2.67296    -2.70669
log_probs/std                         0.42379      0.00703    0.43028     0.41151
log_probs/max                         -1.33231     0.08943    -1.19893    -1.42454
log_probs/min                         -4.23502     0.37832    -3.58085    -4.69866
mean/mean                             -0.00157     0.00002    -0.00152    -0.00159
mean/std                              0.00206      0.00002    0.00209     0.00204
mean/max                              0.00180      0.00007    0.00189     0.00171
mean/min                              -0.00604     0.00010    -0.00589    -0.00617
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4976, 0.5061, 0.4989, 0.5059, 0.5073, 0.4882, 0.4958, 0.5020, 0.4999,
         0.5025, 0.5096, 0.5014, 0.4959, 0.4891, 0.4932, 0.5077, 0.4927, 0.5088,
         0.4963, 0.5143],
        [0.4983, 0.5067, 0.4989, 0.5059, 0.5068, 0.4892, 0.4961, 0.5023, 0.5022,
         0.5027, 0.5080, 0.4996, 0.4962, 0.4883, 0.4912, 0.5077, 0.4922, 0.5084,
         0.4962, 0.5130],
        [0.4979, 0.5065, 0.4979, 0.5063, 0.5069, 0.4883, 0.4966, 0.5003, 0.5043,
         0.5009, 0.5056, 0.4997, 0.4964, 0.4885, 0.4908, 0.5103, 0.4938, 0.5082,
         0.4951, 0.5152],
        [0.4989, 0.5053, 0.4986, 0.5069, 0.5064, 0.4896, 0.4945, 0.4995, 0.5025,
         0.5040, 0.5050, 0.4984, 0.4974, 0.4879, 0.4893, 0.5083, 0.4921, 0.5098,
         0.4922, 0.5146],
        [0.4987, 0.5073, 0.4979, 0.5049, 0.5059, 0.4884, 0.4963, 0.4993, 0.5020,
         0.5025, 0.5084, 0.5001, 0.4968, 0.4885, 0.4923, 0.5063, 0.4942, 0.5095,
         0.4953, 0.5134],
        [0.4978, 0.5041, 0.4977, 0.5061, 0.5055, 0.4884, 0.4948, 0.5001, 0.5011,
         0.5031, 0.5067, 0.5001, 0.4953, 0.4881, 0.4904, 0.5090, 0.4953, 0.5078,
         0.4947, 0.5139],
        [0.4978, 0.5055, 0.4989, 0.5039, 0.5063, 0.4871, 0.4981, 0.5008, 0.5040,
         0.5020, 0.5107, 0.4997, 0.4960, 0.4864, 0.4910, 0.5080, 0.4960, 0.5072,
         0.4974, 0.5125],
        [0.4981, 0.5046, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5012, 0.5030,
         0.5022, 0.5090, 0.4999, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5063,
         0.4936, 0.5131],
        [0.4982, 0.5049, 0.4973, 0.5053, 0.5077, 0.4881, 0.4961, 0.4999, 0.5019,
         0.5010, 0.5060, 0.5014, 0.4975, 0.4887, 0.4922, 0.5084, 0.4928, 0.5091,
         0.4940, 0.5148],
        [0.4984, 0.5057, 0.4991, 0.5051, 0.5071, 0.4884, 0.4959, 0.5014, 0.5021,
         0.5035, 0.5075, 0.5005, 0.4951, 0.4898, 0.4926, 0.5087, 0.4939, 0.5060,
         0.4953, 0.5149]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4976, 0.5061, 0.4989, 0.5059, 0.5073, 0.4882, 0.4958, 0.5020, 0.4999,
        0.5025, 0.5096, 0.5014, 0.4959, 0.4891, 0.4932, 0.5077, 0.4927, 0.5088,
        0.4963, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5067, 0.4989, 0.5059, 0.5068, 0.4892, 0.4961, 0.5023, 0.5022,
        0.5027, 0.5080, 0.4996, 0.4962, 0.4883, 0.4912, 0.5077, 0.4922, 0.5084,
        0.4962, 0.5130], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5065, 0.4979, 0.5063, 0.5069, 0.4883, 0.4966, 0.5003, 0.5043,
        0.5009, 0.5056, 0.4997, 0.4964, 0.4885, 0.4908, 0.5103, 0.4938, 0.5082,
        0.4951, 0.5152], grad_fn=<UnbindBackward>), tensor([0.4989, 0.5053, 0.4986, 0.5069, 0.5064, 0.4896, 0.4945, 0.4995, 0.5025,
        0.5040, 0.5050, 0.4984, 0.4974, 0.4879, 0.4893, 0.5083, 0.4921, 0.5098,
        0.4922, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5073, 0.4979, 0.5049, 0.5059, 0.4884, 0.4963, 0.4993, 0.5020,
        0.5025, 0.5084, 0.5001, 0.4968, 0.4885, 0.4923, 0.5063, 0.4942, 0.5095,
        0.4953, 0.5134], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5041, 0.4977, 0.5061, 0.5055, 0.4884, 0.4948, 0.5001, 0.5011,
        0.5031, 0.5067, 0.5001, 0.4953, 0.4881, 0.4904, 0.5090, 0.4953, 0.5078,
        0.4947, 0.5139], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5055, 0.4989, 0.5039, 0.5063, 0.4871, 0.4981, 0.5008, 0.5040,
        0.5020, 0.5107, 0.4997, 0.4960, 0.4864, 0.4910, 0.5080, 0.4960, 0.5072,
        0.4974, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5046, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5012, 0.5030,
        0.5022, 0.5090, 0.4999, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5063,
        0.4936, 0.5131], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5049, 0.4973, 0.5053, 0.5077, 0.4881, 0.4961, 0.4999, 0.5019,
        0.5010, 0.5060, 0.5014, 0.4975, 0.4887, 0.4922, 0.5084, 0.4928, 0.5091,
        0.4940, 0.5148], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4991, 0.5051, 0.5071, 0.4884, 0.4959, 0.5014, 0.5021,
        0.5035, 0.5075, 0.5005, 0.4951, 0.4898, 0.4926, 0.5087, 0.4939, 0.5060,
        0.4953, 0.5149], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5108, 0.4894, 0.5116, 0.4868, 0.4861, 0.4935, 0.4893, 0.4941, 0.4989,
         0.5070, 0.5026, 0.4989, 0.5022, 0.4909, 0.5036, 0.4918, 0.4856, 0.4989,
         0.5080, 0.4950],
        [0.5089, 0.4894, 0.5110, 0.4855, 0.4860, 0.4921, 0.4881, 0.4950, 0.4983,
         0.5050, 0.5032, 0.4989, 0.5032, 0.4910, 0.5021, 0.4898, 0.4858, 0.4988,
         0.5073, 0.4937],
        [0.5107, 0.4908, 0.5107, 0.4873, 0.4854, 0.4936, 0.4887, 0.4965, 0.5019,
         0.5066, 0.5050, 0.5000, 0.5061, 0.4918, 0.5033, 0.4916, 0.4878, 0.4977,
         0.5069, 0.4926],
        [0.5078, 0.4894, 0.5112, 0.4858, 0.4877, 0.4923, 0.4892, 0.4954, 0.4987,
         0.5086, 0.5038, 0.4991, 0.5049, 0.4908, 0.5030, 0.4910, 0.4863, 0.4988,
         0.5062, 0.4922],
        [0.5096, 0.4912, 0.5127, 0.4866, 0.4871, 0.4924, 0.4904, 0.4927, 0.4962,
         0.5062, 0.5040, 0.4991, 0.5032, 0.4913, 0.5029, 0.4899, 0.4858, 0.4967,
         0.5057, 0.4937],
        [0.5116, 0.4917, 0.5097, 0.4869, 0.4863, 0.4947, 0.4910, 0.4935, 0.4968,
         0.5070, 0.5062, 0.4997, 0.5032, 0.4885, 0.5033, 0.4908, 0.4848, 0.4975,
         0.5064, 0.4936],
        [0.5095, 0.4908, 0.5128, 0.4896, 0.4854, 0.4933, 0.4887, 0.4941, 0.4974,
         0.5069, 0.5065, 0.4989, 0.5050, 0.4896, 0.5029, 0.4890, 0.4841, 0.4998,
         0.5073, 0.4935],
        [0.5101, 0.4907, 0.5106, 0.4854, 0.4861, 0.4928, 0.4891, 0.4954, 0.5005,
         0.5072, 0.5062, 0.5006, 0.5035, 0.4897, 0.5043, 0.4915, 0.4862, 0.4963,
         0.5075, 0.4930],
        [0.5104, 0.4919, 0.5106, 0.4856, 0.4877, 0.4929, 0.4897, 0.4939, 0.4978,
         0.5074, 0.5042, 0.4980, 0.5038, 0.4900, 0.5040, 0.4904, 0.4868, 0.4972,
         0.5079, 0.4936],
        [0.5109, 0.4911, 0.5114, 0.4875, 0.4873, 0.4928, 0.4891, 0.4936, 0.4997,
         0.5054, 0.5047, 0.4986, 0.5034, 0.4891, 0.5005, 0.4927, 0.4842, 0.4982,
         0.5060, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5108, 0.4894, 0.5116, 0.4868, 0.4861, 0.4935, 0.4893, 0.4941, 0.4989,
        0.5070, 0.5026, 0.4989, 0.5022, 0.4909, 0.5036, 0.4918, 0.4856, 0.4989,
        0.5080, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4894, 0.5110, 0.4855, 0.4860, 0.4921, 0.4881, 0.4950, 0.4983,
        0.5050, 0.5032, 0.4989, 0.5032, 0.4910, 0.5021, 0.4898, 0.4858, 0.4988,
        0.5073, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4908, 0.5107, 0.4873, 0.4854, 0.4936, 0.4887, 0.4965, 0.5019,
        0.5066, 0.5050, 0.5000, 0.5061, 0.4918, 0.5033, 0.4916, 0.4878, 0.4977,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4894, 0.5112, 0.4858, 0.4877, 0.4923, 0.4892, 0.4954, 0.4987,
        0.5086, 0.5038, 0.4991, 0.5049, 0.4908, 0.5030, 0.4910, 0.4863, 0.4988,
        0.5062, 0.4922], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4912, 0.5127, 0.4866, 0.4871, 0.4924, 0.4904, 0.4927, 0.4962,
        0.5062, 0.5040, 0.4991, 0.5032, 0.4913, 0.5029, 0.4899, 0.4858, 0.4967,
        0.5057, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4917, 0.5097, 0.4869, 0.4863, 0.4947, 0.4910, 0.4935, 0.4968,
        0.5070, 0.5062, 0.4997, 0.5032, 0.4885, 0.5033, 0.4908, 0.4848, 0.4975,
        0.5064, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4908, 0.5128, 0.4896, 0.4854, 0.4933, 0.4887, 0.4941, 0.4974,
        0.5069, 0.5065, 0.4989, 0.5050, 0.4896, 0.5029, 0.4890, 0.4841, 0.4998,
        0.5073, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4907, 0.5106, 0.4854, 0.4861, 0.4928, 0.4891, 0.4954, 0.5005,
        0.5072, 0.5062, 0.5006, 0.5035, 0.4897, 0.5043, 0.4915, 0.4862, 0.4963,
        0.5075, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5104, 0.4919, 0.5106, 0.4856, 0.4877, 0.4929, 0.4897, 0.4939, 0.4978,
        0.5074, 0.5042, 0.4980, 0.5038, 0.4900, 0.5040, 0.4904, 0.4868, 0.4972,
        0.5079, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4911, 0.5114, 0.4875, 0.4873, 0.4928, 0.4891, 0.4936, 0.4997,
        0.5054, 0.5047, 0.4986, 0.5034, 0.4891, 0.5005, 0.4927, 0.4842, 0.4982,
        0.5060, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5129, 0.4965, 0.4958, 0.5025, 0.5066, 0.4927, 0.5088, 0.4954, 0.4970,
         0.5028, 0.4860, 0.4972, 0.4842, 0.5023, 0.5027, 0.4986, 0.4991, 0.5141,
         0.4936, 0.4971],
        [0.5115, 0.4960, 0.4947, 0.5047, 0.5032, 0.4928, 0.5102, 0.4967, 0.4958,
         0.5027, 0.4848, 0.4963, 0.4866, 0.5010, 0.5001, 0.4977, 0.4978, 0.5147,
         0.4949, 0.4973],
        [0.5131, 0.4979, 0.4968, 0.5021, 0.5044, 0.4925, 0.5098, 0.4981, 0.4967,
         0.5030, 0.4873, 0.4991, 0.4852, 0.5009, 0.4996, 0.4974, 0.4989, 0.5168,
         0.4949, 0.4976],
        [0.5125, 0.4974, 0.4961, 0.5032, 0.5038, 0.4914, 0.5109, 0.4969, 0.4964,
         0.5029, 0.4846, 0.4960, 0.4851, 0.5019, 0.5008, 0.4972, 0.5008, 0.5143,
         0.4956, 0.4972],
        [0.5124, 0.4986, 0.4958, 0.5036, 0.5028, 0.4937, 0.5085, 0.4953, 0.4974,
         0.5034, 0.4883, 0.4982, 0.4828, 0.5021, 0.5000, 0.4990, 0.5000, 0.5154,
         0.4956, 0.4975],
        [0.5138, 0.4971, 0.4951, 0.5029, 0.5056, 0.4928, 0.5076, 0.4971, 0.4955,
         0.5026, 0.4857, 0.4958, 0.4859, 0.5030, 0.5022, 0.5001, 0.4990, 0.5145,
         0.4950, 0.4983],
        [0.5108, 0.4965, 0.4979, 0.5025, 0.5035, 0.4924, 0.5088, 0.4963, 0.4963,
         0.5028, 0.4891, 0.4970, 0.4852, 0.5025, 0.5010, 0.4987, 0.4999, 0.5136,
         0.4952, 0.4998],
        [0.5123, 0.4972, 0.4948, 0.5016, 0.5037, 0.4923, 0.5086, 0.4947, 0.4970,
         0.5026, 0.4858, 0.4986, 0.4866, 0.5008, 0.5007, 0.4982, 0.4991, 0.5155,
         0.4958, 0.4982],
        [0.5126, 0.4981, 0.4952, 0.5015, 0.5043, 0.4925, 0.5099, 0.4967, 0.4967,
         0.5022, 0.4887, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5005, 0.5138,
         0.4955, 0.4990],
        [0.5124, 0.4982, 0.4943, 0.5030, 0.5044, 0.4936, 0.5085, 0.4984, 0.4965,
         0.5035, 0.4884, 0.4983, 0.4856, 0.5002, 0.5005, 0.4980, 0.4991, 0.5156,
         0.4965, 0.4996]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5129, 0.4965, 0.4958, 0.5025, 0.5066, 0.4927, 0.5088, 0.4954, 0.4970,
        0.5028, 0.4860, 0.4972, 0.4842, 0.5023, 0.5027, 0.4986, 0.4991, 0.5141,
        0.4936, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4960, 0.4947, 0.5047, 0.5032, 0.4928, 0.5102, 0.4967, 0.4958,
        0.5027, 0.4848, 0.4963, 0.4866, 0.5010, 0.5001, 0.4977, 0.4978, 0.5147,
        0.4949, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4979, 0.4968, 0.5021, 0.5044, 0.4925, 0.5098, 0.4981, 0.4967,
        0.5030, 0.4873, 0.4991, 0.4852, 0.5009, 0.4996, 0.4974, 0.4989, 0.5168,
        0.4949, 0.4976], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4974, 0.4961, 0.5032, 0.5038, 0.4914, 0.5109, 0.4969, 0.4964,
        0.5029, 0.4846, 0.4960, 0.4851, 0.5019, 0.5008, 0.4972, 0.5008, 0.5143,
        0.4956, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4986, 0.4958, 0.5036, 0.5028, 0.4937, 0.5085, 0.4953, 0.4974,
        0.5034, 0.4883, 0.4982, 0.4828, 0.5021, 0.5000, 0.4990, 0.5000, 0.5154,
        0.4956, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5138, 0.4971, 0.4951, 0.5029, 0.5056, 0.4928, 0.5076, 0.4971, 0.4955,
        0.5026, 0.4857, 0.4958, 0.4859, 0.5030, 0.5022, 0.5001, 0.4990, 0.5145,
        0.4950, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5108, 0.4965, 0.4979, 0.5025, 0.5035, 0.4924, 0.5088, 0.4963, 0.4963,
        0.5028, 0.4891, 0.4970, 0.4852, 0.5025, 0.5010, 0.4987, 0.4999, 0.5136,
        0.4952, 0.4998], grad_fn=<UnbindBackward>), tensor([0.5123, 0.4972, 0.4948, 0.5016, 0.5037, 0.4923, 0.5086, 0.4947, 0.4970,
        0.5026, 0.4858, 0.4986, 0.4866, 0.5008, 0.5007, 0.4982, 0.4991, 0.5155,
        0.4958, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4981, 0.4952, 0.5015, 0.5043, 0.4925, 0.5099, 0.4967, 0.4967,
        0.5022, 0.4887, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5005, 0.5138,
        0.4955, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4982, 0.4943, 0.5030, 0.5044, 0.4936, 0.5085, 0.4984, 0.4965,
        0.5035, 0.4884, 0.4983, 0.4856, 0.5002, 0.5005, 0.4980, 0.4991, 0.5156,
        0.4965, 0.4996], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [7, 9, 0, 1, 8, 2, 3, 4, 6, 5]
replay_buffer._size: [600 600 600 600 600 600 600 600 600 600]
2023-08-12 10:40:15,208 MainThread INFO: EPOCH:2
2023-08-12 10:40:15,208 MainThread INFO: Time Consumed:0.37389492988586426s
2023-08-12 10:40:15,208 MainThread INFO: Total Frames:4500s
  4%|‚ñç         | 3/80 [00:08<02:52,  2.25s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1186.32889
Train_Epoch_Reward                    4129.32455
Running_Training_Average_Rewards      1828.40837
Explore_Time                          0.00354
Train___Time                          0.35964
Eval____Time                          0.00365
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.41254
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.58064
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.85047
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.52161
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.01480
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.72233
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.85048
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12112.36416
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.60918
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.51316
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.97519     0.69013    12.06577    10.16110
alpha_0                               0.99611      0.00042    0.99671     0.99551
alpha_1                               0.99611      0.00042    0.99671     0.99551
alpha_2                               0.99611      0.00042    0.99670     0.99551
alpha_3                               0.99611      0.00042    0.99670     0.99551
alpha_4                               0.99611      0.00042    0.99671     0.99551
alpha_5                               0.99611      0.00042    0.99671     0.99551
alpha_6                               0.99611      0.00042    0.99671     0.99551
alpha_7                               0.99611      0.00042    0.99670     0.99551
alpha_8                               0.99611      0.00042    0.99671     0.99551
alpha_9                               0.99611      0.00042    0.99670     0.99551
Alpha_loss                            -0.02405     0.00285    -0.02004    -0.02805
Training/policy_loss                  -2.67367     0.01238    -2.65900    -2.69657
Training/qf1_loss                     2937.57295   362.58634  3369.54053  2499.06519
Training/qf2_loss                     2937.44736   362.58026  3369.41016  2498.94629
Training/pf_norm                      0.45431      0.03087    0.48082     0.39524
Training/qf1_norm                     27.90501     1.40609    30.11692    26.25478
Training/qf2_norm                     27.98687     1.40844    30.18303    26.32708
log_std/mean                          -0.00761     0.00078    -0.00652    -0.00872
log_std/std                           0.00095      0.00002    0.00099     0.00093
log_std/max                           -0.00564     0.00070    -0.00467    -0.00664
log_std/min                           -0.00962     0.00090    -0.00836    -0.01090
log_probs/mean                        -2.68072     0.01256    -2.66600    -2.70408
log_probs/std                         0.42644      0.01447    0.44994     0.40495
log_probs/max                         -1.26122     0.08909    -1.18272    -1.41237
log_probs/min                         -4.75047     1.66812    -3.58683    -8.00728
mean/mean                             -0.00103     0.00018    -0.00077    -0.00126
mean/std                              0.00244      0.00001    0.00245     0.00243
mean/max                              0.00276      0.00029    0.00319     0.00241
mean/min                              -0.00496     0.00035    -0.00440    -0.00535
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [3, 1, 4, 5, 9, 2, 0, 7, 8, 6]
replay_buffer._size: [750 750 750 750 750 750 750 750 750 750]
2023-08-12 10:40:15,662 MainThread INFO: EPOCH:3
2023-08-12 10:40:15,662 MainThread INFO: Time Consumed:0.34055542945861816s
2023-08-12 10:40:15,662 MainThread INFO: Total Frames:6000s
  5%|‚ñå         | 4/80 [00:09<01:56,  1.53s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1177.65094
Train_Epoch_Reward                    17449.22718
Running_Training_Average_Rewards      2145.27671
Explore_Time                          0.00230
Train___Time                          0.33586
Eval____Time                          0.00197
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.06061
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.60264
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.82284
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.53957
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.01816
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.75603
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.86142
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12024.36610
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.62792
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.56749
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.22122     1.17270    11.50242    8.03513
alpha_0                               0.99462      0.00042    0.99521     0.99402
alpha_1                               0.99461      0.00042    0.99521     0.99402
alpha_2                               0.99461      0.00042    0.99521     0.99402
alpha_3                               0.99461      0.00042    0.99521     0.99402
alpha_4                               0.99461      0.00042    0.99521     0.99402
alpha_5                               0.99461      0.00042    0.99521     0.99402
alpha_6                               0.99461      0.00042    0.99521     0.99402
alpha_7                               0.99461      0.00042    0.99521     0.99402
alpha_8                               0.99461      0.00042    0.99521     0.99402
alpha_9                               0.99461      0.00042    0.99521     0.99402
Alpha_loss                            -0.03414     0.00285    -0.03007    -0.03816
Training/policy_loss                  -2.68439     0.00768    -2.67353    -2.69696
Training/qf1_loss                     2584.85972   613.05203  3112.51465  1389.02734
Training/qf2_loss                     2584.74004   613.04235  3112.38916  1388.92651
Training/pf_norm                      0.42202      0.02696    0.45639     0.37382
Training/qf1_norm                     26.36396     2.37671    28.97303    21.94132
Training/qf2_norm                     26.44997     2.38111    29.09535    22.03412
log_std/mean                          -0.01046     0.00083    -0.00928    -0.01164
log_std/std                           0.00106      0.00004    0.00113     0.00101
log_std/max                           -0.00820     0.00074    -0.00716    -0.00924
log_std/min                           -0.01293     0.00097    -0.01157    -0.01431
log_probs/mean                        -2.69326     0.00779    -2.68166    -2.70555
log_probs/std                         0.41974      0.01257    0.44142     0.40524
log_probs/max                         -1.41465     0.06236    -1.35496    -1.52863
log_probs/min                         -4.81648     0.27916    -4.58335    -5.30755
mean/mean                             -0.00016     0.00029    0.00025     -0.00057
mean/std                              0.00239      0.00003    0.00242     0.00235
mean/max                              0.00376      0.00020    0.00402     0.00344
mean/min                              -0.00342     0.00043    -0.00284    -0.00404
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4976, 0.5061, 0.4990, 0.5060, 0.5073, 0.4883, 0.4959, 0.5021, 0.4998,
         0.5025, 0.5097, 0.5015, 0.4958, 0.4891, 0.4932, 0.5076, 0.4927, 0.5088,
         0.4964, 0.5142],
        [0.4983, 0.5068, 0.4989, 0.5060, 0.5068, 0.4892, 0.4961, 0.5024, 0.5022,
         0.5027, 0.5081, 0.4996, 0.4962, 0.4883, 0.4911, 0.5077, 0.4922, 0.5084,
         0.4962, 0.5130],
        [0.4979, 0.5065, 0.4980, 0.5063, 0.5069, 0.4882, 0.4967, 0.5003, 0.5043,
         0.5009, 0.5056, 0.4996, 0.4964, 0.4885, 0.4908, 0.5103, 0.4938, 0.5083,
         0.4951, 0.5152],
        [0.4989, 0.5053, 0.4986, 0.5069, 0.5064, 0.4895, 0.4946, 0.4995, 0.5026,
         0.5040, 0.5051, 0.4984, 0.4974, 0.4878, 0.4893, 0.5083, 0.4922, 0.5098,
         0.4923, 0.5146],
        [0.4987, 0.5074, 0.4979, 0.5049, 0.5059, 0.4884, 0.4963, 0.4992, 0.5020,
         0.5025, 0.5084, 0.5001, 0.4969, 0.4884, 0.4922, 0.5063, 0.4942, 0.5095,
         0.4954, 0.5134],
        [0.4978, 0.5041, 0.4977, 0.5061, 0.5055, 0.4884, 0.4947, 0.5000, 0.5010,
         0.5031, 0.5067, 0.5001, 0.4952, 0.4881, 0.4904, 0.5090, 0.4953, 0.5078,
         0.4947, 0.5139],
        [0.4979, 0.5055, 0.4989, 0.5039, 0.5063, 0.4871, 0.4980, 0.5007, 0.5039,
         0.5020, 0.5107, 0.4996, 0.4960, 0.4864, 0.4909, 0.5080, 0.4960, 0.5072,
         0.4973, 0.5125],
        [0.4981, 0.5046, 0.4986, 0.5050, 0.5065, 0.4879, 0.4955, 0.5012, 0.5030,
         0.5022, 0.5092, 0.4999, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5063,
         0.4936, 0.5130],
        [0.4981, 0.5049, 0.4973, 0.5053, 0.5078, 0.4881, 0.4962, 0.4999, 0.5019,
         0.5010, 0.5061, 0.5015, 0.4975, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
         0.4940, 0.5147],
        [0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4883, 0.4959, 0.5014, 0.5021,
         0.5035, 0.5076, 0.5004, 0.4951, 0.4899, 0.4926, 0.5087, 0.4939, 0.5059,
         0.4952, 0.5149]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4976, 0.5061, 0.4990, 0.5060, 0.5073, 0.4883, 0.4959, 0.5021, 0.4998,
        0.5025, 0.5097, 0.5015, 0.4958, 0.4891, 0.4932, 0.5076, 0.4927, 0.5088,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5068, 0.4989, 0.5060, 0.5068, 0.4892, 0.4961, 0.5024, 0.5022,
        0.5027, 0.5081, 0.4996, 0.4962, 0.4883, 0.4911, 0.5077, 0.4922, 0.5084,
        0.4962, 0.5130], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5065, 0.4980, 0.5063, 0.5069, 0.4882, 0.4967, 0.5003, 0.5043,
        0.5009, 0.5056, 0.4996, 0.4964, 0.4885, 0.4908, 0.5103, 0.4938, 0.5083,
        0.4951, 0.5152], grad_fn=<UnbindBackward>), tensor([0.4989, 0.5053, 0.4986, 0.5069, 0.5064, 0.4895, 0.4946, 0.4995, 0.5026,
        0.5040, 0.5051, 0.4984, 0.4974, 0.4878, 0.4893, 0.5083, 0.4922, 0.5098,
        0.4923, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5074, 0.4979, 0.5049, 0.5059, 0.4884, 0.4963, 0.4992, 0.5020,
        0.5025, 0.5084, 0.5001, 0.4969, 0.4884, 0.4922, 0.5063, 0.4942, 0.5095,
        0.4954, 0.5134], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5041, 0.4977, 0.5061, 0.5055, 0.4884, 0.4947, 0.5000, 0.5010,
        0.5031, 0.5067, 0.5001, 0.4952, 0.4881, 0.4904, 0.5090, 0.4953, 0.5078,
        0.4947, 0.5139], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5055, 0.4989, 0.5039, 0.5063, 0.4871, 0.4980, 0.5007, 0.5039,
        0.5020, 0.5107, 0.4996, 0.4960, 0.4864, 0.4909, 0.5080, 0.4960, 0.5072,
        0.4973, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5046, 0.4986, 0.5050, 0.5065, 0.4879, 0.4955, 0.5012, 0.5030,
        0.5022, 0.5092, 0.4999, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5063,
        0.4936, 0.5130], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5049, 0.4973, 0.5053, 0.5078, 0.4881, 0.4962, 0.4999, 0.5019,
        0.5010, 0.5061, 0.5015, 0.4975, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
        0.4940, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4883, 0.4959, 0.5014, 0.5021,
        0.5035, 0.5076, 0.5004, 0.4951, 0.4899, 0.4926, 0.5087, 0.4939, 0.5059,
        0.4952, 0.5149], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5108, 0.4893, 0.5118, 0.4867, 0.4860, 0.4935, 0.4893, 0.4942, 0.4988,
         0.5071, 0.5027, 0.4990, 0.5022, 0.4909, 0.5037, 0.4919, 0.4856, 0.4989,
         0.5081, 0.4950],
        [0.5089, 0.4894, 0.5110, 0.4855, 0.4859, 0.4922, 0.4882, 0.4950, 0.4983,
         0.5050, 0.5033, 0.4990, 0.5031, 0.4910, 0.5021, 0.4898, 0.4858, 0.4988,
         0.5073, 0.4936],
        [0.5107, 0.4908, 0.5107, 0.4873, 0.4854, 0.4936, 0.4887, 0.4965, 0.5020,
         0.5066, 0.5050, 0.5000, 0.5061, 0.4917, 0.5034, 0.4916, 0.4877, 0.4977,
         0.5069, 0.4926],
        [0.5078, 0.4894, 0.5112, 0.4858, 0.4876, 0.4923, 0.4893, 0.4954, 0.4987,
         0.5086, 0.5039, 0.4991, 0.5048, 0.4908, 0.5031, 0.4909, 0.4863, 0.4988,
         0.5062, 0.4922],
        [0.5096, 0.4912, 0.5127, 0.4865, 0.4872, 0.4924, 0.4905, 0.4926, 0.4961,
         0.5062, 0.5039, 0.4991, 0.5032, 0.4913, 0.5030, 0.4898, 0.4858, 0.4966,
         0.5057, 0.4937],
        [0.5117, 0.4916, 0.5097, 0.4868, 0.4863, 0.4947, 0.4910, 0.4936, 0.4967,
         0.5070, 0.5061, 0.4996, 0.5033, 0.4885, 0.5033, 0.4909, 0.4849, 0.4974,
         0.5064, 0.4936],
        [0.5095, 0.4908, 0.5127, 0.4894, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
         0.5069, 0.5065, 0.4989, 0.5050, 0.4896, 0.5029, 0.4891, 0.4842, 0.4997,
         0.5073, 0.4935],
        [0.5101, 0.4907, 0.5107, 0.4853, 0.4861, 0.4928, 0.4892, 0.4954, 0.5004,
         0.5072, 0.5063, 0.5006, 0.5035, 0.4896, 0.5043, 0.4917, 0.4862, 0.4963,
         0.5075, 0.4930],
        [0.5104, 0.4919, 0.5106, 0.4855, 0.4878, 0.4929, 0.4899, 0.4940, 0.4977,
         0.5074, 0.5043, 0.4980, 0.5037, 0.4900, 0.5040, 0.4905, 0.4869, 0.4971,
         0.5080, 0.4936],
        [0.5109, 0.4912, 0.5115, 0.4875, 0.4873, 0.4928, 0.4892, 0.4936, 0.4997,
         0.5053, 0.5048, 0.4985, 0.5034, 0.4890, 0.5005, 0.4928, 0.4841, 0.4981,
         0.5060, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5108, 0.4893, 0.5118, 0.4867, 0.4860, 0.4935, 0.4893, 0.4942, 0.4988,
        0.5071, 0.5027, 0.4990, 0.5022, 0.4909, 0.5037, 0.4919, 0.4856, 0.4989,
        0.5081, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4894, 0.5110, 0.4855, 0.4859, 0.4922, 0.4882, 0.4950, 0.4983,
        0.5050, 0.5033, 0.4990, 0.5031, 0.4910, 0.5021, 0.4898, 0.4858, 0.4988,
        0.5073, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4908, 0.5107, 0.4873, 0.4854, 0.4936, 0.4887, 0.4965, 0.5020,
        0.5066, 0.5050, 0.5000, 0.5061, 0.4917, 0.5034, 0.4916, 0.4877, 0.4977,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4894, 0.5112, 0.4858, 0.4876, 0.4923, 0.4893, 0.4954, 0.4987,
        0.5086, 0.5039, 0.4991, 0.5048, 0.4908, 0.5031, 0.4909, 0.4863, 0.4988,
        0.5062, 0.4922], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4912, 0.5127, 0.4865, 0.4872, 0.4924, 0.4905, 0.4926, 0.4961,
        0.5062, 0.5039, 0.4991, 0.5032, 0.4913, 0.5030, 0.4898, 0.4858, 0.4966,
        0.5057, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5117, 0.4916, 0.5097, 0.4868, 0.4863, 0.4947, 0.4910, 0.4936, 0.4967,
        0.5070, 0.5061, 0.4996, 0.5033, 0.4885, 0.5033, 0.4909, 0.4849, 0.4974,
        0.5064, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4908, 0.5127, 0.4894, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
        0.5069, 0.5065, 0.4989, 0.5050, 0.4896, 0.5029, 0.4891, 0.4842, 0.4997,
        0.5073, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4907, 0.5107, 0.4853, 0.4861, 0.4928, 0.4892, 0.4954, 0.5004,
        0.5072, 0.5063, 0.5006, 0.5035, 0.4896, 0.5043, 0.4917, 0.4862, 0.4963,
        0.5075, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5104, 0.4919, 0.5106, 0.4855, 0.4878, 0.4929, 0.4899, 0.4940, 0.4977,
        0.5074, 0.5043, 0.4980, 0.5037, 0.4900, 0.5040, 0.4905, 0.4869, 0.4971,
        0.5080, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4912, 0.5115, 0.4875, 0.4873, 0.4928, 0.4892, 0.4936, 0.4997,
        0.5053, 0.5048, 0.4985, 0.5034, 0.4890, 0.5005, 0.4928, 0.4841, 0.4981,
        0.5060, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5129, 0.4964, 0.4959, 0.5024, 0.5066, 0.4926, 0.5087, 0.4954, 0.4970,
         0.5027, 0.4859, 0.4973, 0.4841, 0.5024, 0.5027, 0.4986, 0.4992, 0.5141,
         0.4936, 0.4970],
        [0.5115, 0.4960, 0.4947, 0.5047, 0.5032, 0.4928, 0.5101, 0.4967, 0.4959,
         0.5027, 0.4848, 0.4963, 0.4866, 0.5010, 0.5001, 0.4977, 0.4979, 0.5147,
         0.4949, 0.4973],
        [0.5131, 0.4979, 0.4968, 0.5021, 0.5044, 0.4924, 0.5098, 0.4981, 0.4967,
         0.5030, 0.4873, 0.4991, 0.4852, 0.5010, 0.4996, 0.4974, 0.4989, 0.5168,
         0.4949, 0.4976],
        [0.5125, 0.4974, 0.4962, 0.5032, 0.5039, 0.4915, 0.5109, 0.4969, 0.4964,
         0.5029, 0.4846, 0.4961, 0.4851, 0.5019, 0.5008, 0.4972, 0.5008, 0.5142,
         0.4956, 0.4971],
        [0.5124, 0.4986, 0.4958, 0.5036, 0.5029, 0.4937, 0.5086, 0.4953, 0.4973,
         0.5034, 0.4881, 0.4982, 0.4829, 0.5022, 0.5000, 0.4990, 0.5000, 0.5153,
         0.4956, 0.4975],
        [0.5139, 0.4971, 0.4952, 0.5029, 0.5056, 0.4927, 0.5075, 0.4970, 0.4955,
         0.5026, 0.4856, 0.4958, 0.4859, 0.5031, 0.5021, 0.5001, 0.4991, 0.5146,
         0.4951, 0.4983],
        [0.5109, 0.4965, 0.4979, 0.5025, 0.5035, 0.4924, 0.5088, 0.4964, 0.4962,
         0.5028, 0.4890, 0.4971, 0.4852, 0.5025, 0.5009, 0.4986, 0.4999, 0.5137,
         0.4952, 0.4997],
        [0.5123, 0.4972, 0.4948, 0.5016, 0.5038, 0.4923, 0.5085, 0.4948, 0.4969,
         0.5026, 0.4858, 0.4986, 0.4866, 0.5009, 0.5005, 0.4982, 0.4992, 0.5155,
         0.4957, 0.4982],
        [0.5127, 0.4980, 0.4952, 0.5016, 0.5043, 0.4925, 0.5098, 0.4968, 0.4966,
         0.5021, 0.4886, 0.4951, 0.4851, 0.5017, 0.5010, 0.4971, 0.5005, 0.5139,
         0.4955, 0.4990],
        [0.5124, 0.4982, 0.4943, 0.5031, 0.5043, 0.4936, 0.5084, 0.4985, 0.4964,
         0.5035, 0.4884, 0.4983, 0.4856, 0.5002, 0.5005, 0.4980, 0.4991, 0.5156,
         0.4966, 0.4997]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5129, 0.4964, 0.4959, 0.5024, 0.5066, 0.4926, 0.5087, 0.4954, 0.4970,
        0.5027, 0.4859, 0.4973, 0.4841, 0.5024, 0.5027, 0.4986, 0.4992, 0.5141,
        0.4936, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4960, 0.4947, 0.5047, 0.5032, 0.4928, 0.5101, 0.4967, 0.4959,
        0.5027, 0.4848, 0.4963, 0.4866, 0.5010, 0.5001, 0.4977, 0.4979, 0.5147,
        0.4949, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4979, 0.4968, 0.5021, 0.5044, 0.4924, 0.5098, 0.4981, 0.4967,
        0.5030, 0.4873, 0.4991, 0.4852, 0.5010, 0.4996, 0.4974, 0.4989, 0.5168,
        0.4949, 0.4976], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4974, 0.4962, 0.5032, 0.5039, 0.4915, 0.5109, 0.4969, 0.4964,
        0.5029, 0.4846, 0.4961, 0.4851, 0.5019, 0.5008, 0.4972, 0.5008, 0.5142,
        0.4956, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4986, 0.4958, 0.5036, 0.5029, 0.4937, 0.5086, 0.4953, 0.4973,
        0.5034, 0.4881, 0.4982, 0.4829, 0.5022, 0.5000, 0.4990, 0.5000, 0.5153,
        0.4956, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5139, 0.4971, 0.4952, 0.5029, 0.5056, 0.4927, 0.5075, 0.4970, 0.4955,
        0.5026, 0.4856, 0.4958, 0.4859, 0.5031, 0.5021, 0.5001, 0.4991, 0.5146,
        0.4951, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4965, 0.4979, 0.5025, 0.5035, 0.4924, 0.5088, 0.4964, 0.4962,
        0.5028, 0.4890, 0.4971, 0.4852, 0.5025, 0.5009, 0.4986, 0.4999, 0.5137,
        0.4952, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5123, 0.4972, 0.4948, 0.5016, 0.5038, 0.4923, 0.5085, 0.4948, 0.4969,
        0.5026, 0.4858, 0.4986, 0.4866, 0.5009, 0.5005, 0.4982, 0.4992, 0.5155,
        0.4957, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4980, 0.4952, 0.5016, 0.5043, 0.4925, 0.5098, 0.4968, 0.4966,
        0.5021, 0.4886, 0.4951, 0.4851, 0.5017, 0.5010, 0.4971, 0.5005, 0.5139,
        0.4955, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4982, 0.4943, 0.5031, 0.5043, 0.4936, 0.5084, 0.4985, 0.4964,
        0.5035, 0.4884, 0.4983, 0.4856, 0.5002, 0.5005, 0.4980, 0.4991, 0.5156,
        0.4966, 0.4997], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [2, 6, 0, 1, 7, 9, 8, 4, 5, 3]
replay_buffer._size: [900 900 900 900 900 900 900 900 900 900]
snapshot at best
2023-08-12 10:40:16,949 MainThread INFO: EPOCH:4
2023-08-12 10:40:16,950 MainThread INFO: Time Consumed:0.8890652656555176s
2023-08-12 10:40:16,950 MainThread INFO: Total Frames:7500s
  6%|‚ñã         | 5/80 [00:10<01:48,  1.45s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1190.47487
Train_Epoch_Reward                    2996.56380
Running_Training_Average_Rewards      819.17052
Explore_Time                          0.00320
Train___Time                          0.37688
Eval____Time                          0.00361
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.30445
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.28982
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.96038
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.47180
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.96230
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.76605
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.85403
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12154.26636
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.37432
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.53453
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.56974     0.64231    11.31464    9.43881
alpha_0                               0.99312      0.00042    0.99372     0.99253
alpha_1                               0.99312      0.00042    0.99372     0.99252
alpha_2                               0.99312      0.00042    0.99372     0.99253
alpha_3                               0.99312      0.00042    0.99372     0.99253
alpha_4                               0.99312      0.00042    0.99372     0.99253
alpha_5                               0.99312      0.00042    0.99372     0.99253
alpha_6                               0.99312      0.00042    0.99372     0.99253
alpha_7                               0.99312      0.00042    0.99372     0.99253
alpha_8                               0.99312      0.00042    0.99372     0.99253
alpha_9                               0.99312      0.00042    0.99372     0.99253
Alpha_loss                            -0.04415     0.00288    -0.04008    -0.04823
Training/policy_loss                  -2.67753     0.00708    -2.66976    -2.68628
Training/qf1_loss                     2571.41826   266.83556  2889.04834  2086.51318
Training/qf2_loss                     2571.29453   266.83013  2888.92114  2086.40112
Training/pf_norm                      0.43894      0.03500    0.48258     0.38452
Training/qf1_norm                     27.11641     1.32027    28.63230    24.77980
Training/qf2_norm                     27.20968     1.32126    28.69367    24.85026
log_std/mean                          -0.01350     0.00089    -0.01224    -0.01477
log_std/std                           0.00123      0.00005    0.00131     0.00116
log_std/max                           -0.01090     0.00080    -0.00977    -0.01204
log_std/min                           -0.01643     0.00102    -0.01501    -0.01789
log_probs/mean                        -2.68804     0.00753    -2.67956    -2.69750
log_probs/std                         0.40256      0.00545    0.40782     0.39254
log_probs/max                         -1.40468     0.06834    -1.30353    -1.46604
log_probs/min                         -4.13592     0.23246    -3.87019    -4.56506
mean/mean                             0.00090      0.00029    0.00128     0.00048
mean/std                              0.00227      0.00004    0.00232     0.00220
mean/max                              0.00443      0.00013    0.00457     0.00423
mean/min                              -0.00210     0.00029    -0.00175    -0.00257
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [3, 5, 8, 1, 2, 4, 0, 9, 7, 6]
replay_buffer._size: [1050 1050 1050 1050 1050 1050 1050 1050 1050 1050]
snapshot at best
2023-08-12 10:40:17,854 MainThread INFO: EPOCH:5
2023-08-12 10:40:17,854 MainThread INFO: Time Consumed:0.8017265796661377s
2023-08-12 10:40:17,854 MainThread INFO: Total Frames:9000s
  8%|‚ñä         | 6/80 [00:11<01:33,  1.26s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1190.82583
Train_Epoch_Reward                    13080.20754
Running_Training_Average_Rewards      1117.53328
Explore_Time                          0.00348
Train___Time                          0.32676
Eval____Time                          0.00262
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.73185
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.12068
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.04619
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.41528
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.00318
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.80328
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.89138
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12154.06888
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.20632
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.59246
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.98305      1.34269    11.31112    7.20601
alpha_0                               0.99163      0.00042    0.99223     0.99104
alpha_1                               0.99163      0.00042    0.99223     0.99104
alpha_2                               0.99164      0.00042    0.99223     0.99104
alpha_3                               0.99164      0.00042    0.99223     0.99104
alpha_4                               0.99163      0.00042    0.99223     0.99104
alpha_5                               0.99163      0.00042    0.99223     0.99104
alpha_6                               0.99163      0.00042    0.99223     0.99104
alpha_7                               0.99163      0.00042    0.99223     0.99104
alpha_8                               0.99163      0.00042    0.99223     0.99104
alpha_9                               0.99163      0.00042    0.99223     0.99104
Alpha_loss                            -0.05428     0.00284    -0.05019    -0.05821
Training/policy_loss                  -2.68829     0.00821    -2.67694    -2.69770
Training/qf1_loss                     1892.52148   529.66330  2806.19092  1158.49658
Training/qf2_loss                     1892.41184   529.65103  2806.05859  1158.40295
Training/pf_norm                      0.39163      0.03142    0.43403     0.35253
Training/qf1_norm                     23.86018     2.75258    28.63284    20.21962
Training/qf2_norm                     23.95570     2.77179    28.77168    20.30800
log_std/mean                          -0.01669     0.00092    -0.01540    -0.01799
log_std/std                           0.00144      0.00007    0.00153     0.00135
log_std/max                           -0.01376     0.00082    -0.01261    -0.01492
log_std/min                           -0.02011     0.00106    -0.01862    -0.02161
log_probs/mean                        -2.70048     0.00829    -2.68965    -2.71029
log_probs/std                         0.40165      0.00402    0.40551     0.39419
log_probs/max                         -1.42020     0.07005    -1.34104    -1.52813
log_probs/min                         -4.67422     0.85152    -3.66684    -5.92113
mean/mean                             0.00151      0.00006    0.00157     0.00140
mean/std                              0.00186      0.00017    0.00212     0.00164
mean/max                              0.00454      0.00019    0.00475     0.00422
mean/min                              -0.00137     0.00016    -0.00121    -0.00165
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4975, 0.5061, 0.4990, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4998,
         0.5025, 0.5098, 0.5014, 0.4959, 0.4891, 0.4932, 0.5076, 0.4927, 0.5088,
         0.4964, 0.5143],
        [0.4983, 0.5067, 0.4989, 0.5059, 0.5068, 0.4893, 0.4961, 0.5024, 0.5022,
         0.5027, 0.5081, 0.4996, 0.4962, 0.4883, 0.4911, 0.5077, 0.4922, 0.5084,
         0.4962, 0.5129],
        [0.4979, 0.5066, 0.4979, 0.5063, 0.5068, 0.4883, 0.4966, 0.5003, 0.5044,
         0.5009, 0.5057, 0.4996, 0.4964, 0.4885, 0.4907, 0.5103, 0.4938, 0.5083,
         0.4951, 0.5152],
        [0.4988, 0.5053, 0.4986, 0.5068, 0.5063, 0.4895, 0.4947, 0.4995, 0.5026,
         0.5039, 0.5052, 0.4984, 0.4974, 0.4878, 0.4894, 0.5083, 0.4923, 0.5098,
         0.4924, 0.5146],
        [0.4987, 0.5074, 0.4979, 0.5048, 0.5058, 0.4884, 0.4962, 0.4992, 0.5021,
         0.5024, 0.5084, 0.5000, 0.4969, 0.4884, 0.4922, 0.5063, 0.4942, 0.5095,
         0.4953, 0.5133],
        [0.4977, 0.5041, 0.4977, 0.5061, 0.5055, 0.4883, 0.4947, 0.5000, 0.5011,
         0.5031, 0.5068, 0.5000, 0.4952, 0.4881, 0.4903, 0.5090, 0.4953, 0.5078,
         0.4946, 0.5139],
        [0.4979, 0.5055, 0.4988, 0.5040, 0.5062, 0.4872, 0.4979, 0.5006, 0.5039,
         0.5020, 0.5106, 0.4996, 0.4960, 0.4865, 0.4909, 0.5080, 0.4959, 0.5073,
         0.4971, 0.5125],
        [0.4981, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
         0.5022, 0.5091, 0.4999, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5062,
         0.4935, 0.5130],
        [0.4982, 0.5049, 0.4973, 0.5053, 0.5078, 0.4881, 0.4962, 0.4999, 0.5019,
         0.5009, 0.5060, 0.5014, 0.4976, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
         0.4940, 0.5147],
        [0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4883, 0.4959, 0.5014, 0.5021,
         0.5035, 0.5076, 0.5004, 0.4950, 0.4899, 0.4926, 0.5088, 0.4939, 0.5058,
         0.4952, 0.5149]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4975, 0.5061, 0.4990, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4998,
        0.5025, 0.5098, 0.5014, 0.4959, 0.4891, 0.4932, 0.5076, 0.4927, 0.5088,
        0.4964, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5067, 0.4989, 0.5059, 0.5068, 0.4893, 0.4961, 0.5024, 0.5022,
        0.5027, 0.5081, 0.4996, 0.4962, 0.4883, 0.4911, 0.5077, 0.4922, 0.5084,
        0.4962, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5066, 0.4979, 0.5063, 0.5068, 0.4883, 0.4966, 0.5003, 0.5044,
        0.5009, 0.5057, 0.4996, 0.4964, 0.4885, 0.4907, 0.5103, 0.4938, 0.5083,
        0.4951, 0.5152], grad_fn=<UnbindBackward>), tensor([0.4988, 0.5053, 0.4986, 0.5068, 0.5063, 0.4895, 0.4947, 0.4995, 0.5026,
        0.5039, 0.5052, 0.4984, 0.4974, 0.4878, 0.4894, 0.5083, 0.4923, 0.5098,
        0.4924, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5074, 0.4979, 0.5048, 0.5058, 0.4884, 0.4962, 0.4992, 0.5021,
        0.5024, 0.5084, 0.5000, 0.4969, 0.4884, 0.4922, 0.5063, 0.4942, 0.5095,
        0.4953, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5041, 0.4977, 0.5061, 0.5055, 0.4883, 0.4947, 0.5000, 0.5011,
        0.5031, 0.5068, 0.5000, 0.4952, 0.4881, 0.4903, 0.5090, 0.4953, 0.5078,
        0.4946, 0.5139], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5055, 0.4988, 0.5040, 0.5062, 0.4872, 0.4979, 0.5006, 0.5039,
        0.5020, 0.5106, 0.4996, 0.4960, 0.4865, 0.4909, 0.5080, 0.4959, 0.5073,
        0.4971, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
        0.5022, 0.5091, 0.4999, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5062,
        0.4935, 0.5130], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5049, 0.4973, 0.5053, 0.5078, 0.4881, 0.4962, 0.4999, 0.5019,
        0.5009, 0.5060, 0.5014, 0.4976, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
        0.4940, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4883, 0.4959, 0.5014, 0.5021,
        0.5035, 0.5076, 0.5004, 0.4950, 0.4899, 0.4926, 0.5088, 0.4939, 0.5058,
        0.4952, 0.5149], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5107, 0.4893, 0.5117, 0.4867, 0.4860, 0.4935, 0.4893, 0.4942, 0.4988,
         0.5071, 0.5027, 0.4990, 0.5022, 0.4909, 0.5038, 0.4918, 0.4856, 0.4988,
         0.5080, 0.4950],
        [0.5089, 0.4894, 0.5110, 0.4854, 0.4859, 0.4922, 0.4882, 0.4950, 0.4982,
         0.5050, 0.5033, 0.4991, 0.5031, 0.4910, 0.5021, 0.4898, 0.4858, 0.4988,
         0.5073, 0.4937],
        [0.5107, 0.4907, 0.5107, 0.4872, 0.4854, 0.4936, 0.4887, 0.4966, 0.5020,
         0.5066, 0.5050, 0.5000, 0.5061, 0.4918, 0.5034, 0.4915, 0.4877, 0.4977,
         0.5069, 0.4926],
        [0.5079, 0.4894, 0.5112, 0.4858, 0.4876, 0.4924, 0.4892, 0.4954, 0.4987,
         0.5086, 0.5039, 0.4991, 0.5048, 0.4908, 0.5031, 0.4909, 0.4863, 0.4987,
         0.5062, 0.4922],
        [0.5096, 0.4912, 0.5128, 0.4865, 0.4872, 0.4924, 0.4905, 0.4925, 0.4960,
         0.5063, 0.5040, 0.4991, 0.5032, 0.4913, 0.5030, 0.4898, 0.4858, 0.4965,
         0.5056, 0.4937],
        [0.5117, 0.4917, 0.5097, 0.4868, 0.4863, 0.4946, 0.4910, 0.4936, 0.4968,
         0.5070, 0.5062, 0.4996, 0.5033, 0.4885, 0.5034, 0.4908, 0.4849, 0.4974,
         0.5064, 0.4935],
        [0.5095, 0.4908, 0.5127, 0.4894, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
         0.5069, 0.5065, 0.4989, 0.5049, 0.4897, 0.5030, 0.4891, 0.4842, 0.4996,
         0.5072, 0.4935],
        [0.5101, 0.4907, 0.5106, 0.4854, 0.4861, 0.4928, 0.4891, 0.4954, 0.5004,
         0.5072, 0.5062, 0.5006, 0.5035, 0.4896, 0.5044, 0.4915, 0.4861, 0.4963,
         0.5076, 0.4930],
        [0.5105, 0.4919, 0.5106, 0.4855, 0.4878, 0.4929, 0.4898, 0.4939, 0.4977,
         0.5074, 0.5043, 0.4980, 0.5038, 0.4900, 0.5040, 0.4903, 0.4868, 0.4971,
         0.5080, 0.4936],
        [0.5110, 0.4912, 0.5115, 0.4875, 0.4873, 0.4928, 0.4892, 0.4936, 0.4997,
         0.5053, 0.5048, 0.4985, 0.5034, 0.4890, 0.5004, 0.4928, 0.4841, 0.4981,
         0.5060, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5107, 0.4893, 0.5117, 0.4867, 0.4860, 0.4935, 0.4893, 0.4942, 0.4988,
        0.5071, 0.5027, 0.4990, 0.5022, 0.4909, 0.5038, 0.4918, 0.4856, 0.4988,
        0.5080, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4894, 0.5110, 0.4854, 0.4859, 0.4922, 0.4882, 0.4950, 0.4982,
        0.5050, 0.5033, 0.4991, 0.5031, 0.4910, 0.5021, 0.4898, 0.4858, 0.4988,
        0.5073, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4907, 0.5107, 0.4872, 0.4854, 0.4936, 0.4887, 0.4966, 0.5020,
        0.5066, 0.5050, 0.5000, 0.5061, 0.4918, 0.5034, 0.4915, 0.4877, 0.4977,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4894, 0.5112, 0.4858, 0.4876, 0.4924, 0.4892, 0.4954, 0.4987,
        0.5086, 0.5039, 0.4991, 0.5048, 0.4908, 0.5031, 0.4909, 0.4863, 0.4987,
        0.5062, 0.4922], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4912, 0.5128, 0.4865, 0.4872, 0.4924, 0.4905, 0.4925, 0.4960,
        0.5063, 0.5040, 0.4991, 0.5032, 0.4913, 0.5030, 0.4898, 0.4858, 0.4965,
        0.5056, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5117, 0.4917, 0.5097, 0.4868, 0.4863, 0.4946, 0.4910, 0.4936, 0.4968,
        0.5070, 0.5062, 0.4996, 0.5033, 0.4885, 0.5034, 0.4908, 0.4849, 0.4974,
        0.5064, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4908, 0.5127, 0.4894, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
        0.5069, 0.5065, 0.4989, 0.5049, 0.4897, 0.5030, 0.4891, 0.4842, 0.4996,
        0.5072, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4907, 0.5106, 0.4854, 0.4861, 0.4928, 0.4891, 0.4954, 0.5004,
        0.5072, 0.5062, 0.5006, 0.5035, 0.4896, 0.5044, 0.4915, 0.4861, 0.4963,
        0.5076, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5105, 0.4919, 0.5106, 0.4855, 0.4878, 0.4929, 0.4898, 0.4939, 0.4977,
        0.5074, 0.5043, 0.4980, 0.5038, 0.4900, 0.5040, 0.4903, 0.4868, 0.4971,
        0.5080, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4912, 0.5115, 0.4875, 0.4873, 0.4928, 0.4892, 0.4936, 0.4997,
        0.5053, 0.5048, 0.4985, 0.5034, 0.4890, 0.5004, 0.4928, 0.4841, 0.4981,
        0.5060, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5129, 0.4964, 0.4959, 0.5025, 0.5067, 0.4926, 0.5088, 0.4952, 0.4970,
         0.5027, 0.4859, 0.4972, 0.4841, 0.5025, 0.5027, 0.4986, 0.4992, 0.5140,
         0.4936, 0.4969],
        [0.5115, 0.4960, 0.4948, 0.5047, 0.5032, 0.4928, 0.5102, 0.4967, 0.4959,
         0.5027, 0.4849, 0.4963, 0.4865, 0.5011, 0.5001, 0.4978, 0.4979, 0.5147,
         0.4950, 0.4973],
        [0.5131, 0.4979, 0.4968, 0.5021, 0.5044, 0.4924, 0.5098, 0.4981, 0.4967,
         0.5030, 0.4872, 0.4991, 0.4852, 0.5010, 0.4997, 0.4974, 0.4989, 0.5168,
         0.4950, 0.4976],
        [0.5126, 0.4974, 0.4962, 0.5032, 0.5039, 0.4914, 0.5109, 0.4968, 0.4964,
         0.5029, 0.4846, 0.4960, 0.4850, 0.5020, 0.5008, 0.4972, 0.5008, 0.5142,
         0.4956, 0.4971],
        [0.5124, 0.4986, 0.4959, 0.5036, 0.5030, 0.4936, 0.5087, 0.4953, 0.4973,
         0.5033, 0.4881, 0.4981, 0.4829, 0.5023, 0.5001, 0.4990, 0.5000, 0.5154,
         0.4957, 0.4976],
        [0.5138, 0.4971, 0.4953, 0.5030, 0.5056, 0.4927, 0.5076, 0.4970, 0.4955,
         0.5025, 0.4857, 0.4958, 0.4858, 0.5031, 0.5022, 0.5001, 0.4992, 0.5145,
         0.4951, 0.4983],
        [0.5109, 0.4965, 0.4978, 0.5025, 0.5036, 0.4924, 0.5088, 0.4963, 0.4963,
         0.5028, 0.4889, 0.4970, 0.4852, 0.5026, 0.5010, 0.4987, 0.4999, 0.5137,
         0.4953, 0.4998],
        [0.5123, 0.4972, 0.4948, 0.5016, 0.5037, 0.4923, 0.5086, 0.4947, 0.4969,
         0.5026, 0.4858, 0.4985, 0.4865, 0.5009, 0.5007, 0.4982, 0.4992, 0.5155,
         0.4958, 0.4982],
        [0.5126, 0.4981, 0.4952, 0.5015, 0.5043, 0.4925, 0.5099, 0.4967, 0.4967,
         0.5021, 0.4886, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5005, 0.5139,
         0.4955, 0.4990],
        [0.5124, 0.4983, 0.4943, 0.5031, 0.5043, 0.4937, 0.5084, 0.4985, 0.4964,
         0.5035, 0.4885, 0.4983, 0.4857, 0.5002, 0.5005, 0.4981, 0.4991, 0.5157,
         0.4967, 0.4997]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5129, 0.4964, 0.4959, 0.5025, 0.5067, 0.4926, 0.5088, 0.4952, 0.4970,
        0.5027, 0.4859, 0.4972, 0.4841, 0.5025, 0.5027, 0.4986, 0.4992, 0.5140,
        0.4936, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4960, 0.4948, 0.5047, 0.5032, 0.4928, 0.5102, 0.4967, 0.4959,
        0.5027, 0.4849, 0.4963, 0.4865, 0.5011, 0.5001, 0.4978, 0.4979, 0.5147,
        0.4950, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4979, 0.4968, 0.5021, 0.5044, 0.4924, 0.5098, 0.4981, 0.4967,
        0.5030, 0.4872, 0.4991, 0.4852, 0.5010, 0.4997, 0.4974, 0.4989, 0.5168,
        0.4950, 0.4976], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4962, 0.5032, 0.5039, 0.4914, 0.5109, 0.4968, 0.4964,
        0.5029, 0.4846, 0.4960, 0.4850, 0.5020, 0.5008, 0.4972, 0.5008, 0.5142,
        0.4956, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4986, 0.4959, 0.5036, 0.5030, 0.4936, 0.5087, 0.4953, 0.4973,
        0.5033, 0.4881, 0.4981, 0.4829, 0.5023, 0.5001, 0.4990, 0.5000, 0.5154,
        0.4957, 0.4976], grad_fn=<UnbindBackward>), tensor([0.5138, 0.4971, 0.4953, 0.5030, 0.5056, 0.4927, 0.5076, 0.4970, 0.4955,
        0.5025, 0.4857, 0.4958, 0.4858, 0.5031, 0.5022, 0.5001, 0.4992, 0.5145,
        0.4951, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4965, 0.4978, 0.5025, 0.5036, 0.4924, 0.5088, 0.4963, 0.4963,
        0.5028, 0.4889, 0.4970, 0.4852, 0.5026, 0.5010, 0.4987, 0.4999, 0.5137,
        0.4953, 0.4998], grad_fn=<UnbindBackward>), tensor([0.5123, 0.4972, 0.4948, 0.5016, 0.5037, 0.4923, 0.5086, 0.4947, 0.4969,
        0.5026, 0.4858, 0.4985, 0.4865, 0.5009, 0.5007, 0.4982, 0.4992, 0.5155,
        0.4958, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4981, 0.4952, 0.5015, 0.5043, 0.4925, 0.5099, 0.4967, 0.4967,
        0.5021, 0.4886, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5005, 0.5139,
        0.4955, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4983, 0.4943, 0.5031, 0.5043, 0.4937, 0.5084, 0.4985, 0.4964,
        0.5035, 0.4885, 0.4983, 0.4857, 0.5002, 0.5005, 0.4981, 0.4991, 0.5157,
        0.4967, 0.4997], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [7, 2, 4, 3, 9, 0, 6, 8, 1, 5]
replay_buffer._size: [1200 1200 1200 1200 1200 1200 1200 1200 1200 1200]
2023-08-12 10:40:18,548 MainThread INFO: EPOCH:6
2023-08-12 10:40:18,548 MainThread INFO: Time Consumed:0.31555891036987305s
2023-08-12 10:40:18,548 MainThread INFO: Total Frames:10500s
  9%|‚ñâ         | 7/80 [00:12<01:18,  1.08s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1184.64221
Train_Epoch_Reward                    8546.10801
Running_Training_Average_Rewards      820.76264
Explore_Time                          0.00292
Train___Time                          0.30852
Eval____Time                          0.00273
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -54.68651
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.01435
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.12298
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.46388
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16380
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.92516
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.00876
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12099.61029
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.10311
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.69960
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.94423      1.25067    10.68228    7.01611
alpha_0                               0.99014      0.00042    0.99074     0.98955
alpha_1                               0.99015      0.00042    0.99074     0.98955
alpha_2                               0.99015      0.00042    0.99074     0.98956
alpha_3                               0.99015      0.00042    0.99074     0.98956
alpha_4                               0.99015      0.00042    0.99074     0.98955
alpha_5                               0.99014      0.00042    0.99074     0.98955
alpha_6                               0.99014      0.00042    0.99074     0.98955
alpha_7                               0.99015      0.00042    0.99074     0.98955
alpha_8                               0.99014      0.00042    0.99074     0.98955
alpha_9                               0.99015      0.00042    0.99074     0.98955
Alpha_loss                            -0.06430     0.00283    -0.06032    -0.06839
Training/policy_loss                  -2.68247     0.00881    -2.66547    -2.68905
Training/qf1_loss                     1948.35491   519.59459  2561.36377  1305.71167
Training/qf2_loss                     1948.24321   519.58222  2561.23096  1305.61938
Training/pf_norm                      0.39993      0.02638    0.43939     0.37019
Training/qf1_norm                     23.81520     2.57198    27.38327    19.84633
Training/qf2_norm                     23.93602     2.60006    27.57654    19.94861
log_std/mean                          -0.01997     0.00094    -0.01865    -0.02132
log_std/std                           0.00168      0.00007    0.00178     0.00158
log_std/max                           -0.01668     0.00082    -0.01552    -0.01786
log_std/min                           -0.02390     0.00111    -0.02235    -0.02547
log_probs/mean                        -2.69613     0.00882    -2.67926    -2.70338
log_probs/std                         0.39498      0.00921    0.40667     0.38386
log_probs/max                         -1.48343     0.02051    -1.44930    -1.51392
log_probs/min                         -4.51834     0.61331    -3.84256    -5.56222
mean/mean                             0.00154      0.00002    0.00155     0.00152
mean/std                              0.00142      0.00011    0.00156     0.00124
mean/max                              0.00388      0.00008    0.00401     0.00380
mean/min                              -0.00092     0.00021    -0.00057    -0.00116
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [7, 8, 6, 1, 5, 4, 2, 3, 9, 0]
replay_buffer._size: [1350 1350 1350 1350 1350 1350 1350 1350 1350 1350]
2023-08-12 10:40:19,023 MainThread INFO: EPOCH:7
2023-08-12 10:40:19,023 MainThread INFO: Time Consumed:0.36926889419555664s
2023-08-12 10:40:19,023 MainThread INFO: Total Frames:12000s
 10%|‚ñà         | 8/80 [00:12<01:03,  1.13it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1187.60056
Train_Epoch_Reward                    21459.93325
Running_Training_Average_Rewards      1436.20829
Explore_Time                          0.00274
Train___Time                          0.36314
Eval____Time                          0.00283
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.22586
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.09219
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.14634
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.52885
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.27887
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.01398
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.09539
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12125.25006
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.18056
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.68242
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.47248      1.17959    10.75751    7.56249
alpha_0                               0.98866      0.00042    0.98925     0.98806
alpha_1                               0.98866      0.00042    0.98925     0.98807
alpha_2                               0.98867      0.00042    0.98926     0.98807
alpha_3                               0.98866      0.00042    0.98926     0.98807
alpha_4                               0.98866      0.00042    0.98926     0.98807
alpha_5                               0.98866      0.00042    0.98925     0.98806
alpha_6                               0.98866      0.00042    0.98925     0.98806
alpha_7                               0.98866      0.00042    0.98925     0.98807
alpha_8                               0.98866      0.00042    0.98925     0.98807
alpha_9                               0.98866      0.00042    0.98926     0.98807
Alpha_loss                            -0.07437     0.00279    -0.07042    -0.07814
Training/policy_loss                  -2.68320     0.01302    -2.66051    -2.69648
Training/qf1_loss                     2159.33667   587.21716  2900.14795  1404.24353
Training/qf2_loss                     2159.21836   587.20806  2900.01904  1404.14392
Training/pf_norm                      0.39056      0.04467    0.46194     0.34006
Training/qf1_norm                     24.95197     2.42513    27.63419    21.03644
Training/qf2_norm                     25.08846     2.43012    27.75384    21.13863
log_std/mean                          -0.02338     0.00098    -0.02200    -0.02476
log_std/std                           0.00194      0.00008    0.00205     0.00183
log_std/max                           -0.01962     0.00083    -0.01845    -0.02080
log_std/min                           -0.02790     0.00114    -0.02630    -0.02952
log_probs/mean                        -2.69834     0.01302    -2.67593    -2.71205
log_probs/std                         0.38619      0.00637    0.39661     0.37801
log_probs/max                         -1.43862     0.09897    -1.35489    -1.62722
log_probs/min                         -4.12959     0.26356    -3.77919    -4.53601
mean/mean                             0.00135      0.00013    0.00152     0.00114
mean/std                              0.00114      0.00003    0.00117     0.00110
mean/max                              0.00376      0.00002    0.00379     0.00372
mean/min                              -0.00037     0.00007    -0.00029    -0.00049
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4975, 0.5062, 0.4990, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
         0.5025, 0.5099, 0.5014, 0.4959, 0.4890, 0.4932, 0.5076, 0.4927, 0.5089,
         0.4964, 0.5142],
        [0.4983, 0.5067, 0.4989, 0.5059, 0.5067, 0.4893, 0.4961, 0.5023, 0.5023,
         0.5027, 0.5081, 0.4996, 0.4962, 0.4883, 0.4911, 0.5076, 0.4921, 0.5085,
         0.4962, 0.5129],
        [0.4979, 0.5066, 0.4980, 0.5063, 0.5068, 0.4883, 0.4967, 0.5002, 0.5044,
         0.5009, 0.5057, 0.4996, 0.4964, 0.4884, 0.4907, 0.5103, 0.4938, 0.5083,
         0.4951, 0.5152],
        [0.4988, 0.5053, 0.4986, 0.5068, 0.5063, 0.4895, 0.4947, 0.4995, 0.5027,
         0.5039, 0.5054, 0.4984, 0.4973, 0.4877, 0.4894, 0.5082, 0.4923, 0.5098,
         0.4924, 0.5145],
        [0.4987, 0.5074, 0.4979, 0.5048, 0.5058, 0.4884, 0.4962, 0.4991, 0.5021,
         0.5025, 0.5085, 0.5000, 0.4969, 0.4884, 0.4922, 0.5062, 0.4943, 0.5096,
         0.4954, 0.5133],
        [0.4978, 0.5041, 0.4977, 0.5061, 0.5055, 0.4883, 0.4947, 0.5000, 0.5011,
         0.5031, 0.5067, 0.5000, 0.4952, 0.4881, 0.4903, 0.5090, 0.4953, 0.5078,
         0.4947, 0.5138],
        [0.4980, 0.5055, 0.4989, 0.5041, 0.5062, 0.4872, 0.4979, 0.5006, 0.5039,
         0.5021, 0.5105, 0.4996, 0.4960, 0.4865, 0.4909, 0.5079, 0.4959, 0.5073,
         0.4972, 0.5126],
        [0.4981, 0.5045, 0.4986, 0.5050, 0.5066, 0.4879, 0.4955, 0.5011, 0.5030,
         0.5022, 0.5093, 0.4999, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5062,
         0.4936, 0.5130],
        [0.4982, 0.5049, 0.4973, 0.5053, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
         0.5009, 0.5061, 0.5014, 0.4976, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
         0.4940, 0.5147],
        [0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4883, 0.4960, 0.5014, 0.5021,
         0.5036, 0.5076, 0.5004, 0.4950, 0.4899, 0.4926, 0.5088, 0.4939, 0.5058,
         0.4953, 0.5150]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4975, 0.5062, 0.4990, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
        0.5025, 0.5099, 0.5014, 0.4959, 0.4890, 0.4932, 0.5076, 0.4927, 0.5089,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5067, 0.4989, 0.5059, 0.5067, 0.4893, 0.4961, 0.5023, 0.5023,
        0.5027, 0.5081, 0.4996, 0.4962, 0.4883, 0.4911, 0.5076, 0.4921, 0.5085,
        0.4962, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5066, 0.4980, 0.5063, 0.5068, 0.4883, 0.4967, 0.5002, 0.5044,
        0.5009, 0.5057, 0.4996, 0.4964, 0.4884, 0.4907, 0.5103, 0.4938, 0.5083,
        0.4951, 0.5152], grad_fn=<UnbindBackward>), tensor([0.4988, 0.5053, 0.4986, 0.5068, 0.5063, 0.4895, 0.4947, 0.4995, 0.5027,
        0.5039, 0.5054, 0.4984, 0.4973, 0.4877, 0.4894, 0.5082, 0.4923, 0.5098,
        0.4924, 0.5145], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5074, 0.4979, 0.5048, 0.5058, 0.4884, 0.4962, 0.4991, 0.5021,
        0.5025, 0.5085, 0.5000, 0.4969, 0.4884, 0.4922, 0.5062, 0.4943, 0.5096,
        0.4954, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5041, 0.4977, 0.5061, 0.5055, 0.4883, 0.4947, 0.5000, 0.5011,
        0.5031, 0.5067, 0.5000, 0.4952, 0.4881, 0.4903, 0.5090, 0.4953, 0.5078,
        0.4947, 0.5138], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5055, 0.4989, 0.5041, 0.5062, 0.4872, 0.4979, 0.5006, 0.5039,
        0.5021, 0.5105, 0.4996, 0.4960, 0.4865, 0.4909, 0.5079, 0.4959, 0.5073,
        0.4972, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5045, 0.4986, 0.5050, 0.5066, 0.4879, 0.4955, 0.5011, 0.5030,
        0.5022, 0.5093, 0.4999, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5062,
        0.4936, 0.5130], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5049, 0.4973, 0.5053, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
        0.5009, 0.5061, 0.5014, 0.4976, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
        0.4940, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4883, 0.4960, 0.5014, 0.5021,
        0.5036, 0.5076, 0.5004, 0.4950, 0.4899, 0.4926, 0.5088, 0.4939, 0.5058,
        0.4953, 0.5150], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5107, 0.4892, 0.5117, 0.4867, 0.4859, 0.4936, 0.4893, 0.4942, 0.4988,
         0.5071, 0.5027, 0.4991, 0.5022, 0.4909, 0.5038, 0.4919, 0.4856, 0.4988,
         0.5080, 0.4950],
        [0.5089, 0.4893, 0.5109, 0.4854, 0.4859, 0.4922, 0.4882, 0.4951, 0.4982,
         0.5050, 0.5032, 0.4991, 0.5032, 0.4911, 0.5022, 0.4897, 0.4859, 0.4987,
         0.5073, 0.4937],
        [0.5107, 0.4907, 0.5107, 0.4872, 0.4854, 0.4937, 0.4887, 0.4966, 0.5020,
         0.5067, 0.5050, 0.5000, 0.5061, 0.4917, 0.5034, 0.4916, 0.4877, 0.4977,
         0.5069, 0.4926],
        [0.5079, 0.4894, 0.5112, 0.4858, 0.4876, 0.4924, 0.4892, 0.4954, 0.4987,
         0.5086, 0.5039, 0.4992, 0.5049, 0.4908, 0.5032, 0.4909, 0.4863, 0.4987,
         0.5062, 0.4922],
        [0.5096, 0.4912, 0.5128, 0.4865, 0.4872, 0.4924, 0.4905, 0.4925, 0.4960,
         0.5063, 0.5039, 0.4992, 0.5032, 0.4913, 0.5031, 0.4897, 0.4858, 0.4965,
         0.5056, 0.4937],
        [0.5115, 0.4916, 0.5098, 0.4868, 0.4863, 0.4945, 0.4909, 0.4936, 0.4968,
         0.5070, 0.5060, 0.4998, 0.5033, 0.4887, 0.5035, 0.4906, 0.4849, 0.4974,
         0.5064, 0.4935],
        [0.5095, 0.4909, 0.5127, 0.4892, 0.4854, 0.4933, 0.4887, 0.4942, 0.4974,
         0.5069, 0.5063, 0.4990, 0.5049, 0.4897, 0.5030, 0.4890, 0.4843, 0.4996,
         0.5073, 0.4934],
        [0.5101, 0.4907, 0.5107, 0.4853, 0.4861, 0.4928, 0.4892, 0.4954, 0.5003,
         0.5072, 0.5063, 0.5006, 0.5035, 0.4895, 0.5044, 0.4916, 0.4861, 0.4963,
         0.5075, 0.4930],
        [0.5105, 0.4919, 0.5106, 0.4855, 0.4878, 0.4929, 0.4898, 0.4939, 0.4977,
         0.5074, 0.5043, 0.4980, 0.5038, 0.4900, 0.5041, 0.4903, 0.4868, 0.4971,
         0.5080, 0.4936],
        [0.5110, 0.4912, 0.5115, 0.4875, 0.4873, 0.4928, 0.4891, 0.4935, 0.4998,
         0.5052, 0.5047, 0.4986, 0.5034, 0.4890, 0.5005, 0.4927, 0.4840, 0.4981,
         0.5059, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5107, 0.4892, 0.5117, 0.4867, 0.4859, 0.4936, 0.4893, 0.4942, 0.4988,
        0.5071, 0.5027, 0.4991, 0.5022, 0.4909, 0.5038, 0.4919, 0.4856, 0.4988,
        0.5080, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4893, 0.5109, 0.4854, 0.4859, 0.4922, 0.4882, 0.4951, 0.4982,
        0.5050, 0.5032, 0.4991, 0.5032, 0.4911, 0.5022, 0.4897, 0.4859, 0.4987,
        0.5073, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4907, 0.5107, 0.4872, 0.4854, 0.4937, 0.4887, 0.4966, 0.5020,
        0.5067, 0.5050, 0.5000, 0.5061, 0.4917, 0.5034, 0.4916, 0.4877, 0.4977,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4894, 0.5112, 0.4858, 0.4876, 0.4924, 0.4892, 0.4954, 0.4987,
        0.5086, 0.5039, 0.4992, 0.5049, 0.4908, 0.5032, 0.4909, 0.4863, 0.4987,
        0.5062, 0.4922], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4912, 0.5128, 0.4865, 0.4872, 0.4924, 0.4905, 0.4925, 0.4960,
        0.5063, 0.5039, 0.4992, 0.5032, 0.4913, 0.5031, 0.4897, 0.4858, 0.4965,
        0.5056, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4916, 0.5098, 0.4868, 0.4863, 0.4945, 0.4909, 0.4936, 0.4968,
        0.5070, 0.5060, 0.4998, 0.5033, 0.4887, 0.5035, 0.4906, 0.4849, 0.4974,
        0.5064, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4909, 0.5127, 0.4892, 0.4854, 0.4933, 0.4887, 0.4942, 0.4974,
        0.5069, 0.5063, 0.4990, 0.5049, 0.4897, 0.5030, 0.4890, 0.4843, 0.4996,
        0.5073, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4907, 0.5107, 0.4853, 0.4861, 0.4928, 0.4892, 0.4954, 0.5003,
        0.5072, 0.5063, 0.5006, 0.5035, 0.4895, 0.5044, 0.4916, 0.4861, 0.4963,
        0.5075, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5105, 0.4919, 0.5106, 0.4855, 0.4878, 0.4929, 0.4898, 0.4939, 0.4977,
        0.5074, 0.5043, 0.4980, 0.5038, 0.4900, 0.5041, 0.4903, 0.4868, 0.4971,
        0.5080, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4912, 0.5115, 0.4875, 0.4873, 0.4928, 0.4891, 0.4935, 0.4998,
        0.5052, 0.5047, 0.4986, 0.5034, 0.4890, 0.5005, 0.4927, 0.4840, 0.4981,
        0.5059, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5129, 0.4963, 0.4960, 0.5025, 0.5067, 0.4925, 0.5088, 0.4952, 0.4970,
         0.5027, 0.4858, 0.4972, 0.4840, 0.5026, 0.5028, 0.4986, 0.4992, 0.5140,
         0.4936, 0.4968],
        [0.5116, 0.4961, 0.4948, 0.5046, 0.5032, 0.4928, 0.5101, 0.4966, 0.4959,
         0.5026, 0.4849, 0.4964, 0.4865, 0.5012, 0.5001, 0.4978, 0.4979, 0.5147,
         0.4950, 0.4973],
        [0.5131, 0.4979, 0.4968, 0.5021, 0.5044, 0.4924, 0.5098, 0.4981, 0.4967,
         0.5029, 0.4872, 0.4991, 0.4852, 0.5011, 0.4997, 0.4975, 0.4989, 0.5168,
         0.4950, 0.4976],
        [0.5126, 0.4974, 0.4962, 0.5032, 0.5039, 0.4915, 0.5108, 0.4968, 0.4964,
         0.5028, 0.4846, 0.4961, 0.4850, 0.5020, 0.5008, 0.4972, 0.5008, 0.5142,
         0.4955, 0.4971],
        [0.5124, 0.4986, 0.4958, 0.5036, 0.5029, 0.4936, 0.5086, 0.4954, 0.4973,
         0.5033, 0.4881, 0.4982, 0.4829, 0.5022, 0.5001, 0.4990, 0.4999, 0.5153,
         0.4957, 0.4975],
        [0.5138, 0.4972, 0.4952, 0.5029, 0.5055, 0.4928, 0.5077, 0.4970, 0.4956,
         0.5025, 0.4856, 0.4959, 0.4858, 0.5030, 0.5021, 0.5000, 0.4991, 0.5145,
         0.4951, 0.4982],
        [0.5110, 0.4965, 0.4978, 0.5025, 0.5036, 0.4924, 0.5088, 0.4964, 0.4962,
         0.5028, 0.4889, 0.4971, 0.4851, 0.5025, 0.5008, 0.4986, 0.4998, 0.5137,
         0.4952, 0.4996],
        [0.5123, 0.4972, 0.4948, 0.5016, 0.5038, 0.4923, 0.5085, 0.4948, 0.4969,
         0.5026, 0.4858, 0.4985, 0.4865, 0.5009, 0.5006, 0.4982, 0.4992, 0.5154,
         0.4958, 0.4982],
        [0.5127, 0.4981, 0.4952, 0.5015, 0.5043, 0.4925, 0.5098, 0.4967, 0.4967,
         0.5021, 0.4886, 0.4951, 0.4851, 0.5017, 0.5010, 0.4971, 0.5005, 0.5138,
         0.4955, 0.4989],
        [0.5124, 0.4984, 0.4942, 0.5030, 0.5043, 0.4937, 0.5084, 0.4986, 0.4965,
         0.5035, 0.4885, 0.4984, 0.4857, 0.5001, 0.5004, 0.4980, 0.4991, 0.5157,
         0.4968, 0.4998]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5129, 0.4963, 0.4960, 0.5025, 0.5067, 0.4925, 0.5088, 0.4952, 0.4970,
        0.5027, 0.4858, 0.4972, 0.4840, 0.5026, 0.5028, 0.4986, 0.4992, 0.5140,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4961, 0.4948, 0.5046, 0.5032, 0.4928, 0.5101, 0.4966, 0.4959,
        0.5026, 0.4849, 0.4964, 0.4865, 0.5012, 0.5001, 0.4978, 0.4979, 0.5147,
        0.4950, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4979, 0.4968, 0.5021, 0.5044, 0.4924, 0.5098, 0.4981, 0.4967,
        0.5029, 0.4872, 0.4991, 0.4852, 0.5011, 0.4997, 0.4975, 0.4989, 0.5168,
        0.4950, 0.4976], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4962, 0.5032, 0.5039, 0.4915, 0.5108, 0.4968, 0.4964,
        0.5028, 0.4846, 0.4961, 0.4850, 0.5020, 0.5008, 0.4972, 0.5008, 0.5142,
        0.4955, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4986, 0.4958, 0.5036, 0.5029, 0.4936, 0.5086, 0.4954, 0.4973,
        0.5033, 0.4881, 0.4982, 0.4829, 0.5022, 0.5001, 0.4990, 0.4999, 0.5153,
        0.4957, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5138, 0.4972, 0.4952, 0.5029, 0.5055, 0.4928, 0.5077, 0.4970, 0.4956,
        0.5025, 0.4856, 0.4959, 0.4858, 0.5030, 0.5021, 0.5000, 0.4991, 0.5145,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4965, 0.4978, 0.5025, 0.5036, 0.4924, 0.5088, 0.4964, 0.4962,
        0.5028, 0.4889, 0.4971, 0.4851, 0.5025, 0.5008, 0.4986, 0.4998, 0.5137,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5123, 0.4972, 0.4948, 0.5016, 0.5038, 0.4923, 0.5085, 0.4948, 0.4969,
        0.5026, 0.4858, 0.4985, 0.4865, 0.5009, 0.5006, 0.4982, 0.4992, 0.5154,
        0.4958, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4981, 0.4952, 0.5015, 0.5043, 0.4925, 0.5098, 0.4967, 0.4967,
        0.5021, 0.4886, 0.4951, 0.4851, 0.5017, 0.5010, 0.4971, 0.5005, 0.5138,
        0.4955, 0.4989], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4984, 0.4942, 0.5030, 0.5043, 0.4937, 0.5084, 0.4986, 0.4965,
        0.5035, 0.4885, 0.4984, 0.4857, 0.5001, 0.5004, 0.4980, 0.4991, 0.5157,
        0.4968, 0.4998], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [5, 7, 0, 4, 6, 2, 8, 1, 9, 3]
replay_buffer._size: [1500 1500 1500 1500 1500 1500 1500 1500 1500 1500]
snapshot at best
2023-08-12 10:40:20,264 MainThread INFO: EPOCH:8
2023-08-12 10:40:20,264 MainThread INFO: Time Consumed:0.8362452983856201s
2023-08-12 10:40:20,264 MainThread INFO: Total Frames:13500s
 11%|‚ñà‚ñè        | 9/80 [00:13<01:10,  1.00it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1191.79593
Train_Epoch_Reward                    8777.40871
Running_Training_Average_Rewards      1292.78167
Explore_Time                          0.00370
Train___Time                          0.34484
Eval____Time                          0.00316
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.12383
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.14855
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.14107
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.52602
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.27201
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.00986
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.09083
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12169.13905
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.23572
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.63185
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.99551      0.40750    9.43646     8.32792
alpha_0                               0.98717      0.00042    0.98777     0.98658
alpha_1                               0.98718      0.00042    0.98777     0.98659
alpha_2                               0.98719      0.00042    0.98778     0.98659
alpha_3                               0.98718      0.00042    0.98777     0.98659
alpha_4                               0.98718      0.00042    0.98777     0.98659
alpha_5                               0.98718      0.00042    0.98777     0.98658
alpha_6                               0.98717      0.00042    0.98777     0.98658
alpha_7                               0.98717      0.00042    0.98777     0.98658
alpha_8                               0.98718      0.00042    0.98777     0.98658
alpha_9                               0.98718      0.00042    0.98777     0.98659
Alpha_loss                            -0.08444     0.00286    -0.08042    -0.08861
Training/policy_loss                  -2.68210     0.01401    -2.65649    -2.69670
Training/qf1_loss                     1899.93042   261.51881  2301.22852  1520.04260
Training/qf2_loss                     1899.81409   261.51943  2301.10767  1519.91992
Training/pf_norm                      0.37057      0.04628    0.45067     0.30576
Training/qf1_norm                     24.00096     0.81463    24.90978    22.64027
Training/qf2_norm                     24.15657     0.79254    25.07155    22.80832
log_std/mean                          -0.02690     0.00102    -0.02547    -0.02834
log_std/std                           0.00223      0.00009    0.00235     0.00211
log_std/max                           -0.02263     0.00087    -0.02143    -0.02388
log_std/min                           -0.03202     0.00119    -0.03035    -0.03369
log_probs/mean                        -2.69861     0.01421    -2.67267    -2.71314
log_probs/std                         0.37713      0.00825    0.38778     0.36866
log_probs/max                         -1.45064     0.08307    -1.32385    -1.53197
log_probs/min                         -4.14034     0.50187    -3.68819    -5.11474
mean/mean                             0.00106      0.00001    0.00107     0.00104
mean/std                              0.00121      0.00001    0.00123     0.00120
mean/max                              0.00379      0.00002    0.00382     0.00377
mean/min                              -0.00054     0.00004    -0.00046    -0.00058
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [8, 2, 4, 0, 5, 9, 6, 7, 1, 3]
replay_buffer._size: [1650 1650 1650 1650 1650 1650 1650 1650 1650 1650]
snapshot at best
2023-08-12 10:40:21,197 MainThread INFO: EPOCH:9
2023-08-12 10:40:21,197 MainThread INFO: Time Consumed:0.8280971050262451s
2023-08-12 10:40:21,197 MainThread INFO: Total Frames:15000s
 12%|‚ñà‚ñé        | 10/80 [00:14<01:08,  1.02it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1195.30836
Train_Epoch_Reward                    6484.29897
Running_Training_Average_Rewards      1224.05470
Explore_Time                          0.00281
Train___Time                          0.32862
Eval____Time                          0.00233
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -54.38880
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.22628
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.08823
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.49871
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16141
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.92526
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.00514
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12206.25107
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.31139
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.56221
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.64997      1.19089    11.65298    8.36543
alpha_0                               0.98569      0.00042    0.98628     0.98510
alpha_1                               0.98570      0.00042    0.98629     0.98511
alpha_2                               0.98571      0.00042    0.98630     0.98511
alpha_3                               0.98570      0.00042    0.98629     0.98511
alpha_4                               0.98570      0.00042    0.98629     0.98511
alpha_5                               0.98569      0.00042    0.98629     0.98510
alpha_6                               0.98569      0.00042    0.98628     0.98510
alpha_7                               0.98569      0.00042    0.98628     0.98510
alpha_8                               0.98570      0.00042    0.98629     0.98510
alpha_9                               0.98570      0.00042    0.98629     0.98511
Alpha_loss                            -0.09459     0.00279    -0.09054    -0.09850
Training/policy_loss                  -2.68750     0.00975    -2.67577    -2.70245
Training/qf1_loss                     1989.13228   504.11750  2882.77319  1469.40491
Training/qf2_loss                     1989.00513   504.10334  2882.61963  1469.29016
Training/pf_norm                      0.38125      0.02643    0.40907     0.33882
Training/qf1_norm                     25.38280     2.48047    29.56311    22.70910
Training/qf2_norm                     25.58238     2.51925    29.84621    22.88958
log_std/mean                          -0.03057     0.00106    -0.02908    -0.03207
log_std/std                           0.00253      0.00008    0.00265     0.00241
log_std/max                           -0.02581     0.00092    -0.02450    -0.02714
log_std/min                           -0.03631     0.00123    -0.03458    -0.03807
log_probs/mean                        -2.70542     0.00970    -2.69380    -2.72055
log_probs/std                         0.37527      0.01183    0.39029     0.36124
log_probs/max                         -1.52527     0.08592    -1.41832    -1.65943
log_probs/min                         -4.63515     0.61926    -4.05126    -5.74432
mean/mean                             0.00076      0.00011    0.00094     0.00065
mean/std                              0.00134      0.00007    0.00143     0.00124
mean/max                              0.00385      0.00006    0.00396     0.00380
mean/min                              -0.00098     0.00017    -0.00073    -0.00117
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4975, 0.5062, 0.4990, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
         0.5025, 0.5099, 0.5014, 0.4959, 0.4890, 0.4932, 0.5076, 0.4927, 0.5089,
         0.4964, 0.5143],
        [0.4983, 0.5068, 0.4989, 0.5060, 0.5067, 0.4893, 0.4961, 0.5023, 0.5023,
         0.5027, 0.5082, 0.4995, 0.4962, 0.4882, 0.4910, 0.5076, 0.4921, 0.5085,
         0.4963, 0.5129],
        [0.4979, 0.5067, 0.4980, 0.5063, 0.5068, 0.4882, 0.4967, 0.5002, 0.5045,
         0.5009, 0.5057, 0.4995, 0.4964, 0.4884, 0.4907, 0.5103, 0.4938, 0.5083,
         0.4951, 0.5152],
        [0.4988, 0.5053, 0.4986, 0.5068, 0.5063, 0.4895, 0.4948, 0.4995, 0.5027,
         0.5039, 0.5055, 0.4985, 0.4973, 0.4877, 0.4894, 0.5082, 0.4923, 0.5097,
         0.4925, 0.5145],
        [0.4987, 0.5075, 0.4979, 0.5049, 0.5058, 0.4884, 0.4962, 0.4991, 0.5021,
         0.5025, 0.5085, 0.4999, 0.4969, 0.4884, 0.4922, 0.5062, 0.4943, 0.5096,
         0.4954, 0.5133],
        [0.4977, 0.5042, 0.4977, 0.5061, 0.5054, 0.4883, 0.4947, 0.5000, 0.5011,
         0.5031, 0.5068, 0.5000, 0.4952, 0.4880, 0.4903, 0.5090, 0.4953, 0.5078,
         0.4946, 0.5138],
        [0.4980, 0.5055, 0.4987, 0.5041, 0.5062, 0.4873, 0.4978, 0.5006, 0.5039,
         0.5021, 0.5104, 0.4995, 0.4961, 0.4866, 0.4908, 0.5079, 0.4958, 0.5073,
         0.4970, 0.5126],
        [0.4981, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
         0.5022, 0.5093, 0.4998, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5061,
         0.4935, 0.5130],
        [0.4982, 0.5049, 0.4972, 0.5053, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
         0.5009, 0.5061, 0.5014, 0.4976, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
         0.4940, 0.5147],
        [0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4883, 0.4959, 0.5014, 0.5021,
         0.5036, 0.5076, 0.5003, 0.4950, 0.4900, 0.4926, 0.5088, 0.4939, 0.5057,
         0.4952, 0.5150]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4975, 0.5062, 0.4990, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
        0.5025, 0.5099, 0.5014, 0.4959, 0.4890, 0.4932, 0.5076, 0.4927, 0.5089,
        0.4964, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5068, 0.4989, 0.5060, 0.5067, 0.4893, 0.4961, 0.5023, 0.5023,
        0.5027, 0.5082, 0.4995, 0.4962, 0.4882, 0.4910, 0.5076, 0.4921, 0.5085,
        0.4963, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5067, 0.4980, 0.5063, 0.5068, 0.4882, 0.4967, 0.5002, 0.5045,
        0.5009, 0.5057, 0.4995, 0.4964, 0.4884, 0.4907, 0.5103, 0.4938, 0.5083,
        0.4951, 0.5152], grad_fn=<UnbindBackward>), tensor([0.4988, 0.5053, 0.4986, 0.5068, 0.5063, 0.4895, 0.4948, 0.4995, 0.5027,
        0.5039, 0.5055, 0.4985, 0.4973, 0.4877, 0.4894, 0.5082, 0.4923, 0.5097,
        0.4925, 0.5145], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5075, 0.4979, 0.5049, 0.5058, 0.4884, 0.4962, 0.4991, 0.5021,
        0.5025, 0.5085, 0.4999, 0.4969, 0.4884, 0.4922, 0.5062, 0.4943, 0.5096,
        0.4954, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5042, 0.4977, 0.5061, 0.5054, 0.4883, 0.4947, 0.5000, 0.5011,
        0.5031, 0.5068, 0.5000, 0.4952, 0.4880, 0.4903, 0.5090, 0.4953, 0.5078,
        0.4946, 0.5138], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5055, 0.4987, 0.5041, 0.5062, 0.4873, 0.4978, 0.5006, 0.5039,
        0.5021, 0.5104, 0.4995, 0.4961, 0.4866, 0.4908, 0.5079, 0.4958, 0.5073,
        0.4970, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
        0.5022, 0.5093, 0.4998, 0.4960, 0.4889, 0.4920, 0.5080, 0.4936, 0.5061,
        0.4935, 0.5130], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5049, 0.4972, 0.5053, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
        0.5009, 0.5061, 0.5014, 0.4976, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
        0.4940, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4883, 0.4959, 0.5014, 0.5021,
        0.5036, 0.5076, 0.5003, 0.4950, 0.4900, 0.4926, 0.5088, 0.4939, 0.5057,
        0.4952, 0.5150], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5107, 0.4893, 0.5118, 0.4867, 0.4859, 0.4936, 0.4893, 0.4942, 0.4988,
         0.5071, 0.5026, 0.4992, 0.5022, 0.4910, 0.5039, 0.4917, 0.4856, 0.4989,
         0.5081, 0.4950],
        [0.5088, 0.4893, 0.5110, 0.4854, 0.4858, 0.4922, 0.4881, 0.4951, 0.4982,
         0.5051, 0.5032, 0.4991, 0.5032, 0.4911, 0.5023, 0.4895, 0.4859, 0.4988,
         0.5073, 0.4936],
        [0.5107, 0.4907, 0.5107, 0.4872, 0.4853, 0.4937, 0.4887, 0.4965, 0.5019,
         0.5067, 0.5050, 0.5001, 0.5061, 0.4917, 0.5035, 0.4915, 0.4877, 0.4978,
         0.5069, 0.4926],
        [0.5078, 0.4894, 0.5113, 0.4858, 0.4875, 0.4924, 0.4892, 0.4954, 0.4987,
         0.5086, 0.5038, 0.4992, 0.5049, 0.4909, 0.5032, 0.4908, 0.4863, 0.4988,
         0.5062, 0.4922],
        [0.5095, 0.4913, 0.5128, 0.4864, 0.4871, 0.4924, 0.4906, 0.4924, 0.4960,
         0.5063, 0.5040, 0.4992, 0.5032, 0.4913, 0.5032, 0.4896, 0.4857, 0.4965,
         0.5056, 0.4937],
        [0.5115, 0.4917, 0.5098, 0.4867, 0.4863, 0.4946, 0.4909, 0.4936, 0.4968,
         0.5070, 0.5062, 0.4998, 0.5033, 0.4886, 0.5035, 0.4906, 0.4849, 0.4974,
         0.5064, 0.4934],
        [0.5094, 0.4909, 0.5126, 0.4892, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
         0.5069, 0.5063, 0.4991, 0.5049, 0.4898, 0.5031, 0.4889, 0.4842, 0.4995,
         0.5072, 0.4934],
        [0.5101, 0.4908, 0.5107, 0.4854, 0.4861, 0.4928, 0.4892, 0.4953, 0.5003,
         0.5072, 0.5063, 0.5007, 0.5035, 0.4895, 0.5045, 0.4914, 0.4861, 0.4963,
         0.5076, 0.4931],
        [0.5105, 0.4920, 0.5106, 0.4855, 0.4878, 0.4929, 0.4898, 0.4939, 0.4977,
         0.5075, 0.5044, 0.4979, 0.5038, 0.4899, 0.5041, 0.4903, 0.4868, 0.4970,
         0.5080, 0.4936],
        [0.5111, 0.4913, 0.5115, 0.4875, 0.4873, 0.4928, 0.4892, 0.4935, 0.4998,
         0.5052, 0.5048, 0.4986, 0.5034, 0.4889, 0.5004, 0.4928, 0.4840, 0.4981,
         0.5059, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5107, 0.4893, 0.5118, 0.4867, 0.4859, 0.4936, 0.4893, 0.4942, 0.4988,
        0.5071, 0.5026, 0.4992, 0.5022, 0.4910, 0.5039, 0.4917, 0.4856, 0.4989,
        0.5081, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4893, 0.5110, 0.4854, 0.4858, 0.4922, 0.4881, 0.4951, 0.4982,
        0.5051, 0.5032, 0.4991, 0.5032, 0.4911, 0.5023, 0.4895, 0.4859, 0.4988,
        0.5073, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4907, 0.5107, 0.4872, 0.4853, 0.4937, 0.4887, 0.4965, 0.5019,
        0.5067, 0.5050, 0.5001, 0.5061, 0.4917, 0.5035, 0.4915, 0.4877, 0.4978,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4894, 0.5113, 0.4858, 0.4875, 0.4924, 0.4892, 0.4954, 0.4987,
        0.5086, 0.5038, 0.4992, 0.5049, 0.4909, 0.5032, 0.4908, 0.4863, 0.4988,
        0.5062, 0.4922], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4913, 0.5128, 0.4864, 0.4871, 0.4924, 0.4906, 0.4924, 0.4960,
        0.5063, 0.5040, 0.4992, 0.5032, 0.4913, 0.5032, 0.4896, 0.4857, 0.4965,
        0.5056, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4917, 0.5098, 0.4867, 0.4863, 0.4946, 0.4909, 0.4936, 0.4968,
        0.5070, 0.5062, 0.4998, 0.5033, 0.4886, 0.5035, 0.4906, 0.4849, 0.4974,
        0.5064, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4909, 0.5126, 0.4892, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
        0.5069, 0.5063, 0.4991, 0.5049, 0.4898, 0.5031, 0.4889, 0.4842, 0.4995,
        0.5072, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4908, 0.5107, 0.4854, 0.4861, 0.4928, 0.4892, 0.4953, 0.5003,
        0.5072, 0.5063, 0.5007, 0.5035, 0.4895, 0.5045, 0.4914, 0.4861, 0.4963,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5105, 0.4920, 0.5106, 0.4855, 0.4878, 0.4929, 0.4898, 0.4939, 0.4977,
        0.5075, 0.5044, 0.4979, 0.5038, 0.4899, 0.5041, 0.4903, 0.4868, 0.4970,
        0.5080, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4913, 0.5115, 0.4875, 0.4873, 0.4928, 0.4892, 0.4935, 0.4998,
        0.5052, 0.5048, 0.4986, 0.5034, 0.4889, 0.5004, 0.4928, 0.4840, 0.4981,
        0.5059, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5129, 0.4963, 0.4961, 0.5024, 0.5067, 0.4925, 0.5088, 0.4952, 0.4970,
         0.5027, 0.4857, 0.4972, 0.4840, 0.5026, 0.5027, 0.4985, 0.4992, 0.5140,
         0.4935, 0.4967],
        [0.5116, 0.4961, 0.4949, 0.5046, 0.5032, 0.4929, 0.5101, 0.4966, 0.4959,
         0.5027, 0.4849, 0.4964, 0.4865, 0.5011, 0.5001, 0.4978, 0.4980, 0.5146,
         0.4949, 0.4973],
        [0.5131, 0.4978, 0.4968, 0.5022, 0.5044, 0.4924, 0.5098, 0.4980, 0.4967,
         0.5029, 0.4871, 0.4990, 0.4851, 0.5011, 0.4997, 0.4975, 0.4990, 0.5167,
         0.4950, 0.4975],
        [0.5126, 0.4974, 0.4963, 0.5032, 0.5039, 0.4915, 0.5108, 0.4968, 0.4964,
         0.5028, 0.4845, 0.4961, 0.4849, 0.5020, 0.5008, 0.4972, 0.5008, 0.5142,
         0.4955, 0.4970],
        [0.5124, 0.4985, 0.4958, 0.5036, 0.5030, 0.4936, 0.5087, 0.4954, 0.4973,
         0.5033, 0.4881, 0.4982, 0.4828, 0.5022, 0.5001, 0.4990, 0.4999, 0.5152,
         0.4956, 0.4975],
        [0.5138, 0.4972, 0.4953, 0.5030, 0.5055, 0.4927, 0.5077, 0.4970, 0.4956,
         0.5025, 0.4857, 0.4959, 0.4857, 0.5031, 0.5022, 0.5001, 0.4991, 0.5145,
         0.4951, 0.4982],
        [0.5110, 0.4966, 0.4978, 0.5025, 0.5037, 0.4924, 0.5088, 0.4963, 0.4963,
         0.5027, 0.4889, 0.4971, 0.4851, 0.5025, 0.5009, 0.4986, 0.4997, 0.5136,
         0.4952, 0.4996],
        [0.5123, 0.4972, 0.4948, 0.5016, 0.5038, 0.4923, 0.5086, 0.4947, 0.4970,
         0.5026, 0.4858, 0.4985, 0.4865, 0.5009, 0.5007, 0.4982, 0.4992, 0.5154,
         0.4958, 0.4982],
        [0.5127, 0.4981, 0.4952, 0.5016, 0.5044, 0.4925, 0.5098, 0.4967, 0.4967,
         0.5021, 0.4886, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5005, 0.5138,
         0.4955, 0.4989],
        [0.5124, 0.4984, 0.4942, 0.5031, 0.5043, 0.4937, 0.5084, 0.4986, 0.4964,
         0.5035, 0.4885, 0.4983, 0.4857, 0.5001, 0.5005, 0.4981, 0.4991, 0.5157,
         0.4968, 0.4998]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5129, 0.4963, 0.4961, 0.5024, 0.5067, 0.4925, 0.5088, 0.4952, 0.4970,
        0.5027, 0.4857, 0.4972, 0.4840, 0.5026, 0.5027, 0.4985, 0.4992, 0.5140,
        0.4935, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4961, 0.4949, 0.5046, 0.5032, 0.4929, 0.5101, 0.4966, 0.4959,
        0.5027, 0.4849, 0.4964, 0.4865, 0.5011, 0.5001, 0.4978, 0.4980, 0.5146,
        0.4949, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4978, 0.4968, 0.5022, 0.5044, 0.4924, 0.5098, 0.4980, 0.4967,
        0.5029, 0.4871, 0.4990, 0.4851, 0.5011, 0.4997, 0.4975, 0.4990, 0.5167,
        0.4950, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4963, 0.5032, 0.5039, 0.4915, 0.5108, 0.4968, 0.4964,
        0.5028, 0.4845, 0.4961, 0.4849, 0.5020, 0.5008, 0.4972, 0.5008, 0.5142,
        0.4955, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4985, 0.4958, 0.5036, 0.5030, 0.4936, 0.5087, 0.4954, 0.4973,
        0.5033, 0.4881, 0.4982, 0.4828, 0.5022, 0.5001, 0.4990, 0.4999, 0.5152,
        0.4956, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5138, 0.4972, 0.4953, 0.5030, 0.5055, 0.4927, 0.5077, 0.4970, 0.4956,
        0.5025, 0.4857, 0.4959, 0.4857, 0.5031, 0.5022, 0.5001, 0.4991, 0.5145,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4966, 0.4978, 0.5025, 0.5037, 0.4924, 0.5088, 0.4963, 0.4963,
        0.5027, 0.4889, 0.4971, 0.4851, 0.5025, 0.5009, 0.4986, 0.4997, 0.5136,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5123, 0.4972, 0.4948, 0.5016, 0.5038, 0.4923, 0.5086, 0.4947, 0.4970,
        0.5026, 0.4858, 0.4985, 0.4865, 0.5009, 0.5007, 0.4982, 0.4992, 0.5154,
        0.4958, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4981, 0.4952, 0.5016, 0.5044, 0.4925, 0.5098, 0.4967, 0.4967,
        0.5021, 0.4886, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5005, 0.5138,
        0.4955, 0.4989], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4984, 0.4942, 0.5031, 0.5043, 0.4937, 0.5084, 0.4986, 0.4964,
        0.5035, 0.4885, 0.4983, 0.4857, 0.5001, 0.5005, 0.4981, 0.4991, 0.5157,
        0.4968, 0.4998], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [1, 3, 4, 2, 0, 9, 5, 7, 8, 6]
replay_buffer._size: [1800 1800 1800 1800 1800 1800 1800 1800 1800 1800]
snapshot at best
2023-08-12 10:40:22,443 MainThread INFO: EPOCH:10
2023-08-12 10:40:22,444 MainThread INFO: Time Consumed:0.850085973739624s
2023-08-12 10:40:22,444 MainThread INFO: Total Frames:16500s
 14%|‚ñà‚ñç        | 11/80 [00:15<01:12,  1.06s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1197.79149
Train_Epoch_Reward                    19512.52024
Running_Training_Average_Rewards      1159.14093
Explore_Time                          0.00325
Train___Time                          0.34474
Eval____Time                          0.00383
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -53.40041
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.25818
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.07662
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.48589
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.12525
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.89954
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.97707
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12230.00789
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.34186
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.52817
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.29342      1.05398    10.91531    7.69089
alpha_0                               0.98421      0.00042    0.98480     0.98362
alpha_1                               0.98422      0.00042    0.98481     0.98363
alpha_2                               0.98423      0.00042    0.98482     0.98364
alpha_3                               0.98422      0.00042    0.98481     0.98363
alpha_4                               0.98422      0.00042    0.98481     0.98363
alpha_5                               0.98421      0.00042    0.98481     0.98362
alpha_6                               0.98421      0.00042    0.98481     0.98362
alpha_7                               0.98421      0.00042    0.98480     0.98362
alpha_8                               0.98422      0.00042    0.98481     0.98363
alpha_9                               0.98422      0.00042    0.98481     0.98363
Alpha_loss                            -0.10455     0.00281    -0.10043    -0.10820
Training/policy_loss                  -2.67931     0.01524    -2.65665    -2.70190
Training/qf1_loss                     1866.04414   549.89165  2752.15845  1145.61328
Training/qf2_loss                     1865.91506   549.87778  2752.00903  1145.50537
Training/pf_norm                      0.38146      0.05422    0.43100     0.29816
Training/qf1_norm                     24.71027     2.20416    28.11134    21.35886
Training/qf2_norm                     24.93777     2.23811    28.37790    21.52843
log_std/mean                          -0.03441     0.00111    -0.03284    -0.03598
log_std/std                           0.00283      0.00009    0.00296     0.00270
log_std/max                           -0.02919     0.00098    -0.02780    -0.03057
log_std/min                           -0.04076     0.00132    -0.03889    -0.04262
log_probs/mean                        -2.69836     0.01544    -2.67579    -2.72133
log_probs/std                         0.36594      0.00564    0.37652     0.35970
log_probs/max                         -1.57923     0.08707    -1.47499    -1.73896
log_probs/min                         -4.08711     0.31417    -3.66407    -4.62133
mean/mean                             0.00067      0.00001    0.00069     0.00066
mean/std                              0.00155      0.00005    0.00164     0.00151
mean/max                              0.00434      0.00014    0.00458     0.00416
mean/min                              -0.00094     0.00009    -0.00083    -0.00111
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [0, 3, 9, 2, 7, 6, 8, 5, 1, 4]
replay_buffer._size: [1950 1950 1950 1950 1950 1950 1950 1950 1950 1950]
snapshot at best
2023-08-12 10:40:23,392 MainThread INFO: EPOCH:11
2023-08-12 10:40:23,392 MainThread INFO: Time Consumed:0.8488070964813232s
2023-08-12 10:40:23,392 MainThread INFO: Total Frames:18000s
 15%|‚ñà‚ñå        | 12/80 [00:16<01:09,  1.02s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1207.20141
Train_Epoch_Reward                    17371.30036
Running_Training_Average_Rewards      1445.60399
Explore_Time                          0.00256
Train___Time                          0.33935
Eval____Time                          0.00256
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.25929
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.31629
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.08404
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.44047
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.08276
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.87038
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.94550
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12321.83666
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.39610
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.42768
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.04577      0.38240    9.73669     8.57391
alpha_0                               0.98274      0.00042    0.98333     0.98215
alpha_1                               0.98274      0.00042    0.98333     0.98215
alpha_2                               0.98275      0.00042    0.98334     0.98216
alpha_3                               0.98274      0.00042    0.98333     0.98215
alpha_4                               0.98275      0.00042    0.98334     0.98216
alpha_5                               0.98274      0.00042    0.98333     0.98215
alpha_6                               0.98274      0.00042    0.98333     0.98215
alpha_7                               0.98273      0.00042    0.98332     0.98214
alpha_8                               0.98274      0.00042    0.98333     0.98215
alpha_9                               0.98274      0.00042    0.98333     0.98215
Alpha_loss                            -0.11472     0.00290    -0.11081    -0.11883
Training/policy_loss                  -2.68464     0.00899    -2.66901    -2.69240
Training/qf1_loss                     1678.60042   258.37722  2021.31177  1295.33362
Training/qf2_loss                     1678.46956   258.36945  2021.16284  1295.21362
Training/pf_norm                      0.35490      0.04291    0.43083     0.30994
Training/qf1_norm                     24.23901     0.78777    25.67977    23.29194
Training/qf2_norm                     24.48793     0.84071    26.03009    23.48289
log_std/mean                          -0.03842     0.00116    -0.03679    -0.04007
log_std/std                           0.00316      0.00010    0.00330     0.00303
log_std/max                           -0.03266     0.00099    -0.03126    -0.03406
log_std/min                           -0.04547     0.00139    -0.04349    -0.04743
log_probs/mean                        -2.70495     0.00922    -2.68883    -2.71303
log_probs/std                         0.35602      0.00584    0.36544     0.35027
log_probs/max                         -1.57365     0.05398    -1.50571    -1.67156
log_probs/min                         -4.17431     0.34494    -3.88034    -4.83531
mean/mean                             0.00071      0.00001    0.00073     0.00069
mean/std                              0.00179      0.00005    0.00185     0.00172
mean/max                              0.00488      0.00005    0.00496     0.00481
mean/min                              -0.00129     0.00016    -0.00105    -0.00147
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4975, 0.5062, 0.4989, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
         0.5025, 0.5099, 0.5013, 0.4959, 0.4890, 0.4932, 0.5076, 0.4927, 0.5089,
         0.4964, 0.5142],
        [0.4983, 0.5068, 0.4989, 0.5060, 0.5067, 0.4893, 0.4961, 0.5024, 0.5023,
         0.5027, 0.5082, 0.4995, 0.4962, 0.4883, 0.4909, 0.5076, 0.4921, 0.5085,
         0.4963, 0.5129],
        [0.4979, 0.5067, 0.4980, 0.5063, 0.5067, 0.4883, 0.4967, 0.5002, 0.5045,
         0.5009, 0.5057, 0.4994, 0.4964, 0.4884, 0.4906, 0.5103, 0.4938, 0.5083,
         0.4951, 0.5152],
        [0.4988, 0.5054, 0.4986, 0.5068, 0.5063, 0.4895, 0.4948, 0.4995, 0.5028,
         0.5038, 0.5056, 0.4984, 0.4973, 0.4876, 0.4894, 0.5081, 0.4924, 0.5097,
         0.4925, 0.5144],
        [0.4987, 0.5075, 0.4979, 0.5049, 0.5058, 0.4883, 0.4962, 0.4991, 0.5021,
         0.5025, 0.5086, 0.4999, 0.4970, 0.4884, 0.4921, 0.5061, 0.4944, 0.5095,
         0.4954, 0.5133],
        [0.4977, 0.5042, 0.4977, 0.5061, 0.5054, 0.4882, 0.4947, 0.5000, 0.5011,
         0.5031, 0.5068, 0.4999, 0.4952, 0.4880, 0.4903, 0.5090, 0.4954, 0.5078,
         0.4946, 0.5138],
        [0.4981, 0.5055, 0.4987, 0.5042, 0.5062, 0.4873, 0.4977, 0.5006, 0.5038,
         0.5021, 0.5104, 0.4995, 0.4961, 0.4867, 0.4907, 0.5079, 0.4957, 0.5074,
         0.4969, 0.5126],
        [0.4981, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
         0.5021, 0.5093, 0.4998, 0.4961, 0.4889, 0.4920, 0.5079, 0.4936, 0.5062,
         0.4934, 0.5130],
        [0.4982, 0.5049, 0.4972, 0.5052, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
         0.5009, 0.5061, 0.5014, 0.4976, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
         0.4940, 0.5147],
        [0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4884, 0.4960, 0.5014, 0.5021,
         0.5036, 0.5076, 0.5004, 0.4950, 0.4900, 0.4927, 0.5088, 0.4939, 0.5057,
         0.4952, 0.5150]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4975, 0.5062, 0.4989, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
        0.5025, 0.5099, 0.5013, 0.4959, 0.4890, 0.4932, 0.5076, 0.4927, 0.5089,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5068, 0.4989, 0.5060, 0.5067, 0.4893, 0.4961, 0.5024, 0.5023,
        0.5027, 0.5082, 0.4995, 0.4962, 0.4883, 0.4909, 0.5076, 0.4921, 0.5085,
        0.4963, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5067, 0.4980, 0.5063, 0.5067, 0.4883, 0.4967, 0.5002, 0.5045,
        0.5009, 0.5057, 0.4994, 0.4964, 0.4884, 0.4906, 0.5103, 0.4938, 0.5083,
        0.4951, 0.5152], grad_fn=<UnbindBackward>), tensor([0.4988, 0.5054, 0.4986, 0.5068, 0.5063, 0.4895, 0.4948, 0.4995, 0.5028,
        0.5038, 0.5056, 0.4984, 0.4973, 0.4876, 0.4894, 0.5081, 0.4924, 0.5097,
        0.4925, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5075, 0.4979, 0.5049, 0.5058, 0.4883, 0.4962, 0.4991, 0.5021,
        0.5025, 0.5086, 0.4999, 0.4970, 0.4884, 0.4921, 0.5061, 0.4944, 0.5095,
        0.4954, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5042, 0.4977, 0.5061, 0.5054, 0.4882, 0.4947, 0.5000, 0.5011,
        0.5031, 0.5068, 0.4999, 0.4952, 0.4880, 0.4903, 0.5090, 0.4954, 0.5078,
        0.4946, 0.5138], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5055, 0.4987, 0.5042, 0.5062, 0.4873, 0.4977, 0.5006, 0.5038,
        0.5021, 0.5104, 0.4995, 0.4961, 0.4867, 0.4907, 0.5079, 0.4957, 0.5074,
        0.4969, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
        0.5021, 0.5093, 0.4998, 0.4961, 0.4889, 0.4920, 0.5079, 0.4936, 0.5062,
        0.4934, 0.5130], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5049, 0.4972, 0.5052, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
        0.5009, 0.5061, 0.5014, 0.4976, 0.4887, 0.4923, 0.5083, 0.4928, 0.5091,
        0.4940, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4992, 0.5051, 0.5071, 0.4884, 0.4960, 0.5014, 0.5021,
        0.5036, 0.5076, 0.5004, 0.4950, 0.4900, 0.4927, 0.5088, 0.4939, 0.5057,
        0.4952, 0.5150], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5107, 0.4892, 0.5118, 0.4867, 0.4858, 0.4936, 0.4893, 0.4942, 0.4988,
         0.5072, 0.5026, 0.4992, 0.5022, 0.4910, 0.5039, 0.4917, 0.4857, 0.4988,
         0.5081, 0.4950],
        [0.5088, 0.4893, 0.5110, 0.4853, 0.4858, 0.4922, 0.4882, 0.4951, 0.4982,
         0.5051, 0.5032, 0.4992, 0.5032, 0.4911, 0.5023, 0.4895, 0.4859, 0.4988,
         0.5073, 0.4936],
        [0.5107, 0.4906, 0.5107, 0.4872, 0.4853, 0.4937, 0.4887, 0.4965, 0.5019,
         0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5035, 0.4915, 0.4877, 0.4978,
         0.5069, 0.4926],
        [0.5079, 0.4894, 0.5113, 0.4858, 0.4875, 0.4925, 0.4893, 0.4954, 0.4987,
         0.5086, 0.5039, 0.4992, 0.5049, 0.4908, 0.5033, 0.4909, 0.4863, 0.4987,
         0.5061, 0.4923],
        [0.5096, 0.4913, 0.5129, 0.4864, 0.4871, 0.4924, 0.4907, 0.4924, 0.4959,
         0.5063, 0.5041, 0.4992, 0.5031, 0.4913, 0.5033, 0.4897, 0.4858, 0.4965,
         0.5056, 0.4937],
        [0.5115, 0.4917, 0.5098, 0.4867, 0.4863, 0.4945, 0.4909, 0.4936, 0.4968,
         0.5070, 0.5062, 0.4998, 0.5034, 0.4886, 0.5036, 0.4907, 0.4850, 0.4974,
         0.5064, 0.4934],
        [0.5095, 0.4909, 0.5126, 0.4891, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
         0.5069, 0.5064, 0.4991, 0.5049, 0.4898, 0.5031, 0.4891, 0.4843, 0.4995,
         0.5073, 0.4934],
        [0.5102, 0.4908, 0.5107, 0.4854, 0.4861, 0.4929, 0.4892, 0.4953, 0.5003,
         0.5072, 0.5063, 0.5007, 0.5035, 0.4895, 0.5045, 0.4915, 0.4861, 0.4963,
         0.5076, 0.4931],
        [0.5105, 0.4920, 0.5106, 0.4855, 0.4879, 0.4929, 0.4897, 0.4939, 0.4977,
         0.5075, 0.5044, 0.4979, 0.5038, 0.4899, 0.5041, 0.4903, 0.4869, 0.4970,
         0.5080, 0.4936],
        [0.5112, 0.4913, 0.5115, 0.4875, 0.4874, 0.4928, 0.4891, 0.4935, 0.4998,
         0.5051, 0.5048, 0.4986, 0.5035, 0.4889, 0.5003, 0.4929, 0.4840, 0.4981,
         0.5059, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5107, 0.4892, 0.5118, 0.4867, 0.4858, 0.4936, 0.4893, 0.4942, 0.4988,
        0.5072, 0.5026, 0.4992, 0.5022, 0.4910, 0.5039, 0.4917, 0.4857, 0.4988,
        0.5081, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4893, 0.5110, 0.4853, 0.4858, 0.4922, 0.4882, 0.4951, 0.4982,
        0.5051, 0.5032, 0.4992, 0.5032, 0.4911, 0.5023, 0.4895, 0.4859, 0.4988,
        0.5073, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4906, 0.5107, 0.4872, 0.4853, 0.4937, 0.4887, 0.4965, 0.5019,
        0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5035, 0.4915, 0.4877, 0.4978,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4894, 0.5113, 0.4858, 0.4875, 0.4925, 0.4893, 0.4954, 0.4987,
        0.5086, 0.5039, 0.4992, 0.5049, 0.4908, 0.5033, 0.4909, 0.4863, 0.4987,
        0.5061, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4913, 0.5129, 0.4864, 0.4871, 0.4924, 0.4907, 0.4924, 0.4959,
        0.5063, 0.5041, 0.4992, 0.5031, 0.4913, 0.5033, 0.4897, 0.4858, 0.4965,
        0.5056, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4917, 0.5098, 0.4867, 0.4863, 0.4945, 0.4909, 0.4936, 0.4968,
        0.5070, 0.5062, 0.4998, 0.5034, 0.4886, 0.5036, 0.4907, 0.4850, 0.4974,
        0.5064, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4909, 0.5126, 0.4891, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
        0.5069, 0.5064, 0.4991, 0.5049, 0.4898, 0.5031, 0.4891, 0.4843, 0.4995,
        0.5073, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5102, 0.4908, 0.5107, 0.4854, 0.4861, 0.4929, 0.4892, 0.4953, 0.5003,
        0.5072, 0.5063, 0.5007, 0.5035, 0.4895, 0.5045, 0.4915, 0.4861, 0.4963,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5105, 0.4920, 0.5106, 0.4855, 0.4879, 0.4929, 0.4897, 0.4939, 0.4977,
        0.5075, 0.5044, 0.4979, 0.5038, 0.4899, 0.5041, 0.4903, 0.4869, 0.4970,
        0.5080, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4913, 0.5115, 0.4875, 0.4874, 0.4928, 0.4891, 0.4935, 0.4998,
        0.5051, 0.5048, 0.4986, 0.5035, 0.4889, 0.5003, 0.4929, 0.4840, 0.4981,
        0.5059, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5129, 0.4963, 0.4961, 0.5025, 0.5068, 0.4925, 0.5088, 0.4951, 0.4970,
         0.5026, 0.4857, 0.4972, 0.4839, 0.5027, 0.5028, 0.4985, 0.4992, 0.5139,
         0.4935, 0.4967],
        [0.5116, 0.4960, 0.4949, 0.5046, 0.5033, 0.4929, 0.5101, 0.4966, 0.4959,
         0.5026, 0.4849, 0.4964, 0.4864, 0.5012, 0.5001, 0.4978, 0.4980, 0.5146,
         0.4949, 0.4973],
        [0.5131, 0.4978, 0.4969, 0.5022, 0.5045, 0.4924, 0.5098, 0.4980, 0.4967,
         0.5029, 0.4871, 0.4990, 0.4851, 0.5011, 0.4998, 0.4975, 0.4990, 0.5167,
         0.4950, 0.4975],
        [0.5126, 0.4974, 0.4963, 0.5033, 0.5040, 0.4914, 0.5108, 0.4967, 0.4964,
         0.5028, 0.4846, 0.4961, 0.4849, 0.5021, 0.5009, 0.4972, 0.5008, 0.5142,
         0.4955, 0.4970],
        [0.5124, 0.4985, 0.4959, 0.5037, 0.5031, 0.4936, 0.5087, 0.4953, 0.4972,
         0.5033, 0.4880, 0.4981, 0.4828, 0.5023, 0.5002, 0.4990, 0.4999, 0.5152,
         0.4956, 0.4975],
        [0.5137, 0.4972, 0.4953, 0.5030, 0.5055, 0.4927, 0.5077, 0.4970, 0.4956,
         0.5024, 0.4857, 0.4959, 0.4857, 0.5031, 0.5022, 0.5001, 0.4992, 0.5145,
         0.4950, 0.4981],
        [0.5111, 0.4966, 0.4979, 0.5025, 0.5037, 0.4923, 0.5088, 0.4963, 0.4963,
         0.5027, 0.4888, 0.4971, 0.4850, 0.5026, 0.5009, 0.4986, 0.4997, 0.5137,
         0.4952, 0.4996],
        [0.5122, 0.4972, 0.4949, 0.5016, 0.5038, 0.4923, 0.5086, 0.4947, 0.4969,
         0.5026, 0.4858, 0.4985, 0.4864, 0.5010, 0.5008, 0.4982, 0.4993, 0.5154,
         0.4959, 0.4982],
        [0.5127, 0.4981, 0.4952, 0.5015, 0.5044, 0.4924, 0.5098, 0.4967, 0.4967,
         0.5021, 0.4885, 0.4950, 0.4850, 0.5018, 0.5011, 0.4971, 0.5005, 0.5139,
         0.4956, 0.4989],
        [0.5124, 0.4985, 0.4942, 0.5030, 0.5043, 0.4937, 0.5084, 0.4986, 0.4964,
         0.5035, 0.4885, 0.4984, 0.4857, 0.5001, 0.5005, 0.4981, 0.4991, 0.5157,
         0.4970, 0.4999]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5129, 0.4963, 0.4961, 0.5025, 0.5068, 0.4925, 0.5088, 0.4951, 0.4970,
        0.5026, 0.4857, 0.4972, 0.4839, 0.5027, 0.5028, 0.4985, 0.4992, 0.5139,
        0.4935, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4960, 0.4949, 0.5046, 0.5033, 0.4929, 0.5101, 0.4966, 0.4959,
        0.5026, 0.4849, 0.4964, 0.4864, 0.5012, 0.5001, 0.4978, 0.4980, 0.5146,
        0.4949, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4978, 0.4969, 0.5022, 0.5045, 0.4924, 0.5098, 0.4980, 0.4967,
        0.5029, 0.4871, 0.4990, 0.4851, 0.5011, 0.4998, 0.4975, 0.4990, 0.5167,
        0.4950, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4963, 0.5033, 0.5040, 0.4914, 0.5108, 0.4967, 0.4964,
        0.5028, 0.4846, 0.4961, 0.4849, 0.5021, 0.5009, 0.4972, 0.5008, 0.5142,
        0.4955, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4985, 0.4959, 0.5037, 0.5031, 0.4936, 0.5087, 0.4953, 0.4972,
        0.5033, 0.4880, 0.4981, 0.4828, 0.5023, 0.5002, 0.4990, 0.4999, 0.5152,
        0.4956, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4972, 0.4953, 0.5030, 0.5055, 0.4927, 0.5077, 0.4970, 0.4956,
        0.5024, 0.4857, 0.4959, 0.4857, 0.5031, 0.5022, 0.5001, 0.4992, 0.5145,
        0.4950, 0.4981], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4966, 0.4979, 0.5025, 0.5037, 0.4923, 0.5088, 0.4963, 0.4963,
        0.5027, 0.4888, 0.4971, 0.4850, 0.5026, 0.5009, 0.4986, 0.4997, 0.5137,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5122, 0.4972, 0.4949, 0.5016, 0.5038, 0.4923, 0.5086, 0.4947, 0.4969,
        0.5026, 0.4858, 0.4985, 0.4864, 0.5010, 0.5008, 0.4982, 0.4993, 0.5154,
        0.4959, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4981, 0.4952, 0.5015, 0.5044, 0.4924, 0.5098, 0.4967, 0.4967,
        0.5021, 0.4885, 0.4950, 0.4850, 0.5018, 0.5011, 0.4971, 0.5005, 0.5139,
        0.4956, 0.4989], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4985, 0.4942, 0.5030, 0.5043, 0.4937, 0.5084, 0.4986, 0.4964,
        0.5035, 0.4885, 0.4984, 0.4857, 0.5001, 0.5005, 0.4981, 0.4991, 0.5157,
        0.4970, 0.4999], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [9, 0, 4, 8, 1, 2, 5, 7, 3, 6]
replay_buffer._size: [2100 2100 2100 2100 2100 2100 2100 2100 2100 2100]
snapshot at best
2023-08-12 10:40:24,711 MainThread INFO: EPOCH:12
2023-08-12 10:40:24,712 MainThread INFO: Time Consumed:0.9098944664001465s
2023-08-12 10:40:24,712 MainThread INFO: Total Frames:19500s
 16%|‚ñà‚ñã        | 13/80 [00:18<01:14,  1.12s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1218.80196
Train_Epoch_Reward                    21443.48090
Running_Training_Average_Rewards      1944.24338
Explore_Time                          0.00317
Train___Time                          0.34808
Eval____Time                          0.00305
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.51075
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.31077
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.17489
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.39897
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16970
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.94222
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.01446
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12442.27735
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.38797
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.34803
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.56188      1.13306    11.66344    8.29429
alpha_0                               0.98126      0.00042    0.98185     0.98067
alpha_1                               0.98127      0.00042    0.98186     0.98068
alpha_2                               0.98128      0.00042    0.98187     0.98069
alpha_3                               0.98127      0.00042    0.98186     0.98068
alpha_4                               0.98127      0.00042    0.98186     0.98068
alpha_5                               0.98126      0.00042    0.98185     0.98067
alpha_6                               0.98126      0.00042    0.98185     0.98068
alpha_7                               0.98126      0.00042    0.98185     0.98067
alpha_8                               0.98127      0.00042    0.98186     0.98068
alpha_9                               0.98127      0.00042    0.98186     0.98068
Alpha_loss                            -0.12475     0.00293    -0.12047    -0.12900
Training/policy_loss                  -2.68138     0.01430    -2.66530    -2.70265
Training/qf1_loss                     1666.04321   725.96036  3109.66748  1183.95227
Training/qf2_loss                     1665.90132   725.94451  3109.49463  1183.82593
Training/pf_norm                      0.37460      0.03338    0.40810     0.32719
Training/qf1_norm                     25.34983     2.38696    29.78873    22.70123
Training/qf2_norm                     25.63673     2.43255    30.16242    22.95437
log_std/mean                          -0.04260     0.00121    -0.04091    -0.04432
log_std/std                           0.00352      0.00009    0.00366     0.00339
log_std/max                           -0.03619     0.00104    -0.03474    -0.03767
log_std/min                           -0.05044     0.00143    -0.04844    -0.05248
log_probs/mean                        -2.70283     0.01465    -2.68639    -2.72428
log_probs/std                         0.35817      0.00916    0.37159     0.34550
log_probs/max                         -1.65022     0.08914    -1.52149    -1.77951
log_probs/min                         -5.09512     0.36345    -4.51469    -5.63430
mean/mean                             0.00070      0.00002    0.00073     0.00067
mean/std                              0.00198      0.00010    0.00213     0.00184
mean/max                              0.00516      0.00021    0.00545     0.00485
mean/min                              -0.00165     0.00005    -0.00157    -0.00174
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [9, 6, 7, 1, 8, 3, 0, 2, 5, 4]
replay_buffer._size: [2250 2250 2250 2250 2250 2250 2250 2250 2250 2250]
snapshot at best
2023-08-12 10:40:25,708 MainThread INFO: EPOCH:13
2023-08-12 10:40:25,708 MainThread INFO: Time Consumed:0.8846280574798584s
2023-08-12 10:40:25,708 MainThread INFO: Total Frames:21000s
 18%|‚ñà‚ñä        | 14/80 [00:19<01:11,  1.08s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1224.68024
Train_Epoch_Reward                    14950.10239
Running_Training_Average_Rewards      1792.16279
Explore_Time                          0.00322
Train___Time                          0.33366
Eval____Time                          0.00272
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.43178
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.35632
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.19180
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.40446
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.20340
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.97667
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.04684
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12493.15003
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.43266
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.30370
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.03915      0.68699    9.81974     8.03670
alpha_0                               0.97979      0.00042    0.98038     0.97920
alpha_1                               0.97979      0.00042    0.98038     0.97920
alpha_2                               0.97981      0.00042    0.98040     0.97922
alpha_3                               0.97980      0.00042    0.98038     0.97921
alpha_4                               0.97980      0.00042    0.98039     0.97921
alpha_5                               0.97979      0.00042    0.98038     0.97920
alpha_6                               0.97979      0.00042    0.98038     0.97921
alpha_7                               0.97978      0.00042    0.98037     0.97920
alpha_8                               0.97980      0.00042    0.98038     0.97921
alpha_9                               0.97980      0.00042    0.98039     0.97921
Alpha_loss                            -0.13494     0.00301    -0.13081    -0.13935
Training/policy_loss                  -2.68655     0.00984    -2.67472    -2.70444
Training/qf1_loss                     1589.75962   267.22671  1902.79858  1284.35620
Training/qf2_loss                     1589.62515   267.22463  1902.66431  1284.21729
Training/pf_norm                      0.34894      0.03588    0.38722     0.28848
Training/qf1_norm                     24.33945     1.42208    25.95337    22.27763
Training/qf2_norm                     24.58642     1.39311    26.20166    22.60822
log_std/mean                          -0.04695     0.00125    -0.04519    -0.04873
log_std/std                           0.00387      0.00009    0.00400     0.00374
log_std/max                           -0.03998     0.00110    -0.03846    -0.04156
log_std/min                           -0.05557     0.00148    -0.05348    -0.05767
log_probs/mean                        -2.70911     0.01024    -2.69689    -2.72775
log_probs/std                         0.34351      0.00893    0.35050     0.32601
log_probs/max                         -1.64002     0.03596    -1.59116    -1.68683
log_probs/min                         -4.55343     0.57601    -3.69477    -5.32723
mean/mean                             0.00053      0.00014    0.00066     0.00028
mean/std                              0.00284      0.00035    0.00332     0.00233
mean/max                              0.00638      0.00040    0.00686     0.00575
mean/min                              -0.00274     0.00060    -0.00196    -0.00369
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5062, 0.4989, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
         0.5025, 0.5100, 0.5013, 0.4960, 0.4890, 0.4931, 0.5076, 0.4926, 0.5089,
         0.4964, 0.5143],
        [0.4983, 0.5068, 0.4989, 0.5060, 0.5067, 0.4894, 0.4961, 0.5023, 0.5023,
         0.5027, 0.5082, 0.4994, 0.4962, 0.4882, 0.4909, 0.5076, 0.4921, 0.5085,
         0.4963, 0.5129],
        [0.4979, 0.5068, 0.4980, 0.5063, 0.5067, 0.4883, 0.4966, 0.5002, 0.5045,
         0.5009, 0.5058, 0.4994, 0.4964, 0.4884, 0.4906, 0.5103, 0.4938, 0.5083,
         0.4951, 0.5152],
        [0.4988, 0.5054, 0.4986, 0.5068, 0.5063, 0.4894, 0.4948, 0.4995, 0.5028,
         0.5038, 0.5057, 0.4984, 0.4973, 0.4876, 0.4894, 0.5081, 0.4924, 0.5097,
         0.4926, 0.5144],
        [0.4987, 0.5075, 0.4979, 0.5049, 0.5058, 0.4884, 0.4962, 0.4990, 0.5021,
         0.5025, 0.5086, 0.4998, 0.4970, 0.4883, 0.4921, 0.5061, 0.4944, 0.5096,
         0.4954, 0.5132],
        [0.4977, 0.5042, 0.4977, 0.5061, 0.5054, 0.4882, 0.4947, 0.4999, 0.5011,
         0.5031, 0.5069, 0.4999, 0.4952, 0.4880, 0.4902, 0.5090, 0.4953, 0.5078,
         0.4946, 0.5138],
        [0.4981, 0.5055, 0.4987, 0.5042, 0.5062, 0.4874, 0.4976, 0.5005, 0.5038,
         0.5021, 0.5103, 0.4995, 0.4961, 0.4867, 0.4906, 0.5080, 0.4957, 0.5074,
         0.4969, 0.5126],
        [0.4981, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
         0.5021, 0.5093, 0.4998, 0.4961, 0.4889, 0.4920, 0.5080, 0.4936, 0.5061,
         0.4934, 0.5130],
        [0.4982, 0.5049, 0.4972, 0.5052, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
         0.5009, 0.5061, 0.5014, 0.4977, 0.4886, 0.4923, 0.5083, 0.4928, 0.5091,
         0.4940, 0.5147],
        [0.4984, 0.5058, 0.4993, 0.5051, 0.5071, 0.4883, 0.4960, 0.5014, 0.5021,
         0.5036, 0.5076, 0.5004, 0.4949, 0.4900, 0.4927, 0.5089, 0.4940, 0.5056,
         0.4953, 0.5150]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5062, 0.4989, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
        0.5025, 0.5100, 0.5013, 0.4960, 0.4890, 0.4931, 0.5076, 0.4926, 0.5089,
        0.4964, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5068, 0.4989, 0.5060, 0.5067, 0.4894, 0.4961, 0.5023, 0.5023,
        0.5027, 0.5082, 0.4994, 0.4962, 0.4882, 0.4909, 0.5076, 0.4921, 0.5085,
        0.4963, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5068, 0.4980, 0.5063, 0.5067, 0.4883, 0.4966, 0.5002, 0.5045,
        0.5009, 0.5058, 0.4994, 0.4964, 0.4884, 0.4906, 0.5103, 0.4938, 0.5083,
        0.4951, 0.5152], grad_fn=<UnbindBackward>), tensor([0.4988, 0.5054, 0.4986, 0.5068, 0.5063, 0.4894, 0.4948, 0.4995, 0.5028,
        0.5038, 0.5057, 0.4984, 0.4973, 0.4876, 0.4894, 0.5081, 0.4924, 0.5097,
        0.4926, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5075, 0.4979, 0.5049, 0.5058, 0.4884, 0.4962, 0.4990, 0.5021,
        0.5025, 0.5086, 0.4998, 0.4970, 0.4883, 0.4921, 0.5061, 0.4944, 0.5096,
        0.4954, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5042, 0.4977, 0.5061, 0.5054, 0.4882, 0.4947, 0.4999, 0.5011,
        0.5031, 0.5069, 0.4999, 0.4952, 0.4880, 0.4902, 0.5090, 0.4953, 0.5078,
        0.4946, 0.5138], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5055, 0.4987, 0.5042, 0.5062, 0.4874, 0.4976, 0.5005, 0.5038,
        0.5021, 0.5103, 0.4995, 0.4961, 0.4867, 0.4906, 0.5080, 0.4957, 0.5074,
        0.4969, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
        0.5021, 0.5093, 0.4998, 0.4961, 0.4889, 0.4920, 0.5080, 0.4936, 0.5061,
        0.4934, 0.5130], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5049, 0.4972, 0.5052, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
        0.5009, 0.5061, 0.5014, 0.4977, 0.4886, 0.4923, 0.5083, 0.4928, 0.5091,
        0.4940, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4993, 0.5051, 0.5071, 0.4883, 0.4960, 0.5014, 0.5021,
        0.5036, 0.5076, 0.5004, 0.4949, 0.4900, 0.4927, 0.5089, 0.4940, 0.5056,
        0.4953, 0.5150], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5107, 0.4892, 0.5118, 0.4867, 0.4858, 0.4937, 0.4893, 0.4942, 0.4988,
         0.5072, 0.5025, 0.4992, 0.5022, 0.4911, 0.5040, 0.4916, 0.4856, 0.4989,
         0.5081, 0.4950],
        [0.5087, 0.4893, 0.5109, 0.4853, 0.4858, 0.4922, 0.4881, 0.4951, 0.4982,
         0.5051, 0.5032, 0.4992, 0.5032, 0.4912, 0.5024, 0.4894, 0.4859, 0.4988,
         0.5073, 0.4936],
        [0.5107, 0.4906, 0.5107, 0.4871, 0.4853, 0.4937, 0.4887, 0.4965, 0.5018,
         0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5036, 0.4915, 0.4877, 0.4978,
         0.5069, 0.4926],
        [0.5078, 0.4893, 0.5113, 0.4858, 0.4875, 0.4926, 0.4893, 0.4954, 0.4987,
         0.5086, 0.5039, 0.4993, 0.5049, 0.4908, 0.5033, 0.4909, 0.4863, 0.4987,
         0.5061, 0.4923],
        [0.5095, 0.4912, 0.5129, 0.4864, 0.4872, 0.4924, 0.4907, 0.4924, 0.4958,
         0.5063, 0.5041, 0.4992, 0.5032, 0.4913, 0.5033, 0.4897, 0.4858, 0.4964,
         0.5056, 0.4937],
        [0.5115, 0.4917, 0.5099, 0.4867, 0.4864, 0.4945, 0.4909, 0.4936, 0.4969,
         0.5069, 0.5062, 0.4998, 0.5034, 0.4887, 0.5035, 0.4906, 0.4850, 0.4973,
         0.5064, 0.4934],
        [0.5094, 0.4909, 0.5126, 0.4890, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
         0.5069, 0.5062, 0.4992, 0.5049, 0.4899, 0.5031, 0.4890, 0.4844, 0.4994,
         0.5073, 0.4934],
        [0.5101, 0.4908, 0.5107, 0.4853, 0.4861, 0.4928, 0.4891, 0.4953, 0.5002,
         0.5072, 0.5063, 0.5007, 0.5035, 0.4895, 0.5045, 0.4914, 0.4860, 0.4962,
         0.5076, 0.4931],
        [0.5105, 0.4921, 0.5107, 0.4854, 0.4879, 0.4929, 0.4898, 0.4939, 0.4976,
         0.5075, 0.5044, 0.4979, 0.5038, 0.4899, 0.5042, 0.4903, 0.4869, 0.4970,
         0.5081, 0.4936],
        [0.5111, 0.4913, 0.5115, 0.4875, 0.4873, 0.4928, 0.4892, 0.4935, 0.4998,
         0.5052, 0.5049, 0.4986, 0.5034, 0.4889, 0.5004, 0.4930, 0.4839, 0.4981,
         0.5058, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5107, 0.4892, 0.5118, 0.4867, 0.4858, 0.4937, 0.4893, 0.4942, 0.4988,
        0.5072, 0.5025, 0.4992, 0.5022, 0.4911, 0.5040, 0.4916, 0.4856, 0.4989,
        0.5081, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4893, 0.5109, 0.4853, 0.4858, 0.4922, 0.4881, 0.4951, 0.4982,
        0.5051, 0.5032, 0.4992, 0.5032, 0.4912, 0.5024, 0.4894, 0.4859, 0.4988,
        0.5073, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4906, 0.5107, 0.4871, 0.4853, 0.4937, 0.4887, 0.4965, 0.5018,
        0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5036, 0.4915, 0.4877, 0.4978,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4893, 0.5113, 0.4858, 0.4875, 0.4926, 0.4893, 0.4954, 0.4987,
        0.5086, 0.5039, 0.4993, 0.5049, 0.4908, 0.5033, 0.4909, 0.4863, 0.4987,
        0.5061, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5129, 0.4864, 0.4872, 0.4924, 0.4907, 0.4924, 0.4958,
        0.5063, 0.5041, 0.4992, 0.5032, 0.4913, 0.5033, 0.4897, 0.4858, 0.4964,
        0.5056, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4917, 0.5099, 0.4867, 0.4864, 0.4945, 0.4909, 0.4936, 0.4969,
        0.5069, 0.5062, 0.4998, 0.5034, 0.4887, 0.5035, 0.4906, 0.4850, 0.4973,
        0.5064, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4909, 0.5126, 0.4890, 0.4855, 0.4933, 0.4887, 0.4941, 0.4974,
        0.5069, 0.5062, 0.4992, 0.5049, 0.4899, 0.5031, 0.4890, 0.4844, 0.4994,
        0.5073, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4908, 0.5107, 0.4853, 0.4861, 0.4928, 0.4891, 0.4953, 0.5002,
        0.5072, 0.5063, 0.5007, 0.5035, 0.4895, 0.5045, 0.4914, 0.4860, 0.4962,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5105, 0.4921, 0.5107, 0.4854, 0.4879, 0.4929, 0.4898, 0.4939, 0.4976,
        0.5075, 0.5044, 0.4979, 0.5038, 0.4899, 0.5042, 0.4903, 0.4869, 0.4970,
        0.5081, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4913, 0.5115, 0.4875, 0.4873, 0.4928, 0.4892, 0.4935, 0.4998,
        0.5052, 0.5049, 0.4986, 0.5034, 0.4889, 0.5004, 0.4930, 0.4839, 0.4981,
        0.5058, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5128, 0.4963, 0.4962, 0.5025, 0.5068, 0.4925, 0.5089, 0.4951, 0.4970,
         0.5026, 0.4857, 0.4972, 0.4840, 0.5026, 0.5027, 0.4985, 0.4992, 0.5139,
         0.4935, 0.4967],
        [0.5116, 0.4961, 0.4950, 0.5046, 0.5033, 0.4929, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4849, 0.4964, 0.4864, 0.5012, 0.5002, 0.4978, 0.4980, 0.5146,
         0.4949, 0.4973],
        [0.5132, 0.4978, 0.4969, 0.5022, 0.5045, 0.4924, 0.5099, 0.4980, 0.4967,
         0.5029, 0.4870, 0.4990, 0.4851, 0.5012, 0.4998, 0.4975, 0.4990, 0.5167,
         0.4950, 0.4975],
        [0.5126, 0.4974, 0.4964, 0.5033, 0.5040, 0.4914, 0.5108, 0.4967, 0.4964,
         0.5028, 0.4845, 0.4961, 0.4848, 0.5022, 0.5009, 0.4972, 0.5009, 0.5142,
         0.4955, 0.4970],
        [0.5124, 0.4985, 0.4959, 0.5037, 0.5031, 0.4935, 0.5087, 0.4953, 0.4972,
         0.5033, 0.4879, 0.4981, 0.4828, 0.5023, 0.5001, 0.4990, 0.5000, 0.5152,
         0.4956, 0.4975],
        [0.5137, 0.4971, 0.4954, 0.5030, 0.5055, 0.4927, 0.5077, 0.4969, 0.4956,
         0.5025, 0.4857, 0.4960, 0.4857, 0.5031, 0.5022, 0.5001, 0.4992, 0.5145,
         0.4951, 0.4982],
        [0.5112, 0.4966, 0.4978, 0.5025, 0.5037, 0.4924, 0.5088, 0.4963, 0.4963,
         0.5028, 0.4888, 0.4971, 0.4851, 0.5026, 0.5009, 0.4986, 0.4997, 0.5136,
         0.4951, 0.4996],
        [0.5123, 0.4972, 0.4949, 0.5016, 0.5038, 0.4923, 0.5085, 0.4947, 0.4970,
         0.5026, 0.4858, 0.4985, 0.4864, 0.5010, 0.5007, 0.4982, 0.4993, 0.5154,
         0.4958, 0.4982],
        [0.5127, 0.4980, 0.4952, 0.5015, 0.5044, 0.4925, 0.5098, 0.4967, 0.4967,
         0.5021, 0.4885, 0.4950, 0.4851, 0.5018, 0.5011, 0.4971, 0.5005, 0.5139,
         0.4956, 0.4990],
        [0.5124, 0.4984, 0.4941, 0.5030, 0.5043, 0.4938, 0.5083, 0.4987, 0.4964,
         0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5004, 0.4981, 0.4991, 0.5157,
         0.4969, 0.4999]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5128, 0.4963, 0.4962, 0.5025, 0.5068, 0.4925, 0.5089, 0.4951, 0.4970,
        0.5026, 0.4857, 0.4972, 0.4840, 0.5026, 0.5027, 0.4985, 0.4992, 0.5139,
        0.4935, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4961, 0.4950, 0.5046, 0.5033, 0.4929, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4849, 0.4964, 0.4864, 0.5012, 0.5002, 0.4978, 0.4980, 0.5146,
        0.4949, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5132, 0.4978, 0.4969, 0.5022, 0.5045, 0.4924, 0.5099, 0.4980, 0.4967,
        0.5029, 0.4870, 0.4990, 0.4851, 0.5012, 0.4998, 0.4975, 0.4990, 0.5167,
        0.4950, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4964, 0.5033, 0.5040, 0.4914, 0.5108, 0.4967, 0.4964,
        0.5028, 0.4845, 0.4961, 0.4848, 0.5022, 0.5009, 0.4972, 0.5009, 0.5142,
        0.4955, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4985, 0.4959, 0.5037, 0.5031, 0.4935, 0.5087, 0.4953, 0.4972,
        0.5033, 0.4879, 0.4981, 0.4828, 0.5023, 0.5001, 0.4990, 0.5000, 0.5152,
        0.4956, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4971, 0.4954, 0.5030, 0.5055, 0.4927, 0.5077, 0.4969, 0.4956,
        0.5025, 0.4857, 0.4960, 0.4857, 0.5031, 0.5022, 0.5001, 0.4992, 0.5145,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4966, 0.4978, 0.5025, 0.5037, 0.4924, 0.5088, 0.4963, 0.4963,
        0.5028, 0.4888, 0.4971, 0.4851, 0.5026, 0.5009, 0.4986, 0.4997, 0.5136,
        0.4951, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5123, 0.4972, 0.4949, 0.5016, 0.5038, 0.4923, 0.5085, 0.4947, 0.4970,
        0.5026, 0.4858, 0.4985, 0.4864, 0.5010, 0.5007, 0.4982, 0.4993, 0.5154,
        0.4958, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4980, 0.4952, 0.5015, 0.5044, 0.4925, 0.5098, 0.4967, 0.4967,
        0.5021, 0.4885, 0.4950, 0.4851, 0.5018, 0.5011, 0.4971, 0.5005, 0.5139,
        0.4956, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4984, 0.4941, 0.5030, 0.5043, 0.4938, 0.5083, 0.4987, 0.4964,
        0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5004, 0.4981, 0.4991, 0.5157,
        0.4969, 0.4999], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [6, 8, 2, 3, 4, 1, 9, 0, 7, 5]
replay_buffer._size: [2400 2400 2400 2400 2400 2400 2400 2400 2400 2400]
snapshot at best
2023-08-12 10:40:27,046 MainThread INFO: EPOCH:14
2023-08-12 10:40:27,047 MainThread INFO: Time Consumed:0.9186477661132812s
2023-08-12 10:40:27,047 MainThread INFO: Total Frames:22500s
 19%|‚ñà‚ñâ        | 15/80 [00:20<01:14,  1.15s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1237.94880
Train_Epoch_Reward                    4014.67917
Running_Training_Average_Rewards      1346.94208
Explore_Time                          0.00312
Train___Time                          0.34404
Eval____Time                          0.00273
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.87165
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.33163
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.28980
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.33918
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.26643
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.03062
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.09867
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12631.33678
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.40257
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.21825
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.64412      0.56279    10.25159    8.66885
alpha_0                               0.97832      0.00042    0.97891     0.97773
alpha_1                               0.97832      0.00042    0.97891     0.97773
alpha_2                               0.97834      0.00042    0.97893     0.97775
alpha_3                               0.97833      0.00042    0.97891     0.97774
alpha_4                               0.97833      0.00042    0.97892     0.97774
alpha_5                               0.97832      0.00042    0.97890     0.97773
alpha_6                               0.97832      0.00042    0.97891     0.97774
alpha_7                               0.97831      0.00042    0.97890     0.97773
alpha_8                               0.97832      0.00042    0.97891     0.97774
alpha_9                               0.97833      0.00042    0.97891     0.97774
Alpha_loss                            -0.14509     0.00293    -0.14092    -0.14950
Training/policy_loss                  -2.68928     0.01323    -2.66690    -2.70529
Training/qf1_loss                     1916.54780   296.70439  2419.02612  1575.53931
Training/qf2_loss                     1916.39517   296.69731  2418.85474  1575.39709
Training/pf_norm                      0.33006      0.03623    0.39157     0.28141
Training/qf1_norm                     25.64361     1.17780    26.92862    23.61307
Training/qf2_norm                     25.96436     1.19473    27.21896    23.90946
log_std/mean                          -0.05140     0.00127    -0.04961    -0.05322
log_std/std                           0.00423      0.00011    0.00441     0.00409
log_std/max                           -0.04379     0.00106    -0.04230    -0.04531
log_std/min                           -0.06082     0.00154    -0.05868    -0.06304
log_probs/mean                        -2.71282     0.01360    -2.69005    -2.72958
log_probs/std                         0.33267      0.00927    0.34028     0.31485
log_probs/max                         -1.69164     0.02512    -1.64978    -1.72783
log_probs/min                         -4.40668     0.68021    -3.72464    -5.64667
mean/mean                             -0.00043     0.00036    0.00008     -0.00092
mean/std                              0.00389      0.00021    0.00414     0.00354
mean/max                              0.00689      0.00014    0.00707     0.00666
mean/min                              -0.00524     0.00059    -0.00430    -0.00597
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [0, 3, 9, 8, 5, 4, 7, 2, 6, 1]
replay_buffer._size: [2550 2550 2550 2550 2550 2550 2550 2550 2550 2550]
snapshot at best
2023-08-12 10:40:28,017 MainThread INFO: EPOCH:15
2023-08-12 10:40:28,017 MainThread INFO: Time Consumed:0.8800199031829834s
2023-08-12 10:40:28,017 MainThread INFO: Total Frames:24000s
 20%|‚ñà‚ñà        | 16/80 [00:21<01:10,  1.10s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1266.00829
Train_Epoch_Reward                    15746.38647
Running_Training_Average_Rewards      1157.03893
Explore_Time                          0.00262
Train___Time                          0.33801
Eval____Time                          0.00239
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.34688
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.41427
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.40736
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.24149
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.31526
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.07045
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.13685
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12906.49070
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.48398
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.99132
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.66310      0.91485    10.83088    8.21528
alpha_0                               0.97685      0.00042    0.97744     0.97627
alpha_1                               0.97685      0.00042    0.97744     0.97626
alpha_2                               0.97687      0.00041    0.97746     0.97629
alpha_3                               0.97686      0.00042    0.97744     0.97627
alpha_4                               0.97686      0.00041    0.97745     0.97628
alpha_5                               0.97685      0.00042    0.97743     0.97626
alpha_6                               0.97685      0.00042    0.97744     0.97627
alpha_7                               0.97684      0.00042    0.97743     0.97626
alpha_8                               0.97685      0.00042    0.97744     0.97627
alpha_9                               0.97686      0.00042    0.97744     0.97627
Alpha_loss                            -0.15527     0.00283    -0.15107    -0.15923
Training/policy_loss                  -2.69222     0.00845    -2.68236    -2.70429
Training/qf1_loss                     1902.35979   457.68535  2475.56494  1262.41162
Training/qf2_loss                     1902.19336   457.68086  2475.39355  1262.24976
Training/pf_norm                      0.33982      0.04477    0.39871     0.27301
Training/qf1_norm                     25.74923     1.90322    28.17345    22.74406
Training/qf2_norm                     26.15703     1.88346    28.55702    23.19914
log_std/mean                          -0.05591     0.00129    -0.05410    -0.05775
log_std/std                           0.00459      0.00009    0.00472     0.00445
log_std/max                           -0.04761     0.00108    -0.04609    -0.04914
log_std/min                           -0.06629     0.00155    -0.06407    -0.06850
log_probs/mean                        -2.71665     0.00857    -2.70676    -2.72898
log_probs/std                         0.33223      0.01335    0.35053     0.31202
log_probs/max                         -1.69549     0.03981    -1.63470    -1.74615
log_probs/min                         -4.61972     0.61011    -3.91534    -5.64880
mean/mean                             -0.00137     0.00021    -0.00110    -0.00169
mean/std                              0.00405      0.00011    0.00417     0.00389
mean/max                              0.00583      0.00046    0.00643     0.00518
mean/min                              -0.00626     0.00012    -0.00609    -0.00646
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5062, 0.4989, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
         0.5024, 0.5100, 0.5013, 0.4960, 0.4890, 0.4931, 0.5076, 0.4926, 0.5090,
         0.4964, 0.5142],
        [0.4983, 0.5068, 0.4989, 0.5060, 0.5067, 0.4894, 0.4960, 0.5024, 0.5023,
         0.5028, 0.5083, 0.4994, 0.4962, 0.4882, 0.4909, 0.5076, 0.4921, 0.5085,
         0.4963, 0.5129],
        [0.4979, 0.5068, 0.4980, 0.5064, 0.5067, 0.4883, 0.4966, 0.5002, 0.5045,
         0.5009, 0.5058, 0.4993, 0.4965, 0.4883, 0.4906, 0.5103, 0.4938, 0.5083,
         0.4951, 0.5152],
        [0.4987, 0.5054, 0.4987, 0.5068, 0.5063, 0.4895, 0.4949, 0.4996, 0.5027,
         0.5038, 0.5058, 0.4985, 0.4972, 0.4876, 0.4894, 0.5081, 0.4925, 0.5098,
         0.4927, 0.5144],
        [0.4987, 0.5075, 0.4980, 0.5049, 0.5058, 0.4883, 0.4962, 0.4990, 0.5021,
         0.5025, 0.5086, 0.4998, 0.4970, 0.4883, 0.4921, 0.5061, 0.4944, 0.5096,
         0.4954, 0.5133],
        [0.4977, 0.5042, 0.4977, 0.5061, 0.5053, 0.4882, 0.4947, 0.4999, 0.5011,
         0.5031, 0.5069, 0.4999, 0.4952, 0.4880, 0.4903, 0.5089, 0.4954, 0.5078,
         0.4946, 0.5138],
        [0.4980, 0.5055, 0.4988, 0.5043, 0.5061, 0.4874, 0.4977, 0.5005, 0.5038,
         0.5021, 0.5103, 0.4994, 0.4961, 0.4866, 0.4907, 0.5080, 0.4957, 0.5074,
         0.4968, 0.5127],
        [0.4981, 0.5045, 0.4986, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
         0.5021, 0.5094, 0.4998, 0.4960, 0.4888, 0.4920, 0.5080, 0.4936, 0.5061,
         0.4935, 0.5129],
        [0.4982, 0.5049, 0.4972, 0.5052, 0.5078, 0.4881, 0.4963, 0.4998, 0.5019,
         0.5009, 0.5062, 0.5014, 0.4977, 0.4886, 0.4923, 0.5083, 0.4928, 0.5091,
         0.4940, 0.5147],
        [0.4984, 0.5058, 0.4993, 0.5051, 0.5071, 0.4883, 0.4960, 0.5014, 0.5020,
         0.5037, 0.5076, 0.5003, 0.4948, 0.4900, 0.4927, 0.5089, 0.4940, 0.5056,
         0.4953, 0.5151]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5062, 0.4989, 0.5060, 0.5073, 0.4883, 0.4958, 0.5020, 0.4997,
        0.5024, 0.5100, 0.5013, 0.4960, 0.4890, 0.4931, 0.5076, 0.4926, 0.5090,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5068, 0.4989, 0.5060, 0.5067, 0.4894, 0.4960, 0.5024, 0.5023,
        0.5028, 0.5083, 0.4994, 0.4962, 0.4882, 0.4909, 0.5076, 0.4921, 0.5085,
        0.4963, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5068, 0.4980, 0.5064, 0.5067, 0.4883, 0.4966, 0.5002, 0.5045,
        0.5009, 0.5058, 0.4993, 0.4965, 0.4883, 0.4906, 0.5103, 0.4938, 0.5083,
        0.4951, 0.5152], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5054, 0.4987, 0.5068, 0.5063, 0.4895, 0.4949, 0.4996, 0.5027,
        0.5038, 0.5058, 0.4985, 0.4972, 0.4876, 0.4894, 0.5081, 0.4925, 0.5098,
        0.4927, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5075, 0.4980, 0.5049, 0.5058, 0.4883, 0.4962, 0.4990, 0.5021,
        0.5025, 0.5086, 0.4998, 0.4970, 0.4883, 0.4921, 0.5061, 0.4944, 0.5096,
        0.4954, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5042, 0.4977, 0.5061, 0.5053, 0.4882, 0.4947, 0.4999, 0.5011,
        0.5031, 0.5069, 0.4999, 0.4952, 0.4880, 0.4903, 0.5089, 0.4954, 0.5078,
        0.4946, 0.5138], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5055, 0.4988, 0.5043, 0.5061, 0.4874, 0.4977, 0.5005, 0.5038,
        0.5021, 0.5103, 0.4994, 0.4961, 0.4866, 0.4907, 0.5080, 0.4957, 0.5074,
        0.4968, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5045, 0.4986, 0.5050, 0.5065, 0.4879, 0.4955, 0.5011, 0.5031,
        0.5021, 0.5094, 0.4998, 0.4960, 0.4888, 0.4920, 0.5080, 0.4936, 0.5061,
        0.4935, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5049, 0.4972, 0.5052, 0.5078, 0.4881, 0.4963, 0.4998, 0.5019,
        0.5009, 0.5062, 0.5014, 0.4977, 0.4886, 0.4923, 0.5083, 0.4928, 0.5091,
        0.4940, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4993, 0.5051, 0.5071, 0.4883, 0.4960, 0.5014, 0.5020,
        0.5037, 0.5076, 0.5003, 0.4948, 0.4900, 0.4927, 0.5089, 0.4940, 0.5056,
        0.4953, 0.5151], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5107, 0.4892, 0.5118, 0.4866, 0.4857, 0.4937, 0.4892, 0.4942, 0.4988,
         0.5072, 0.5025, 0.4993, 0.5022, 0.4911, 0.5040, 0.4916, 0.4857, 0.4989,
         0.5081, 0.4950],
        [0.5088, 0.4893, 0.5109, 0.4853, 0.4857, 0.4923, 0.4881, 0.4952, 0.4981,
         0.5050, 0.5033, 0.4993, 0.5032, 0.4912, 0.5024, 0.4895, 0.4860, 0.4987,
         0.5073, 0.4936],
        [0.5106, 0.4906, 0.5107, 0.4871, 0.4853, 0.4938, 0.4887, 0.4964, 0.5017,
         0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5037, 0.4914, 0.4877, 0.4978,
         0.5069, 0.4926],
        [0.5078, 0.4893, 0.5114, 0.4858, 0.4874, 0.4926, 0.4893, 0.4954, 0.4986,
         0.5086, 0.5039, 0.4993, 0.5049, 0.4909, 0.5034, 0.4909, 0.4863, 0.4988,
         0.5061, 0.4923],
        [0.5096, 0.4913, 0.5129, 0.4864, 0.4871, 0.4924, 0.4907, 0.4924, 0.4959,
         0.5063, 0.5040, 0.4992, 0.5032, 0.4913, 0.5034, 0.4896, 0.4857, 0.4965,
         0.5056, 0.4937],
        [0.5114, 0.4917, 0.5099, 0.4867, 0.4863, 0.4945, 0.4908, 0.4936, 0.4969,
         0.5070, 0.5061, 0.4999, 0.5035, 0.4888, 0.5037, 0.4904, 0.4850, 0.4973,
         0.5064, 0.4934],
        [0.5094, 0.4910, 0.5125, 0.4889, 0.4854, 0.4933, 0.4887, 0.4942, 0.4975,
         0.5070, 0.5062, 0.4992, 0.5049, 0.4899, 0.5032, 0.4889, 0.4844, 0.4994,
         0.5072, 0.4933],
        [0.5101, 0.4908, 0.5107, 0.4853, 0.4861, 0.4927, 0.4891, 0.4953, 0.5001,
         0.5072, 0.5063, 0.5007, 0.5035, 0.4895, 0.5045, 0.4914, 0.4860, 0.4962,
         0.5076, 0.4931],
        [0.5106, 0.4921, 0.5107, 0.4854, 0.4879, 0.4929, 0.4897, 0.4939, 0.4976,
         0.5075, 0.5044, 0.4979, 0.5039, 0.4899, 0.5043, 0.4903, 0.4869, 0.4970,
         0.5082, 0.4936],
        [0.5111, 0.4913, 0.5116, 0.4875, 0.4873, 0.4928, 0.4892, 0.4935, 0.4998,
         0.5051, 0.5049, 0.4986, 0.5034, 0.4888, 0.5004, 0.4930, 0.4838, 0.4980,
         0.5058, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5107, 0.4892, 0.5118, 0.4866, 0.4857, 0.4937, 0.4892, 0.4942, 0.4988,
        0.5072, 0.5025, 0.4993, 0.5022, 0.4911, 0.5040, 0.4916, 0.4857, 0.4989,
        0.5081, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4893, 0.5109, 0.4853, 0.4857, 0.4923, 0.4881, 0.4952, 0.4981,
        0.5050, 0.5033, 0.4993, 0.5032, 0.4912, 0.5024, 0.4895, 0.4860, 0.4987,
        0.5073, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5106, 0.4906, 0.5107, 0.4871, 0.4853, 0.4938, 0.4887, 0.4964, 0.5017,
        0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5037, 0.4914, 0.4877, 0.4978,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4893, 0.5114, 0.4858, 0.4874, 0.4926, 0.4893, 0.4954, 0.4986,
        0.5086, 0.5039, 0.4993, 0.5049, 0.4909, 0.5034, 0.4909, 0.4863, 0.4988,
        0.5061, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4913, 0.5129, 0.4864, 0.4871, 0.4924, 0.4907, 0.4924, 0.4959,
        0.5063, 0.5040, 0.4992, 0.5032, 0.4913, 0.5034, 0.4896, 0.4857, 0.4965,
        0.5056, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4917, 0.5099, 0.4867, 0.4863, 0.4945, 0.4908, 0.4936, 0.4969,
        0.5070, 0.5061, 0.4999, 0.5035, 0.4888, 0.5037, 0.4904, 0.4850, 0.4973,
        0.5064, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4910, 0.5125, 0.4889, 0.4854, 0.4933, 0.4887, 0.4942, 0.4975,
        0.5070, 0.5062, 0.4992, 0.5049, 0.4899, 0.5032, 0.4889, 0.4844, 0.4994,
        0.5072, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4908, 0.5107, 0.4853, 0.4861, 0.4927, 0.4891, 0.4953, 0.5001,
        0.5072, 0.5063, 0.5007, 0.5035, 0.4895, 0.5045, 0.4914, 0.4860, 0.4962,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5106, 0.4921, 0.5107, 0.4854, 0.4879, 0.4929, 0.4897, 0.4939, 0.4976,
        0.5075, 0.5044, 0.4979, 0.5039, 0.4899, 0.5043, 0.4903, 0.4869, 0.4970,
        0.5082, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4913, 0.5116, 0.4875, 0.4873, 0.4928, 0.4892, 0.4935, 0.4998,
        0.5051, 0.5049, 0.4986, 0.5034, 0.4888, 0.5004, 0.4930, 0.4838, 0.4980,
        0.5058, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5128, 0.4963, 0.4962, 0.5025, 0.5068, 0.4924, 0.5089, 0.4951, 0.4970,
         0.5026, 0.4856, 0.4973, 0.4839, 0.5027, 0.5027, 0.4985, 0.4992, 0.5139,
         0.4935, 0.4967],
        [0.5116, 0.4960, 0.4950, 0.5046, 0.5033, 0.4928, 0.5102, 0.4966, 0.4960,
         0.5026, 0.4849, 0.4965, 0.4863, 0.5013, 0.5002, 0.4978, 0.4981, 0.5147,
         0.4949, 0.4973],
        [0.5131, 0.4978, 0.4969, 0.5022, 0.5045, 0.4924, 0.5099, 0.4980, 0.4966,
         0.5029, 0.4869, 0.4990, 0.4850, 0.5012, 0.4998, 0.4975, 0.4990, 0.5167,
         0.4950, 0.4974],
        [0.5126, 0.4973, 0.4964, 0.5032, 0.5040, 0.4914, 0.5108, 0.4967, 0.4964,
         0.5027, 0.4845, 0.4962, 0.4848, 0.5021, 0.5008, 0.4972, 0.5009, 0.5142,
         0.4955, 0.4970],
        [0.5124, 0.4984, 0.4959, 0.5037, 0.5031, 0.4936, 0.5087, 0.4953, 0.4972,
         0.5033, 0.4879, 0.4981, 0.4828, 0.5023, 0.5002, 0.4990, 0.5000, 0.5152,
         0.4956, 0.4975],
        [0.5137, 0.4972, 0.4955, 0.5029, 0.5055, 0.4927, 0.5078, 0.4969, 0.4956,
         0.5025, 0.4856, 0.4960, 0.4856, 0.5031, 0.5021, 0.5000, 0.4992, 0.5145,
         0.4951, 0.4981],
        [0.5111, 0.4965, 0.4978, 0.5025, 0.5037, 0.4924, 0.5088, 0.4964, 0.4963,
         0.5028, 0.4886, 0.4971, 0.4851, 0.5025, 0.5009, 0.4986, 0.4998, 0.5137,
         0.4952, 0.4995],
        [0.5123, 0.4972, 0.4949, 0.5015, 0.5039, 0.4923, 0.5085, 0.4947, 0.4970,
         0.5025, 0.4858, 0.4986, 0.4864, 0.5009, 0.5006, 0.4982, 0.4993, 0.5153,
         0.4958, 0.4982],
        [0.5127, 0.4980, 0.4952, 0.5015, 0.5045, 0.4925, 0.5098, 0.4967, 0.4967,
         0.5021, 0.4885, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5005, 0.5138,
         0.4956, 0.4990],
        [0.5124, 0.4984, 0.4940, 0.5030, 0.5044, 0.4938, 0.5083, 0.4988, 0.4964,
         0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5004, 0.4981, 0.4992, 0.5156,
         0.4970, 0.4999]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5128, 0.4963, 0.4962, 0.5025, 0.5068, 0.4924, 0.5089, 0.4951, 0.4970,
        0.5026, 0.4856, 0.4973, 0.4839, 0.5027, 0.5027, 0.4985, 0.4992, 0.5139,
        0.4935, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4960, 0.4950, 0.5046, 0.5033, 0.4928, 0.5102, 0.4966, 0.4960,
        0.5026, 0.4849, 0.4965, 0.4863, 0.5013, 0.5002, 0.4978, 0.4981, 0.5147,
        0.4949, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4978, 0.4969, 0.5022, 0.5045, 0.4924, 0.5099, 0.4980, 0.4966,
        0.5029, 0.4869, 0.4990, 0.4850, 0.5012, 0.4998, 0.4975, 0.4990, 0.5167,
        0.4950, 0.4974], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4973, 0.4964, 0.5032, 0.5040, 0.4914, 0.5108, 0.4967, 0.4964,
        0.5027, 0.4845, 0.4962, 0.4848, 0.5021, 0.5008, 0.4972, 0.5009, 0.5142,
        0.4955, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4984, 0.4959, 0.5037, 0.5031, 0.4936, 0.5087, 0.4953, 0.4972,
        0.5033, 0.4879, 0.4981, 0.4828, 0.5023, 0.5002, 0.4990, 0.5000, 0.5152,
        0.4956, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4972, 0.4955, 0.5029, 0.5055, 0.4927, 0.5078, 0.4969, 0.4956,
        0.5025, 0.4856, 0.4960, 0.4856, 0.5031, 0.5021, 0.5000, 0.4992, 0.5145,
        0.4951, 0.4981], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4965, 0.4978, 0.5025, 0.5037, 0.4924, 0.5088, 0.4964, 0.4963,
        0.5028, 0.4886, 0.4971, 0.4851, 0.5025, 0.5009, 0.4986, 0.4998, 0.5137,
        0.4952, 0.4995], grad_fn=<UnbindBackward>), tensor([0.5123, 0.4972, 0.4949, 0.5015, 0.5039, 0.4923, 0.5085, 0.4947, 0.4970,
        0.5025, 0.4858, 0.4986, 0.4864, 0.5009, 0.5006, 0.4982, 0.4993, 0.5153,
        0.4958, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4980, 0.4952, 0.5015, 0.5045, 0.4925, 0.5098, 0.4967, 0.4967,
        0.5021, 0.4885, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5005, 0.5138,
        0.4956, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4984, 0.4940, 0.5030, 0.5044, 0.4938, 0.5083, 0.4988, 0.4964,
        0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5004, 0.4981, 0.4992, 0.5156,
        0.4970, 0.4999], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [7, 9, 4, 8, 2, 3, 5, 0, 1, 6]
replay_buffer._size: [2700 2700 2700 2700 2700 2700 2700 2700 2700 2700]
snapshot at best
2023-08-12 10:40:29,211 MainThread INFO: EPOCH:16
2023-08-12 10:40:29,212 MainThread INFO: Time Consumed:0.8363628387451172s
2023-08-12 10:40:29,212 MainThread INFO: Total Frames:25500s
 21%|‚ñà‚ñà‚ñè       | 17/80 [00:22<01:10,  1.13s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1268.14948
Train_Epoch_Reward                    8256.26241
Running_Training_Average_Rewards      933.91093
Explore_Time                          0.00316
Train___Time                          0.33400
Eval____Time                          0.00308
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.83856
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.56527
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.34772
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.27795
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.26725
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.03655
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.10365
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12936.48637
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.64042
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.91419
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.68514     0.83693    11.75053    9.41827
alpha_0                               0.97539      0.00041    0.97597     0.97480
alpha_1                               0.97538      0.00041    0.97597     0.97480
alpha_2                               0.97541      0.00041    0.97599     0.97482
alpha_3                               0.97539      0.00041    0.97598     0.97480
alpha_4                               0.97540      0.00041    0.97598     0.97481
alpha_5                               0.97538      0.00041    0.97597     0.97479
alpha_6                               0.97539      0.00041    0.97597     0.97480
alpha_7                               0.97538      0.00041    0.97596     0.97479
alpha_8                               0.97539      0.00042    0.97597     0.97480
alpha_9                               0.97539      0.00041    0.97597     0.97480
Alpha_loss                            -0.16535     0.00287    -0.16148    -0.16927
Training/policy_loss                  -2.69098     0.00929    -2.67988    -2.70520
Training/qf1_loss                     2463.99353   685.11159  3613.03564  1711.33948
Training/qf2_loss                     2463.82036   685.10938  3612.85547  1711.17126
Training/pf_norm                      0.30100      0.05088    0.37762     0.23493
Training/qf1_norm                     27.96418     1.75357    30.21183    25.36021
Training/qf2_norm                     28.32713     1.72645    30.57685    25.74987
log_std/mean                          -0.06060     0.00134    -0.05872    -0.06250
log_std/std                           0.00496      0.00010    0.00509     0.00482
log_std/max                           -0.05168     0.00109    -0.05005    -0.05321
log_std/min                           -0.07170     0.00158    -0.06952    -0.07395
log_probs/mean                        -2.71614     0.00952    -2.70469    -2.73086
log_probs/std                         0.31456      0.00668    0.32770     0.30917
log_probs/max                         -1.71695     0.06063    -1.64101    -1.79841
log_probs/min                         -4.40825     0.26572    -4.12120    -4.83670
mean/mean                             -0.00219     0.00022    -0.00189    -0.00251
mean/std                              0.00369      0.00011    0.00384     0.00353
mean/max                              0.00441      0.00036    0.00488     0.00388
mean/min                              -0.00705     0.00028    -0.00671    -0.00745
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [3, 4, 8, 5, 9, 2, 7, 1, 6, 0]
replay_buffer._size: [2850 2850 2850 2850 2850 2850 2850 2850 2850 2850]
2023-08-12 10:40:29,637 MainThread INFO: EPOCH:17
2023-08-12 10:40:29,637 MainThread INFO: Time Consumed:0.3366677761077881s
2023-08-12 10:40:29,637 MainThread INFO: Total Frames:27000s
 22%|‚ñà‚ñà‚ñé       | 18/80 [00:23<00:57,  1.08it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1252.32955
Train_Epoch_Reward                    37739.33358
Running_Training_Average_Rewards      2058.06608
Explore_Time                          0.00281
Train___Time                          0.33071
Eval____Time                          0.00266
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.98013
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.70484
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.18103
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.39437
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.16389
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.96128
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.02969
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12775.47362
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.78735
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.97548
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.75925     0.52817    11.73917    10.32211
alpha_0                               0.97392      0.00041    0.97451     0.97333
alpha_1                               0.97392      0.00041    0.97450     0.97333
alpha_2                               0.97394      0.00041    0.97453     0.97336
alpha_3                               0.97392      0.00041    0.97451     0.97334
alpha_4                               0.97393      0.00041    0.97452     0.97335
alpha_5                               0.97391      0.00041    0.97450     0.97333
alpha_6                               0.97392      0.00041    0.97451     0.97333
alpha_7                               0.97391      0.00041    0.97450     0.97333
alpha_8                               0.97392      0.00041    0.97451     0.97334
alpha_9                               0.97392      0.00041    0.97451     0.97334
Alpha_loss                            -0.17570     0.00296    -0.17154    -0.18003
Training/policy_loss                  -2.69983     0.00626    -2.69161    -2.70990
Training/qf1_loss                     2401.90959   296.90618  2718.84912  1966.01599
Training/qf2_loss                     2401.71128   296.90092  2718.66211  1965.84082
Training/pf_norm                      0.26749      0.03141    0.30902     0.22189
Training/qf1_norm                     28.21342     1.10110    30.25290    27.27020
Training/qf2_norm                     28.70518     1.06302    30.62996    27.65022
log_std/mean                          -0.06536     0.00134    -0.06347    -0.06724
log_std/std                           0.00536      0.00012    0.00553     0.00520
log_std/max                           -0.05568     0.00111    -0.05410    -0.05721
log_std/min                           -0.07723     0.00150    -0.07506    -0.07927
log_probs/mean                        -2.72592     0.00652    -2.71747    -2.73646
log_probs/std                         0.30916      0.00863    0.31921     0.29848
log_probs/max                         -1.77137     0.03835    -1.71141    -1.83051
log_probs/min                         -4.18879     0.29590    -3.70712    -4.58666
mean/mean                             -0.00302     0.00019    -0.00273    -0.00323
mean/std                              0.00298      0.00026    0.00338     0.00265
mean/max                              0.00254      0.00061    0.00344     0.00178
mean/min                              -0.00742     0.00011    -0.00726    -0.00756
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5062, 0.4990, 0.5061, 0.5072, 0.4883, 0.4958, 0.5020, 0.4997,
         0.5024, 0.5100, 0.5013, 0.4960, 0.4889, 0.4931, 0.5076, 0.4926, 0.5090,
         0.4964, 0.5142],
        [0.4983, 0.5069, 0.4989, 0.5061, 0.5067, 0.4894, 0.4961, 0.5024, 0.5023,
         0.5028, 0.5083, 0.4994, 0.4962, 0.4882, 0.4908, 0.5076, 0.4921, 0.5085,
         0.4964, 0.5129],
        [0.4979, 0.5068, 0.4980, 0.5064, 0.5067, 0.4883, 0.4966, 0.5002, 0.5045,
         0.5009, 0.5059, 0.4993, 0.4964, 0.4883, 0.4905, 0.5103, 0.4938, 0.5083,
         0.4952, 0.5152],
        [0.4987, 0.5055, 0.4986, 0.5067, 0.5063, 0.4894, 0.4949, 0.4996, 0.5028,
         0.5038, 0.5058, 0.4984, 0.4973, 0.4876, 0.4894, 0.5081, 0.4925, 0.5098,
         0.4927, 0.5144],
        [0.4987, 0.5076, 0.4980, 0.5049, 0.5057, 0.4883, 0.4961, 0.4990, 0.5021,
         0.5025, 0.5087, 0.4998, 0.4970, 0.4883, 0.4921, 0.5061, 0.4944, 0.5096,
         0.4954, 0.5132],
        [0.4976, 0.5043, 0.4977, 0.5060, 0.5054, 0.4883, 0.4947, 0.4999, 0.5010,
         0.5031, 0.5069, 0.4999, 0.4952, 0.4880, 0.4902, 0.5089, 0.4954, 0.5079,
         0.4947, 0.5138],
        [0.4980, 0.5055, 0.4987, 0.5043, 0.5061, 0.4875, 0.4976, 0.5005, 0.5038,
         0.5020, 0.5103, 0.4995, 0.4961, 0.4867, 0.4907, 0.5080, 0.4957, 0.5075,
         0.4968, 0.5126],
        [0.4980, 0.5045, 0.4985, 0.5050, 0.5065, 0.4880, 0.4955, 0.5010, 0.5031,
         0.5021, 0.5094, 0.4998, 0.4960, 0.4889, 0.4920, 0.5079, 0.4936, 0.5062,
         0.4934, 0.5129],
        [0.4982, 0.5049, 0.4972, 0.5052, 0.5079, 0.4881, 0.4963, 0.4998, 0.5019,
         0.5009, 0.5062, 0.5014, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5091,
         0.4940, 0.5146],
        [0.4984, 0.5058, 0.4993, 0.5051, 0.5071, 0.4883, 0.4961, 0.5014, 0.5020,
         0.5037, 0.5076, 0.5003, 0.4948, 0.4901, 0.4927, 0.5089, 0.4940, 0.5055,
         0.4953, 0.5151]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5062, 0.4990, 0.5061, 0.5072, 0.4883, 0.4958, 0.5020, 0.4997,
        0.5024, 0.5100, 0.5013, 0.4960, 0.4889, 0.4931, 0.5076, 0.4926, 0.5090,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5069, 0.4989, 0.5061, 0.5067, 0.4894, 0.4961, 0.5024, 0.5023,
        0.5028, 0.5083, 0.4994, 0.4962, 0.4882, 0.4908, 0.5076, 0.4921, 0.5085,
        0.4964, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5068, 0.4980, 0.5064, 0.5067, 0.4883, 0.4966, 0.5002, 0.5045,
        0.5009, 0.5059, 0.4993, 0.4964, 0.4883, 0.4905, 0.5103, 0.4938, 0.5083,
        0.4952, 0.5152], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5055, 0.4986, 0.5067, 0.5063, 0.4894, 0.4949, 0.4996, 0.5028,
        0.5038, 0.5058, 0.4984, 0.4973, 0.4876, 0.4894, 0.5081, 0.4925, 0.5098,
        0.4927, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5076, 0.4980, 0.5049, 0.5057, 0.4883, 0.4961, 0.4990, 0.5021,
        0.5025, 0.5087, 0.4998, 0.4970, 0.4883, 0.4921, 0.5061, 0.4944, 0.5096,
        0.4954, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5043, 0.4977, 0.5060, 0.5054, 0.4883, 0.4947, 0.4999, 0.5010,
        0.5031, 0.5069, 0.4999, 0.4952, 0.4880, 0.4902, 0.5089, 0.4954, 0.5079,
        0.4947, 0.5138], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5055, 0.4987, 0.5043, 0.5061, 0.4875, 0.4976, 0.5005, 0.5038,
        0.5020, 0.5103, 0.4995, 0.4961, 0.4867, 0.4907, 0.5080, 0.4957, 0.5075,
        0.4968, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5045, 0.4985, 0.5050, 0.5065, 0.4880, 0.4955, 0.5010, 0.5031,
        0.5021, 0.5094, 0.4998, 0.4960, 0.4889, 0.4920, 0.5079, 0.4936, 0.5062,
        0.4934, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5049, 0.4972, 0.5052, 0.5079, 0.4881, 0.4963, 0.4998, 0.5019,
        0.5009, 0.5062, 0.5014, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5091,
        0.4940, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4993, 0.5051, 0.5071, 0.4883, 0.4961, 0.5014, 0.5020,
        0.5037, 0.5076, 0.5003, 0.4948, 0.4901, 0.4927, 0.5089, 0.4940, 0.5055,
        0.4953, 0.5151], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5106, 0.4892, 0.5119, 0.4866, 0.4857, 0.4937, 0.4892, 0.4943, 0.4988,
         0.5072, 0.5025, 0.4994, 0.5022, 0.4911, 0.5041, 0.4916, 0.4857, 0.4989,
         0.5081, 0.4950],
        [0.5087, 0.4893, 0.5110, 0.4853, 0.4856, 0.4923, 0.4881, 0.4952, 0.4981,
         0.5051, 0.5033, 0.4994, 0.5032, 0.4912, 0.5025, 0.4893, 0.4860, 0.4988,
         0.5073, 0.4936],
        [0.5106, 0.4906, 0.5108, 0.4871, 0.4853, 0.4938, 0.4887, 0.4964, 0.5017,
         0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5037, 0.4914, 0.4877, 0.4979,
         0.5069, 0.4926],
        [0.5078, 0.4893, 0.5114, 0.4858, 0.4874, 0.4926, 0.4893, 0.4954, 0.4986,
         0.5086, 0.5038, 0.4994, 0.5049, 0.4909, 0.5034, 0.4908, 0.4863, 0.4987,
         0.5061, 0.4923],
        [0.5095, 0.4913, 0.5129, 0.4863, 0.4871, 0.4925, 0.4907, 0.4924, 0.4958,
         0.5063, 0.5041, 0.4993, 0.5031, 0.4913, 0.5034, 0.4897, 0.4858, 0.4964,
         0.5055, 0.4937],
        [0.5115, 0.4916, 0.5099, 0.4867, 0.4863, 0.4945, 0.4909, 0.4937, 0.4969,
         0.5069, 0.5061, 0.4998, 0.5035, 0.4888, 0.5036, 0.4905, 0.4850, 0.4973,
         0.5064, 0.4934],
        [0.5095, 0.4909, 0.5125, 0.4889, 0.4855, 0.4934, 0.4887, 0.4942, 0.4974,
         0.5069, 0.5062, 0.4992, 0.5049, 0.4899, 0.5031, 0.4891, 0.4845, 0.4994,
         0.5073, 0.4934],
        [0.5101, 0.4908, 0.5107, 0.4853, 0.4862, 0.4928, 0.4891, 0.4953, 0.5001,
         0.5072, 0.5063, 0.5007, 0.5035, 0.4894, 0.5045, 0.4914, 0.4860, 0.4962,
         0.5076, 0.4931],
        [0.5106, 0.4921, 0.5107, 0.4853, 0.4880, 0.4929, 0.4897, 0.4939, 0.4976,
         0.5075, 0.5044, 0.4979, 0.5039, 0.4899, 0.5042, 0.4903, 0.4869, 0.4969,
         0.5082, 0.4936],
        [0.5112, 0.4914, 0.5116, 0.4875, 0.4874, 0.4928, 0.4892, 0.4935, 0.4998,
         0.5051, 0.5049, 0.4986, 0.5034, 0.4888, 0.5003, 0.4931, 0.4838, 0.4980,
         0.5058, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5106, 0.4892, 0.5119, 0.4866, 0.4857, 0.4937, 0.4892, 0.4943, 0.4988,
        0.5072, 0.5025, 0.4994, 0.5022, 0.4911, 0.5041, 0.4916, 0.4857, 0.4989,
        0.5081, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4893, 0.5110, 0.4853, 0.4856, 0.4923, 0.4881, 0.4952, 0.4981,
        0.5051, 0.5033, 0.4994, 0.5032, 0.4912, 0.5025, 0.4893, 0.4860, 0.4988,
        0.5073, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5106, 0.4906, 0.5108, 0.4871, 0.4853, 0.4938, 0.4887, 0.4964, 0.5017,
        0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5037, 0.4914, 0.4877, 0.4979,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4893, 0.5114, 0.4858, 0.4874, 0.4926, 0.4893, 0.4954, 0.4986,
        0.5086, 0.5038, 0.4994, 0.5049, 0.4909, 0.5034, 0.4908, 0.4863, 0.4987,
        0.5061, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4913, 0.5129, 0.4863, 0.4871, 0.4925, 0.4907, 0.4924, 0.4958,
        0.5063, 0.5041, 0.4993, 0.5031, 0.4913, 0.5034, 0.4897, 0.4858, 0.4964,
        0.5055, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4916, 0.5099, 0.4867, 0.4863, 0.4945, 0.4909, 0.4937, 0.4969,
        0.5069, 0.5061, 0.4998, 0.5035, 0.4888, 0.5036, 0.4905, 0.4850, 0.4973,
        0.5064, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4909, 0.5125, 0.4889, 0.4855, 0.4934, 0.4887, 0.4942, 0.4974,
        0.5069, 0.5062, 0.4992, 0.5049, 0.4899, 0.5031, 0.4891, 0.4845, 0.4994,
        0.5073, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4908, 0.5107, 0.4853, 0.4862, 0.4928, 0.4891, 0.4953, 0.5001,
        0.5072, 0.5063, 0.5007, 0.5035, 0.4894, 0.5045, 0.4914, 0.4860, 0.4962,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5106, 0.4921, 0.5107, 0.4853, 0.4880, 0.4929, 0.4897, 0.4939, 0.4976,
        0.5075, 0.5044, 0.4979, 0.5039, 0.4899, 0.5042, 0.4903, 0.4869, 0.4969,
        0.5082, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4914, 0.5116, 0.4875, 0.4874, 0.4928, 0.4892, 0.4935, 0.4998,
        0.5051, 0.5049, 0.4986, 0.5034, 0.4888, 0.5003, 0.4931, 0.4838, 0.4980,
        0.5058, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5128, 0.4962, 0.4963, 0.5025, 0.5068, 0.4925, 0.5089, 0.4951, 0.4970,
         0.5026, 0.4856, 0.4973, 0.4840, 0.5026, 0.5026, 0.4985, 0.4992, 0.5139,
         0.4935, 0.4966],
        [0.5116, 0.4960, 0.4950, 0.5046, 0.5034, 0.4930, 0.5101, 0.4967, 0.4959,
         0.5026, 0.4849, 0.4965, 0.4863, 0.5012, 0.5001, 0.4978, 0.4980, 0.5146,
         0.4949, 0.4972],
        [0.5131, 0.4977, 0.4970, 0.5022, 0.5046, 0.4923, 0.5099, 0.4980, 0.4966,
         0.5028, 0.4869, 0.4990, 0.4850, 0.5012, 0.4998, 0.4975, 0.4990, 0.5167,
         0.4950, 0.4974],
        [0.5126, 0.4974, 0.4964, 0.5032, 0.5040, 0.4915, 0.5108, 0.4967, 0.4964,
         0.5028, 0.4844, 0.4962, 0.4848, 0.5021, 0.5009, 0.4972, 0.5008, 0.5142,
         0.4955, 0.4969],
        [0.5124, 0.4984, 0.4960, 0.5037, 0.5032, 0.4935, 0.5087, 0.4953, 0.4972,
         0.5032, 0.4878, 0.4980, 0.4828, 0.5024, 0.5002, 0.4990, 0.5000, 0.5152,
         0.4956, 0.4975],
        [0.5137, 0.4972, 0.4955, 0.5029, 0.5055, 0.4927, 0.5078, 0.4969, 0.4956,
         0.5025, 0.4856, 0.4960, 0.4856, 0.5031, 0.5021, 0.5000, 0.4993, 0.5145,
         0.4951, 0.4982],
        [0.5112, 0.4965, 0.4978, 0.5026, 0.5037, 0.4923, 0.5088, 0.4963, 0.4963,
         0.5028, 0.4885, 0.4971, 0.4850, 0.5026, 0.5009, 0.4986, 0.4999, 0.5138,
         0.4953, 0.4996],
        [0.5123, 0.4972, 0.4949, 0.5015, 0.5038, 0.4923, 0.5085, 0.4947, 0.4970,
         0.5025, 0.4858, 0.4986, 0.4864, 0.5010, 0.5007, 0.4983, 0.4993, 0.5154,
         0.4958, 0.4982],
        [0.5128, 0.4981, 0.4952, 0.5015, 0.5045, 0.4925, 0.5098, 0.4967, 0.4967,
         0.5020, 0.4885, 0.4950, 0.4850, 0.5018, 0.5011, 0.4971, 0.5005, 0.5139,
         0.4956, 0.4990],
        [0.5125, 0.4984, 0.4941, 0.5030, 0.5044, 0.4938, 0.5082, 0.4988, 0.4964,
         0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5005, 0.4981, 0.4992, 0.5157,
         0.4970, 0.5000]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5128, 0.4962, 0.4963, 0.5025, 0.5068, 0.4925, 0.5089, 0.4951, 0.4970,
        0.5026, 0.4856, 0.4973, 0.4840, 0.5026, 0.5026, 0.4985, 0.4992, 0.5139,
        0.4935, 0.4966], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4960, 0.4950, 0.5046, 0.5034, 0.4930, 0.5101, 0.4967, 0.4959,
        0.5026, 0.4849, 0.4965, 0.4863, 0.5012, 0.5001, 0.4978, 0.4980, 0.5146,
        0.4949, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4977, 0.4970, 0.5022, 0.5046, 0.4923, 0.5099, 0.4980, 0.4966,
        0.5028, 0.4869, 0.4990, 0.4850, 0.5012, 0.4998, 0.4975, 0.4990, 0.5167,
        0.4950, 0.4974], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4964, 0.5032, 0.5040, 0.4915, 0.5108, 0.4967, 0.4964,
        0.5028, 0.4844, 0.4962, 0.4848, 0.5021, 0.5009, 0.4972, 0.5008, 0.5142,
        0.4955, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5124, 0.4984, 0.4960, 0.5037, 0.5032, 0.4935, 0.5087, 0.4953, 0.4972,
        0.5032, 0.4878, 0.4980, 0.4828, 0.5024, 0.5002, 0.4990, 0.5000, 0.5152,
        0.4956, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4972, 0.4955, 0.5029, 0.5055, 0.4927, 0.5078, 0.4969, 0.4956,
        0.5025, 0.4856, 0.4960, 0.4856, 0.5031, 0.5021, 0.5000, 0.4993, 0.5145,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4965, 0.4978, 0.5026, 0.5037, 0.4923, 0.5088, 0.4963, 0.4963,
        0.5028, 0.4885, 0.4971, 0.4850, 0.5026, 0.5009, 0.4986, 0.4999, 0.5138,
        0.4953, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5123, 0.4972, 0.4949, 0.5015, 0.5038, 0.4923, 0.5085, 0.4947, 0.4970,
        0.5025, 0.4858, 0.4986, 0.4864, 0.5010, 0.5007, 0.4983, 0.4993, 0.5154,
        0.4958, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4952, 0.5015, 0.5045, 0.4925, 0.5098, 0.4967, 0.4967,
        0.5020, 0.4885, 0.4950, 0.4850, 0.5018, 0.5011, 0.4971, 0.5005, 0.5139,
        0.4956, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4984, 0.4941, 0.5030, 0.5044, 0.4938, 0.5082, 0.4988, 0.4964,
        0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5005, 0.4981, 0.4992, 0.5157,
        0.4970, 0.5000], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [7, 0, 1, 3, 6, 9, 5, 2, 4, 8]
replay_buffer._size: [3000 3000 3000 3000 3000 3000 3000 3000 3000 3000]
2023-08-12 10:40:30,424 MainThread INFO: EPOCH:18
2023-08-12 10:40:30,424 MainThread INFO: Time Consumed:0.35132312774658203s
2023-08-12 10:40:30,424 MainThread INFO: Total Frames:28500s
 24%|‚ñà‚ñà‚ñç       | 19/80 [00:23<00:53,  1.14it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1235.62015
Train_Epoch_Reward                    20415.23432
Running_Training_Average_Rewards      2213.69434
Explore_Time                          0.00371
Train___Time                          0.34287
Eval____Time                          0.00260
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.12740
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.85455
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.96770
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.47251
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.94507
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.79539
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.86582
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12605.19000
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.94458
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.01546
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.38041     0.73918    12.22894    9.99836
alpha_0                               0.97246      0.00041    0.97304     0.97187
alpha_1                               0.97246      0.00041    0.97304     0.97187
alpha_2                               0.97248      0.00041    0.97307     0.97190
alpha_3                               0.97246      0.00041    0.97304     0.97187
alpha_4                               0.97247      0.00041    0.97305     0.97188
alpha_5                               0.97245      0.00041    0.97303     0.97186
alpha_6                               0.97246      0.00041    0.97304     0.97187
alpha_7                               0.97245      0.00041    0.97303     0.97186
alpha_8                               0.97246      0.00041    0.97304     0.97187
alpha_9                               0.97246      0.00041    0.97304     0.97187
Alpha_loss                            -0.18592     0.00293    -0.18179    -0.19023
Training/policy_loss                  -2.70295     0.00514    -2.69644    -2.71180
Training/qf1_loss                     2442.16270   179.47456  2663.94507  2124.47046
Training/qf2_loss                     2441.93911   179.45522  2663.70947  2124.28687
Training/pf_norm                      0.23375      0.02245    0.26992     0.19918
Training/qf1_norm                     29.62720     1.56509    31.34595    26.66816
Training/qf2_norm                     30.22116     1.64821    31.98870    27.09579
log_std/mean                          -0.06998     0.00126    -0.06818    -0.07176
log_std/std                           0.00573      0.00011    0.00588     0.00557
log_std/max                           -0.05956     0.00105    -0.05806    -0.06102
log_std/min                           -0.08247     0.00134    -0.08048    -0.08423
log_probs/mean                        -2.72971     0.00535    -2.72307    -2.73899
log_probs/std                         0.30544      0.00798    0.31659     0.29434
log_probs/max                         -1.80664     0.05689    -1.71558    -1.89280
log_probs/min                         -4.88311     0.58502    -4.05524    -5.68515
mean/mean                             -0.00291     0.00021    -0.00260    -0.00319
mean/std                              0.00240      0.00011    0.00257     0.00226
mean/max                              0.00161      0.00007    0.00172     0.00151
mean/min                              -0.00671     0.00045    -0.00603    -0.00722
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [8, 4, 2, 0, 3, 5, 9, 6, 7, 1]
replay_buffer._size: [3150 3150 3150 3150 3150 3150 3150 3150 3150 3150]
2023-08-12 10:40:30,865 MainThread INFO: EPOCH:19
2023-08-12 10:40:30,865 MainThread INFO: Time Consumed:0.33055591583251953s
2023-08-12 10:40:30,865 MainThread INFO: Total Frames:30000s
 25%|‚ñà‚ñà‚ñå       | 20/80 [00:24<00:44,  1.34it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1209.75070
Train_Epoch_Reward                    17864.08516
Running_Training_Average_Rewards      2533.95510
Explore_Time                          0.00290
Train___Time                          0.32494
Eval____Time                          0.00229
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.01639
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.95237
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.73660
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.59649
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.76414
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.65928
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.73161
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12344.16833
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -27.05050
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.15395
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.61433     0.45437    12.41255    11.21057
alpha_0                               0.97100      0.00041    0.97158     0.97041
alpha_1                               0.97099      0.00041    0.97158     0.97041
alpha_2                               0.97102      0.00041    0.97161     0.97044
alpha_3                               0.97100      0.00041    0.97158     0.97041
alpha_4                               0.97101      0.00041    0.97159     0.97042
alpha_5                               0.97098      0.00041    0.97157     0.97040
alpha_6                               0.97099      0.00041    0.97158     0.97041
alpha_7                               0.97099      0.00041    0.97157     0.97040
alpha_8                               0.97100      0.00041    0.97158     0.97041
alpha_9                               0.97100      0.00041    0.97158     0.97041
Alpha_loss                            -0.19592     0.00289    -0.19195    -0.20013
Training/policy_loss                  -2.69856     0.00427    -2.69117    -2.70388
Training/qf1_loss                     2782.82075   215.80086  3136.72827  2587.61304
Training/qf2_loss                     2782.58208   215.80520  3136.49292  2587.37866
Training/pf_norm                      0.25317      0.02963    0.28409     0.19955
Training/qf1_norm                     30.21086     0.94574    31.86306    29.35054
Training/qf2_norm                     30.84310     0.92373    32.42955    29.95546
log_std/mean                          -0.07439     0.00125    -0.07265    -0.07615
log_std/std                           0.00610      0.00009    0.00625     0.00599
log_std/max                           -0.06336     0.00091    -0.06211    -0.06459
log_std/min                           -0.08750     0.00148    -0.08549    -0.08960
log_probs/mean                        -2.72550     0.00437    -2.71798    -2.73105
log_probs/std                         0.30957      0.01668    0.33790     0.28906
log_probs/max                         -1.79616     0.01797    -1.77471    -1.81920
log_probs/min                         -5.31437     0.90500    -4.21325    -6.62975
mean/mean                             -0.00196     0.00031    -0.00153    -0.00238
mean/std                              0.00227      0.00002    0.00230     0.00225
mean/max                              0.00262      0.00043    0.00317     0.00200
mean/min                              -0.00475     0.00067    -0.00385    -0.00568
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5063, 0.4989, 0.5061, 0.5072, 0.4883, 0.4957, 0.5019, 0.4997,
         0.5024, 0.5100, 0.5012, 0.4960, 0.4889, 0.4930, 0.5076, 0.4926, 0.5090,
         0.4964, 0.5142],
        [0.4983, 0.5069, 0.4989, 0.5061, 0.5067, 0.4895, 0.4960, 0.5024, 0.5023,
         0.5028, 0.5083, 0.4994, 0.4962, 0.4882, 0.4908, 0.5076, 0.4920, 0.5085,
         0.4963, 0.5128],
        [0.4979, 0.5069, 0.4980, 0.5064, 0.5066, 0.4883, 0.4966, 0.5002, 0.5045,
         0.5009, 0.5060, 0.4993, 0.4964, 0.4883, 0.4905, 0.5102, 0.4938, 0.5084,
         0.4952, 0.5151],
        [0.4988, 0.5055, 0.4986, 0.5068, 0.5063, 0.4894, 0.4948, 0.4996, 0.5028,
         0.5037, 0.5059, 0.4984, 0.4973, 0.4876, 0.4894, 0.5080, 0.4925, 0.5098,
         0.4928, 0.5144],
        [0.4987, 0.5076, 0.4979, 0.5049, 0.5057, 0.4884, 0.4961, 0.4990, 0.5021,
         0.5025, 0.5086, 0.4997, 0.4970, 0.4883, 0.4921, 0.5061, 0.4944, 0.5096,
         0.4954, 0.5132],
        [0.4977, 0.5043, 0.4976, 0.5060, 0.5054, 0.4883, 0.4947, 0.4999, 0.5010,
         0.5030, 0.5070, 0.4999, 0.4952, 0.4880, 0.4903, 0.5089, 0.4954, 0.5079,
         0.4947, 0.5137],
        [0.4981, 0.5055, 0.4986, 0.5043, 0.5062, 0.4875, 0.4975, 0.5005, 0.5037,
         0.5021, 0.5103, 0.4995, 0.4961, 0.4868, 0.4906, 0.5079, 0.4956, 0.5075,
         0.4967, 0.5126],
        [0.4980, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4954, 0.5010, 0.5032,
         0.5021, 0.5094, 0.4997, 0.4961, 0.4889, 0.4920, 0.5079, 0.4935, 0.5061,
         0.4933, 0.5129],
        [0.4981, 0.5049, 0.4972, 0.5052, 0.5079, 0.4881, 0.4962, 0.4998, 0.5019,
         0.5009, 0.5062, 0.5014, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5091,
         0.4940, 0.5146],
        [0.4984, 0.5058, 0.4993, 0.5051, 0.5071, 0.4883, 0.4960, 0.5015, 0.5021,
         0.5037, 0.5076, 0.5003, 0.4948, 0.4901, 0.4927, 0.5090, 0.4940, 0.5055,
         0.4953, 0.5151]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5063, 0.4989, 0.5061, 0.5072, 0.4883, 0.4957, 0.5019, 0.4997,
        0.5024, 0.5100, 0.5012, 0.4960, 0.4889, 0.4930, 0.5076, 0.4926, 0.5090,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5069, 0.4989, 0.5061, 0.5067, 0.4895, 0.4960, 0.5024, 0.5023,
        0.5028, 0.5083, 0.4994, 0.4962, 0.4882, 0.4908, 0.5076, 0.4920, 0.5085,
        0.4963, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5069, 0.4980, 0.5064, 0.5066, 0.4883, 0.4966, 0.5002, 0.5045,
        0.5009, 0.5060, 0.4993, 0.4964, 0.4883, 0.4905, 0.5102, 0.4938, 0.5084,
        0.4952, 0.5151], grad_fn=<UnbindBackward>), tensor([0.4988, 0.5055, 0.4986, 0.5068, 0.5063, 0.4894, 0.4948, 0.4996, 0.5028,
        0.5037, 0.5059, 0.4984, 0.4973, 0.4876, 0.4894, 0.5080, 0.4925, 0.5098,
        0.4928, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5076, 0.4979, 0.5049, 0.5057, 0.4884, 0.4961, 0.4990, 0.5021,
        0.5025, 0.5086, 0.4997, 0.4970, 0.4883, 0.4921, 0.5061, 0.4944, 0.5096,
        0.4954, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5043, 0.4976, 0.5060, 0.5054, 0.4883, 0.4947, 0.4999, 0.5010,
        0.5030, 0.5070, 0.4999, 0.4952, 0.4880, 0.4903, 0.5089, 0.4954, 0.5079,
        0.4947, 0.5137], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5055, 0.4986, 0.5043, 0.5062, 0.4875, 0.4975, 0.5005, 0.5037,
        0.5021, 0.5103, 0.4995, 0.4961, 0.4868, 0.4906, 0.5079, 0.4956, 0.5075,
        0.4967, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4954, 0.5010, 0.5032,
        0.5021, 0.5094, 0.4997, 0.4961, 0.4889, 0.4920, 0.5079, 0.4935, 0.5061,
        0.4933, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5049, 0.4972, 0.5052, 0.5079, 0.4881, 0.4962, 0.4998, 0.5019,
        0.5009, 0.5062, 0.5014, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5091,
        0.4940, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4993, 0.5051, 0.5071, 0.4883, 0.4960, 0.5015, 0.5021,
        0.5037, 0.5076, 0.5003, 0.4948, 0.4901, 0.4927, 0.5090, 0.4940, 0.5055,
        0.4953, 0.5151], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5106, 0.4892, 0.5118, 0.4866, 0.4856, 0.4938, 0.4893, 0.4942, 0.4988,
         0.5072, 0.5026, 0.4994, 0.5022, 0.4912, 0.5042, 0.4915, 0.4857, 0.4989,
         0.5081, 0.4950],
        [0.5087, 0.4893, 0.5110, 0.4852, 0.4856, 0.4923, 0.4881, 0.4952, 0.4981,
         0.5051, 0.5033, 0.4994, 0.5033, 0.4912, 0.5025, 0.4892, 0.4860, 0.4987,
         0.5073, 0.4936],
        [0.5106, 0.4906, 0.5108, 0.4871, 0.4852, 0.4938, 0.4887, 0.4964, 0.5016,
         0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5037, 0.4913, 0.4876, 0.4979,
         0.5070, 0.4926],
        [0.5078, 0.4893, 0.5114, 0.4858, 0.4874, 0.4927, 0.4893, 0.4954, 0.4986,
         0.5086, 0.5039, 0.4994, 0.5049, 0.4909, 0.5035, 0.4908, 0.4863, 0.4987,
         0.5061, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4863, 0.4871, 0.4925, 0.4907, 0.4924, 0.4957,
         0.5064, 0.5041, 0.4993, 0.5032, 0.4913, 0.5034, 0.4896, 0.4858, 0.4964,
         0.5056, 0.4937],
        [0.5114, 0.4917, 0.5100, 0.4866, 0.4863, 0.4945, 0.4908, 0.4937, 0.4968,
         0.5069, 0.5061, 0.4999, 0.5035, 0.4889, 0.5036, 0.4905, 0.4850, 0.4973,
         0.5064, 0.4934],
        [0.5095, 0.4910, 0.5126, 0.4888, 0.4856, 0.4934, 0.4887, 0.4942, 0.4975,
         0.5070, 0.5062, 0.4993, 0.5049, 0.4899, 0.5032, 0.4891, 0.4845, 0.4993,
         0.5073, 0.4933],
        [0.5101, 0.4909, 0.5108, 0.4853, 0.4862, 0.4928, 0.4891, 0.4952, 0.5001,
         0.5072, 0.5064, 0.5008, 0.5035, 0.4894, 0.5046, 0.4914, 0.4860, 0.4962,
         0.5076, 0.4931],
        [0.5106, 0.4922, 0.5107, 0.4853, 0.4880, 0.4930, 0.4898, 0.4939, 0.4976,
         0.5075, 0.5045, 0.4978, 0.5039, 0.4899, 0.5043, 0.4903, 0.4870, 0.4968,
         0.5082, 0.4936],
        [0.5112, 0.4914, 0.5116, 0.4875, 0.4874, 0.4928, 0.4892, 0.4934, 0.4999,
         0.5051, 0.5049, 0.4986, 0.5034, 0.4888, 0.5003, 0.4930, 0.4837, 0.4980,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5106, 0.4892, 0.5118, 0.4866, 0.4856, 0.4938, 0.4893, 0.4942, 0.4988,
        0.5072, 0.5026, 0.4994, 0.5022, 0.4912, 0.5042, 0.4915, 0.4857, 0.4989,
        0.5081, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4893, 0.5110, 0.4852, 0.4856, 0.4923, 0.4881, 0.4952, 0.4981,
        0.5051, 0.5033, 0.4994, 0.5033, 0.4912, 0.5025, 0.4892, 0.4860, 0.4987,
        0.5073, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5106, 0.4906, 0.5108, 0.4871, 0.4852, 0.4938, 0.4887, 0.4964, 0.5016,
        0.5067, 0.5049, 0.5001, 0.5061, 0.4917, 0.5037, 0.4913, 0.4876, 0.4979,
        0.5070, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4893, 0.5114, 0.4858, 0.4874, 0.4927, 0.4893, 0.4954, 0.4986,
        0.5086, 0.5039, 0.4994, 0.5049, 0.4909, 0.5035, 0.4908, 0.4863, 0.4987,
        0.5061, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4863, 0.4871, 0.4925, 0.4907, 0.4924, 0.4957,
        0.5064, 0.5041, 0.4993, 0.5032, 0.4913, 0.5034, 0.4896, 0.4858, 0.4964,
        0.5056, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4917, 0.5100, 0.4866, 0.4863, 0.4945, 0.4908, 0.4937, 0.4968,
        0.5069, 0.5061, 0.4999, 0.5035, 0.4889, 0.5036, 0.4905, 0.4850, 0.4973,
        0.5064, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4910, 0.5126, 0.4888, 0.4856, 0.4934, 0.4887, 0.4942, 0.4975,
        0.5070, 0.5062, 0.4993, 0.5049, 0.4899, 0.5032, 0.4891, 0.4845, 0.4993,
        0.5073, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4909, 0.5108, 0.4853, 0.4862, 0.4928, 0.4891, 0.4952, 0.5001,
        0.5072, 0.5064, 0.5008, 0.5035, 0.4894, 0.5046, 0.4914, 0.4860, 0.4962,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5106, 0.4922, 0.5107, 0.4853, 0.4880, 0.4930, 0.4898, 0.4939, 0.4976,
        0.5075, 0.5045, 0.4978, 0.5039, 0.4899, 0.5043, 0.4903, 0.4870, 0.4968,
        0.5082, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4914, 0.5116, 0.4875, 0.4874, 0.4928, 0.4892, 0.4934, 0.4999,
        0.5051, 0.5049, 0.4986, 0.5034, 0.4888, 0.5003, 0.4930, 0.4837, 0.4980,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5128, 0.4962, 0.4963, 0.5025, 0.5067, 0.4925, 0.5089, 0.4951, 0.4970,
         0.5026, 0.4856, 0.4973, 0.4839, 0.5026, 0.5026, 0.4985, 0.4992, 0.5139,
         0.4935, 0.4967],
        [0.5116, 0.4960, 0.4951, 0.5046, 0.5033, 0.4929, 0.5102, 0.4966, 0.4960,
         0.5026, 0.4848, 0.4965, 0.4862, 0.5012, 0.5002, 0.4978, 0.4981, 0.5146,
         0.4949, 0.4972],
        [0.5131, 0.4977, 0.4970, 0.5022, 0.5046, 0.4924, 0.5100, 0.4980, 0.4966,
         0.5028, 0.4868, 0.4990, 0.4850, 0.5012, 0.4998, 0.4974, 0.4990, 0.5166,
         0.4949, 0.4973],
        [0.5126, 0.4974, 0.4965, 0.5033, 0.5040, 0.4914, 0.5109, 0.4967, 0.4964,
         0.5027, 0.4844, 0.4962, 0.4848, 0.5022, 0.5009, 0.4972, 0.5009, 0.5142,
         0.4954, 0.4969],
        [0.5125, 0.4984, 0.4960, 0.5037, 0.5032, 0.4935, 0.5087, 0.4953, 0.4972,
         0.5032, 0.4878, 0.4981, 0.4828, 0.5024, 0.5002, 0.4990, 0.5000, 0.5152,
         0.4956, 0.4975],
        [0.5137, 0.4972, 0.4956, 0.5030, 0.5055, 0.4927, 0.5077, 0.4969, 0.4956,
         0.5025, 0.4856, 0.4961, 0.4856, 0.5031, 0.5020, 0.5001, 0.4992, 0.5145,
         0.4951, 0.4981],
        [0.5112, 0.4965, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4963, 0.4962,
         0.5027, 0.4885, 0.4971, 0.4850, 0.5026, 0.5009, 0.4987, 0.4997, 0.5137,
         0.4952, 0.4995],
        [0.5122, 0.4972, 0.4949, 0.5015, 0.5039, 0.4923, 0.5085, 0.4946, 0.4970,
         0.5025, 0.4858, 0.4985, 0.4864, 0.5010, 0.5008, 0.4983, 0.4994, 0.5153,
         0.4958, 0.4982],
        [0.5128, 0.4980, 0.4952, 0.5015, 0.5045, 0.4924, 0.5097, 0.4967, 0.4967,
         0.5020, 0.4885, 0.4950, 0.4850, 0.5018, 0.5011, 0.4971, 0.5006, 0.5139,
         0.4957, 0.4990],
        [0.5125, 0.4985, 0.4940, 0.5030, 0.5044, 0.4938, 0.5082, 0.4988, 0.4964,
         0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5006, 0.4981, 0.4992, 0.5156,
         0.4970, 0.5000]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5128, 0.4962, 0.4963, 0.5025, 0.5067, 0.4925, 0.5089, 0.4951, 0.4970,
        0.5026, 0.4856, 0.4973, 0.4839, 0.5026, 0.5026, 0.4985, 0.4992, 0.5139,
        0.4935, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4960, 0.4951, 0.5046, 0.5033, 0.4929, 0.5102, 0.4966, 0.4960,
        0.5026, 0.4848, 0.4965, 0.4862, 0.5012, 0.5002, 0.4978, 0.4981, 0.5146,
        0.4949, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4977, 0.4970, 0.5022, 0.5046, 0.4924, 0.5100, 0.4980, 0.4966,
        0.5028, 0.4868, 0.4990, 0.4850, 0.5012, 0.4998, 0.4974, 0.4990, 0.5166,
        0.4949, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4965, 0.5033, 0.5040, 0.4914, 0.5109, 0.4967, 0.4964,
        0.5027, 0.4844, 0.4962, 0.4848, 0.5022, 0.5009, 0.4972, 0.5009, 0.5142,
        0.4954, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4984, 0.4960, 0.5037, 0.5032, 0.4935, 0.5087, 0.4953, 0.4972,
        0.5032, 0.4878, 0.4981, 0.4828, 0.5024, 0.5002, 0.4990, 0.5000, 0.5152,
        0.4956, 0.4975], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4972, 0.4956, 0.5030, 0.5055, 0.4927, 0.5077, 0.4969, 0.4956,
        0.5025, 0.4856, 0.4961, 0.4856, 0.5031, 0.5020, 0.5001, 0.4992, 0.5145,
        0.4951, 0.4981], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4965, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4963, 0.4962,
        0.5027, 0.4885, 0.4971, 0.4850, 0.5026, 0.5009, 0.4987, 0.4997, 0.5137,
        0.4952, 0.4995], grad_fn=<UnbindBackward>), tensor([0.5122, 0.4972, 0.4949, 0.5015, 0.5039, 0.4923, 0.5085, 0.4946, 0.4970,
        0.5025, 0.4858, 0.4985, 0.4864, 0.5010, 0.5008, 0.4983, 0.4994, 0.5153,
        0.4958, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4980, 0.4952, 0.5015, 0.5045, 0.4924, 0.5097, 0.4967, 0.4967,
        0.5020, 0.4885, 0.4950, 0.4850, 0.5018, 0.5011, 0.4971, 0.5006, 0.5139,
        0.4957, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4985, 0.4940, 0.5030, 0.5044, 0.4938, 0.5082, 0.4988, 0.4964,
        0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5006, 0.4981, 0.4992, 0.5156,
        0.4970, 0.5000], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [0, 1, 7, 8, 4, 3, 2, 5, 6, 9]
replay_buffer._size: [3300 3300 3300 3300 3300 3300 3300 3300 3300 3300]
2023-08-12 10:40:31,618 MainThread INFO: EPOCH:20
2023-08-12 10:40:31,618 MainThread INFO: Time Consumed:0.33384156227111816s
2023-08-12 10:40:31,619 MainThread INFO: Total Frames:31500s
 26%|‚ñà‚ñà‚ñã       | 21/80 [00:25<00:43,  1.34it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1207.57887
Train_Epoch_Reward                    41879.55788
Running_Training_Average_Rewards      2671.96258
Explore_Time                          0.00339
Train___Time                          0.32608
Eval____Time                          0.00251
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.41974
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.82917
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.78802
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.56746
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.81818
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.70015
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.77241
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12322.81542
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.91906
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.21256
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.66496     0.62176    11.84263    10.09888
alpha_0                               0.96954      0.00041    0.97012     0.96895
alpha_1                               0.96953      0.00041    0.97012     0.96895
alpha_2                               0.96956      0.00041    0.97015     0.96898
alpha_3                               0.96954      0.00041    0.97012     0.96895
alpha_4                               0.96954      0.00041    0.97013     0.96896
alpha_5                               0.96952      0.00041    0.97011     0.96894
alpha_6                               0.96953      0.00041    0.97012     0.96895
alpha_7                               0.96953      0.00041    0.97011     0.96895
alpha_8                               0.96954      0.00041    0.97012     0.96895
alpha_9                               0.96954      0.00041    0.97012     0.96895
Alpha_loss                            -0.20604     0.00299    -0.20184    -0.21030
Training/policy_loss                  -2.69824     0.00397    -2.69343    -2.70485
Training/qf1_loss                     2239.39326   423.55155  3072.27368  1910.37964
Training/qf2_loss                     2239.16094   423.55515  3072.04810  1910.12793
Training/pf_norm                      0.22240      0.02875    0.26951     0.18222
Training/qf1_norm                     28.27421     1.29871    30.71744    27.13457
Training/qf2_norm                     28.89075     1.26570    31.24892    27.89376
log_std/mean                          -0.07887     0.00127    -0.07709    -0.08065
log_std/std                           0.00648      0.00011    0.00664     0.00631
log_std/max                           -0.06688     0.00092    -0.06562    -0.06814
log_std/min                           -0.09276     0.00149    -0.09057    -0.09459
log_probs/mean                        -2.72547     0.00422    -2.72035    -2.73249
log_probs/std                         0.28639      0.00303    0.29068     0.28272
log_probs/max                         -1.87262     0.04526    -1.80702    -1.93026
log_probs/min                         -4.40907     0.29108    -4.06842    -4.93006
mean/mean                             -0.00125     0.00016    -0.00100    -0.00143
mean/std                              0.00215      0.00008    0.00222     0.00202
mean/max                              0.00324      0.00008    0.00336     0.00311
mean/min                              -0.00390     0.00005    -0.00381    -0.00395
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [3, 5, 9, 7, 8, 4, 2, 0, 6, 1]
replay_buffer._size: [3450 3450 3450 3450 3450 3450 3450 3450 3450 3450]
2023-08-12 10:40:32,001 MainThread INFO: EPOCH:21
2023-08-12 10:40:32,002 MainThread INFO: Time Consumed:0.2977721691131592s
2023-08-12 10:40:32,002 MainThread INFO: Total Frames:33000s
 28%|‚ñà‚ñà‚ñä       | 22/80 [00:25<00:36,  1.57it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1207.41971
Train_Epoch_Reward                    10924.38738
Running_Training_Average_Rewards      2355.60101
Explore_Time                          0.00240
Train___Time                          0.29201
Eval____Time                          0.00288
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.16765
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.59468
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.87149
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.44210
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.79733
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.68643
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.75899
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12324.45446
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.66763
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.27106
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.52473     1.27556    11.93552    8.62961
alpha_0                               0.96808      0.00041    0.96866     0.96750
alpha_1                               0.96808      0.00041    0.96866     0.96749
alpha_2                               0.96811      0.00041    0.96869     0.96753
alpha_3                               0.96808      0.00041    0.96866     0.96750
alpha_4                               0.96809      0.00041    0.96867     0.96750
alpha_5                               0.96807      0.00041    0.96865     0.96749
alpha_6                               0.96807      0.00041    0.96866     0.96749
alpha_7                               0.96807      0.00041    0.96866     0.96749
alpha_8                               0.96808      0.00041    0.96866     0.96750
alpha_9                               0.96808      0.00041    0.96866     0.96750
Alpha_loss                            -0.21606     0.00289    -0.21183    -0.22015
Training/policy_loss                  -2.69505     0.00355    -2.68954    -2.70005
Training/qf1_loss                     2269.70564   622.07128  2942.22095  1419.76184
Training/qf2_loss                     2269.43669   622.04144  2941.88916  1419.51965
Training/pf_norm                      0.23864      0.01980    0.27011     0.21839
Training/qf1_norm                     28.12604     2.71634    31.07636    24.04814
Training/qf2_norm                     28.92681     2.83344    32.05900    24.80393
log_std/mean                          -0.08330     0.00126    -0.08155    -0.08510
log_std/std                           0.00687      0.00011    0.00703     0.00671
log_std/max                           -0.07026     0.00099    -0.06882    -0.07173
log_std/min                           -0.09834     0.00170    -0.09637    -0.10119
log_probs/mean                        -2.72243     0.00368    -2.71668    -2.72762
log_probs/std                         0.28653      0.00852    0.30124     0.27770
log_probs/max                         -1.88417     0.05106    -1.84215    -1.97569
log_probs/min                         -4.82767     1.03716    -3.66538    -6.73142
mean/mean                             -0.00057     0.00022    -0.00025    -0.00088
mean/std                              0.00197      0.00002    0.00200     0.00195
mean/max                              0.00314      0.00016    0.00342     0.00300
mean/min                              -0.00367     0.00019    -0.00337    -0.00389
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5063, 0.4989, 0.5061, 0.5072, 0.4884, 0.4957, 0.5019, 0.4998,
         0.5024, 0.5100, 0.5012, 0.4960, 0.4889, 0.4930, 0.5076, 0.4926, 0.5091,
         0.4964, 0.5142],
        [0.4983, 0.5069, 0.4990, 0.5061, 0.5067, 0.4895, 0.4960, 0.5024, 0.5023,
         0.5027, 0.5083, 0.4994, 0.4962, 0.4882, 0.4908, 0.5076, 0.4920, 0.5086,
         0.4964, 0.5128],
        [0.4979, 0.5069, 0.4980, 0.5064, 0.5066, 0.4884, 0.4966, 0.5002, 0.5044,
         0.5010, 0.5060, 0.4993, 0.4964, 0.4883, 0.4905, 0.5101, 0.4937, 0.5084,
         0.4952, 0.5151],
        [0.4987, 0.5055, 0.4986, 0.5067, 0.5062, 0.4895, 0.4949, 0.4996, 0.5028,
         0.5037, 0.5059, 0.4985, 0.4972, 0.4877, 0.4895, 0.5080, 0.4924, 0.5099,
         0.4928, 0.5143],
        [0.4987, 0.5075, 0.4979, 0.5049, 0.5057, 0.4885, 0.4961, 0.4989, 0.5021,
         0.5025, 0.5086, 0.4998, 0.4969, 0.4883, 0.4921, 0.5061, 0.4944, 0.5097,
         0.4954, 0.5132],
        [0.4977, 0.5043, 0.4976, 0.5060, 0.5054, 0.4883, 0.4947, 0.4999, 0.5010,
         0.5030, 0.5070, 0.4999, 0.4952, 0.4880, 0.4903, 0.5088, 0.4954, 0.5079,
         0.4948, 0.5137],
        [0.4980, 0.5055, 0.4986, 0.5043, 0.5062, 0.4875, 0.4976, 0.5006, 0.5036,
         0.5020, 0.5103, 0.4995, 0.4961, 0.4868, 0.4907, 0.5080, 0.4956, 0.5075,
         0.4967, 0.5126],
        [0.4980, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4954, 0.5010, 0.5031,
         0.5020, 0.5095, 0.4997, 0.4961, 0.4889, 0.4920, 0.5079, 0.4935, 0.5062,
         0.4933, 0.5129],
        [0.4981, 0.5049, 0.4972, 0.5052, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
         0.5009, 0.5063, 0.5014, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5091,
         0.4940, 0.5146],
        [0.4984, 0.5058, 0.4993, 0.5051, 0.5072, 0.4882, 0.4960, 0.5015, 0.5021,
         0.5038, 0.5076, 0.5003, 0.4948, 0.4902, 0.4927, 0.5091, 0.4940, 0.5054,
         0.4952, 0.5152]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5063, 0.4989, 0.5061, 0.5072, 0.4884, 0.4957, 0.5019, 0.4998,
        0.5024, 0.5100, 0.5012, 0.4960, 0.4889, 0.4930, 0.5076, 0.4926, 0.5091,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5069, 0.4990, 0.5061, 0.5067, 0.4895, 0.4960, 0.5024, 0.5023,
        0.5027, 0.5083, 0.4994, 0.4962, 0.4882, 0.4908, 0.5076, 0.4920, 0.5086,
        0.4964, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5069, 0.4980, 0.5064, 0.5066, 0.4884, 0.4966, 0.5002, 0.5044,
        0.5010, 0.5060, 0.4993, 0.4964, 0.4883, 0.4905, 0.5101, 0.4937, 0.5084,
        0.4952, 0.5151], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5055, 0.4986, 0.5067, 0.5062, 0.4895, 0.4949, 0.4996, 0.5028,
        0.5037, 0.5059, 0.4985, 0.4972, 0.4877, 0.4895, 0.5080, 0.4924, 0.5099,
        0.4928, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5075, 0.4979, 0.5049, 0.5057, 0.4885, 0.4961, 0.4989, 0.5021,
        0.5025, 0.5086, 0.4998, 0.4969, 0.4883, 0.4921, 0.5061, 0.4944, 0.5097,
        0.4954, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5043, 0.4976, 0.5060, 0.5054, 0.4883, 0.4947, 0.4999, 0.5010,
        0.5030, 0.5070, 0.4999, 0.4952, 0.4880, 0.4903, 0.5088, 0.4954, 0.5079,
        0.4948, 0.5137], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5055, 0.4986, 0.5043, 0.5062, 0.4875, 0.4976, 0.5006, 0.5036,
        0.5020, 0.5103, 0.4995, 0.4961, 0.4868, 0.4907, 0.5080, 0.4956, 0.5075,
        0.4967, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5045, 0.4985, 0.5050, 0.5065, 0.4879, 0.4954, 0.5010, 0.5031,
        0.5020, 0.5095, 0.4997, 0.4961, 0.4889, 0.4920, 0.5079, 0.4935, 0.5062,
        0.4933, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5049, 0.4972, 0.5052, 0.5078, 0.4881, 0.4962, 0.4998, 0.5019,
        0.5009, 0.5063, 0.5014, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5091,
        0.4940, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4993, 0.5051, 0.5072, 0.4882, 0.4960, 0.5015, 0.5021,
        0.5038, 0.5076, 0.5003, 0.4948, 0.4902, 0.4927, 0.5091, 0.4940, 0.5054,
        0.4952, 0.5152], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5106, 0.4891, 0.5118, 0.4866, 0.4856, 0.4938, 0.4893, 0.4942, 0.4988,
         0.5073, 0.5025, 0.4995, 0.5023, 0.4912, 0.5041, 0.4915, 0.4857, 0.4988,
         0.5081, 0.4950],
        [0.5087, 0.4892, 0.5109, 0.4853, 0.4857, 0.4924, 0.4881, 0.4952, 0.4981,
         0.5051, 0.5033, 0.4995, 0.5033, 0.4913, 0.5025, 0.4892, 0.4861, 0.4987,
         0.5073, 0.4936],
        [0.5106, 0.4905, 0.5108, 0.4871, 0.4853, 0.4938, 0.4887, 0.4964, 0.5016,
         0.5067, 0.5048, 0.5001, 0.5061, 0.4918, 0.5037, 0.4913, 0.4876, 0.4979,
         0.5069, 0.4926],
        [0.5078, 0.4893, 0.5114, 0.4859, 0.4874, 0.4927, 0.4893, 0.4954, 0.4986,
         0.5086, 0.5038, 0.4995, 0.5050, 0.4910, 0.5035, 0.4907, 0.4863, 0.4987,
         0.5061, 0.4923],
        [0.5095, 0.4912, 0.5129, 0.4863, 0.4871, 0.4925, 0.4907, 0.4924, 0.4957,
         0.5064, 0.5040, 0.4994, 0.5032, 0.4914, 0.5035, 0.4895, 0.4858, 0.4964,
         0.5055, 0.4936],
        [0.5114, 0.4917, 0.5100, 0.4866, 0.4863, 0.4944, 0.4908, 0.4937, 0.4969,
         0.5069, 0.5060, 0.4999, 0.5035, 0.4889, 0.5037, 0.4904, 0.4850, 0.4973,
         0.5064, 0.4933],
        [0.5095, 0.4910, 0.5126, 0.4887, 0.4855, 0.4934, 0.4888, 0.4942, 0.4975,
         0.5070, 0.5062, 0.4993, 0.5049, 0.4900, 0.5032, 0.4891, 0.4845, 0.4992,
         0.5073, 0.4933],
        [0.5101, 0.4909, 0.5108, 0.4853, 0.4862, 0.4928, 0.4891, 0.4952, 0.5000,
         0.5072, 0.5064, 0.5008, 0.5035, 0.4894, 0.5046, 0.4914, 0.4860, 0.4961,
         0.5077, 0.4931],
        [0.5106, 0.4922, 0.5108, 0.4853, 0.4880, 0.4929, 0.4897, 0.4940, 0.4976,
         0.5075, 0.5046, 0.4978, 0.5039, 0.4898, 0.5043, 0.4903, 0.4869, 0.4968,
         0.5082, 0.4936],
        [0.5112, 0.4915, 0.5116, 0.4874, 0.4875, 0.4928, 0.4892, 0.4933, 0.4999,
         0.5050, 0.5049, 0.4986, 0.5035, 0.4888, 0.5003, 0.4930, 0.4837, 0.4979,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5106, 0.4891, 0.5118, 0.4866, 0.4856, 0.4938, 0.4893, 0.4942, 0.4988,
        0.5073, 0.5025, 0.4995, 0.5023, 0.4912, 0.5041, 0.4915, 0.4857, 0.4988,
        0.5081, 0.4950], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4892, 0.5109, 0.4853, 0.4857, 0.4924, 0.4881, 0.4952, 0.4981,
        0.5051, 0.5033, 0.4995, 0.5033, 0.4913, 0.5025, 0.4892, 0.4861, 0.4987,
        0.5073, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5106, 0.4905, 0.5108, 0.4871, 0.4853, 0.4938, 0.4887, 0.4964, 0.5016,
        0.5067, 0.5048, 0.5001, 0.5061, 0.4918, 0.5037, 0.4913, 0.4876, 0.4979,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4893, 0.5114, 0.4859, 0.4874, 0.4927, 0.4893, 0.4954, 0.4986,
        0.5086, 0.5038, 0.4995, 0.5050, 0.4910, 0.5035, 0.4907, 0.4863, 0.4987,
        0.5061, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5129, 0.4863, 0.4871, 0.4925, 0.4907, 0.4924, 0.4957,
        0.5064, 0.5040, 0.4994, 0.5032, 0.4914, 0.5035, 0.4895, 0.4858, 0.4964,
        0.5055, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4917, 0.5100, 0.4866, 0.4863, 0.4944, 0.4908, 0.4937, 0.4969,
        0.5069, 0.5060, 0.4999, 0.5035, 0.4889, 0.5037, 0.4904, 0.4850, 0.4973,
        0.5064, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4910, 0.5126, 0.4887, 0.4855, 0.4934, 0.4888, 0.4942, 0.4975,
        0.5070, 0.5062, 0.4993, 0.5049, 0.4900, 0.5032, 0.4891, 0.4845, 0.4992,
        0.5073, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4909, 0.5108, 0.4853, 0.4862, 0.4928, 0.4891, 0.4952, 0.5000,
        0.5072, 0.5064, 0.5008, 0.5035, 0.4894, 0.5046, 0.4914, 0.4860, 0.4961,
        0.5077, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5106, 0.4922, 0.5108, 0.4853, 0.4880, 0.4929, 0.4897, 0.4940, 0.4976,
        0.5075, 0.5046, 0.4978, 0.5039, 0.4898, 0.5043, 0.4903, 0.4869, 0.4968,
        0.5082, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4915, 0.5116, 0.4874, 0.4875, 0.4928, 0.4892, 0.4933, 0.4999,
        0.5050, 0.5049, 0.4986, 0.5035, 0.4888, 0.5003, 0.4930, 0.4837, 0.4979,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5128, 0.4963, 0.4963, 0.5025, 0.5067, 0.4924, 0.5089, 0.4951, 0.4970,
         0.5026, 0.4856, 0.4973, 0.4839, 0.5027, 0.5026, 0.4985, 0.4992, 0.5140,
         0.4936, 0.4967],
        [0.5116, 0.4960, 0.4951, 0.5045, 0.5034, 0.4930, 0.5102, 0.4966, 0.4960,
         0.5026, 0.4848, 0.4965, 0.4862, 0.5013, 0.5002, 0.4978, 0.4980, 0.5147,
         0.4949, 0.4972],
        [0.5132, 0.4977, 0.4971, 0.5022, 0.5046, 0.4923, 0.5100, 0.4980, 0.4966,
         0.5028, 0.4867, 0.4990, 0.4850, 0.5013, 0.4998, 0.4975, 0.4990, 0.5167,
         0.4949, 0.4973],
        [0.5126, 0.4974, 0.4965, 0.5032, 0.5040, 0.4914, 0.5108, 0.4966, 0.4964,
         0.5028, 0.4843, 0.4962, 0.4848, 0.5022, 0.5009, 0.4973, 0.5009, 0.5142,
         0.4954, 0.4969],
        [0.5125, 0.4984, 0.4960, 0.5037, 0.5031, 0.4936, 0.5087, 0.4953, 0.4972,
         0.5032, 0.4877, 0.4981, 0.4828, 0.5024, 0.5002, 0.4990, 0.4999, 0.5152,
         0.4956, 0.4974],
        [0.5137, 0.4972, 0.4956, 0.5030, 0.5054, 0.4927, 0.5077, 0.4969, 0.4956,
         0.5025, 0.4856, 0.4961, 0.4856, 0.5031, 0.5020, 0.5000, 0.4992, 0.5145,
         0.4951, 0.4981],
        [0.5112, 0.4965, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4963, 0.4962,
         0.5027, 0.4884, 0.4971, 0.4850, 0.5026, 0.5009, 0.4987, 0.4998, 0.5137,
         0.4953, 0.4995],
        [0.5122, 0.4972, 0.4950, 0.5015, 0.5039, 0.4923, 0.5085, 0.4946, 0.4970,
         0.5025, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4983, 0.4994, 0.5154,
         0.4959, 0.4983],
        [0.5127, 0.4980, 0.4952, 0.5014, 0.5046, 0.4924, 0.5098, 0.4967, 0.4967,
         0.5020, 0.4885, 0.4950, 0.4850, 0.5018, 0.5012, 0.4971, 0.5006, 0.5139,
         0.4956, 0.4990],
        [0.5125, 0.4985, 0.4940, 0.5030, 0.5045, 0.4938, 0.5082, 0.4988, 0.4964,
         0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5007, 0.4981, 0.4992, 0.5156,
         0.4970, 0.5001]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5128, 0.4963, 0.4963, 0.5025, 0.5067, 0.4924, 0.5089, 0.4951, 0.4970,
        0.5026, 0.4856, 0.4973, 0.4839, 0.5027, 0.5026, 0.4985, 0.4992, 0.5140,
        0.4936, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4960, 0.4951, 0.5045, 0.5034, 0.4930, 0.5102, 0.4966, 0.4960,
        0.5026, 0.4848, 0.4965, 0.4862, 0.5013, 0.5002, 0.4978, 0.4980, 0.5147,
        0.4949, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5132, 0.4977, 0.4971, 0.5022, 0.5046, 0.4923, 0.5100, 0.4980, 0.4966,
        0.5028, 0.4867, 0.4990, 0.4850, 0.5013, 0.4998, 0.4975, 0.4990, 0.5167,
        0.4949, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4965, 0.5032, 0.5040, 0.4914, 0.5108, 0.4966, 0.4964,
        0.5028, 0.4843, 0.4962, 0.4848, 0.5022, 0.5009, 0.4973, 0.5009, 0.5142,
        0.4954, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4984, 0.4960, 0.5037, 0.5031, 0.4936, 0.5087, 0.4953, 0.4972,
        0.5032, 0.4877, 0.4981, 0.4828, 0.5024, 0.5002, 0.4990, 0.4999, 0.5152,
        0.4956, 0.4974], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4972, 0.4956, 0.5030, 0.5054, 0.4927, 0.5077, 0.4969, 0.4956,
        0.5025, 0.4856, 0.4961, 0.4856, 0.5031, 0.5020, 0.5000, 0.4992, 0.5145,
        0.4951, 0.4981], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4965, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4963, 0.4962,
        0.5027, 0.4884, 0.4971, 0.4850, 0.5026, 0.5009, 0.4987, 0.4998, 0.5137,
        0.4953, 0.4995], grad_fn=<UnbindBackward>), tensor([0.5122, 0.4972, 0.4950, 0.5015, 0.5039, 0.4923, 0.5085, 0.4946, 0.4970,
        0.5025, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4983, 0.4994, 0.5154,
        0.4959, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4980, 0.4952, 0.5014, 0.5046, 0.4924, 0.5098, 0.4967, 0.4967,
        0.5020, 0.4885, 0.4950, 0.4850, 0.5018, 0.5012, 0.4971, 0.5006, 0.5139,
        0.4956, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4985, 0.4940, 0.5030, 0.5045, 0.4938, 0.5082, 0.4988, 0.4964,
        0.5035, 0.4886, 0.4983, 0.4857, 0.5000, 0.5007, 0.4981, 0.4992, 0.5156,
        0.4970, 0.5001], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [2, 0, 4, 1, 9, 3, 6, 5, 7, 8]
replay_buffer._size: [3600 3600 3600 3600 3600 3600 3600 3600 3600 3600]
2023-08-12 10:40:32,780 MainThread INFO: EPOCH:22
2023-08-12 10:40:32,780 MainThread INFO: Time Consumed:0.3736691474914551s
2023-08-12 10:40:32,780 MainThread INFO: Total Frames:34500s
 29%|‚ñà‚ñà‚ñâ       | 23/80 [00:26<00:38,  1.47it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1192.33962
Train_Epoch_Reward                    2866.30001
Running_Training_Average_Rewards      1855.67484
Explore_Time                          0.00319
Train___Time                          0.36640
Eval____Time                          0.00264
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.74082
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.44403
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.84886
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.45603
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.78739
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.67838
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.75088
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12170.05041
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.50968
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.43818
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.31743     0.49923    12.00831    10.50846
alpha_0                               0.96662      0.00041    0.96721     0.96604
alpha_1                               0.96662      0.00041    0.96720     0.96604
alpha_2                               0.96665      0.00041    0.96723     0.96607
alpha_3                               0.96662      0.00041    0.96721     0.96604
alpha_4                               0.96663      0.00041    0.96721     0.96605
alpha_5                               0.96661      0.00041    0.96720     0.96603
alpha_6                               0.96662      0.00041    0.96720     0.96604
alpha_7                               0.96662      0.00041    0.96720     0.96604
alpha_8                               0.96662      0.00041    0.96720     0.96604
alpha_9                               0.96663      0.00041    0.96721     0.96604
Alpha_loss                            -0.22635     0.00287    -0.22221    -0.23054
Training/policy_loss                  -2.70073     0.00513    -2.69393    -2.70812
Training/qf1_loss                     2529.58540   341.79564  2945.97510  2154.70752
Training/qf2_loss                     2529.29634   341.77702  2945.66162  2154.40796
Training/pf_norm                      0.21857      0.02750    0.26132     0.19428
Training/qf1_norm                     29.95412     1.11928    31.47761    28.11267
Training/qf2_norm                     30.78432     1.21839    32.38950    28.74848
log_std/mean                          -0.08782     0.00126    -0.08602    -0.08961
log_std/std                           0.00731      0.00012    0.00746     0.00711
log_std/max                           -0.07388     0.00093    -0.07277    -0.07537
log_std/min                           -0.10414     0.00136    -0.10224    -0.10563
log_probs/mean                        -2.72773     0.00525    -2.72060    -2.73529
log_probs/std                         0.26764      0.00253    0.27114     0.26471
log_probs/max                         -1.92252     0.01145    -1.90395    -1.93522
log_probs/min                         -4.26915     0.22689    -4.08185    -4.71584
mean/mean                             0.00059      0.00044    0.00122     -0.00002
mean/std                              0.00201      0.00003    0.00205     0.00197
mean/max                              0.00407      0.00042    0.00476     0.00353
mean/min                              -0.00232     0.00058    -0.00147    -0.00309
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [6, 7, 5, 4, 2, 9, 1, 8, 3, 0]
replay_buffer._size: [3750 3750 3750 3750 3750 3750 3750 3750 3750 3750]
2023-08-12 10:40:33,196 MainThread INFO: EPOCH:23
2023-08-12 10:40:33,196 MainThread INFO: Time Consumed:0.3251821994781494s
2023-08-12 10:40:33,196 MainThread INFO: Total Frames:36000s
 30%|‚ñà‚ñà‚ñà       | 24/80 [00:26<00:33,  1.65it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1189.39677
Train_Epoch_Reward                    20242.56585
Running_Training_Average_Rewards      1134.44177
Explore_Time                          0.00234
Train___Time                          0.32014
Eval____Time                          0.00228
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.67657
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.27445
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -17.94164
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.43478
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.89837
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.76434
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.83427
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12139.65328
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.33009
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.53105
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.72599     1.09395    12.20194    9.12084
alpha_0                               0.96517      0.00041    0.96575     0.96459
alpha_1                               0.96516      0.00041    0.96574     0.96458
alpha_2                               0.96520      0.00041    0.96578     0.96462
alpha_3                               0.96517      0.00041    0.96575     0.96459
alpha_4                               0.96517      0.00041    0.96576     0.96459
alpha_5                               0.96516      0.00041    0.96574     0.96458
alpha_6                               0.96516      0.00041    0.96575     0.96458
alpha_7                               0.96517      0.00041    0.96575     0.96459
alpha_8                               0.96517      0.00041    0.96575     0.96459
alpha_9                               0.96517      0.00041    0.96575     0.96459
Alpha_loss                            -0.23658     0.00286    -0.23252    -0.24054
Training/policy_loss                  -2.70394     0.00169    -2.70155    -2.70669
Training/qf1_loss                     2214.31885   610.44046  3195.58838  1365.89941
Training/qf2_loss                     2214.04509   610.43272  3195.29810  1365.64075
Training/pf_norm                      0.20457      0.01396    0.22249     0.18434
Training/qf1_norm                     28.79249     2.35467    31.90463    25.30324
Training/qf2_norm                     29.52711     2.33553    32.65866    26.05001
log_std/mean                          -0.09242     0.00128    -0.09061    -0.09423
log_std/std                           0.00772      0.00011    0.00788     0.00757
log_std/max                           -0.07765     0.00113    -0.07595    -0.07930
log_std/min                           -0.10966     0.00143    -0.10783    -0.11145
log_probs/mean                        -2.73084     0.00176    -2.72852    -2.73372
log_probs/std                         0.27144      0.01027    0.29083     0.26283
log_probs/max                         -1.91185     0.04068    -1.84997    -1.97824
log_probs/min                         -4.60638     0.29779    -4.19032    -5.03648
mean/mean                             0.00220      0.00042    0.00275     0.00156
mean/std                              0.00212      0.00008    0.00225     0.00202
mean/max                              0.00581      0.00034    0.00623     0.00523
mean/min                              -0.00122     0.00005    -0.00116    -0.00129
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5063, 0.4989, 0.5062, 0.5072, 0.4883, 0.4957, 0.5019, 0.4998,
         0.5024, 0.5100, 0.5011, 0.4960, 0.4889, 0.4929, 0.5076, 0.4927, 0.5090,
         0.4964, 0.5142],
        [0.4983, 0.5070, 0.4989, 0.5061, 0.5067, 0.4895, 0.4960, 0.5025, 0.5023,
         0.5028, 0.5083, 0.4993, 0.4962, 0.4882, 0.4907, 0.5076, 0.4920, 0.5086,
         0.4964, 0.5128],
        [0.4979, 0.5069, 0.4980, 0.5064, 0.5066, 0.4884, 0.4966, 0.5002, 0.5044,
         0.5010, 0.5061, 0.4992, 0.4965, 0.4883, 0.4904, 0.5101, 0.4937, 0.5084,
         0.4952, 0.5151],
        [0.4987, 0.5056, 0.4986, 0.5068, 0.5062, 0.4894, 0.4948, 0.4996, 0.5027,
         0.5037, 0.5059, 0.4984, 0.4973, 0.4877, 0.4894, 0.5080, 0.4924, 0.5099,
         0.4928, 0.5143],
        [0.4987, 0.5076, 0.4979, 0.5049, 0.5056, 0.4885, 0.4960, 0.4989, 0.5021,
         0.5024, 0.5087, 0.4997, 0.4969, 0.4883, 0.4920, 0.5061, 0.4944, 0.5097,
         0.4954, 0.5132],
        [0.4977, 0.5044, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.4999, 0.5010,
         0.5030, 0.5070, 0.4999, 0.4953, 0.4880, 0.4902, 0.5088, 0.4953, 0.5079,
         0.4947, 0.5136],
        [0.4981, 0.5055, 0.4985, 0.5044, 0.5061, 0.4875, 0.4975, 0.5006, 0.5036,
         0.5019, 0.5103, 0.4996, 0.4961, 0.4869, 0.4906, 0.5080, 0.4956, 0.5076,
         0.4968, 0.5126],
        [0.4980, 0.5045, 0.4984, 0.5050, 0.5066, 0.4879, 0.4954, 0.5010, 0.5031,
         0.5020, 0.5095, 0.4997, 0.4962, 0.4889, 0.4919, 0.5079, 0.4935, 0.5061,
         0.4933, 0.5129],
        [0.4981, 0.5049, 0.4972, 0.5052, 0.5079, 0.4881, 0.4962, 0.4998, 0.5019,
         0.5009, 0.5063, 0.5013, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5091,
         0.4940, 0.5146],
        [0.4984, 0.5058, 0.4994, 0.5050, 0.5072, 0.4883, 0.4961, 0.5015, 0.5020,
         0.5038, 0.5076, 0.5003, 0.4947, 0.4902, 0.4927, 0.5091, 0.4940, 0.5054,
         0.4953, 0.5152]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5063, 0.4989, 0.5062, 0.5072, 0.4883, 0.4957, 0.5019, 0.4998,
        0.5024, 0.5100, 0.5011, 0.4960, 0.4889, 0.4929, 0.5076, 0.4927, 0.5090,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5070, 0.4989, 0.5061, 0.5067, 0.4895, 0.4960, 0.5025, 0.5023,
        0.5028, 0.5083, 0.4993, 0.4962, 0.4882, 0.4907, 0.5076, 0.4920, 0.5086,
        0.4964, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5069, 0.4980, 0.5064, 0.5066, 0.4884, 0.4966, 0.5002, 0.5044,
        0.5010, 0.5061, 0.4992, 0.4965, 0.4883, 0.4904, 0.5101, 0.4937, 0.5084,
        0.4952, 0.5151], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5056, 0.4986, 0.5068, 0.5062, 0.4894, 0.4948, 0.4996, 0.5027,
        0.5037, 0.5059, 0.4984, 0.4973, 0.4877, 0.4894, 0.5080, 0.4924, 0.5099,
        0.4928, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5076, 0.4979, 0.5049, 0.5056, 0.4885, 0.4960, 0.4989, 0.5021,
        0.5024, 0.5087, 0.4997, 0.4969, 0.4883, 0.4920, 0.5061, 0.4944, 0.5097,
        0.4954, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5044, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.4999, 0.5010,
        0.5030, 0.5070, 0.4999, 0.4953, 0.4880, 0.4902, 0.5088, 0.4953, 0.5079,
        0.4947, 0.5136], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5055, 0.4985, 0.5044, 0.5061, 0.4875, 0.4975, 0.5006, 0.5036,
        0.5019, 0.5103, 0.4996, 0.4961, 0.4869, 0.4906, 0.5080, 0.4956, 0.5076,
        0.4968, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5045, 0.4984, 0.5050, 0.5066, 0.4879, 0.4954, 0.5010, 0.5031,
        0.5020, 0.5095, 0.4997, 0.4962, 0.4889, 0.4919, 0.5079, 0.4935, 0.5061,
        0.4933, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5049, 0.4972, 0.5052, 0.5079, 0.4881, 0.4962, 0.4998, 0.5019,
        0.5009, 0.5063, 0.5013, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5091,
        0.4940, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4994, 0.5050, 0.5072, 0.4883, 0.4961, 0.5015, 0.5020,
        0.5038, 0.5076, 0.5003, 0.4947, 0.4902, 0.4927, 0.5091, 0.4940, 0.5054,
        0.4953, 0.5152], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5105, 0.4892, 0.5119, 0.4866, 0.4855, 0.4938, 0.4893, 0.4942, 0.4988,
         0.5073, 0.5026, 0.4995, 0.5022, 0.4912, 0.5043, 0.4915, 0.4857, 0.4988,
         0.5081, 0.4949],
        [0.5087, 0.4893, 0.5109, 0.4852, 0.4856, 0.4924, 0.4882, 0.4952, 0.4981,
         0.5051, 0.5034, 0.4995, 0.5033, 0.4913, 0.5026, 0.4892, 0.4861, 0.4987,
         0.5073, 0.4935],
        [0.5105, 0.4906, 0.5107, 0.4871, 0.4852, 0.4939, 0.4888, 0.4963, 0.5016,
         0.5067, 0.5049, 0.5002, 0.5061, 0.4917, 0.5038, 0.4913, 0.4876, 0.4979,
         0.5069, 0.4926],
        [0.5078, 0.4893, 0.5114, 0.4859, 0.4873, 0.4928, 0.4893, 0.4953, 0.4986,
         0.5086, 0.5039, 0.4995, 0.5050, 0.4910, 0.5035, 0.4908, 0.4863, 0.4987,
         0.5061, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4863, 0.4871, 0.4925, 0.4907, 0.4924, 0.4957,
         0.5064, 0.5041, 0.4994, 0.5032, 0.4914, 0.5035, 0.4895, 0.4857, 0.4964,
         0.5055, 0.4937],
        [0.5113, 0.4916, 0.5100, 0.4866, 0.4864, 0.4945, 0.4907, 0.4937, 0.4969,
         0.5069, 0.5061, 0.5000, 0.5036, 0.4890, 0.5037, 0.4903, 0.4850, 0.4973,
         0.5064, 0.4933],
        [0.5094, 0.4910, 0.5125, 0.4886, 0.4856, 0.4934, 0.4887, 0.4942, 0.4975,
         0.5070, 0.5061, 0.4993, 0.5049, 0.4900, 0.5033, 0.4891, 0.4846, 0.4992,
         0.5073, 0.4933],
        [0.5101, 0.4910, 0.5109, 0.4853, 0.4862, 0.4928, 0.4892, 0.4951, 0.5000,
         0.5072, 0.5065, 0.5008, 0.5035, 0.4894, 0.5047, 0.4914, 0.4859, 0.4961,
         0.5076, 0.4931],
        [0.5107, 0.4923, 0.5108, 0.4852, 0.4881, 0.4930, 0.4898, 0.4940, 0.4976,
         0.5075, 0.5047, 0.4977, 0.5039, 0.4898, 0.5043, 0.4904, 0.4870, 0.4968,
         0.5082, 0.4936],
        [0.5113, 0.4915, 0.5117, 0.4874, 0.4875, 0.4928, 0.4892, 0.4934, 0.4999,
         0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5002, 0.4932, 0.4837, 0.4979,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5105, 0.4892, 0.5119, 0.4866, 0.4855, 0.4938, 0.4893, 0.4942, 0.4988,
        0.5073, 0.5026, 0.4995, 0.5022, 0.4912, 0.5043, 0.4915, 0.4857, 0.4988,
        0.5081, 0.4949], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4893, 0.5109, 0.4852, 0.4856, 0.4924, 0.4882, 0.4952, 0.4981,
        0.5051, 0.5034, 0.4995, 0.5033, 0.4913, 0.5026, 0.4892, 0.4861, 0.4987,
        0.5073, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5105, 0.4906, 0.5107, 0.4871, 0.4852, 0.4939, 0.4888, 0.4963, 0.5016,
        0.5067, 0.5049, 0.5002, 0.5061, 0.4917, 0.5038, 0.4913, 0.4876, 0.4979,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4893, 0.5114, 0.4859, 0.4873, 0.4928, 0.4893, 0.4953, 0.4986,
        0.5086, 0.5039, 0.4995, 0.5050, 0.4910, 0.5035, 0.4908, 0.4863, 0.4987,
        0.5061, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4863, 0.4871, 0.4925, 0.4907, 0.4924, 0.4957,
        0.5064, 0.5041, 0.4994, 0.5032, 0.4914, 0.5035, 0.4895, 0.4857, 0.4964,
        0.5055, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4916, 0.5100, 0.4866, 0.4864, 0.4945, 0.4907, 0.4937, 0.4969,
        0.5069, 0.5061, 0.5000, 0.5036, 0.4890, 0.5037, 0.4903, 0.4850, 0.4973,
        0.5064, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4910, 0.5125, 0.4886, 0.4856, 0.4934, 0.4887, 0.4942, 0.4975,
        0.5070, 0.5061, 0.4993, 0.5049, 0.4900, 0.5033, 0.4891, 0.4846, 0.4992,
        0.5073, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4910, 0.5109, 0.4853, 0.4862, 0.4928, 0.4892, 0.4951, 0.5000,
        0.5072, 0.5065, 0.5008, 0.5035, 0.4894, 0.5047, 0.4914, 0.4859, 0.4961,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4923, 0.5108, 0.4852, 0.4881, 0.4930, 0.4898, 0.4940, 0.4976,
        0.5075, 0.5047, 0.4977, 0.5039, 0.4898, 0.5043, 0.4904, 0.4870, 0.4968,
        0.5082, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4915, 0.5117, 0.4874, 0.4875, 0.4928, 0.4892, 0.4934, 0.4999,
        0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5002, 0.4932, 0.4837, 0.4979,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5127, 0.4962, 0.4963, 0.5025, 0.5067, 0.4925, 0.5090, 0.4951, 0.4970,
         0.5026, 0.4856, 0.4973, 0.4840, 0.5026, 0.5026, 0.4984, 0.4992, 0.5140,
         0.4935, 0.4967],
        [0.5115, 0.4959, 0.4952, 0.5046, 0.5034, 0.4929, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4965, 0.4861, 0.5013, 0.5002, 0.4978, 0.4981, 0.5146,
         0.4948, 0.4972],
        [0.5131, 0.4976, 0.4971, 0.5023, 0.5047, 0.4923, 0.5101, 0.4979, 0.4965,
         0.5028, 0.4867, 0.4989, 0.4849, 0.5013, 0.4999, 0.4975, 0.4991, 0.5166,
         0.4949, 0.4973],
        [0.5126, 0.4974, 0.4966, 0.5033, 0.5040, 0.4914, 0.5109, 0.4966, 0.4964,
         0.5028, 0.4844, 0.4962, 0.4847, 0.5022, 0.5009, 0.4973, 0.5009, 0.5142,
         0.4954, 0.4969],
        [0.5126, 0.4984, 0.4960, 0.5037, 0.5032, 0.4936, 0.5087, 0.4953, 0.4972,
         0.5032, 0.4877, 0.4981, 0.4827, 0.5024, 0.5002, 0.4991, 0.4999, 0.5152,
         0.4956, 0.4974],
        [0.5137, 0.4972, 0.4956, 0.5030, 0.5054, 0.4927, 0.5078, 0.4969, 0.4957,
         0.5025, 0.4856, 0.4961, 0.4855, 0.5031, 0.5021, 0.5000, 0.4992, 0.5145,
         0.4951, 0.4981],
        [0.5113, 0.4965, 0.4979, 0.5025, 0.5038, 0.4924, 0.5088, 0.4963, 0.4962,
         0.5028, 0.4884, 0.4972, 0.4850, 0.5026, 0.5009, 0.4987, 0.4997, 0.5136,
         0.4951, 0.4995],
        [0.5121, 0.4971, 0.4950, 0.5015, 0.5039, 0.4922, 0.5085, 0.4945, 0.4969,
         0.5025, 0.4858, 0.4985, 0.4864, 0.5011, 0.5008, 0.4983, 0.4995, 0.5153,
         0.4958, 0.4983],
        [0.5128, 0.4980, 0.4952, 0.5015, 0.5046, 0.4924, 0.5098, 0.4966, 0.4967,
         0.5020, 0.4885, 0.4949, 0.4850, 0.5018, 0.5012, 0.4971, 0.5006, 0.5139,
         0.4957, 0.4991],
        [0.5125, 0.4984, 0.4940, 0.5030, 0.5045, 0.4938, 0.5081, 0.4988, 0.4964,
         0.5035, 0.4886, 0.4983, 0.4857, 0.5001, 0.5006, 0.4982, 0.4992, 0.5156,
         0.4970, 0.5001]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5127, 0.4962, 0.4963, 0.5025, 0.5067, 0.4925, 0.5090, 0.4951, 0.4970,
        0.5026, 0.4856, 0.4973, 0.4840, 0.5026, 0.5026, 0.4984, 0.4992, 0.5140,
        0.4935, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4959, 0.4952, 0.5046, 0.5034, 0.4929, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4965, 0.4861, 0.5013, 0.5002, 0.4978, 0.4981, 0.5146,
        0.4948, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4976, 0.4971, 0.5023, 0.5047, 0.4923, 0.5101, 0.4979, 0.4965,
        0.5028, 0.4867, 0.4989, 0.4849, 0.5013, 0.4999, 0.4975, 0.4991, 0.5166,
        0.4949, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4966, 0.5033, 0.5040, 0.4914, 0.5109, 0.4966, 0.4964,
        0.5028, 0.4844, 0.4962, 0.4847, 0.5022, 0.5009, 0.4973, 0.5009, 0.5142,
        0.4954, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4984, 0.4960, 0.5037, 0.5032, 0.4936, 0.5087, 0.4953, 0.4972,
        0.5032, 0.4877, 0.4981, 0.4827, 0.5024, 0.5002, 0.4991, 0.4999, 0.5152,
        0.4956, 0.4974], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4972, 0.4956, 0.5030, 0.5054, 0.4927, 0.5078, 0.4969, 0.4957,
        0.5025, 0.4856, 0.4961, 0.4855, 0.5031, 0.5021, 0.5000, 0.4992, 0.5145,
        0.4951, 0.4981], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4965, 0.4979, 0.5025, 0.5038, 0.4924, 0.5088, 0.4963, 0.4962,
        0.5028, 0.4884, 0.4972, 0.4850, 0.5026, 0.5009, 0.4987, 0.4997, 0.5136,
        0.4951, 0.4995], grad_fn=<UnbindBackward>), tensor([0.5121, 0.4971, 0.4950, 0.5015, 0.5039, 0.4922, 0.5085, 0.4945, 0.4969,
        0.5025, 0.4858, 0.4985, 0.4864, 0.5011, 0.5008, 0.4983, 0.4995, 0.5153,
        0.4958, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4980, 0.4952, 0.5015, 0.5046, 0.4924, 0.5098, 0.4966, 0.4967,
        0.5020, 0.4885, 0.4949, 0.4850, 0.5018, 0.5012, 0.4971, 0.5006, 0.5139,
        0.4957, 0.4991], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4984, 0.4940, 0.5030, 0.5045, 0.4938, 0.5081, 0.4988, 0.4964,
        0.5035, 0.4886, 0.4983, 0.4857, 0.5001, 0.5006, 0.4982, 0.4992, 0.5156,
        0.4970, 0.5001], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [1, 5, 6, 2, 0, 4, 3, 7, 8, 9]
replay_buffer._size: [3900 3900 3900 3900 3900 3900 3900 3900 3900 3900]
2023-08-12 10:40:34,004 MainThread INFO: EPOCH:24
2023-08-12 10:40:34,004 MainThread INFO: Time Consumed:0.38988423347473145s
2023-08-12 10:40:34,004 MainThread INFO: Total Frames:37500s
 31%|‚ñà‚ñà‚ñà‚ñè      | 25/80 [00:27<00:36,  1.50it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1201.43911
Train_Epoch_Reward                    31744.08600
Running_Training_Average_Rewards      1828.43173
Explore_Time                          0.00407
Train___Time                          0.38100
Eval____Time                          0.00321
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.01467
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.02511
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.20438
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.35555
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.18065
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.98180
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.04656
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12260.82521
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.06731
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.55803
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.71710     1.43565    12.78030    8.60109
alpha_0                               0.96372      0.00041    0.96430     0.96314
alpha_1                               0.96371      0.00041    0.96429     0.96313
alpha_2                               0.96375      0.00041    0.96433     0.96317
alpha_3                               0.96372      0.00041    0.96430     0.96315
alpha_4                               0.96372      0.00041    0.96430     0.96314
alpha_5                               0.96371      0.00041    0.96429     0.96313
alpha_6                               0.96371      0.00041    0.96429     0.96314
alpha_7                               0.96372      0.00041    0.96430     0.96314
alpha_8                               0.96372      0.00041    0.96430     0.96314
alpha_9                               0.96372      0.00041    0.96430     0.96315
Alpha_loss                            -0.24650     0.00290    -0.24237    -0.25047
Training/policy_loss                  -2.69896     0.00371    -2.69576    -2.70593
Training/qf1_loss                     2335.83203   609.38501  3169.57983  1491.30176
Training/qf2_loss                     2335.51113   609.35388  3169.24194  1491.04541
Training/pf_norm                      0.20721      0.03596    0.25379     0.15652
Training/qf1_norm                     28.92855     3.10008    33.32180    24.39004
Training/qf2_norm                     29.88086     3.19889    34.25601    25.08940
log_std/mean                          -0.09695     0.00131    -0.09507    -0.09879
log_std/std                           0.00809      0.00013    0.00829     0.00792
log_std/max                           -0.08137     0.00103    -0.08004    -0.08300
log_std/min                           -0.11513     0.00192    -0.11204    -0.11782
log_probs/mean                        -2.72537     0.00376    -2.72211    -2.73245
log_probs/std                         0.26503      0.00816    0.27878     0.25346
log_probs/max                         -1.95627     0.04412    -1.91220    -2.03693
log_probs/min                         -4.72118     0.45160    -4.21487    -5.50269
mean/mean                             0.00336      0.00025    0.00365     0.00296
mean/std                              0.00249      0.00009    0.00257     0.00234
mean/max                              0.00696      0.00029    0.00725     0.00647
mean/min                              -0.00146     0.00007    -0.00135    -0.00155
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [6, 8, 5, 3, 1, 9, 0, 2, 7, 4]
replay_buffer._size: [4050 4050 4050 4050 4050 4050 4050 4050 4050 4050]
2023-08-12 10:40:34,468 MainThread INFO: EPOCH:25
2023-08-12 10:40:34,468 MainThread INFO: Time Consumed:0.3603231906890869s
2023-08-12 10:40:34,468 MainThread INFO: Total Frames:39000s
 32%|‚ñà‚ñà‚ñà‚ñé      | 26/80 [00:28<00:32,  1.65it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1208.15705
Train_Epoch_Reward                    7576.08661
Running_Training_Average_Rewards      1985.42462
Explore_Time                          0.00251
Train___Time                          0.35544
Eval____Time                          0.00193
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -56.03576
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.79496
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.46069
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.34002
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.51854
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.23786
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.29635
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12336.72008
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -25.83203
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.63340
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.04921     1.22569    11.76735    8.73211
alpha_0                               0.96227      0.00041    0.96285     0.96169
alpha_1                               0.96226      0.00041    0.96284     0.96168
alpha_2                               0.96230      0.00041    0.96288     0.96172
alpha_3                               0.96228      0.00041    0.96286     0.96170
alpha_4                               0.96227      0.00041    0.96285     0.96169
alpha_5                               0.96226      0.00041    0.96284     0.96168
alpha_6                               0.96227      0.00041    0.96285     0.96169
alpha_7                               0.96227      0.00041    0.96285     0.96169
alpha_8                               0.96227      0.00041    0.96285     0.96169
alpha_9                               0.96228      0.00041    0.96286     0.96170
Alpha_loss                            -0.25693     0.00291    -0.25252    -0.26090
Training/policy_loss                  -2.70717     0.00581    -2.69762    -2.71358
Training/qf1_loss                     2053.53318   544.12360  2707.19922  1444.34998
Training/qf2_loss                     2053.20823   544.10300  2706.87183  1444.02173
Training/pf_norm                      0.16082      0.02915    0.20723     0.12649
Training/qf1_norm                     27.60420     2.67479    31.32209    24.78162
Training/qf2_norm                     28.57587     2.71849    32.21161    25.64262
log_std/mean                          -0.10141     0.00118    -0.09973    -0.10308
log_std/std                           0.00846      0.00009    0.00860     0.00833
log_std/max                           -0.08504     0.00104    -0.08355    -0.08650
log_std/min                           -0.12006     0.00148    -0.11807    -0.12168
log_probs/mean                        -2.73343     0.00590    -2.72389    -2.74000
log_probs/std                         0.25947      0.00698    0.27016     0.25000
log_probs/max                         -1.99585     0.03460    -1.94082    -2.03597
log_probs/min                         -4.49059     0.41571    -3.88055    -5.04832
mean/mean                             0.00415      0.00029    0.00460     0.00377
mean/std                              0.00275      0.00011    0.00290     0.00258
mean/max                              0.00786      0.00038    0.00841     0.00728
mean/min                              -0.00138     0.00014    -0.00114    -0.00152
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5063, 0.4990, 0.5062, 0.5072, 0.4883, 0.4957, 0.5019, 0.4998,
         0.5024, 0.5100, 0.5011, 0.4960, 0.4889, 0.4929, 0.5076, 0.4926, 0.5091,
         0.4964, 0.5142],
        [0.4983, 0.5070, 0.4989, 0.5061, 0.5067, 0.4896, 0.4960, 0.5025, 0.5023,
         0.5027, 0.5083, 0.4993, 0.4963, 0.4882, 0.4907, 0.5077, 0.4919, 0.5086,
         0.4964, 0.5128],
        [0.4979, 0.5069, 0.4980, 0.5064, 0.5066, 0.4884, 0.4966, 0.5003, 0.5044,
         0.5010, 0.5061, 0.4992, 0.4964, 0.4883, 0.4904, 0.5101, 0.4937, 0.5085,
         0.4953, 0.5150],
        [0.4987, 0.5056, 0.4986, 0.5068, 0.5063, 0.4895, 0.4948, 0.4997, 0.5027,
         0.5037, 0.5060, 0.4985, 0.4973, 0.4877, 0.4894, 0.5080, 0.4924, 0.5099,
         0.4929, 0.5143],
        [0.4987, 0.5075, 0.4980, 0.5049, 0.5056, 0.4885, 0.4960, 0.4989, 0.5020,
         0.5025, 0.5087, 0.4997, 0.4969, 0.4882, 0.4920, 0.5061, 0.4944, 0.5097,
         0.4954, 0.5132],
        [0.4976, 0.5044, 0.4977, 0.5061, 0.5054, 0.4883, 0.4947, 0.4999, 0.5010,
         0.5030, 0.5071, 0.4999, 0.4953, 0.4880, 0.4902, 0.5088, 0.4954, 0.5079,
         0.4949, 0.5136],
        [0.4979, 0.5056, 0.4986, 0.5043, 0.5061, 0.4875, 0.4975, 0.5006, 0.5036,
         0.5019, 0.5104, 0.4995, 0.4961, 0.4868, 0.4907, 0.5081, 0.4956, 0.5075,
         0.4968, 0.5127],
        [0.4980, 0.5045, 0.4984, 0.5050, 0.5065, 0.4879, 0.4954, 0.5010, 0.5031,
         0.5020, 0.5095, 0.4997, 0.4961, 0.4889, 0.4920, 0.5079, 0.4935, 0.5061,
         0.4933, 0.5128],
        [0.4982, 0.5049, 0.4972, 0.5051, 0.5078, 0.4881, 0.4962, 0.4999, 0.5019,
         0.5008, 0.5063, 0.5013, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5090,
         0.4940, 0.5145],
        [0.4984, 0.5058, 0.4994, 0.5050, 0.5072, 0.4882, 0.4960, 0.5015, 0.5020,
         0.5038, 0.5076, 0.5002, 0.4947, 0.4903, 0.4927, 0.5091, 0.4940, 0.5053,
         0.4952, 0.5152]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5063, 0.4990, 0.5062, 0.5072, 0.4883, 0.4957, 0.5019, 0.4998,
        0.5024, 0.5100, 0.5011, 0.4960, 0.4889, 0.4929, 0.5076, 0.4926, 0.5091,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5070, 0.4989, 0.5061, 0.5067, 0.4896, 0.4960, 0.5025, 0.5023,
        0.5027, 0.5083, 0.4993, 0.4963, 0.4882, 0.4907, 0.5077, 0.4919, 0.5086,
        0.4964, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5069, 0.4980, 0.5064, 0.5066, 0.4884, 0.4966, 0.5003, 0.5044,
        0.5010, 0.5061, 0.4992, 0.4964, 0.4883, 0.4904, 0.5101, 0.4937, 0.5085,
        0.4953, 0.5150], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5056, 0.4986, 0.5068, 0.5063, 0.4895, 0.4948, 0.4997, 0.5027,
        0.5037, 0.5060, 0.4985, 0.4973, 0.4877, 0.4894, 0.5080, 0.4924, 0.5099,
        0.4929, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5075, 0.4980, 0.5049, 0.5056, 0.4885, 0.4960, 0.4989, 0.5020,
        0.5025, 0.5087, 0.4997, 0.4969, 0.4882, 0.4920, 0.5061, 0.4944, 0.5097,
        0.4954, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5044, 0.4977, 0.5061, 0.5054, 0.4883, 0.4947, 0.4999, 0.5010,
        0.5030, 0.5071, 0.4999, 0.4953, 0.4880, 0.4902, 0.5088, 0.4954, 0.5079,
        0.4949, 0.5136], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5056, 0.4986, 0.5043, 0.5061, 0.4875, 0.4975, 0.5006, 0.5036,
        0.5019, 0.5104, 0.4995, 0.4961, 0.4868, 0.4907, 0.5081, 0.4956, 0.5075,
        0.4968, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5045, 0.4984, 0.5050, 0.5065, 0.4879, 0.4954, 0.5010, 0.5031,
        0.5020, 0.5095, 0.4997, 0.4961, 0.4889, 0.4920, 0.5079, 0.4935, 0.5061,
        0.4933, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5049, 0.4972, 0.5051, 0.5078, 0.4881, 0.4962, 0.4999, 0.5019,
        0.5008, 0.5063, 0.5013, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5090,
        0.4940, 0.5145], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4994, 0.5050, 0.5072, 0.4882, 0.4960, 0.5015, 0.5020,
        0.5038, 0.5076, 0.5002, 0.4947, 0.4903, 0.4927, 0.5091, 0.4940, 0.5053,
        0.4952, 0.5152], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5105, 0.4892, 0.5120, 0.4865, 0.4855, 0.4938, 0.4893, 0.4943, 0.4987,
         0.5073, 0.5025, 0.4996, 0.5023, 0.4913, 0.5043, 0.4914, 0.4857, 0.4988,
         0.5081, 0.4948],
        [0.5087, 0.4893, 0.5109, 0.4852, 0.4856, 0.4924, 0.4882, 0.4952, 0.4980,
         0.5052, 0.5033, 0.4995, 0.5033, 0.4913, 0.5027, 0.4890, 0.4861, 0.4987,
         0.5073, 0.4935],
        [0.5105, 0.4905, 0.5108, 0.4870, 0.4852, 0.4939, 0.4887, 0.4964, 0.5015,
         0.5068, 0.5048, 0.5002, 0.5061, 0.4918, 0.5038, 0.4912, 0.4876, 0.4980,
         0.5069, 0.4926],
        [0.5079, 0.4893, 0.5115, 0.4859, 0.4873, 0.4928, 0.4894, 0.4954, 0.4986,
         0.5086, 0.5039, 0.4995, 0.5050, 0.4910, 0.5036, 0.4908, 0.4863, 0.4987,
         0.5061, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4863, 0.4871, 0.4925, 0.4908, 0.4924, 0.4956,
         0.5064, 0.5041, 0.4994, 0.5032, 0.4914, 0.5035, 0.4895, 0.4857, 0.4963,
         0.5055, 0.4936],
        [0.5113, 0.4917, 0.5100, 0.4866, 0.4863, 0.4944, 0.4908, 0.4937, 0.4969,
         0.5069, 0.5061, 0.5000, 0.5036, 0.4890, 0.5037, 0.4904, 0.4850, 0.4972,
         0.5063, 0.4933],
        [0.5094, 0.4911, 0.5125, 0.4886, 0.4855, 0.4934, 0.4888, 0.4942, 0.4976,
         0.5070, 0.5061, 0.4993, 0.5049, 0.4901, 0.5033, 0.4890, 0.4846, 0.4991,
         0.5072, 0.4932],
        [0.5101, 0.4910, 0.5109, 0.4853, 0.4862, 0.4928, 0.4891, 0.4951, 0.5000,
         0.5072, 0.5065, 0.5008, 0.5035, 0.4894, 0.5047, 0.4913, 0.4859, 0.4961,
         0.5076, 0.4931],
        [0.5107, 0.4923, 0.5108, 0.4853, 0.4881, 0.4929, 0.4896, 0.4939, 0.4977,
         0.5076, 0.5046, 0.4978, 0.5040, 0.4898, 0.5043, 0.4902, 0.4869, 0.4968,
         0.5082, 0.4936],
        [0.5113, 0.4916, 0.5116, 0.4874, 0.4875, 0.4928, 0.4892, 0.4933, 0.4999,
         0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5003, 0.4931, 0.4837, 0.4978,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5105, 0.4892, 0.5120, 0.4865, 0.4855, 0.4938, 0.4893, 0.4943, 0.4987,
        0.5073, 0.5025, 0.4996, 0.5023, 0.4913, 0.5043, 0.4914, 0.4857, 0.4988,
        0.5081, 0.4948], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4893, 0.5109, 0.4852, 0.4856, 0.4924, 0.4882, 0.4952, 0.4980,
        0.5052, 0.5033, 0.4995, 0.5033, 0.4913, 0.5027, 0.4890, 0.4861, 0.4987,
        0.5073, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5105, 0.4905, 0.5108, 0.4870, 0.4852, 0.4939, 0.4887, 0.4964, 0.5015,
        0.5068, 0.5048, 0.5002, 0.5061, 0.4918, 0.5038, 0.4912, 0.4876, 0.4980,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4893, 0.5115, 0.4859, 0.4873, 0.4928, 0.4894, 0.4954, 0.4986,
        0.5086, 0.5039, 0.4995, 0.5050, 0.4910, 0.5036, 0.4908, 0.4863, 0.4987,
        0.5061, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4863, 0.4871, 0.4925, 0.4908, 0.4924, 0.4956,
        0.5064, 0.5041, 0.4994, 0.5032, 0.4914, 0.5035, 0.4895, 0.4857, 0.4963,
        0.5055, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4917, 0.5100, 0.4866, 0.4863, 0.4944, 0.4908, 0.4937, 0.4969,
        0.5069, 0.5061, 0.5000, 0.5036, 0.4890, 0.5037, 0.4904, 0.4850, 0.4972,
        0.5063, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4911, 0.5125, 0.4886, 0.4855, 0.4934, 0.4888, 0.4942, 0.4976,
        0.5070, 0.5061, 0.4993, 0.5049, 0.4901, 0.5033, 0.4890, 0.4846, 0.4991,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4910, 0.5109, 0.4853, 0.4862, 0.4928, 0.4891, 0.4951, 0.5000,
        0.5072, 0.5065, 0.5008, 0.5035, 0.4894, 0.5047, 0.4913, 0.4859, 0.4961,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4923, 0.5108, 0.4853, 0.4881, 0.4929, 0.4896, 0.4939, 0.4977,
        0.5076, 0.5046, 0.4978, 0.5040, 0.4898, 0.5043, 0.4902, 0.4869, 0.4968,
        0.5082, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4916, 0.5116, 0.4874, 0.4875, 0.4928, 0.4892, 0.4933, 0.4999,
        0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5003, 0.4931, 0.4837, 0.4978,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5127, 0.4963, 0.4963, 0.5025, 0.5067, 0.4926, 0.5090, 0.4953, 0.4970,
         0.5026, 0.4856, 0.4974, 0.4840, 0.5025, 0.5024, 0.4985, 0.4992, 0.5140,
         0.4935, 0.4967],
        [0.5115, 0.4960, 0.4952, 0.5045, 0.5034, 0.4930, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4965, 0.4861, 0.5013, 0.5002, 0.4978, 0.4980, 0.5146,
         0.4948, 0.4972],
        [0.5131, 0.4976, 0.4971, 0.5022, 0.5046, 0.4923, 0.5100, 0.4980, 0.4965,
         0.5028, 0.4866, 0.4990, 0.4849, 0.5013, 0.4998, 0.4975, 0.4990, 0.5166,
         0.4948, 0.4972],
        [0.5126, 0.4973, 0.4966, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4964,
         0.5027, 0.4843, 0.4963, 0.4847, 0.5022, 0.5009, 0.4973, 0.5010, 0.5143,
         0.4955, 0.4969],
        [0.5125, 0.4983, 0.4961, 0.5037, 0.5032, 0.4936, 0.5087, 0.4953, 0.4971,
         0.5032, 0.4876, 0.4981, 0.4827, 0.5025, 0.5002, 0.4991, 0.5000, 0.5152,
         0.4956, 0.4974],
        [0.5137, 0.4971, 0.4957, 0.5030, 0.5054, 0.4927, 0.5078, 0.4969, 0.4957,
         0.5025, 0.4856, 0.4961, 0.4855, 0.5031, 0.5020, 0.5001, 0.4993, 0.5145,
         0.4951, 0.4982],
        [0.5112, 0.4964, 0.4979, 0.5025, 0.5038, 0.4924, 0.5088, 0.4964, 0.4962,
         0.5028, 0.4883, 0.4971, 0.4850, 0.5026, 0.5009, 0.4987, 0.4999, 0.5137,
         0.4952, 0.4996],
        [0.5121, 0.4972, 0.4950, 0.5014, 0.5039, 0.4923, 0.5085, 0.4945, 0.4970,
         0.5024, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4984, 0.4995, 0.5153,
         0.4958, 0.4983],
        [0.5127, 0.4981, 0.4952, 0.5013, 0.5046, 0.4925, 0.5098, 0.4966, 0.4968,
         0.5020, 0.4885, 0.4949, 0.4850, 0.5018, 0.5012, 0.4971, 0.5006, 0.5138,
         0.4956, 0.4991],
        [0.5125, 0.4984, 0.4940, 0.5030, 0.5046, 0.4938, 0.5082, 0.4988, 0.4964,
         0.5034, 0.4886, 0.4982, 0.4857, 0.5001, 0.5008, 0.4981, 0.4992, 0.5156,
         0.4970, 0.5001]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5127, 0.4963, 0.4963, 0.5025, 0.5067, 0.4926, 0.5090, 0.4953, 0.4970,
        0.5026, 0.4856, 0.4974, 0.4840, 0.5025, 0.5024, 0.4985, 0.4992, 0.5140,
        0.4935, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4960, 0.4952, 0.5045, 0.5034, 0.4930, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4965, 0.4861, 0.5013, 0.5002, 0.4978, 0.4980, 0.5146,
        0.4948, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4976, 0.4971, 0.5022, 0.5046, 0.4923, 0.5100, 0.4980, 0.4965,
        0.5028, 0.4866, 0.4990, 0.4849, 0.5013, 0.4998, 0.4975, 0.4990, 0.5166,
        0.4948, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4973, 0.4966, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4964,
        0.5027, 0.4843, 0.4963, 0.4847, 0.5022, 0.5009, 0.4973, 0.5010, 0.5143,
        0.4955, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4983, 0.4961, 0.5037, 0.5032, 0.4936, 0.5087, 0.4953, 0.4971,
        0.5032, 0.4876, 0.4981, 0.4827, 0.5025, 0.5002, 0.4991, 0.5000, 0.5152,
        0.4956, 0.4974], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4971, 0.4957, 0.5030, 0.5054, 0.4927, 0.5078, 0.4969, 0.4957,
        0.5025, 0.4856, 0.4961, 0.4855, 0.5031, 0.5020, 0.5001, 0.4993, 0.5145,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4964, 0.4979, 0.5025, 0.5038, 0.4924, 0.5088, 0.4964, 0.4962,
        0.5028, 0.4883, 0.4971, 0.4850, 0.5026, 0.5009, 0.4987, 0.4999, 0.5137,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5121, 0.4972, 0.4950, 0.5014, 0.5039, 0.4923, 0.5085, 0.4945, 0.4970,
        0.5024, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4984, 0.4995, 0.5153,
        0.4958, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4981, 0.4952, 0.5013, 0.5046, 0.4925, 0.5098, 0.4966, 0.4968,
        0.5020, 0.4885, 0.4949, 0.4850, 0.5018, 0.5012, 0.4971, 0.5006, 0.5138,
        0.4956, 0.4991], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4984, 0.4940, 0.5030, 0.5046, 0.4938, 0.5082, 0.4988, 0.4964,
        0.5034, 0.4886, 0.4982, 0.4857, 0.5001, 0.5008, 0.4981, 0.4992, 0.5156,
        0.4970, 0.5001], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [0, 9, 3, 7, 2, 6, 1, 8, 4, 5]
replay_buffer._size: [4200 4200 4200 4200 4200 4200 4200 4200 4200 4200]
2023-08-12 10:40:35,321 MainThread INFO: EPOCH:26
2023-08-12 10:40:35,321 MainThread INFO: Time Consumed:0.3808422088623047s
2023-08-12 10:40:35,321 MainThread INFO: Total Frames:40500s
 34%|‚ñà‚ñà‚ñà‚ñç      | 27/80 [00:28<00:36,  1.47it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1223.90202
Train_Epoch_Reward                    7691.44520
Running_Training_Average_Rewards      1567.05393
Explore_Time                          0.00314
Train___Time                          0.37414
Eval____Time                          0.00223
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -53.74633
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.75837
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.67999
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.37162
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.85919
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.49821
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.55129
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12492.87706
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -25.79770
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.59415
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.01263     0.66868    11.85255    10.00158
alpha_0                               0.96082      0.00041    0.96140     0.96024
alpha_1                               0.96081      0.00041    0.96139     0.96023
alpha_2                               0.96085      0.00041    0.96143     0.96028
alpha_3                               0.96083      0.00041    0.96141     0.96025
alpha_4                               0.96082      0.00041    0.96140     0.96025
alpha_5                               0.96082      0.00041    0.96139     0.96024
alpha_6                               0.96082      0.00041    0.96140     0.96024
alpha_7                               0.96082      0.00041    0.96140     0.96025
alpha_8                               0.96082      0.00041    0.96140     0.96025
alpha_9                               0.96083      0.00041    0.96141     0.96025
Alpha_loss                            -0.26701     0.00283    -0.26292    -0.27098
Training/policy_loss                  -2.70678     0.00199    -2.70506    -2.71046
Training/qf1_loss                     2312.59734   246.08201  2567.01514  1941.06702
Training/qf2_loss                     2312.19675   246.07024  2566.57007  1940.65955
Training/pf_norm                      0.17076      0.02640    0.21049     0.13351
Training/qf1_norm                     29.88849     1.51221    31.95074    27.78616
Training/qf2_norm                     31.16474     1.60969    33.37984    29.17159
log_std/mean                          -0.10553     0.00116    -0.10387    -0.10716
log_std/std                           0.00877      0.00010    0.00889     0.00861
log_std/max                           -0.08854     0.00101    -0.08720    -0.08998
log_std/min                           -0.12523     0.00144    -0.12298    -0.12732
log_probs/mean                        -2.73237     0.00208    -2.73038    -2.73629
log_probs/std                         0.25717      0.01133    0.27863     0.24690
log_probs/max                         -2.02017     0.02147    -1.98564    -2.05277
log_probs/min                         -4.82369     0.66089    -4.28328    -5.92105
mean/mean                             0.00531      0.00030    0.00568     0.00483
mean/std                              0.00338      0.00029    0.00384     0.00301
mean/max                              0.01079      0.00104    0.01222     0.00925
mean/min                              -0.00095     0.00009    -0.00081    -0.00109
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [5, 0, 9, 6, 3, 4, 2, 1, 7, 8]
replay_buffer._size: [4350 4350 4350 4350 4350 4350 4350 4350 4350 4350]
2023-08-12 10:40:35,810 MainThread INFO: EPOCH:27
2023-08-12 10:40:35,810 MainThread INFO: Time Consumed:0.37917304039001465s
2023-08-12 10:40:35,810 MainThread INFO: Total Frames:42000s
 35%|‚ñà‚ñà‚ñà‚ñå      | 28/80 [00:29<00:32,  1.60it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1244.07643
Train_Epoch_Reward                    7002.57571
Running_Training_Average_Rewards      742.33692
Explore_Time                          0.00266
Train___Time                          0.37327
Eval____Time                          0.00262
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -45.78034
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.71283
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.93454
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.38830
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.22410
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.77802
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.82524
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12687.69738
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -25.75822
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.53148
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.57217     0.69843    11.88827    9.91140
alpha_0                               0.95938      0.00041    0.95995     0.95880
alpha_1                               0.95937      0.00041    0.95995     0.95879
alpha_2                               0.95941      0.00041    0.95999     0.95883
alpha_3                               0.95939      0.00041    0.95996     0.95881
alpha_4                               0.95938      0.00041    0.95996     0.95880
alpha_5                               0.95937      0.00041    0.95995     0.95880
alpha_6                               0.95937      0.00041    0.95995     0.95880
alpha_7                               0.95938      0.00041    0.95996     0.95880
alpha_8                               0.95938      0.00041    0.95996     0.95880
alpha_9                               0.95939      0.00041    0.95997     0.95881
Alpha_loss                            -0.27675     0.00281    -0.27287    -0.28074
Training/policy_loss                  -2.69856     0.00262    -2.69416    -2.70185
Training/qf1_loss                     2146.97070   307.08220  2724.03320  1833.69202
Training/qf2_loss                     2146.58413   307.06086  2723.61670  1833.36450
Training/pf_norm                      0.18453      0.02396    0.21684     0.14465
Training/qf1_norm                     28.99990     1.51617    31.84669    27.54311
Training/qf2_norm                     30.17834     1.58913    33.11055    28.43323
log_std/mean                          -0.10947     0.00115    -0.10786    -0.11112
log_std/std                           0.00904      0.00008    0.00916     0.00898
log_std/max                           -0.09180     0.00087    -0.09062    -0.09297
log_std/min                           -0.12963     0.00122    -0.12760    -0.13108
log_probs/mean                        -2.72295     0.00287    -2.71845    -2.72688
log_probs/std                         0.25369      0.01304    0.27819     0.24178
log_probs/max                         -2.01579     0.01901    -1.97856    -2.03194
log_probs/min                         -4.52589     0.53566    -3.78568    -5.22993
mean/mean                             0.00585      0.00004    0.00590     0.00578
mean/std                              0.00435      0.00018    0.00456     0.00405
mean/max                              0.01332      0.00033    0.01365     0.01274
mean/min                              -0.00117     0.00004    -0.00110    -0.00123
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5063, 0.4989, 0.5061, 0.5071, 0.4884, 0.4956, 0.5018, 0.4998,
         0.5024, 0.5100, 0.5010, 0.4960, 0.4889, 0.4928, 0.5076, 0.4927, 0.5091,
         0.4963, 0.5142],
        [0.4983, 0.5071, 0.4989, 0.5061, 0.5068, 0.4896, 0.4960, 0.5026, 0.5022,
         0.5028, 0.5083, 0.4993, 0.4962, 0.4883, 0.4906, 0.5077, 0.4920, 0.5086,
         0.4963, 0.5127],
        [0.4979, 0.5070, 0.4980, 0.5064, 0.5065, 0.4885, 0.4965, 0.5002, 0.5043,
         0.5010, 0.5062, 0.4993, 0.4964, 0.4883, 0.4904, 0.5100, 0.4936, 0.5085,
         0.4954, 0.5150],
        [0.4987, 0.5057, 0.4985, 0.5068, 0.5062, 0.4895, 0.4948, 0.4997, 0.5026,
         0.5037, 0.5059, 0.4985, 0.4972, 0.4878, 0.4895, 0.5079, 0.4923, 0.5100,
         0.4929, 0.5142],
        [0.4987, 0.5076, 0.4979, 0.5050, 0.5056, 0.4885, 0.4959, 0.4990, 0.5020,
         0.5025, 0.5087, 0.4996, 0.4969, 0.4883, 0.4920, 0.5061, 0.4944, 0.5097,
         0.4954, 0.5132],
        [0.4976, 0.5044, 0.4976, 0.5060, 0.5053, 0.4882, 0.4947, 0.5000, 0.5010,
         0.5030, 0.5071, 0.4998, 0.4953, 0.4880, 0.4902, 0.5088, 0.4953, 0.5080,
         0.4948, 0.5136],
        [0.4980, 0.5055, 0.4984, 0.5044, 0.5061, 0.4875, 0.4974, 0.5007, 0.5035,
         0.5019, 0.5103, 0.4996, 0.4960, 0.4869, 0.4907, 0.5080, 0.4955, 0.5076,
         0.4967, 0.5126],
        [0.4980, 0.5045, 0.4984, 0.5050, 0.5065, 0.4880, 0.4954, 0.5010, 0.5031,
         0.5019, 0.5095, 0.4998, 0.4962, 0.4889, 0.4919, 0.5079, 0.4935, 0.5062,
         0.4933, 0.5128],
        [0.4981, 0.5049, 0.4973, 0.5051, 0.5079, 0.4881, 0.4962, 0.4998, 0.5018,
         0.5008, 0.5064, 0.5014, 0.4976, 0.4886, 0.4923, 0.5082, 0.4928, 0.5090,
         0.4941, 0.5144],
        [0.4984, 0.5058, 0.4994, 0.5050, 0.5072, 0.4883, 0.4961, 0.5015, 0.5020,
         0.5038, 0.5076, 0.5003, 0.4946, 0.4903, 0.4927, 0.5091, 0.4940, 0.5053,
         0.4953, 0.5152]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5063, 0.4989, 0.5061, 0.5071, 0.4884, 0.4956, 0.5018, 0.4998,
        0.5024, 0.5100, 0.5010, 0.4960, 0.4889, 0.4928, 0.5076, 0.4927, 0.5091,
        0.4963, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5071, 0.4989, 0.5061, 0.5068, 0.4896, 0.4960, 0.5026, 0.5022,
        0.5028, 0.5083, 0.4993, 0.4962, 0.4883, 0.4906, 0.5077, 0.4920, 0.5086,
        0.4963, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5070, 0.4980, 0.5064, 0.5065, 0.4885, 0.4965, 0.5002, 0.5043,
        0.5010, 0.5062, 0.4993, 0.4964, 0.4883, 0.4904, 0.5100, 0.4936, 0.5085,
        0.4954, 0.5150], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5057, 0.4985, 0.5068, 0.5062, 0.4895, 0.4948, 0.4997, 0.5026,
        0.5037, 0.5059, 0.4985, 0.4972, 0.4878, 0.4895, 0.5079, 0.4923, 0.5100,
        0.4929, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5076, 0.4979, 0.5050, 0.5056, 0.4885, 0.4959, 0.4990, 0.5020,
        0.5025, 0.5087, 0.4996, 0.4969, 0.4883, 0.4920, 0.5061, 0.4944, 0.5097,
        0.4954, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5044, 0.4976, 0.5060, 0.5053, 0.4882, 0.4947, 0.5000, 0.5010,
        0.5030, 0.5071, 0.4998, 0.4953, 0.4880, 0.4902, 0.5088, 0.4953, 0.5080,
        0.4948, 0.5136], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5055, 0.4984, 0.5044, 0.5061, 0.4875, 0.4974, 0.5007, 0.5035,
        0.5019, 0.5103, 0.4996, 0.4960, 0.4869, 0.4907, 0.5080, 0.4955, 0.5076,
        0.4967, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5045, 0.4984, 0.5050, 0.5065, 0.4880, 0.4954, 0.5010, 0.5031,
        0.5019, 0.5095, 0.4998, 0.4962, 0.4889, 0.4919, 0.5079, 0.4935, 0.5062,
        0.4933, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5049, 0.4973, 0.5051, 0.5079, 0.4881, 0.4962, 0.4998, 0.5018,
        0.5008, 0.5064, 0.5014, 0.4976, 0.4886, 0.4923, 0.5082, 0.4928, 0.5090,
        0.4941, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5058, 0.4994, 0.5050, 0.5072, 0.4883, 0.4961, 0.5015, 0.5020,
        0.5038, 0.5076, 0.5003, 0.4946, 0.4903, 0.4927, 0.5091, 0.4940, 0.5053,
        0.4953, 0.5152], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5105, 0.4891, 0.5119, 0.4865, 0.4855, 0.4939, 0.4894, 0.4943, 0.4988,
         0.5073, 0.5027, 0.4996, 0.5023, 0.4913, 0.5043, 0.4915, 0.4858, 0.4988,
         0.5081, 0.4949],
        [0.5088, 0.4893, 0.5109, 0.4852, 0.4856, 0.4925, 0.4883, 0.4953, 0.4981,
         0.5051, 0.5035, 0.4996, 0.5033, 0.4913, 0.5026, 0.4893, 0.4861, 0.4986,
         0.5072, 0.4935],
        [0.5106, 0.4904, 0.5108, 0.4870, 0.4853, 0.4940, 0.4887, 0.4964, 0.5015,
         0.5067, 0.5048, 0.5002, 0.5061, 0.4918, 0.5037, 0.4912, 0.4876, 0.4980,
         0.5069, 0.4926],
        [0.5078, 0.4892, 0.5115, 0.4859, 0.4873, 0.4928, 0.4893, 0.4954, 0.4985,
         0.5086, 0.5038, 0.4996, 0.5050, 0.4911, 0.5036, 0.4907, 0.4863, 0.4987,
         0.5060, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4863, 0.4871, 0.4925, 0.4908, 0.4923, 0.4956,
         0.5064, 0.5042, 0.4995, 0.5032, 0.4914, 0.5036, 0.4895, 0.4857, 0.4963,
         0.5055, 0.4936],
        [0.5113, 0.4917, 0.5100, 0.4866, 0.4863, 0.4944, 0.4907, 0.4937, 0.4970,
         0.5069, 0.5061, 0.5000, 0.5036, 0.4891, 0.5038, 0.4903, 0.4851, 0.4972,
         0.5063, 0.4932],
        [0.5094, 0.4910, 0.5125, 0.4885, 0.4856, 0.4935, 0.4887, 0.4942, 0.4975,
         0.5070, 0.5061, 0.4994, 0.5049, 0.4901, 0.5034, 0.4891, 0.4846, 0.4991,
         0.5073, 0.4933],
        [0.5102, 0.4909, 0.5109, 0.4853, 0.4863, 0.4928, 0.4891, 0.4951, 0.5000,
         0.5072, 0.5064, 0.5008, 0.5035, 0.4894, 0.5047, 0.4914, 0.4859, 0.4961,
         0.5077, 0.4931],
        [0.5107, 0.4922, 0.5108, 0.4851, 0.4881, 0.4929, 0.4897, 0.4940, 0.4975,
         0.5075, 0.5045, 0.4977, 0.5040, 0.4898, 0.5043, 0.4903, 0.4870, 0.4968,
         0.5083, 0.4936],
        [0.5113, 0.4916, 0.5117, 0.4874, 0.4876, 0.4928, 0.4892, 0.4934, 0.4999,
         0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5002, 0.4932, 0.4837, 0.4978,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5105, 0.4891, 0.5119, 0.4865, 0.4855, 0.4939, 0.4894, 0.4943, 0.4988,
        0.5073, 0.5027, 0.4996, 0.5023, 0.4913, 0.5043, 0.4915, 0.4858, 0.4988,
        0.5081, 0.4949], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4893, 0.5109, 0.4852, 0.4856, 0.4925, 0.4883, 0.4953, 0.4981,
        0.5051, 0.5035, 0.4996, 0.5033, 0.4913, 0.5026, 0.4893, 0.4861, 0.4986,
        0.5072, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5106, 0.4904, 0.5108, 0.4870, 0.4853, 0.4940, 0.4887, 0.4964, 0.5015,
        0.5067, 0.5048, 0.5002, 0.5061, 0.4918, 0.5037, 0.4912, 0.4876, 0.4980,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4892, 0.5115, 0.4859, 0.4873, 0.4928, 0.4893, 0.4954, 0.4985,
        0.5086, 0.5038, 0.4996, 0.5050, 0.4911, 0.5036, 0.4907, 0.4863, 0.4987,
        0.5060, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4863, 0.4871, 0.4925, 0.4908, 0.4923, 0.4956,
        0.5064, 0.5042, 0.4995, 0.5032, 0.4914, 0.5036, 0.4895, 0.4857, 0.4963,
        0.5055, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4917, 0.5100, 0.4866, 0.4863, 0.4944, 0.4907, 0.4937, 0.4970,
        0.5069, 0.5061, 0.5000, 0.5036, 0.4891, 0.5038, 0.4903, 0.4851, 0.4972,
        0.5063, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4910, 0.5125, 0.4885, 0.4856, 0.4935, 0.4887, 0.4942, 0.4975,
        0.5070, 0.5061, 0.4994, 0.5049, 0.4901, 0.5034, 0.4891, 0.4846, 0.4991,
        0.5073, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5102, 0.4909, 0.5109, 0.4853, 0.4863, 0.4928, 0.4891, 0.4951, 0.5000,
        0.5072, 0.5064, 0.5008, 0.5035, 0.4894, 0.5047, 0.4914, 0.4859, 0.4961,
        0.5077, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5107, 0.4922, 0.5108, 0.4851, 0.4881, 0.4929, 0.4897, 0.4940, 0.4975,
        0.5075, 0.5045, 0.4977, 0.5040, 0.4898, 0.5043, 0.4903, 0.4870, 0.4968,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4916, 0.5117, 0.4874, 0.4876, 0.4928, 0.4892, 0.4934, 0.4999,
        0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5002, 0.4932, 0.4837, 0.4978,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5127, 0.4963, 0.4964, 0.5025, 0.5067, 0.4925, 0.5090, 0.4951, 0.4969,
         0.5026, 0.4856, 0.4974, 0.4839, 0.5026, 0.5026, 0.4985, 0.4992, 0.5141,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4952, 0.5046, 0.5035, 0.4929, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4965, 0.4860, 0.5014, 0.5002, 0.4979, 0.4981, 0.5147,
         0.4948, 0.4972],
        [0.5132, 0.4976, 0.4972, 0.5023, 0.5046, 0.4923, 0.5100, 0.4979, 0.4965,
         0.5028, 0.4866, 0.4990, 0.4849, 0.5014, 0.4998, 0.4975, 0.4991, 0.5167,
         0.4948, 0.4972],
        [0.5127, 0.4974, 0.4966, 0.5032, 0.5040, 0.4914, 0.5108, 0.4966, 0.4964,
         0.5027, 0.4843, 0.4963, 0.4847, 0.5022, 0.5008, 0.4973, 0.5009, 0.5143,
         0.4954, 0.4969],
        [0.5126, 0.4983, 0.4961, 0.5037, 0.5032, 0.4935, 0.5087, 0.4953, 0.4971,
         0.5032, 0.4876, 0.4981, 0.4827, 0.5025, 0.5002, 0.4991, 0.5000, 0.5151,
         0.4956, 0.4974],
        [0.5136, 0.4972, 0.4957, 0.5030, 0.5054, 0.4927, 0.5078, 0.4969, 0.4956,
         0.5025, 0.4856, 0.4962, 0.4855, 0.5032, 0.5021, 0.5001, 0.4993, 0.5145,
         0.4951, 0.4981],
        [0.5113, 0.4965, 0.4979, 0.5025, 0.5038, 0.4923, 0.5088, 0.4963, 0.4962,
         0.5027, 0.4883, 0.4971, 0.4849, 0.5027, 0.5009, 0.4987, 0.4997, 0.5137,
         0.4952, 0.4995],
        [0.5121, 0.4972, 0.4950, 0.5014, 0.5039, 0.4922, 0.5084, 0.4945, 0.4970,
         0.5024, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4984, 0.4995, 0.5154,
         0.4959, 0.4983],
        [0.5128, 0.4981, 0.4951, 0.5013, 0.5046, 0.4924, 0.5097, 0.4967, 0.4967,
         0.5020, 0.4885, 0.4950, 0.4850, 0.5018, 0.5011, 0.4971, 0.5006, 0.5139,
         0.4957, 0.4991],
        [0.5125, 0.4984, 0.4940, 0.5029, 0.5046, 0.4938, 0.5081, 0.4988, 0.4964,
         0.5034, 0.4885, 0.4983, 0.4857, 0.5001, 0.5007, 0.4982, 0.4992, 0.5156,
         0.4971, 0.5002]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5127, 0.4963, 0.4964, 0.5025, 0.5067, 0.4925, 0.5090, 0.4951, 0.4969,
        0.5026, 0.4856, 0.4974, 0.4839, 0.5026, 0.5026, 0.4985, 0.4992, 0.5141,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4952, 0.5046, 0.5035, 0.4929, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4965, 0.4860, 0.5014, 0.5002, 0.4979, 0.4981, 0.5147,
        0.4948, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5132, 0.4976, 0.4972, 0.5023, 0.5046, 0.4923, 0.5100, 0.4979, 0.4965,
        0.5028, 0.4866, 0.4990, 0.4849, 0.5014, 0.4998, 0.4975, 0.4991, 0.5167,
        0.4948, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4974, 0.4966, 0.5032, 0.5040, 0.4914, 0.5108, 0.4966, 0.4964,
        0.5027, 0.4843, 0.4963, 0.4847, 0.5022, 0.5008, 0.4973, 0.5009, 0.5143,
        0.4954, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4983, 0.4961, 0.5037, 0.5032, 0.4935, 0.5087, 0.4953, 0.4971,
        0.5032, 0.4876, 0.4981, 0.4827, 0.5025, 0.5002, 0.4991, 0.5000, 0.5151,
        0.4956, 0.4974], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4972, 0.4957, 0.5030, 0.5054, 0.4927, 0.5078, 0.4969, 0.4956,
        0.5025, 0.4856, 0.4962, 0.4855, 0.5032, 0.5021, 0.5001, 0.4993, 0.5145,
        0.4951, 0.4981], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4965, 0.4979, 0.5025, 0.5038, 0.4923, 0.5088, 0.4963, 0.4962,
        0.5027, 0.4883, 0.4971, 0.4849, 0.5027, 0.5009, 0.4987, 0.4997, 0.5137,
        0.4952, 0.4995], grad_fn=<UnbindBackward>), tensor([0.5121, 0.4972, 0.4950, 0.5014, 0.5039, 0.4922, 0.5084, 0.4945, 0.4970,
        0.5024, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4984, 0.4995, 0.5154,
        0.4959, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4951, 0.5013, 0.5046, 0.4924, 0.5097, 0.4967, 0.4967,
        0.5020, 0.4885, 0.4950, 0.4850, 0.5018, 0.5011, 0.4971, 0.5006, 0.5139,
        0.4957, 0.4991], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4984, 0.4940, 0.5029, 0.5046, 0.4938, 0.5081, 0.4988, 0.4964,
        0.5034, 0.4885, 0.4983, 0.4857, 0.5001, 0.5007, 0.4982, 0.4992, 0.5156,
        0.4971, 0.5002], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [1, 5, 4, 0, 2, 3, 8, 9, 7, 6]
replay_buffer._size: [4500 4500 4500 4500 4500 4500 4500 4500 4500 4500]
2023-08-12 10:40:36,627 MainThread INFO: EPOCH:28
2023-08-12 10:40:36,627 MainThread INFO: Time Consumed:0.34273600578308105s
2023-08-12 10:40:36,627 MainThread INFO: Total Frames:43500s
 36%|‚ñà‚ñà‚ñà‚ñã      | 29/80 [00:30<00:34,  1.46it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1265.57604
Train_Epoch_Reward                    14440.06747
Running_Training_Average_Rewards      971.13628
Explore_Time                          0.00405
Train___Time                          0.33522
Eval____Time                          0.00265
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.07618
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.66687
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.21285
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.40023
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.60339
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.07007
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -22.11041
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12908.07701
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -25.71954
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.45708
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.68843     0.72152    12.78692    10.74692
alpha_0                               0.95794      0.00041    0.95851     0.95736
alpha_1                               0.95793      0.00041    0.95850     0.95735
alpha_2                               0.95797      0.00041    0.95855     0.95739
alpha_3                               0.95794      0.00041    0.95852     0.95737
alpha_4                               0.95794      0.00041    0.95851     0.95736
alpha_5                               0.95793      0.00041    0.95851     0.95736
alpha_6                               0.95793      0.00041    0.95851     0.95736
alpha_7                               0.95794      0.00041    0.95851     0.95736
alpha_8                               0.95794      0.00041    0.95851     0.95736
alpha_9                               0.95795      0.00041    0.95853     0.95738
Alpha_loss                            -0.28729     0.00311    -0.28284    -0.29126
Training/policy_loss                  -2.70913     0.00949    -2.69836    -2.72014
Training/qf1_loss                     2691.11863   497.35661  3501.35742  1951.47205
Training/qf2_loss                     2690.65925   497.31571  3500.85278  1951.07678
Training/pf_norm                      0.15422      0.04269    0.20004     0.08536
Training/qf1_norm                     31.72503     1.70281    34.31495    29.52203
Training/qf2_norm                     33.15482     1.86857    35.88289    30.67653
log_std/mean                          -0.11350     0.00113    -0.11188    -0.11508
log_std/std                           0.00938      0.00009    0.00951     0.00928
log_std/max                           -0.09517     0.00098    -0.09368    -0.09655
log_std/min                           -0.13480     0.00185    -0.13229    -0.13720
log_probs/mean                        -2.73287     0.00965    -2.72172    -2.74403
log_probs/std                         0.25126      0.01250    0.27031     0.23522
log_probs/max                         -2.04926     0.03270    -2.00631    -2.10340
log_probs/min                         -4.83139     0.48524    -4.29608    -5.47783
mean/mean                             0.00577      0.00008    0.00591     0.00567
mean/std                              0.00457      0.00004    0.00465     0.00454
mean/max                              0.01384      0.00025    0.01428     0.01360
mean/min                              -0.00063     0.00018    -0.00043    -0.00091
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [2, 9, 8, 3, 7, 5, 0, 6, 1, 4]
replay_buffer._size: [4650 4650 4650 4650 4650 4650 4650 4650 4650 4650]
snapshot at best
2023-08-12 10:40:37,589 MainThread INFO: EPOCH:29
2023-08-12 10:40:37,589 MainThread INFO: Time Consumed:0.8418762683868408s
2023-08-12 10:40:37,589 MainThread INFO: Total Frames:45000s
 38%|‚ñà‚ñà‚ñà‚ñä      | 30/80 [00:31<00:38,  1.30it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1289.99459
Train_Epoch_Reward                    11836.88040
Running_Training_Average_Rewards      1109.31745
Explore_Time                          0.00288
Train___Time                          0.34940
Eval____Time                          0.00216
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.59455
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.73160
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.34111
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.34119
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.71504
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.16040
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -22.19810
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 13153.08912
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -25.78652
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.27473
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.96062      0.76418    10.68591    8.94647
alpha_0                               0.95650      0.00041    0.95707     0.95592
alpha_1                               0.95649      0.00041    0.95706     0.95591
alpha_2                               0.95653      0.00041    0.95711     0.95595
alpha_3                               0.95650      0.00041    0.95708     0.95593
alpha_4                               0.95650      0.00041    0.95707     0.95592
alpha_5                               0.95649      0.00041    0.95707     0.95592
alpha_6                               0.95649      0.00041    0.95707     0.95592
alpha_7                               0.95650      0.00041    0.95707     0.95592
alpha_8                               0.95650      0.00041    0.95707     0.95592
alpha_9                               0.95651      0.00041    0.95709     0.95594
Alpha_loss                            -0.29742     0.00277    -0.29363    -0.30153
Training/policy_loss                  -2.71058     0.00529    -2.70187    -2.71624
Training/qf1_loss                     1767.92192   394.46882  2220.98193  1314.90161
Training/qf2_loss                     1767.48486   394.45836  2220.52515  1314.47729
Training/pf_norm                      0.13431      0.02751    0.18357     0.10577
Training/qf1_norm                     28.00222     1.65951    29.47837    25.85817
Training/qf2_norm                     29.37645     1.70672    30.92357    27.21284
log_std/mean                          -0.11702     0.00092    -0.11574    -0.11831
log_std/std                           0.00962      0.00005    0.00971     0.00956
log_std/max                           -0.09812     0.00069    -0.09701    -0.09911
log_std/min                           -0.13980     0.00123    -0.13779    -0.14142
log_probs/mean                        -2.73306     0.00558    -2.72364    -2.73910
log_probs/std                         0.24245      0.01456    0.26261     0.22671
log_probs/max                         -2.07189     0.01647    -2.05075    -2.09039
log_probs/min                         -4.94169     0.61269    -4.29637    -6.08387
mean/mean                             0.00528      0.00027    0.00564     0.00488
mean/std                              0.00478      0.00003    0.00481     0.00475
mean/max                              0.01452      0.00018    0.01471     0.01420
mean/min                              -0.00096     0.00033    -0.00051    -0.00141
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5063, 0.4989, 0.5062, 0.5071, 0.4884, 0.4956, 0.5018, 0.4999,
         0.5024, 0.5099, 0.5010, 0.4961, 0.4889, 0.4928, 0.5077, 0.4927, 0.5092,
         0.4963, 0.5142],
        [0.4983, 0.5070, 0.4989, 0.5062, 0.5067, 0.4896, 0.4960, 0.5025, 0.5023,
         0.5028, 0.5083, 0.4993, 0.4963, 0.4882, 0.4906, 0.5077, 0.4919, 0.5087,
         0.4964, 0.5128],
        [0.4979, 0.5070, 0.4981, 0.5065, 0.5065, 0.4885, 0.4965, 0.5003, 0.5043,
         0.5011, 0.5062, 0.4992, 0.4965, 0.4883, 0.4904, 0.5099, 0.4936, 0.5086,
         0.4954, 0.5150],
        [0.4987, 0.5057, 0.4985, 0.5068, 0.5062, 0.4895, 0.4947, 0.4997, 0.5026,
         0.5037, 0.5059, 0.4985, 0.4972, 0.4878, 0.4895, 0.5079, 0.4923, 0.5100,
         0.4929, 0.5143],
        [0.4987, 0.5075, 0.4980, 0.5050, 0.5056, 0.4885, 0.4959, 0.4989, 0.5021,
         0.5025, 0.5087, 0.4996, 0.4969, 0.4883, 0.4919, 0.5062, 0.4944, 0.5097,
         0.4953, 0.5132],
        [0.4976, 0.5044, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.4999, 0.5010,
         0.5030, 0.5072, 0.4998, 0.4953, 0.4880, 0.4902, 0.5087, 0.4953, 0.5080,
         0.4948, 0.5136],
        [0.4978, 0.5056, 0.4985, 0.5044, 0.5061, 0.4875, 0.4975, 0.5007, 0.5035,
         0.5018, 0.5104, 0.4995, 0.4961, 0.4868, 0.4908, 0.5082, 0.4956, 0.5076,
         0.4968, 0.5127],
        [0.4979, 0.5046, 0.4984, 0.5050, 0.5066, 0.4879, 0.4954, 0.5011, 0.5031,
         0.5019, 0.5095, 0.4997, 0.4962, 0.4888, 0.4919, 0.5080, 0.4935, 0.5061,
         0.4934, 0.5128],
        [0.4981, 0.5049, 0.4973, 0.5051, 0.5079, 0.4881, 0.4962, 0.4999, 0.5019,
         0.5008, 0.5064, 0.5013, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5090,
         0.4941, 0.5145],
        [0.4984, 0.5057, 0.4994, 0.5050, 0.5072, 0.4883, 0.4961, 0.5016, 0.5020,
         0.5038, 0.5076, 0.5003, 0.4946, 0.4904, 0.4927, 0.5092, 0.4940, 0.5053,
         0.4952, 0.5153]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5063, 0.4989, 0.5062, 0.5071, 0.4884, 0.4956, 0.5018, 0.4999,
        0.5024, 0.5099, 0.5010, 0.4961, 0.4889, 0.4928, 0.5077, 0.4927, 0.5092,
        0.4963, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5070, 0.4989, 0.5062, 0.5067, 0.4896, 0.4960, 0.5025, 0.5023,
        0.5028, 0.5083, 0.4993, 0.4963, 0.4882, 0.4906, 0.5077, 0.4919, 0.5087,
        0.4964, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5070, 0.4981, 0.5065, 0.5065, 0.4885, 0.4965, 0.5003, 0.5043,
        0.5011, 0.5062, 0.4992, 0.4965, 0.4883, 0.4904, 0.5099, 0.4936, 0.5086,
        0.4954, 0.5150], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5057, 0.4985, 0.5068, 0.5062, 0.4895, 0.4947, 0.4997, 0.5026,
        0.5037, 0.5059, 0.4985, 0.4972, 0.4878, 0.4895, 0.5079, 0.4923, 0.5100,
        0.4929, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5075, 0.4980, 0.5050, 0.5056, 0.4885, 0.4959, 0.4989, 0.5021,
        0.5025, 0.5087, 0.4996, 0.4969, 0.4883, 0.4919, 0.5062, 0.4944, 0.5097,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5044, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.4999, 0.5010,
        0.5030, 0.5072, 0.4998, 0.4953, 0.4880, 0.4902, 0.5087, 0.4953, 0.5080,
        0.4948, 0.5136], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5056, 0.4985, 0.5044, 0.5061, 0.4875, 0.4975, 0.5007, 0.5035,
        0.5018, 0.5104, 0.4995, 0.4961, 0.4868, 0.4908, 0.5082, 0.4956, 0.5076,
        0.4968, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5046, 0.4984, 0.5050, 0.5066, 0.4879, 0.4954, 0.5011, 0.5031,
        0.5019, 0.5095, 0.4997, 0.4962, 0.4888, 0.4919, 0.5080, 0.4935, 0.5061,
        0.4934, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5049, 0.4973, 0.5051, 0.5079, 0.4881, 0.4962, 0.4999, 0.5019,
        0.5008, 0.5064, 0.5013, 0.4977, 0.4886, 0.4923, 0.5082, 0.4928, 0.5090,
        0.4941, 0.5145], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4994, 0.5050, 0.5072, 0.4883, 0.4961, 0.5016, 0.5020,
        0.5038, 0.5076, 0.5003, 0.4946, 0.4904, 0.4927, 0.5092, 0.4940, 0.5053,
        0.4952, 0.5153], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5104, 0.4892, 0.5119, 0.4866, 0.4855, 0.4939, 0.4893, 0.4942, 0.4988,
         0.5073, 0.5026, 0.4997, 0.5023, 0.4914, 0.5043, 0.4914, 0.4858, 0.4988,
         0.5080, 0.4948],
        [0.5087, 0.4893, 0.5109, 0.4852, 0.4856, 0.4925, 0.4882, 0.4952, 0.4981,
         0.5052, 0.5034, 0.4996, 0.5034, 0.4914, 0.5027, 0.4889, 0.4862, 0.4986,
         0.5072, 0.4935],
        [0.5105, 0.4905, 0.5108, 0.4870, 0.4852, 0.4940, 0.4887, 0.4964, 0.5015,
         0.5068, 0.5047, 0.5002, 0.5061, 0.4918, 0.5038, 0.4911, 0.4876, 0.4980,
         0.5070, 0.4926],
        [0.5079, 0.4893, 0.5115, 0.4859, 0.4872, 0.4929, 0.4893, 0.4954, 0.4985,
         0.5086, 0.5038, 0.4997, 0.5050, 0.4911, 0.5037, 0.4907, 0.4863, 0.4987,
         0.5060, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4863, 0.4872, 0.4926, 0.4908, 0.4923, 0.4956,
         0.5065, 0.5042, 0.4995, 0.5032, 0.4914, 0.5036, 0.4895, 0.4857, 0.4963,
         0.5054, 0.4936],
        [0.5113, 0.4916, 0.5100, 0.4866, 0.4863, 0.4945, 0.4907, 0.4938, 0.4970,
         0.5069, 0.5060, 0.5000, 0.5037, 0.4892, 0.5038, 0.4903, 0.4850, 0.4972,
         0.5063, 0.4933],
        [0.5094, 0.4911, 0.5125, 0.4884, 0.4855, 0.4935, 0.4888, 0.4942, 0.4976,
         0.5071, 0.5061, 0.4994, 0.5049, 0.4901, 0.5034, 0.4891, 0.4847, 0.4990,
         0.5072, 0.4932],
        [0.5101, 0.4911, 0.5110, 0.4853, 0.4862, 0.4927, 0.4892, 0.4950, 0.4999,
         0.5072, 0.5065, 0.5008, 0.5035, 0.4893, 0.5048, 0.4913, 0.4858, 0.4961,
         0.5076, 0.4931],
        [0.5108, 0.4923, 0.5108, 0.4852, 0.4882, 0.4929, 0.4896, 0.4940, 0.4976,
         0.5075, 0.5046, 0.4977, 0.5040, 0.4898, 0.5043, 0.4902, 0.4869, 0.4968,
         0.5083, 0.4936],
        [0.5113, 0.4917, 0.5117, 0.4874, 0.4876, 0.4928, 0.4892, 0.4933, 0.4999,
         0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5003, 0.4931, 0.4836, 0.4978,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5104, 0.4892, 0.5119, 0.4866, 0.4855, 0.4939, 0.4893, 0.4942, 0.4988,
        0.5073, 0.5026, 0.4997, 0.5023, 0.4914, 0.5043, 0.4914, 0.4858, 0.4988,
        0.5080, 0.4948], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4893, 0.5109, 0.4852, 0.4856, 0.4925, 0.4882, 0.4952, 0.4981,
        0.5052, 0.5034, 0.4996, 0.5034, 0.4914, 0.5027, 0.4889, 0.4862, 0.4986,
        0.5072, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5105, 0.4905, 0.5108, 0.4870, 0.4852, 0.4940, 0.4887, 0.4964, 0.5015,
        0.5068, 0.5047, 0.5002, 0.5061, 0.4918, 0.5038, 0.4911, 0.4876, 0.4980,
        0.5070, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4893, 0.5115, 0.4859, 0.4872, 0.4929, 0.4893, 0.4954, 0.4985,
        0.5086, 0.5038, 0.4997, 0.5050, 0.4911, 0.5037, 0.4907, 0.4863, 0.4987,
        0.5060, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4863, 0.4872, 0.4926, 0.4908, 0.4923, 0.4956,
        0.5065, 0.5042, 0.4995, 0.5032, 0.4914, 0.5036, 0.4895, 0.4857, 0.4963,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4916, 0.5100, 0.4866, 0.4863, 0.4945, 0.4907, 0.4938, 0.4970,
        0.5069, 0.5060, 0.5000, 0.5037, 0.4892, 0.5038, 0.4903, 0.4850, 0.4972,
        0.5063, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4911, 0.5125, 0.4884, 0.4855, 0.4935, 0.4888, 0.4942, 0.4976,
        0.5071, 0.5061, 0.4994, 0.5049, 0.4901, 0.5034, 0.4891, 0.4847, 0.4990,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4911, 0.5110, 0.4853, 0.4862, 0.4927, 0.4892, 0.4950, 0.4999,
        0.5072, 0.5065, 0.5008, 0.5035, 0.4893, 0.5048, 0.4913, 0.4858, 0.4961,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5108, 0.4923, 0.5108, 0.4852, 0.4882, 0.4929, 0.4896, 0.4940, 0.4976,
        0.5075, 0.5046, 0.4977, 0.5040, 0.4898, 0.5043, 0.4902, 0.4869, 0.4968,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4917, 0.5117, 0.4874, 0.4876, 0.4928, 0.4892, 0.4933, 0.4999,
        0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5003, 0.4931, 0.4836, 0.4978,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5127, 0.4963, 0.4964, 0.5025, 0.5066, 0.4926, 0.5090, 0.4952, 0.4969,
         0.5026, 0.4857, 0.4974, 0.4840, 0.5025, 0.5025, 0.4984, 0.4992, 0.5141,
         0.4936, 0.4968],
        [0.5115, 0.4959, 0.4953, 0.5046, 0.5034, 0.4930, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4965, 0.4860, 0.5013, 0.5003, 0.4978, 0.4980, 0.5146,
         0.4948, 0.4972],
        [0.5132, 0.4976, 0.4972, 0.5023, 0.5046, 0.4923, 0.5101, 0.4980, 0.4965,
         0.5028, 0.4865, 0.4990, 0.4849, 0.5013, 0.4998, 0.4974, 0.4990, 0.5166,
         0.4948, 0.4971],
        [0.5126, 0.4973, 0.4967, 0.5032, 0.5040, 0.4914, 0.5109, 0.4966, 0.4964,
         0.5027, 0.4842, 0.4964, 0.4847, 0.5022, 0.5008, 0.4973, 0.5010, 0.5143,
         0.4954, 0.4969],
        [0.5125, 0.4982, 0.4961, 0.5037, 0.5032, 0.4935, 0.5087, 0.4952, 0.4970,
         0.5032, 0.4875, 0.4980, 0.4827, 0.5026, 0.5003, 0.4991, 0.5000, 0.5152,
         0.4957, 0.4974],
        [0.5136, 0.4972, 0.4958, 0.5030, 0.5054, 0.4927, 0.5078, 0.4968, 0.4957,
         0.5025, 0.4856, 0.4962, 0.4855, 0.5031, 0.5020, 0.5001, 0.4993, 0.5145,
         0.4951, 0.4982],
        [0.5112, 0.4964, 0.4979, 0.5025, 0.5038, 0.4924, 0.5088, 0.4964, 0.4961,
         0.5028, 0.4882, 0.4971, 0.4850, 0.5026, 0.5009, 0.4987, 0.4999, 0.5137,
         0.4952, 0.4996],
        [0.5121, 0.4971, 0.4950, 0.5013, 0.5040, 0.4923, 0.5084, 0.4945, 0.4970,
         0.5024, 0.4858, 0.4986, 0.4865, 0.5010, 0.5008, 0.4984, 0.4995, 0.5153,
         0.4958, 0.4983],
        [0.5127, 0.4981, 0.4952, 0.5013, 0.5046, 0.4925, 0.5097, 0.4967, 0.4967,
         0.5020, 0.4885, 0.4949, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5139,
         0.4956, 0.4991],
        [0.5125, 0.4984, 0.4940, 0.5029, 0.5047, 0.4938, 0.5081, 0.4988, 0.4964,
         0.5034, 0.4885, 0.4982, 0.4857, 0.5001, 0.5008, 0.4982, 0.4992, 0.5155,
         0.4970, 0.5002]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5127, 0.4963, 0.4964, 0.5025, 0.5066, 0.4926, 0.5090, 0.4952, 0.4969,
        0.5026, 0.4857, 0.4974, 0.4840, 0.5025, 0.5025, 0.4984, 0.4992, 0.5141,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4959, 0.4953, 0.5046, 0.5034, 0.4930, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4965, 0.4860, 0.5013, 0.5003, 0.4978, 0.4980, 0.5146,
        0.4948, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5132, 0.4976, 0.4972, 0.5023, 0.5046, 0.4923, 0.5101, 0.4980, 0.4965,
        0.5028, 0.4865, 0.4990, 0.4849, 0.5013, 0.4998, 0.4974, 0.4990, 0.5166,
        0.4948, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4973, 0.4967, 0.5032, 0.5040, 0.4914, 0.5109, 0.4966, 0.4964,
        0.5027, 0.4842, 0.4964, 0.4847, 0.5022, 0.5008, 0.4973, 0.5010, 0.5143,
        0.4954, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4982, 0.4961, 0.5037, 0.5032, 0.4935, 0.5087, 0.4952, 0.4970,
        0.5032, 0.4875, 0.4980, 0.4827, 0.5026, 0.5003, 0.4991, 0.5000, 0.5152,
        0.4957, 0.4974], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4972, 0.4958, 0.5030, 0.5054, 0.4927, 0.5078, 0.4968, 0.4957,
        0.5025, 0.4856, 0.4962, 0.4855, 0.5031, 0.5020, 0.5001, 0.4993, 0.5145,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4964, 0.4979, 0.5025, 0.5038, 0.4924, 0.5088, 0.4964, 0.4961,
        0.5028, 0.4882, 0.4971, 0.4850, 0.5026, 0.5009, 0.4987, 0.4999, 0.5137,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5121, 0.4971, 0.4950, 0.5013, 0.5040, 0.4923, 0.5084, 0.4945, 0.4970,
        0.5024, 0.4858, 0.4986, 0.4865, 0.5010, 0.5008, 0.4984, 0.4995, 0.5153,
        0.4958, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4981, 0.4952, 0.5013, 0.5046, 0.4925, 0.5097, 0.4967, 0.4967,
        0.5020, 0.4885, 0.4949, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5139,
        0.4956, 0.4991], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4984, 0.4940, 0.5029, 0.5047, 0.4938, 0.5081, 0.4988, 0.4964,
        0.5034, 0.4885, 0.4982, 0.4857, 0.5001, 0.5008, 0.4982, 0.4992, 0.5155,
        0.4970, 0.5002], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [2, 9, 1, 5, 7, 4, 6, 3, 8, 0]
replay_buffer._size: [4800 4800 4800 4800 4800 4800 4800 4800 4800 4800]
2023-08-12 10:40:38,366 MainThread INFO: EPOCH:30
2023-08-12 10:40:38,366 MainThread INFO: Time Consumed:0.3649928569793701s
2023-08-12 10:40:38,366 MainThread INFO: Total Frames:46500s
 39%|‚ñà‚ñà‚ñà‚ñâ      | 31/80 [00:31<00:37,  1.30it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1285.90874
Train_Epoch_Reward                    12595.57729
Running_Training_Average_Rewards      1295.75084
Explore_Time                          0.00313
Train___Time                          0.35793
Eval____Time                          0.00287
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.24965
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.90540
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.28582
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.47457
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.78472
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.22106
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -22.25659
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 13117.51080
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -25.97481
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.27074
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.56773      0.86186    10.63533    8.34352
alpha_0                               0.95506      0.00041    0.95563     0.95448
alpha_1                               0.95505      0.00041    0.95563     0.95448
alpha_2                               0.95509      0.00041    0.95567     0.95452
alpha_3                               0.95506      0.00041    0.95564     0.95449
alpha_4                               0.95506      0.00041    0.95563     0.95448
alpha_5                               0.95506      0.00041    0.95563     0.95448
alpha_6                               0.95505      0.00041    0.95563     0.95448
alpha_7                               0.95506      0.00041    0.95563     0.95448
alpha_8                               0.95506      0.00041    0.95564     0.95449
alpha_9                               0.95508      0.00041    0.95565     0.95450
Alpha_loss                            -0.30733     0.00255    -0.30376    -0.31086
Training/policy_loss                  -2.70727     0.00634    -2.69736    -2.71686
Training/qf1_loss                     1701.77920   440.31766  2466.71899  1250.30151
Training/qf2_loss                     1701.30491   440.29851  2466.18652  1249.80542
Training/pf_norm                      0.12995      0.02426    0.14725     0.08222
Training/qf1_norm                     27.32034     1.90152    29.79699    24.70437
Training/qf2_norm                     28.85560     1.89207    31.53831    26.41649
log_std/mean                          -0.11981     0.00069    -0.11881    -0.12075
log_std/std                           0.00980      0.00005    0.00987     0.00972
log_std/max                           -0.10052     0.00082    -0.09939    -0.10172
log_std/min                           -0.14299     0.00065    -0.14213    -0.14382
log_probs/mean                        -2.72827     0.00711    -2.71718    -2.73892
log_probs/std                         0.23174      0.00756    0.24455     0.22263
log_probs/max                         -2.09665     0.01783    -2.06988    -2.12348
log_probs/min                         -4.40968     0.20136    -4.14720    -4.72248
mean/mean                             0.00412      0.00038    0.00462     0.00354
mean/std                              0.00493      0.00014    0.00510     0.00474
mean/max                              0.01366      0.00020    0.01391     0.01329
mean/min                              -0.00259     0.00062    -0.00173    -0.00348
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [9, 4, 6, 7, 1, 8, 0, 3, 5, 2]
replay_buffer._size: [4950 4950 4950 4950 4950 4950 4950 4950 4950 4950]
2023-08-12 10:40:38,863 MainThread INFO: EPOCH:31
2023-08-12 10:40:38,863 MainThread INFO: Time Consumed:0.3871316909790039s
2023-08-12 10:40:38,863 MainThread INFO: Total Frames:48000s
 40%|‚ñà‚ñà‚ñà‚ñà      | 32/80 [00:32<00:32,  1.47it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1260.28129
Train_Epoch_Reward                    5640.45344
Running_Training_Average_Rewards      1002.43037
Explore_Time                          0.00318
Train___Time                          0.38150
Eval____Time                          0.00203
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.00284
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.05165
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.07190
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.65615
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.69662
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.16144
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -22.19726
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12856.20055
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.13300
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.41673
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.58594      0.51353    10.39464    8.87843
alpha_0                               0.95362      0.00041    0.95420     0.95305
alpha_1                               0.95362      0.00041    0.95419     0.95304
alpha_2                               0.95365      0.00041    0.95423     0.95308
alpha_3                               0.95363      0.00041    0.95420     0.95306
alpha_4                               0.95362      0.00041    0.95420     0.95305
alpha_5                               0.95362      0.00041    0.95420     0.95305
alpha_6                               0.95362      0.00041    0.95419     0.95304
alpha_7                               0.95362      0.00041    0.95420     0.95305
alpha_8                               0.95363      0.00041    0.95420     0.95305
alpha_9                               0.95364      0.00041    0.95421     0.95307
Alpha_loss                            -0.31791     0.00290    -0.31384    -0.32177
Training/policy_loss                  -2.71801     0.00476    -2.71462    -2.72735
Training/qf1_loss                     1758.10303   341.04781  2387.74463  1410.78186
Training/qf2_loss                     1757.58682   341.00770  2387.15527  1410.35413
Training/pf_norm                      0.11965      0.02509    0.15695     0.08603
Training/qf1_norm                     27.50589     1.23238    29.54038    25.82876
Training/qf2_norm                     29.19312     1.42066    31.52507    27.11216
log_std/mean                          -0.12202     0.00059    -0.12117    -0.12282
log_std/std                           0.00988      0.00005    0.00993     0.00982
log_std/max                           -0.10255     0.00045    -0.10206    -0.10322
log_std/min                           -0.14501     0.00082    -0.14375    -0.14610
log_probs/mean                        -2.73803     0.00469    -2.73411    -2.74708
log_probs/std                         0.26360      0.01759    0.29857     0.25256
log_probs/max                         -2.08001     0.04598    -2.02388    -2.14964
log_probs/min                         -6.17315     1.01337    -5.30562    -8.15575
mean/mean                             0.00277      0.00033    0.00328     0.00233
mean/std                              0.00500      0.00004    0.00506     0.00495
mean/max                              0.01212      0.00049    0.01287     0.01148
mean/min                              -0.00444     0.00035    -0.00390    -0.00493
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5063, 0.4989, 0.5062, 0.5070, 0.4884, 0.4955, 0.5018, 0.4999,
         0.5024, 0.5099, 0.5010, 0.4960, 0.4888, 0.4928, 0.5076, 0.4926, 0.5092,
         0.4963, 0.5141],
        [0.4983, 0.5071, 0.4989, 0.5061, 0.5067, 0.4897, 0.4959, 0.5026, 0.5023,
         0.5028, 0.5083, 0.4992, 0.4963, 0.4882, 0.4906, 0.5077, 0.4919, 0.5086,
         0.4964, 0.5127],
        [0.4979, 0.5070, 0.4981, 0.5065, 0.5065, 0.4886, 0.4965, 0.5003, 0.5043,
         0.5011, 0.5063, 0.4991, 0.4964, 0.4883, 0.4903, 0.5099, 0.4936, 0.5086,
         0.4954, 0.5149],
        [0.4987, 0.5058, 0.4986, 0.5069, 0.5063, 0.4896, 0.4947, 0.4998, 0.5026,
         0.5037, 0.5060, 0.4985, 0.4972, 0.4878, 0.4894, 0.5080, 0.4923, 0.5100,
         0.4929, 0.5142],
        [0.4986, 0.5076, 0.4980, 0.5050, 0.5056, 0.4885, 0.4958, 0.4990, 0.5020,
         0.5025, 0.5087, 0.4996, 0.4969, 0.4882, 0.4919, 0.5062, 0.4944, 0.5097,
         0.4954, 0.5132],
        [0.4976, 0.5045, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.5000, 0.5010,
         0.5030, 0.5072, 0.4999, 0.4953, 0.4880, 0.4902, 0.5087, 0.4953, 0.5080,
         0.4949, 0.5136],
        [0.4979, 0.5055, 0.4984, 0.5044, 0.5061, 0.4875, 0.4974, 0.5007, 0.5034,
         0.5018, 0.5103, 0.4996, 0.4960, 0.4869, 0.4908, 0.5081, 0.4955, 0.5076,
         0.4967, 0.5126],
        [0.4979, 0.5046, 0.4984, 0.5050, 0.5065, 0.4879, 0.4953, 0.5011, 0.5031,
         0.5019, 0.5095, 0.4997, 0.4962, 0.4889, 0.4919, 0.5079, 0.4935, 0.5062,
         0.4933, 0.5128],
        [0.4981, 0.5049, 0.4973, 0.5051, 0.5079, 0.4881, 0.4961, 0.4999, 0.5019,
         0.5008, 0.5064, 0.5013, 0.4976, 0.4886, 0.4923, 0.5082, 0.4928, 0.5090,
         0.4941, 0.5144],
        [0.4984, 0.5057, 0.4994, 0.5050, 0.5072, 0.4883, 0.4961, 0.5016, 0.5020,
         0.5039, 0.5076, 0.5003, 0.4946, 0.4904, 0.4927, 0.5092, 0.4940, 0.5052,
         0.4953, 0.5153]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5063, 0.4989, 0.5062, 0.5070, 0.4884, 0.4955, 0.5018, 0.4999,
        0.5024, 0.5099, 0.5010, 0.4960, 0.4888, 0.4928, 0.5076, 0.4926, 0.5092,
        0.4963, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5071, 0.4989, 0.5061, 0.5067, 0.4897, 0.4959, 0.5026, 0.5023,
        0.5028, 0.5083, 0.4992, 0.4963, 0.4882, 0.4906, 0.5077, 0.4919, 0.5086,
        0.4964, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5070, 0.4981, 0.5065, 0.5065, 0.4886, 0.4965, 0.5003, 0.5043,
        0.5011, 0.5063, 0.4991, 0.4964, 0.4883, 0.4903, 0.5099, 0.4936, 0.5086,
        0.4954, 0.5149], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5058, 0.4986, 0.5069, 0.5063, 0.4896, 0.4947, 0.4998, 0.5026,
        0.5037, 0.5060, 0.4985, 0.4972, 0.4878, 0.4894, 0.5080, 0.4923, 0.5100,
        0.4929, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5076, 0.4980, 0.5050, 0.5056, 0.4885, 0.4958, 0.4990, 0.5020,
        0.5025, 0.5087, 0.4996, 0.4969, 0.4882, 0.4919, 0.5062, 0.4944, 0.5097,
        0.4954, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5045, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.5000, 0.5010,
        0.5030, 0.5072, 0.4999, 0.4953, 0.4880, 0.4902, 0.5087, 0.4953, 0.5080,
        0.4949, 0.5136], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5055, 0.4984, 0.5044, 0.5061, 0.4875, 0.4974, 0.5007, 0.5034,
        0.5018, 0.5103, 0.4996, 0.4960, 0.4869, 0.4908, 0.5081, 0.4955, 0.5076,
        0.4967, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5046, 0.4984, 0.5050, 0.5065, 0.4879, 0.4953, 0.5011, 0.5031,
        0.5019, 0.5095, 0.4997, 0.4962, 0.4889, 0.4919, 0.5079, 0.4935, 0.5062,
        0.4933, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5049, 0.4973, 0.5051, 0.5079, 0.4881, 0.4961, 0.4999, 0.5019,
        0.5008, 0.5064, 0.5013, 0.4976, 0.4886, 0.4923, 0.5082, 0.4928, 0.5090,
        0.4941, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4994, 0.5050, 0.5072, 0.4883, 0.4961, 0.5016, 0.5020,
        0.5039, 0.5076, 0.5003, 0.4946, 0.4904, 0.4927, 0.5092, 0.4940, 0.5052,
        0.4953, 0.5153], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5104, 0.4891, 0.5119, 0.4865, 0.4855, 0.4939, 0.4893, 0.4943, 0.4988,
         0.5073, 0.5026, 0.4998, 0.5024, 0.4914, 0.5043, 0.4914, 0.4858, 0.4987,
         0.5080, 0.4948],
        [0.5087, 0.4894, 0.5109, 0.4852, 0.4856, 0.4926, 0.4882, 0.4952, 0.4981,
         0.5051, 0.5035, 0.4997, 0.5033, 0.4914, 0.5027, 0.4890, 0.4862, 0.4985,
         0.5072, 0.4935],
        [0.5105, 0.4905, 0.5108, 0.4870, 0.4852, 0.4940, 0.4888, 0.4963, 0.5014,
         0.5068, 0.5048, 0.5002, 0.5061, 0.4918, 0.5039, 0.4911, 0.4876, 0.4980,
         0.5069, 0.4926],
        [0.5079, 0.4893, 0.5115, 0.4859, 0.4872, 0.4929, 0.4895, 0.4953, 0.4985,
         0.5086, 0.5039, 0.4997, 0.5050, 0.4912, 0.5037, 0.4908, 0.4863, 0.4987,
         0.5060, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4926, 0.4909, 0.4923, 0.4956,
         0.5065, 0.5042, 0.4995, 0.5032, 0.4914, 0.5037, 0.4894, 0.4857, 0.4963,
         0.5055, 0.4936],
        [0.5113, 0.4917, 0.5101, 0.4865, 0.4863, 0.4944, 0.4908, 0.4938, 0.4970,
         0.5069, 0.5061, 0.5000, 0.5036, 0.4892, 0.5038, 0.4903, 0.4851, 0.4972,
         0.5063, 0.4932],
        [0.5095, 0.4911, 0.5125, 0.4884, 0.4856, 0.4935, 0.4888, 0.4942, 0.4976,
         0.5071, 0.5062, 0.4994, 0.5049, 0.4902, 0.5034, 0.4891, 0.4846, 0.4990,
         0.5072, 0.4932],
        [0.5101, 0.4911, 0.5110, 0.4853, 0.4863, 0.4928, 0.4892, 0.4950, 0.4999,
         0.5072, 0.5066, 0.5008, 0.5035, 0.4893, 0.5048, 0.4913, 0.4858, 0.4960,
         0.5076, 0.4931],
        [0.5108, 0.4924, 0.5108, 0.4852, 0.4882, 0.4929, 0.4896, 0.4940, 0.4977,
         0.5075, 0.5046, 0.4977, 0.5040, 0.4898, 0.5043, 0.4902, 0.4869, 0.4967,
         0.5083, 0.4936],
        [0.5113, 0.4917, 0.5117, 0.4874, 0.4877, 0.4928, 0.4892, 0.4933, 0.4999,
         0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5003, 0.4931, 0.4836, 0.4978,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5104, 0.4891, 0.5119, 0.4865, 0.4855, 0.4939, 0.4893, 0.4943, 0.4988,
        0.5073, 0.5026, 0.4998, 0.5024, 0.4914, 0.5043, 0.4914, 0.4858, 0.4987,
        0.5080, 0.4948], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4894, 0.5109, 0.4852, 0.4856, 0.4926, 0.4882, 0.4952, 0.4981,
        0.5051, 0.5035, 0.4997, 0.5033, 0.4914, 0.5027, 0.4890, 0.4862, 0.4985,
        0.5072, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5105, 0.4905, 0.5108, 0.4870, 0.4852, 0.4940, 0.4888, 0.4963, 0.5014,
        0.5068, 0.5048, 0.5002, 0.5061, 0.4918, 0.5039, 0.4911, 0.4876, 0.4980,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4893, 0.5115, 0.4859, 0.4872, 0.4929, 0.4895, 0.4953, 0.4985,
        0.5086, 0.5039, 0.4997, 0.5050, 0.4912, 0.5037, 0.4908, 0.4863, 0.4987,
        0.5060, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4926, 0.4909, 0.4923, 0.4956,
        0.5065, 0.5042, 0.4995, 0.5032, 0.4914, 0.5037, 0.4894, 0.4857, 0.4963,
        0.5055, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4917, 0.5101, 0.4865, 0.4863, 0.4944, 0.4908, 0.4938, 0.4970,
        0.5069, 0.5061, 0.5000, 0.5036, 0.4892, 0.5038, 0.4903, 0.4851, 0.4972,
        0.5063, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4911, 0.5125, 0.4884, 0.4856, 0.4935, 0.4888, 0.4942, 0.4976,
        0.5071, 0.5062, 0.4994, 0.5049, 0.4902, 0.5034, 0.4891, 0.4846, 0.4990,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4911, 0.5110, 0.4853, 0.4863, 0.4928, 0.4892, 0.4950, 0.4999,
        0.5072, 0.5066, 0.5008, 0.5035, 0.4893, 0.5048, 0.4913, 0.4858, 0.4960,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5108, 0.4924, 0.5108, 0.4852, 0.4882, 0.4929, 0.4896, 0.4940, 0.4977,
        0.5075, 0.5046, 0.4977, 0.5040, 0.4898, 0.5043, 0.4902, 0.4869, 0.4967,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4917, 0.5117, 0.4874, 0.4877, 0.4928, 0.4892, 0.4933, 0.4999,
        0.5050, 0.5050, 0.4986, 0.5035, 0.4888, 0.5003, 0.4931, 0.4836, 0.4978,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5127, 0.4963, 0.4965, 0.5025, 0.5066, 0.4925, 0.5091, 0.4952, 0.4969,
         0.5026, 0.4856, 0.4975, 0.4839, 0.5025, 0.5025, 0.4985, 0.4993, 0.5141,
         0.4936, 0.4968],
        [0.5115, 0.4959, 0.4953, 0.5046, 0.5035, 0.4930, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4966, 0.4860, 0.5014, 0.5003, 0.4978, 0.4981, 0.5147,
         0.4948, 0.4971],
        [0.5131, 0.4974, 0.4973, 0.5023, 0.5047, 0.4922, 0.5102, 0.4979, 0.4964,
         0.5028, 0.4865, 0.4990, 0.4848, 0.5014, 0.4999, 0.4975, 0.4991, 0.5166,
         0.4948, 0.4971],
        [0.5126, 0.4973, 0.4967, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4964,
         0.5027, 0.4843, 0.4964, 0.4847, 0.5022, 0.5009, 0.4973, 0.5010, 0.5143,
         0.4954, 0.4969],
        [0.5126, 0.4982, 0.4961, 0.5038, 0.5032, 0.4935, 0.5087, 0.4953, 0.4970,
         0.5032, 0.4875, 0.4981, 0.4826, 0.5025, 0.5003, 0.4991, 0.5000, 0.5151,
         0.4956, 0.4973],
        [0.5136, 0.4972, 0.4958, 0.5030, 0.5054, 0.4927, 0.5078, 0.4969, 0.4956,
         0.5025, 0.4856, 0.4962, 0.4855, 0.5032, 0.5020, 0.5002, 0.4993, 0.5145,
         0.4951, 0.4981],
        [0.5113, 0.4964, 0.4979, 0.5026, 0.5039, 0.4923, 0.5089, 0.4963, 0.4961,
         0.5027, 0.4882, 0.4971, 0.4850, 0.5027, 0.5010, 0.4987, 0.4998, 0.5137,
         0.4952, 0.4996],
        [0.5120, 0.4971, 0.4950, 0.5013, 0.5040, 0.4923, 0.5084, 0.4944, 0.4970,
         0.5024, 0.4858, 0.4985, 0.4864, 0.5011, 0.5009, 0.4984, 0.4996, 0.5153,
         0.4958, 0.4984],
        [0.5127, 0.4981, 0.4951, 0.5012, 0.5046, 0.4925, 0.5097, 0.4967, 0.4968,
         0.5019, 0.4885, 0.4949, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5139,
         0.4956, 0.4992],
        [0.5126, 0.4985, 0.4940, 0.5029, 0.5047, 0.4939, 0.5081, 0.4988, 0.4964,
         0.5034, 0.4885, 0.4982, 0.4857, 0.5001, 0.5008, 0.4982, 0.4992, 0.5155,
         0.4971, 0.5002]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5127, 0.4963, 0.4965, 0.5025, 0.5066, 0.4925, 0.5091, 0.4952, 0.4969,
        0.5026, 0.4856, 0.4975, 0.4839, 0.5025, 0.5025, 0.4985, 0.4993, 0.5141,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4959, 0.4953, 0.5046, 0.5035, 0.4930, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4966, 0.4860, 0.5014, 0.5003, 0.4978, 0.4981, 0.5147,
        0.4948, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4974, 0.4973, 0.5023, 0.5047, 0.4922, 0.5102, 0.4979, 0.4964,
        0.5028, 0.4865, 0.4990, 0.4848, 0.5014, 0.4999, 0.4975, 0.4991, 0.5166,
        0.4948, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4973, 0.4967, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4964,
        0.5027, 0.4843, 0.4964, 0.4847, 0.5022, 0.5009, 0.4973, 0.5010, 0.5143,
        0.4954, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4982, 0.4961, 0.5038, 0.5032, 0.4935, 0.5087, 0.4953, 0.4970,
        0.5032, 0.4875, 0.4981, 0.4826, 0.5025, 0.5003, 0.4991, 0.5000, 0.5151,
        0.4956, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4972, 0.4958, 0.5030, 0.5054, 0.4927, 0.5078, 0.4969, 0.4956,
        0.5025, 0.4856, 0.4962, 0.4855, 0.5032, 0.5020, 0.5002, 0.4993, 0.5145,
        0.4951, 0.4981], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4964, 0.4979, 0.5026, 0.5039, 0.4923, 0.5089, 0.4963, 0.4961,
        0.5027, 0.4882, 0.4971, 0.4850, 0.5027, 0.5010, 0.4987, 0.4998, 0.5137,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4971, 0.4950, 0.5013, 0.5040, 0.4923, 0.5084, 0.4944, 0.4970,
        0.5024, 0.4858, 0.4985, 0.4864, 0.5011, 0.5009, 0.4984, 0.4996, 0.5153,
        0.4958, 0.4984], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4981, 0.4951, 0.5012, 0.5046, 0.4925, 0.5097, 0.4967, 0.4968,
        0.5019, 0.4885, 0.4949, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5139,
        0.4956, 0.4992], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4985, 0.4940, 0.5029, 0.5047, 0.4939, 0.5081, 0.4988, 0.4964,
        0.5034, 0.4885, 0.4982, 0.4857, 0.5001, 0.5008, 0.4982, 0.4992, 0.5155,
        0.4971, 0.5002], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [1, 8, 3, 2, 5, 0, 4, 6, 9, 7]
replay_buffer._size: [5100 5100 5100 5100 5100 5100 5100 5100 5100 5100]
2023-08-12 10:40:39,687 MainThread INFO: EPOCH:32
2023-08-12 10:40:39,687 MainThread INFO: Time Consumed:0.35869288444519043s
2023-08-12 10:40:39,687 MainThread INFO: Total Frames:49500s
 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 33/80 [00:33<00:34,  1.37it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1237.14563
Train_Epoch_Reward                    18562.03561
Running_Training_Average_Rewards      1226.60221
Explore_Time                          0.00396
Train___Time                          0.34861
Eval____Time                          0.00268
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.27560
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.20499
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.84956
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.81158
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.57684
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.07443
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -22.11219
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12623.19197
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.29671
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.53380
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.07320     0.98145    11.38597    8.79193
alpha_0                               0.95219      0.00040    0.95276     0.95162
alpha_1                               0.95219      0.00040    0.95276     0.95161
alpha_2                               0.95222      0.00041    0.95279     0.95165
alpha_3                               0.95220      0.00040    0.95277     0.95163
alpha_4                               0.95219      0.00040    0.95276     0.95162
alpha_5                               0.95219      0.00040    0.95276     0.95162
alpha_6                               0.95218      0.00041    0.95276     0.95161
alpha_7                               0.95219      0.00041    0.95276     0.95162
alpha_8                               0.95219      0.00041    0.95277     0.95162
alpha_9                               0.95221      0.00041    0.95278     0.95163
Alpha_loss                            -0.32746     0.00310    -0.32269    -0.33151
Training/policy_loss                  -2.70821     0.00851    -2.69322    -2.71751
Training/qf1_loss                     2135.09639   529.15225  2677.49023  1355.60474
Training/qf2_loss                     2134.52151   529.10171  2676.88281  1355.07959
Training/pf_norm                      0.10680      0.02704    0.13289     0.07173
Training/qf1_norm                     28.82816     2.23752    31.72910    25.91542
Training/qf2_norm                     30.70527     2.45978    33.72007    27.63587
log_std/mean                          -0.12397     0.00052    -0.12322    -0.12466
log_std/std                           0.01001      0.00005    0.01008     0.00993
log_std/max                           -0.10438     0.00044    -0.10367    -0.10486
log_std/min                           -0.14670     0.00082    -0.14554    -0.14750
log_probs/mean                        -2.72612     0.00878    -2.71101    -2.73579
log_probs/std                         0.22772      0.00392    0.23381     0.22146
log_probs/max                         -2.06957     0.04243    -2.01722    -2.11894
log_probs/min                         -4.55338     0.45246    -4.06699    -5.38350
mean/mean                             0.00212      0.00006    0.00221     0.00205
mean/std                              0.00509      0.00007    0.00515     0.00496
mean/max                              0.01120      0.00006    0.01128     0.01113
mean/min                              -0.00551     0.00024    -0.00509    -0.00572
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [6, 3, 1, 7, 8, 9, 4, 2, 0, 5]
replay_buffer._size: [5250 5250 5250 5250 5250 5250 5250 5250 5250 5250]
2023-08-12 10:40:40,195 MainThread INFO: EPOCH:33
2023-08-12 10:40:40,195 MainThread INFO: Time Consumed:0.38820362091064453s
2023-08-12 10:40:40,195 MainThread INFO: Total Frames:51000s
 42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 34/80 [00:33<00:30,  1.51it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1211.11919
Train_Epoch_Reward                    7299.82670
Running_Training_Average_Rewards      1050.07719
Explore_Time                          0.00333
Train___Time                          0.38167
Eval____Time                          0.00253
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -46.55663
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.31463
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.60726
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.94082
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.40298
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.94306
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.98312
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12361.02861
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.41415
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.67403
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.50449     0.79918    11.60007    9.63917
alpha_0                               0.95076      0.00040    0.95133     0.95019
alpha_1                               0.95076      0.00040    0.95133     0.95018
alpha_2                               0.95079      0.00040    0.95136     0.95022
alpha_3                               0.95077      0.00040    0.95134     0.95020
alpha_4                               0.95076      0.00040    0.95133     0.95019
alpha_5                               0.95076      0.00040    0.95133     0.95019
alpha_6                               0.95075      0.00040    0.95132     0.95018
alpha_7                               0.95076      0.00040    0.95133     0.95018
alpha_8                               0.95076      0.00040    0.95133     0.95019
alpha_9                               0.95077      0.00040    0.95135     0.95020
Alpha_loss                            -0.33783     0.00296    -0.33359    -0.34213
Training/policy_loss                  -2.71491     0.00456    -2.70896    -2.72043
Training/qf1_loss                     2082.51340   322.54335  2549.28296  1596.44434
Training/qf2_loss                     2081.84949   322.50502  2548.54126  1595.81702
Training/pf_norm                      0.13574      0.02096    0.16417     0.10699
Training/qf1_norm                     30.05441     1.85631    32.71511    28.11434
Training/qf2_norm                     32.27433     2.05534    35.26481    30.18137
log_std/mean                          -0.12563     0.00052    -0.12497    -0.12639
log_std/std                           0.01000      0.00003    0.01003     0.00995
log_std/max                           -0.10560     0.00034    -0.10520    -0.10617
log_std/min                           -0.14774     0.00086    -0.14653    -0.14891
log_probs/mean                        -2.73111     0.00441    -2.72515    -2.73623
log_probs/std                         0.24470      0.00461    0.25021     0.23824
log_probs/max                         -2.09090     0.04825    -2.01708    -2.13606
log_probs/min                         -5.00210     0.37524    -4.38411    -5.56107
mean/mean                             0.00254      0.00017    0.00272     0.00227
mean/std                              0.00511      0.00004    0.00515     0.00505
mean/max                              0.01102      0.00015    0.01119     0.01073
mean/min                              -0.00566     0.00003    -0.00561    -0.00570
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5064, 0.4988, 0.5062, 0.5070, 0.4884, 0.4955, 0.5018, 0.4999,
         0.5024, 0.5099, 0.5009, 0.4961, 0.4888, 0.4927, 0.5077, 0.4926, 0.5093,
         0.4962, 0.5141],
        [0.4982, 0.5071, 0.4989, 0.5062, 0.5067, 0.4897, 0.4959, 0.5026, 0.5023,
         0.5028, 0.5083, 0.4992, 0.4963, 0.4882, 0.4906, 0.5077, 0.4919, 0.5087,
         0.4964, 0.5127],
        [0.4979, 0.5070, 0.4981, 0.5065, 0.5065, 0.4886, 0.4964, 0.5004, 0.5042,
         0.5011, 0.5063, 0.4992, 0.4964, 0.4883, 0.4903, 0.5099, 0.4936, 0.5086,
         0.4955, 0.5149],
        [0.4987, 0.5058, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.4998, 0.5026,
         0.5037, 0.5060, 0.4985, 0.4972, 0.4879, 0.4895, 0.5079, 0.4923, 0.5100,
         0.4930, 0.5142],
        [0.4986, 0.5076, 0.4980, 0.5050, 0.5056, 0.4885, 0.4958, 0.4990, 0.5020,
         0.5025, 0.5087, 0.4995, 0.4969, 0.4883, 0.4918, 0.5062, 0.4944, 0.5097,
         0.4953, 0.5132],
        [0.4976, 0.5045, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.5000, 0.5010,
         0.5030, 0.5072, 0.4998, 0.4953, 0.4880, 0.4903, 0.5087, 0.4952, 0.5081,
         0.4948, 0.5135],
        [0.4979, 0.5055, 0.4984, 0.5044, 0.5061, 0.4875, 0.4973, 0.5007, 0.5034,
         0.5018, 0.5103, 0.4997, 0.4960, 0.4870, 0.4907, 0.5081, 0.4955, 0.5076,
         0.4968, 0.5126],
        [0.4979, 0.5046, 0.4983, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5031,
         0.5019, 0.5095, 0.4998, 0.4962, 0.4889, 0.4919, 0.5079, 0.4934, 0.5062,
         0.4933, 0.5128],
        [0.4981, 0.5049, 0.4974, 0.5051, 0.5079, 0.4881, 0.4961, 0.4999, 0.5018,
         0.5008, 0.5064, 0.5014, 0.4976, 0.4886, 0.4923, 0.5082, 0.4928, 0.5089,
         0.4941, 0.5144],
        [0.4984, 0.5057, 0.4994, 0.5050, 0.5073, 0.4883, 0.4961, 0.5016, 0.5020,
         0.5039, 0.5076, 0.5003, 0.4946, 0.4905, 0.4927, 0.5093, 0.4940, 0.5052,
         0.4952, 0.5153]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5064, 0.4988, 0.5062, 0.5070, 0.4884, 0.4955, 0.5018, 0.4999,
        0.5024, 0.5099, 0.5009, 0.4961, 0.4888, 0.4927, 0.5077, 0.4926, 0.5093,
        0.4962, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5071, 0.4989, 0.5062, 0.5067, 0.4897, 0.4959, 0.5026, 0.5023,
        0.5028, 0.5083, 0.4992, 0.4963, 0.4882, 0.4906, 0.5077, 0.4919, 0.5087,
        0.4964, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5070, 0.4981, 0.5065, 0.5065, 0.4886, 0.4964, 0.5004, 0.5042,
        0.5011, 0.5063, 0.4992, 0.4964, 0.4883, 0.4903, 0.5099, 0.4936, 0.5086,
        0.4955, 0.5149], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5058, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.4998, 0.5026,
        0.5037, 0.5060, 0.4985, 0.4972, 0.4879, 0.4895, 0.5079, 0.4923, 0.5100,
        0.4930, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5076, 0.4980, 0.5050, 0.5056, 0.4885, 0.4958, 0.4990, 0.5020,
        0.5025, 0.5087, 0.4995, 0.4969, 0.4883, 0.4918, 0.5062, 0.4944, 0.5097,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5045, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.5000, 0.5010,
        0.5030, 0.5072, 0.4998, 0.4953, 0.4880, 0.4903, 0.5087, 0.4952, 0.5081,
        0.4948, 0.5135], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5055, 0.4984, 0.5044, 0.5061, 0.4875, 0.4973, 0.5007, 0.5034,
        0.5018, 0.5103, 0.4997, 0.4960, 0.4870, 0.4907, 0.5081, 0.4955, 0.5076,
        0.4968, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5046, 0.4983, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5031,
        0.5019, 0.5095, 0.4998, 0.4962, 0.4889, 0.4919, 0.5079, 0.4934, 0.5062,
        0.4933, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5049, 0.4974, 0.5051, 0.5079, 0.4881, 0.4961, 0.4999, 0.5018,
        0.5008, 0.5064, 0.5014, 0.4976, 0.4886, 0.4923, 0.5082, 0.4928, 0.5089,
        0.4941, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4994, 0.5050, 0.5073, 0.4883, 0.4961, 0.5016, 0.5020,
        0.5039, 0.5076, 0.5003, 0.4946, 0.4905, 0.4927, 0.5093, 0.4940, 0.5052,
        0.4952, 0.5153], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5104, 0.4892, 0.5119, 0.4866, 0.4854, 0.4939, 0.4893, 0.4942, 0.4987,
         0.5074, 0.5025, 0.4998, 0.5024, 0.4915, 0.5044, 0.4912, 0.4858, 0.4988,
         0.5079, 0.4948],
        [0.5087, 0.4894, 0.5109, 0.4852, 0.4855, 0.4926, 0.4882, 0.4952, 0.4981,
         0.5052, 0.5034, 0.4997, 0.5034, 0.4915, 0.5028, 0.4889, 0.4862, 0.4985,
         0.5072, 0.4934],
        [0.5104, 0.4905, 0.5108, 0.4870, 0.4852, 0.4940, 0.4888, 0.4964, 0.5013,
         0.5068, 0.5047, 0.5002, 0.5060, 0.4918, 0.5039, 0.4911, 0.4875, 0.4980,
         0.5069, 0.4926],
        [0.5079, 0.4893, 0.5115, 0.4860, 0.4872, 0.4930, 0.4895, 0.4953, 0.4985,
         0.5086, 0.5039, 0.4997, 0.5050, 0.4912, 0.5038, 0.4907, 0.4863, 0.4987,
         0.5060, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
         0.5065, 0.5043, 0.4996, 0.5032, 0.4914, 0.5037, 0.4894, 0.4856, 0.4962,
         0.5054, 0.4936],
        [0.5113, 0.4917, 0.5101, 0.4866, 0.4863, 0.4945, 0.4907, 0.4938, 0.4971,
         0.5068, 0.5060, 0.5002, 0.5037, 0.4893, 0.5039, 0.4901, 0.4851, 0.4972,
         0.5063, 0.4932],
        [0.5094, 0.4911, 0.5125, 0.4884, 0.4857, 0.4935, 0.4888, 0.4942, 0.4976,
         0.5071, 0.5061, 0.4994, 0.5049, 0.4902, 0.5034, 0.4891, 0.4847, 0.4989,
         0.5072, 0.4932],
        [0.5101, 0.4910, 0.5110, 0.4853, 0.4864, 0.4927, 0.4891, 0.4950, 0.4999,
         0.5072, 0.5065, 0.5009, 0.5035, 0.4893, 0.5048, 0.4912, 0.4858, 0.4960,
         0.5076, 0.4931],
        [0.5108, 0.4924, 0.5108, 0.4851, 0.4883, 0.4929, 0.4896, 0.4940, 0.4977,
         0.5075, 0.5046, 0.4976, 0.5040, 0.4898, 0.5043, 0.4903, 0.4868, 0.4967,
         0.5084, 0.4936],
        [0.5113, 0.4918, 0.5117, 0.4873, 0.4877, 0.4928, 0.4892, 0.4933, 0.4999,
         0.5050, 0.5051, 0.4986, 0.5035, 0.4888, 0.5003, 0.4932, 0.4836, 0.4977,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5104, 0.4892, 0.5119, 0.4866, 0.4854, 0.4939, 0.4893, 0.4942, 0.4987,
        0.5074, 0.5025, 0.4998, 0.5024, 0.4915, 0.5044, 0.4912, 0.4858, 0.4988,
        0.5079, 0.4948], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4894, 0.5109, 0.4852, 0.4855, 0.4926, 0.4882, 0.4952, 0.4981,
        0.5052, 0.5034, 0.4997, 0.5034, 0.4915, 0.5028, 0.4889, 0.4862, 0.4985,
        0.5072, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5104, 0.4905, 0.5108, 0.4870, 0.4852, 0.4940, 0.4888, 0.4964, 0.5013,
        0.5068, 0.5047, 0.5002, 0.5060, 0.4918, 0.5039, 0.4911, 0.4875, 0.4980,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4893, 0.5115, 0.4860, 0.4872, 0.4930, 0.4895, 0.4953, 0.4985,
        0.5086, 0.5039, 0.4997, 0.5050, 0.4912, 0.5038, 0.4907, 0.4863, 0.4987,
        0.5060, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
        0.5065, 0.5043, 0.4996, 0.5032, 0.4914, 0.5037, 0.4894, 0.4856, 0.4962,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4917, 0.5101, 0.4866, 0.4863, 0.4945, 0.4907, 0.4938, 0.4971,
        0.5068, 0.5060, 0.5002, 0.5037, 0.4893, 0.5039, 0.4901, 0.4851, 0.4972,
        0.5063, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4911, 0.5125, 0.4884, 0.4857, 0.4935, 0.4888, 0.4942, 0.4976,
        0.5071, 0.5061, 0.4994, 0.5049, 0.4902, 0.5034, 0.4891, 0.4847, 0.4989,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4910, 0.5110, 0.4853, 0.4864, 0.4927, 0.4891, 0.4950, 0.4999,
        0.5072, 0.5065, 0.5009, 0.5035, 0.4893, 0.5048, 0.4912, 0.4858, 0.4960,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5108, 0.4924, 0.5108, 0.4851, 0.4883, 0.4929, 0.4896, 0.4940, 0.4977,
        0.5075, 0.5046, 0.4976, 0.5040, 0.4898, 0.5043, 0.4903, 0.4868, 0.4967,
        0.5084, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4918, 0.5117, 0.4873, 0.4877, 0.4928, 0.4892, 0.4933, 0.4999,
        0.5050, 0.5051, 0.4986, 0.5035, 0.4888, 0.5003, 0.4932, 0.4836, 0.4977,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5127, 0.4963, 0.4965, 0.5025, 0.5066, 0.4926, 0.5091, 0.4952, 0.4969,
         0.5027, 0.4856, 0.4975, 0.4839, 0.5024, 0.5025, 0.4984, 0.4992, 0.5141,
         0.4936, 0.4968],
        [0.5115, 0.4959, 0.4954, 0.5046, 0.5034, 0.4930, 0.5103, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4965, 0.4859, 0.5014, 0.5003, 0.4978, 0.4981, 0.5147,
         0.4947, 0.4971],
        [0.5131, 0.4974, 0.4973, 0.5023, 0.5047, 0.4923, 0.5102, 0.4979, 0.4964,
         0.5028, 0.4864, 0.4990, 0.4848, 0.5014, 0.4998, 0.4975, 0.4991, 0.5166,
         0.4947, 0.4970],
        [0.5126, 0.4973, 0.4968, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4964,
         0.5027, 0.4842, 0.4964, 0.4846, 0.5022, 0.5008, 0.4973, 0.5010, 0.5143,
         0.4954, 0.4969],
        [0.5125, 0.4982, 0.4962, 0.5038, 0.5032, 0.4935, 0.5088, 0.4953, 0.4970,
         0.5032, 0.4875, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
         0.4956, 0.4973],
        [0.5136, 0.4973, 0.4959, 0.5030, 0.5053, 0.4927, 0.5079, 0.4968, 0.4956,
         0.5026, 0.4856, 0.4962, 0.4854, 0.5032, 0.5021, 0.5001, 0.4994, 0.5145,
         0.4951, 0.4982],
        [0.5113, 0.4964, 0.4979, 0.5025, 0.5038, 0.4923, 0.5089, 0.4963, 0.4961,
         0.5027, 0.4882, 0.4971, 0.4850, 0.5027, 0.5010, 0.4987, 0.4998, 0.5137,
         0.4952, 0.4996],
        [0.5121, 0.4972, 0.4950, 0.5013, 0.5040, 0.4923, 0.5084, 0.4944, 0.4970,
         0.5024, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4985, 0.4996, 0.5153,
         0.4959, 0.4984],
        [0.5128, 0.4981, 0.4951, 0.5012, 0.5047, 0.4925, 0.5097, 0.4967, 0.4968,
         0.5020, 0.4885, 0.4950, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5139,
         0.4956, 0.4992],
        [0.5125, 0.4984, 0.4940, 0.5029, 0.5047, 0.4938, 0.5080, 0.4988, 0.4964,
         0.5034, 0.4885, 0.4982, 0.4857, 0.5001, 0.5009, 0.4982, 0.4993, 0.5155,
         0.4971, 0.5003]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5127, 0.4963, 0.4965, 0.5025, 0.5066, 0.4926, 0.5091, 0.4952, 0.4969,
        0.5027, 0.4856, 0.4975, 0.4839, 0.5024, 0.5025, 0.4984, 0.4992, 0.5141,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4959, 0.4954, 0.5046, 0.5034, 0.4930, 0.5103, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4965, 0.4859, 0.5014, 0.5003, 0.4978, 0.4981, 0.5147,
        0.4947, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4974, 0.4973, 0.5023, 0.5047, 0.4923, 0.5102, 0.4979, 0.4964,
        0.5028, 0.4864, 0.4990, 0.4848, 0.5014, 0.4998, 0.4975, 0.4991, 0.5166,
        0.4947, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4973, 0.4968, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4964,
        0.5027, 0.4842, 0.4964, 0.4846, 0.5022, 0.5008, 0.4973, 0.5010, 0.5143,
        0.4954, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4982, 0.4962, 0.5038, 0.5032, 0.4935, 0.5088, 0.4953, 0.4970,
        0.5032, 0.4875, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
        0.4956, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4973, 0.4959, 0.5030, 0.5053, 0.4927, 0.5079, 0.4968, 0.4956,
        0.5026, 0.4856, 0.4962, 0.4854, 0.5032, 0.5021, 0.5001, 0.4994, 0.5145,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4964, 0.4979, 0.5025, 0.5038, 0.4923, 0.5089, 0.4963, 0.4961,
        0.5027, 0.4882, 0.4971, 0.4850, 0.5027, 0.5010, 0.4987, 0.4998, 0.5137,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5121, 0.4972, 0.4950, 0.5013, 0.5040, 0.4923, 0.5084, 0.4944, 0.4970,
        0.5024, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4985, 0.4996, 0.5153,
        0.4959, 0.4984], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4951, 0.5012, 0.5047, 0.4925, 0.5097, 0.4967, 0.4968,
        0.5020, 0.4885, 0.4950, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5139,
        0.4956, 0.4992], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4984, 0.4940, 0.5029, 0.5047, 0.4938, 0.5080, 0.4988, 0.4964,
        0.5034, 0.4885, 0.4982, 0.4857, 0.5001, 0.5009, 0.4982, 0.4993, 0.5155,
        0.4971, 0.5003], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [5, 0, 3, 9, 2, 4, 1, 8, 6, 7]
replay_buffer._size: [5400 5400 5400 5400 5400 5400 5400 5400 5400 5400]
2023-08-12 10:40:41,053 MainThread INFO: EPOCH:34
2023-08-12 10:40:41,053 MainThread INFO: Time Consumed:0.29377031326293945s
2023-08-12 10:40:41,053 MainThread INFO: Total Frames:52500s
 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 35/80 [00:34<00:32,  1.39it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1208.72670
Train_Epoch_Reward                    42858.26134
Running_Training_Average_Rewards      2290.67079
Explore_Time                          0.00339
Train___Time                          0.28630
Eval____Time                          0.00236
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.13843
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.24043
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.62191
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.90905
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.38557
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.92965
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.96827
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12342.49923
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.33580
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.70311
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.89521     0.49391    11.74489    10.24373
alpha_0                               0.94933      0.00040    0.94991     0.94876
alpha_1                               0.94933      0.00040    0.94990     0.94876
alpha_2                               0.94936      0.00040    0.94993     0.94879
alpha_3                               0.94934      0.00040    0.94991     0.94877
alpha_4                               0.94933      0.00040    0.94990     0.94876
alpha_5                               0.94933      0.00040    0.94990     0.94876
alpha_6                               0.94932      0.00040    0.94989     0.94875
alpha_7                               0.94933      0.00040    0.94990     0.94876
alpha_8                               0.94933      0.00040    0.94990     0.94876
alpha_9                               0.94935      0.00040    0.94992     0.94877
Alpha_loss                            -0.34831     0.00315    -0.34394    -0.35278
Training/policy_loss                  -2.72375     0.00597    -2.71688    -2.73200
Training/qf1_loss                     2331.62319   356.21611  2996.27783  1930.18530
Training/qf2_loss                     2330.90627   356.26240  2995.62061  1929.40588
Training/pf_norm                      0.09881      0.02290    0.12757     0.06973
Training/qf1_norm                     31.13915     1.06979    33.08678    29.85743
Training/qf2_norm                     33.51901     0.89324    35.09418    32.57396
log_std/mean                          -0.12745     0.00044    -0.12682    -0.12805
log_std/std                           0.01012      0.00004    0.01018     0.01008
log_std/max                           -0.10674     0.00042    -0.10634    -0.10750
log_std/min                           -0.15015     0.00077    -0.14933    -0.15120
log_probs/mean                        -2.73816     0.00571    -2.73159    -2.74609
log_probs/std                         0.24114      0.01173    0.25959     0.22302
log_probs/max                         -2.11715     0.03727    -2.05249    -2.15911
log_probs/min                         -4.90792     0.65010    -4.31825    -6.16769
mean/mean                             0.00237      0.00020    0.00265     0.00209
mean/std                              0.00500      0.00004    0.00505     0.00495
mean/max                              0.00998      0.00037    0.01056     0.00952
mean/min                              -0.00593     0.00008    -0.00578    -0.00603
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [2, 8, 0, 6, 1, 3, 7, 5, 9, 4]
replay_buffer._size: [5550 5550 5550 5550 5550 5550 5550 5550 5550 5550]
2023-08-12 10:40:41,549 MainThread INFO: EPOCH:35
2023-08-12 10:40:41,550 MainThread INFO: Time Consumed:0.3790287971496582s
2023-08-12 10:40:41,550 MainThread INFO: Total Frames:54000s
 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 36/80 [00:35<00:28,  1.54it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1222.68338
Train_Epoch_Reward                    48113.18401
Running_Training_Average_Rewards      3275.70907
Explore_Time                          0.00768
Train___Time                          0.33531
Eval____Time                          0.00203
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.81796
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.07491
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.72892
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.73128
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.33161
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.88603
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.92504
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12478.10801
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.15590
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.62262
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.73187     0.69077    11.83538    10.03501
alpha_0                               0.94791      0.00040    0.94848     0.94734
alpha_1                               0.94790      0.00040    0.94847     0.94733
alpha_2                               0.94794      0.00040    0.94851     0.94737
alpha_3                               0.94791      0.00040    0.94848     0.94734
alpha_4                               0.94791      0.00040    0.94848     0.94734
alpha_5                               0.94791      0.00040    0.94848     0.94734
alpha_6                               0.94789      0.00040    0.94847     0.94732
alpha_7                               0.94790      0.00040    0.94847     0.94733
alpha_8                               0.94791      0.00040    0.94848     0.94734
alpha_9                               0.94792      0.00040    0.94849     0.94735
Alpha_loss                            -0.35816     0.00284    -0.35425    -0.36232
Training/policy_loss                  -2.72088     0.00216    -2.71773    -2.72389
Training/qf1_loss                     2233.05459   289.39629  2679.15771  1924.51233
Training/qf2_loss                     2232.34155   289.38060  2678.51270  1923.85376
Training/pf_norm                      0.13286      0.02976    0.16397     0.07674
Training/qf1_norm                     30.98422     1.64600    33.35302    29.22557
Training/qf2_norm                     33.27979     1.74181    35.26223    31.31842
log_std/mean                          -0.12891     0.00040    -0.12836    -0.12948
log_std/std                           0.01022      0.00003    0.01026     0.01017
log_std/max                           -0.10814     0.00033    -0.10767    -0.10861
log_std/min                           -0.15202     0.00096    -0.15052    -0.15347
log_probs/mean                        -2.73286     0.00216    -2.72946    -2.73547
log_probs/std                         0.24164      0.01242    0.25655     0.22582
log_probs/max                         -2.15879     0.04089    -2.09148    -2.21741
log_probs/min                         -4.95249     0.46842    -4.49805    -5.78115
mean/mean                             0.00188      0.00006    0.00196     0.00180
mean/std                              0.00461      0.00023    0.00491     0.00427
mean/max                              0.00871      0.00039    0.00926     0.00822
mean/min                              -0.00554     0.00039    -0.00491    -0.00601
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4974, 0.5064, 0.4988, 0.5062, 0.5070, 0.4885, 0.4955, 0.5019, 0.4999,
         0.5024, 0.5099, 0.5010, 0.4960, 0.4888, 0.4927, 0.5076, 0.4926, 0.5093,
         0.4963, 0.5141],
        [0.4982, 0.5071, 0.4989, 0.5062, 0.5067, 0.4897, 0.4960, 0.5026, 0.5022,
         0.5028, 0.5083, 0.4993, 0.4962, 0.4882, 0.4905, 0.5077, 0.4919, 0.5087,
         0.4964, 0.5127],
        [0.4979, 0.5071, 0.4981, 0.5066, 0.5065, 0.4886, 0.4964, 0.5004, 0.5042,
         0.5012, 0.5064, 0.4991, 0.4964, 0.4882, 0.4903, 0.5099, 0.4935, 0.5087,
         0.4955, 0.5148],
        [0.4987, 0.5059, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.4999, 0.5025,
         0.5038, 0.5060, 0.4984, 0.4972, 0.4879, 0.4894, 0.5080, 0.4922, 0.5100,
         0.4930, 0.5142],
        [0.4986, 0.5076, 0.4980, 0.5051, 0.5056, 0.4885, 0.4958, 0.4990, 0.5020,
         0.5025, 0.5087, 0.4995, 0.4968, 0.4882, 0.4918, 0.5062, 0.4945, 0.5097,
         0.4953, 0.5132],
        [0.4975, 0.5045, 0.4977, 0.5061, 0.5053, 0.4883, 0.4947, 0.5000, 0.5009,
         0.5030, 0.5073, 0.4998, 0.4953, 0.4880, 0.4902, 0.5087, 0.4953, 0.5081,
         0.4949, 0.5135],
        [0.4978, 0.5055, 0.4984, 0.5044, 0.5061, 0.4875, 0.4974, 0.5008, 0.5034,
         0.5018, 0.5103, 0.4997, 0.4960, 0.4870, 0.4908, 0.5081, 0.4955, 0.5076,
         0.4968, 0.5127],
        [0.4979, 0.5046, 0.4984, 0.5050, 0.5065, 0.4880, 0.4954, 0.5011, 0.5031,
         0.5019, 0.5095, 0.4998, 0.4962, 0.4888, 0.4919, 0.5080, 0.4935, 0.5062,
         0.4934, 0.5128],
        [0.4981, 0.5048, 0.4973, 0.5050, 0.5079, 0.4881, 0.4961, 0.5000, 0.5019,
         0.5008, 0.5064, 0.5013, 0.4976, 0.4886, 0.4923, 0.5082, 0.4928, 0.5089,
         0.4941, 0.5144],
        [0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4883, 0.4961, 0.5017, 0.5020,
         0.5039, 0.5076, 0.5003, 0.4945, 0.4905, 0.4927, 0.5093, 0.4940, 0.5051,
         0.4952, 0.5153]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4974, 0.5064, 0.4988, 0.5062, 0.5070, 0.4885, 0.4955, 0.5019, 0.4999,
        0.5024, 0.5099, 0.5010, 0.4960, 0.4888, 0.4927, 0.5076, 0.4926, 0.5093,
        0.4963, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5071, 0.4989, 0.5062, 0.5067, 0.4897, 0.4960, 0.5026, 0.5022,
        0.5028, 0.5083, 0.4993, 0.4962, 0.4882, 0.4905, 0.5077, 0.4919, 0.5087,
        0.4964, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5071, 0.4981, 0.5066, 0.5065, 0.4886, 0.4964, 0.5004, 0.5042,
        0.5012, 0.5064, 0.4991, 0.4964, 0.4882, 0.4903, 0.5099, 0.4935, 0.5087,
        0.4955, 0.5148], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5059, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.4999, 0.5025,
        0.5038, 0.5060, 0.4984, 0.4972, 0.4879, 0.4894, 0.5080, 0.4922, 0.5100,
        0.4930, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5076, 0.4980, 0.5051, 0.5056, 0.4885, 0.4958, 0.4990, 0.5020,
        0.5025, 0.5087, 0.4995, 0.4968, 0.4882, 0.4918, 0.5062, 0.4945, 0.5097,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4975, 0.5045, 0.4977, 0.5061, 0.5053, 0.4883, 0.4947, 0.5000, 0.5009,
        0.5030, 0.5073, 0.4998, 0.4953, 0.4880, 0.4902, 0.5087, 0.4953, 0.5081,
        0.4949, 0.5135], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5055, 0.4984, 0.5044, 0.5061, 0.4875, 0.4974, 0.5008, 0.5034,
        0.5018, 0.5103, 0.4997, 0.4960, 0.4870, 0.4908, 0.5081, 0.4955, 0.5076,
        0.4968, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5046, 0.4984, 0.5050, 0.5065, 0.4880, 0.4954, 0.5011, 0.5031,
        0.5019, 0.5095, 0.4998, 0.4962, 0.4888, 0.4919, 0.5080, 0.4935, 0.5062,
        0.4934, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5048, 0.4973, 0.5050, 0.5079, 0.4881, 0.4961, 0.5000, 0.5019,
        0.5008, 0.5064, 0.5013, 0.4976, 0.4886, 0.4923, 0.5082, 0.4928, 0.5089,
        0.4941, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4883, 0.4961, 0.5017, 0.5020,
        0.5039, 0.5076, 0.5003, 0.4945, 0.4905, 0.4927, 0.5093, 0.4940, 0.5051,
        0.4952, 0.5153], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5104, 0.4891, 0.5120, 0.4865, 0.4854, 0.4939, 0.4893, 0.4943, 0.4987,
         0.5074, 0.5026, 0.4999, 0.5024, 0.4915, 0.5044, 0.4913, 0.4858, 0.4987,
         0.5080, 0.4948],
        [0.5087, 0.4894, 0.5109, 0.4851, 0.4855, 0.4926, 0.4883, 0.4953, 0.4980,
         0.5052, 0.5035, 0.4998, 0.5033, 0.4915, 0.5028, 0.4890, 0.4862, 0.4985,
         0.5072, 0.4934],
        [0.5104, 0.4905, 0.5109, 0.4869, 0.4852, 0.4941, 0.4888, 0.4964, 0.5013,
         0.5069, 0.5048, 0.5002, 0.5060, 0.4918, 0.5039, 0.4911, 0.4875, 0.4981,
         0.5069, 0.4926],
        [0.5078, 0.4894, 0.5115, 0.4859, 0.4871, 0.4930, 0.4896, 0.4953, 0.4985,
         0.5086, 0.5039, 0.4998, 0.5050, 0.4912, 0.5038, 0.4907, 0.4863, 0.4986,
         0.5060, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
         0.5065, 0.5043, 0.4996, 0.5031, 0.4914, 0.5037, 0.4894, 0.4856, 0.4963,
         0.5054, 0.4936],
        [0.5112, 0.4917, 0.5101, 0.4865, 0.4863, 0.4944, 0.4907, 0.4938, 0.4970,
         0.5069, 0.5061, 0.5001, 0.5037, 0.4893, 0.5039, 0.4902, 0.4851, 0.4972,
         0.5063, 0.4931],
        [0.5094, 0.4911, 0.5125, 0.4883, 0.4856, 0.4936, 0.4888, 0.4942, 0.4976,
         0.5071, 0.5061, 0.4995, 0.5049, 0.4902, 0.5035, 0.4891, 0.4847, 0.4989,
         0.5072, 0.4932],
        [0.5101, 0.4911, 0.5111, 0.4853, 0.4864, 0.4927, 0.4891, 0.4950, 0.4998,
         0.5072, 0.5065, 0.5009, 0.5035, 0.4894, 0.5048, 0.4912, 0.4857, 0.4960,
         0.5076, 0.4931],
        [0.5109, 0.4924, 0.5109, 0.4851, 0.4883, 0.4929, 0.4896, 0.4940, 0.4977,
         0.5075, 0.5047, 0.4976, 0.5040, 0.4897, 0.5043, 0.4904, 0.4869, 0.4967,
         0.5083, 0.4936],
        [0.5113, 0.4918, 0.5118, 0.4873, 0.4877, 0.4928, 0.4892, 0.4933, 0.4999,
         0.5050, 0.5051, 0.4986, 0.5035, 0.4888, 0.5003, 0.4932, 0.4836, 0.4977,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5104, 0.4891, 0.5120, 0.4865, 0.4854, 0.4939, 0.4893, 0.4943, 0.4987,
        0.5074, 0.5026, 0.4999, 0.5024, 0.4915, 0.5044, 0.4913, 0.4858, 0.4987,
        0.5080, 0.4948], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4894, 0.5109, 0.4851, 0.4855, 0.4926, 0.4883, 0.4953, 0.4980,
        0.5052, 0.5035, 0.4998, 0.5033, 0.4915, 0.5028, 0.4890, 0.4862, 0.4985,
        0.5072, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5104, 0.4905, 0.5109, 0.4869, 0.4852, 0.4941, 0.4888, 0.4964, 0.5013,
        0.5069, 0.5048, 0.5002, 0.5060, 0.4918, 0.5039, 0.4911, 0.4875, 0.4981,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4894, 0.5115, 0.4859, 0.4871, 0.4930, 0.4896, 0.4953, 0.4985,
        0.5086, 0.5039, 0.4998, 0.5050, 0.4912, 0.5038, 0.4907, 0.4863, 0.4986,
        0.5060, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
        0.5065, 0.5043, 0.4996, 0.5031, 0.4914, 0.5037, 0.4894, 0.4856, 0.4963,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4917, 0.5101, 0.4865, 0.4863, 0.4944, 0.4907, 0.4938, 0.4970,
        0.5069, 0.5061, 0.5001, 0.5037, 0.4893, 0.5039, 0.4902, 0.4851, 0.4972,
        0.5063, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4911, 0.5125, 0.4883, 0.4856, 0.4936, 0.4888, 0.4942, 0.4976,
        0.5071, 0.5061, 0.4995, 0.5049, 0.4902, 0.5035, 0.4891, 0.4847, 0.4989,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4911, 0.5111, 0.4853, 0.4864, 0.4927, 0.4891, 0.4950, 0.4998,
        0.5072, 0.5065, 0.5009, 0.5035, 0.4894, 0.5048, 0.4912, 0.4857, 0.4960,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4924, 0.5109, 0.4851, 0.4883, 0.4929, 0.4896, 0.4940, 0.4977,
        0.5075, 0.5047, 0.4976, 0.5040, 0.4897, 0.5043, 0.4904, 0.4869, 0.4967,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4918, 0.5118, 0.4873, 0.4877, 0.4928, 0.4892, 0.4933, 0.4999,
        0.5050, 0.5051, 0.4986, 0.5035, 0.4888, 0.5003, 0.4932, 0.4836, 0.4977,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5127, 0.4963, 0.4965, 0.5025, 0.5066, 0.4926, 0.5091, 0.4953, 0.4969,
         0.5027, 0.4855, 0.4976, 0.4839, 0.5024, 0.5024, 0.4984, 0.4992, 0.5142,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4954, 0.5046, 0.5035, 0.4931, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4966, 0.4859, 0.5014, 0.5002, 0.4979, 0.4981, 0.5147,
         0.4947, 0.4971],
        [0.5131, 0.4973, 0.4973, 0.5023, 0.5047, 0.4922, 0.5102, 0.4979, 0.4964,
         0.5028, 0.4864, 0.4989, 0.4848, 0.5015, 0.4998, 0.4975, 0.4991, 0.5166,
         0.4947, 0.4970],
        [0.5125, 0.4973, 0.4968, 0.5033, 0.5041, 0.4914, 0.5110, 0.4966, 0.4963,
         0.5027, 0.4842, 0.4964, 0.4846, 0.5022, 0.5008, 0.4974, 0.5010, 0.5143,
         0.4953, 0.4969],
        [0.5126, 0.4982, 0.4962, 0.5038, 0.5033, 0.4936, 0.5087, 0.4953, 0.4969,
         0.5032, 0.4874, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
         0.4955, 0.4973],
        [0.5136, 0.4972, 0.4959, 0.5031, 0.5053, 0.4927, 0.5078, 0.4968, 0.4956,
         0.5025, 0.4856, 0.4962, 0.4854, 0.5032, 0.5019, 0.5002, 0.4994, 0.5145,
         0.4951, 0.4982],
        [0.5113, 0.4964, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4963, 0.4961,
         0.5027, 0.4881, 0.4971, 0.4849, 0.5027, 0.5009, 0.4987, 0.4999, 0.5138,
         0.4953, 0.4996],
        [0.5121, 0.4972, 0.4950, 0.5012, 0.5040, 0.4923, 0.5084, 0.4945, 0.4970,
         0.5024, 0.4858, 0.4986, 0.4865, 0.5011, 0.5008, 0.4985, 0.4996, 0.5153,
         0.4958, 0.4984],
        [0.5128, 0.4981, 0.4951, 0.5012, 0.5047, 0.4924, 0.5096, 0.4967, 0.4968,
         0.5019, 0.4885, 0.4950, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5139,
         0.4957, 0.4992],
        [0.5126, 0.4984, 0.4939, 0.5029, 0.5048, 0.4938, 0.5080, 0.4988, 0.4964,
         0.5034, 0.4885, 0.4981, 0.4857, 0.5001, 0.5009, 0.4982, 0.4993, 0.5155,
         0.4970, 0.5003]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5127, 0.4963, 0.4965, 0.5025, 0.5066, 0.4926, 0.5091, 0.4953, 0.4969,
        0.5027, 0.4855, 0.4976, 0.4839, 0.5024, 0.5024, 0.4984, 0.4992, 0.5142,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4954, 0.5046, 0.5035, 0.4931, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4966, 0.4859, 0.5014, 0.5002, 0.4979, 0.4981, 0.5147,
        0.4947, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4973, 0.4973, 0.5023, 0.5047, 0.4922, 0.5102, 0.4979, 0.4964,
        0.5028, 0.4864, 0.4989, 0.4848, 0.5015, 0.4998, 0.4975, 0.4991, 0.5166,
        0.4947, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4973, 0.4968, 0.5033, 0.5041, 0.4914, 0.5110, 0.4966, 0.4963,
        0.5027, 0.4842, 0.4964, 0.4846, 0.5022, 0.5008, 0.4974, 0.5010, 0.5143,
        0.4953, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4982, 0.4962, 0.5038, 0.5033, 0.4936, 0.5087, 0.4953, 0.4969,
        0.5032, 0.4874, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
        0.4955, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4972, 0.4959, 0.5031, 0.5053, 0.4927, 0.5078, 0.4968, 0.4956,
        0.5025, 0.4856, 0.4962, 0.4854, 0.5032, 0.5019, 0.5002, 0.4994, 0.5145,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4964, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4963, 0.4961,
        0.5027, 0.4881, 0.4971, 0.4849, 0.5027, 0.5009, 0.4987, 0.4999, 0.5138,
        0.4953, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5121, 0.4972, 0.4950, 0.5012, 0.5040, 0.4923, 0.5084, 0.4945, 0.4970,
        0.5024, 0.4858, 0.4986, 0.4865, 0.5011, 0.5008, 0.4985, 0.4996, 0.5153,
        0.4958, 0.4984], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4951, 0.5012, 0.5047, 0.4924, 0.5096, 0.4967, 0.4968,
        0.5019, 0.4885, 0.4950, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5139,
        0.4957, 0.4992], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4984, 0.4939, 0.5029, 0.5048, 0.4938, 0.5080, 0.4988, 0.4964,
        0.5034, 0.4885, 0.4981, 0.4857, 0.5001, 0.5009, 0.4982, 0.4993, 0.5155,
        0.4970, 0.5003], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [8, 2, 9, 7, 5, 1, 3, 0, 4, 6]
replay_buffer._size: [5700 5700 5700 5700 5700 5700 5700 5700 5700 5700]
2023-08-12 10:40:42,333 MainThread INFO: EPOCH:36
2023-08-12 10:40:42,334 MainThread INFO: Time Consumed:0.4014749526977539s
2023-08-12 10:40:42,334 MainThread INFO: Total Frames:55500s
 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 37/80 [00:35<00:29,  1.44it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1238.19490
Train_Epoch_Reward                    12058.64654
Running_Training_Average_Rewards      3434.33640
Explore_Time                          0.00385
Train___Time                          0.39359
Eval____Time                          0.00253
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.83051
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.18620
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.70627
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.63852
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.20276
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.78585
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.82720
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12634.81855
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.26334
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.42891
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.03883     1.02430    12.21456    9.64879
alpha_0                               0.94648      0.00040    0.94705     0.94591
alpha_1                               0.94648      0.00040    0.94705     0.94591
alpha_2                               0.94651      0.00040    0.94708     0.94594
alpha_3                               0.94649      0.00040    0.94706     0.94592
alpha_4                               0.94648      0.00040    0.94705     0.94591
alpha_5                               0.94648      0.00040    0.94705     0.94592
alpha_6                               0.94647      0.00040    0.94704     0.94590
alpha_7                               0.94648      0.00040    0.94704     0.94591
alpha_8                               0.94648      0.00040    0.94705     0.94591
alpha_9                               0.94649      0.00040    0.94706     0.94592
Alpha_loss                            -0.36821     0.00264    -0.36454    -0.37215
Training/policy_loss                  -2.72247     0.00472    -2.71707    -2.72804
Training/qf1_loss                     2416.84805   483.35987  3126.23633  1770.23438
Training/qf2_loss                     2416.05710   483.34713  3125.39917  1769.49255
Training/pf_norm                      0.16604      0.02091    0.19071     0.13847
Training/qf1_norm                     31.98904     2.37729    34.66775    28.82649
Training/qf2_norm                     34.56493     2.38343    37.35435    31.24668
log_std/mean                          -0.13022     0.00032    -0.12975    -0.13064
log_std/std                           0.01021      0.00005    0.01027     0.01015
log_std/max                           -0.10965     0.00029    -0.10914    -0.10996
log_std/min                           -0.15388     0.00019    -0.15372    -0.15422
log_probs/mean                        -2.73152     0.00539    -2.72572    -2.73845
log_probs/std                         0.23650      0.00645    0.24437     0.22908
log_probs/max                         -2.12732     0.03557    -2.07334    -2.18255
log_probs/min                         -5.01962     0.70398    -4.28901    -6.31003
mean/mean                             0.00233      0.00010    0.00244     0.00214
mean/std                              0.00398      0.00016    0.00414     0.00374
mean/max                              0.00779      0.00017    0.00805     0.00758
mean/min                              -0.00393     0.00048    -0.00320    -0.00450
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [7, 0, 4, 3, 1, 2, 8, 5, 9, 6]
replay_buffer._size: [5850 5850 5850 5850 5850 5850 5850 5850 5850 5850]
2023-08-12 10:40:42,892 MainThread INFO: EPOCH:37
2023-08-12 10:40:42,892 MainThread INFO: Time Consumed:0.45623159408569336s
2023-08-12 10:40:42,892 MainThread INFO: Total Frames:57000s
 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 38/80 [00:36<00:27,  1.52it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1250.46737
Train_Epoch_Reward                    19921.31052
Running_Training_Average_Rewards      2669.77137
Explore_Time                          0.00238
Train___Time                          0.44925
Eval____Time                          0.00248
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.30963
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.16472
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.71663
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.50048
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.06512
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.67570
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.71979
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12757.35886
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.23385
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.29924
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.37698     1.08037    12.71236    9.60237
alpha_0                               0.94506      0.00040    0.94563     0.94449
alpha_1                               0.94505      0.00040    0.94562     0.94449
alpha_2                               0.94509      0.00040    0.94566     0.94452
alpha_3                               0.94506      0.00040    0.94563     0.94450
alpha_4                               0.94506      0.00040    0.94563     0.94449
alpha_5                               0.94506      0.00040    0.94563     0.94449
alpha_6                               0.94505      0.00040    0.94562     0.94448
alpha_7                               0.94505      0.00040    0.94562     0.94449
alpha_8                               0.94506      0.00040    0.94563     0.94449
alpha_9                               0.94507      0.00040    0.94564     0.94450
Alpha_loss                            -0.37829     0.00258    -0.37448    -0.38199
Training/policy_loss                  -2.72416     0.00499    -2.71729    -2.73002
Training/qf1_loss                     2530.32505   364.35442  3100.97998  1990.34253
Training/qf2_loss                     2529.37930   364.33492  3100.08203  1989.45703
Training/pf_norm                      0.12368      0.03126    0.16989     0.07412
Training/qf1_norm                     33.08862     2.65888    36.36015    28.74239
Training/qf2_norm                     36.27019     2.81447    39.19982    31.76106
log_std/mean                          -0.13120     0.00026    -0.13089    -0.13159
log_std/std                           0.01017      0.00004    0.01021     0.01013
log_std/max                           -0.11083     0.00032    -0.11036    -0.11116
log_std/min                           -0.15408     0.00046    -0.15338    -0.15461
log_probs/mean                        -2.73080     0.00601    -2.72273    -2.73792
log_probs/std                         0.23810      0.01934    0.26695     0.21378
log_probs/max                         -2.11968     0.02110    -2.09843    -2.15248
log_probs/min                         -5.34596     0.99249    -4.42058    -7.14156
mean/mean                             0.00211      0.00013    0.00232     0.00196
mean/std                              0.00325      0.00016    0.00354     0.00308
mean/max                              0.00760      0.00009    0.00776     0.00751
mean/min                              -0.00272     0.00019    -0.00243    -0.00298
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4973, 0.5064, 0.4989, 0.5063, 0.5069, 0.4885, 0.4954, 0.5018, 0.4999,
         0.5024, 0.5099, 0.5009, 0.4961, 0.4888, 0.4926, 0.5077, 0.4926, 0.5093,
         0.4962, 0.5141],
        [0.4982, 0.5072, 0.4989, 0.5062, 0.5067, 0.4897, 0.4959, 0.5027, 0.5022,
         0.5028, 0.5083, 0.4992, 0.4963, 0.4882, 0.4905, 0.5077, 0.4919, 0.5087,
         0.4964, 0.5127],
        [0.4979, 0.5071, 0.4982, 0.5066, 0.5064, 0.4887, 0.4964, 0.5004, 0.5042,
         0.5012, 0.5065, 0.4991, 0.4964, 0.4882, 0.4903, 0.5098, 0.4935, 0.5087,
         0.4956, 0.5148],
        [0.4987, 0.5059, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.4999, 0.5025,
         0.5037, 0.5060, 0.4985, 0.4972, 0.4879, 0.4895, 0.5080, 0.4922, 0.5100,
         0.4930, 0.5142],
        [0.4986, 0.5076, 0.4980, 0.5051, 0.5056, 0.4886, 0.4958, 0.4990, 0.5020,
         0.5025, 0.5087, 0.4995, 0.4968, 0.4882, 0.4918, 0.5062, 0.4945, 0.5098,
         0.4953, 0.5132],
        [0.4975, 0.5046, 0.4977, 0.5060, 0.5053, 0.4883, 0.4947, 0.5000, 0.5009,
         0.5029, 0.5073, 0.4999, 0.4953, 0.4880, 0.4902, 0.5086, 0.4953, 0.5081,
         0.4950, 0.5135],
        [0.4978, 0.5056, 0.4984, 0.5044, 0.5061, 0.4876, 0.4973, 0.5007, 0.5033,
         0.5017, 0.5104, 0.4997, 0.4960, 0.4870, 0.4908, 0.5082, 0.4955, 0.5077,
         0.4969, 0.5127],
        [0.4979, 0.5046, 0.4984, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5030,
         0.5019, 0.5096, 0.4998, 0.4962, 0.4888, 0.4919, 0.5079, 0.4934, 0.5062,
         0.4934, 0.5127],
        [0.4981, 0.5048, 0.4974, 0.5050, 0.5079, 0.4881, 0.4961, 0.5000, 0.5018,
         0.5008, 0.5065, 0.5013, 0.4976, 0.4886, 0.4922, 0.5082, 0.4928, 0.5088,
         0.4941, 0.5143],
        [0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4883, 0.4961, 0.5016, 0.5020,
         0.5039, 0.5076, 0.5003, 0.4945, 0.4905, 0.4927, 0.5093, 0.4940, 0.5051,
         0.4952, 0.5153]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4973, 0.5064, 0.4989, 0.5063, 0.5069, 0.4885, 0.4954, 0.5018, 0.4999,
        0.5024, 0.5099, 0.5009, 0.4961, 0.4888, 0.4926, 0.5077, 0.4926, 0.5093,
        0.4962, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5072, 0.4989, 0.5062, 0.5067, 0.4897, 0.4959, 0.5027, 0.5022,
        0.5028, 0.5083, 0.4992, 0.4963, 0.4882, 0.4905, 0.5077, 0.4919, 0.5087,
        0.4964, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5071, 0.4982, 0.5066, 0.5064, 0.4887, 0.4964, 0.5004, 0.5042,
        0.5012, 0.5065, 0.4991, 0.4964, 0.4882, 0.4903, 0.5098, 0.4935, 0.5087,
        0.4956, 0.5148], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5059, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.4999, 0.5025,
        0.5037, 0.5060, 0.4985, 0.4972, 0.4879, 0.4895, 0.5080, 0.4922, 0.5100,
        0.4930, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5076, 0.4980, 0.5051, 0.5056, 0.4886, 0.4958, 0.4990, 0.5020,
        0.5025, 0.5087, 0.4995, 0.4968, 0.4882, 0.4918, 0.5062, 0.4945, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4975, 0.5046, 0.4977, 0.5060, 0.5053, 0.4883, 0.4947, 0.5000, 0.5009,
        0.5029, 0.5073, 0.4999, 0.4953, 0.4880, 0.4902, 0.5086, 0.4953, 0.5081,
        0.4950, 0.5135], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5056, 0.4984, 0.5044, 0.5061, 0.4876, 0.4973, 0.5007, 0.5033,
        0.5017, 0.5104, 0.4997, 0.4960, 0.4870, 0.4908, 0.5082, 0.4955, 0.5077,
        0.4969, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5046, 0.4984, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5030,
        0.5019, 0.5096, 0.4998, 0.4962, 0.4888, 0.4919, 0.5079, 0.4934, 0.5062,
        0.4934, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5048, 0.4974, 0.5050, 0.5079, 0.4881, 0.4961, 0.5000, 0.5018,
        0.5008, 0.5065, 0.5013, 0.4976, 0.4886, 0.4922, 0.5082, 0.4928, 0.5088,
        0.4941, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4883, 0.4961, 0.5016, 0.5020,
        0.5039, 0.5076, 0.5003, 0.4945, 0.4905, 0.4927, 0.5093, 0.4940, 0.5051,
        0.4952, 0.5153], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5103, 0.4891, 0.5120, 0.4865, 0.4854, 0.4940, 0.4894, 0.4943, 0.4987,
         0.5074, 0.5026, 0.4999, 0.5024, 0.4916, 0.5044, 0.4914, 0.4858, 0.4987,
         0.5079, 0.4948],
        [0.5087, 0.4894, 0.5109, 0.4851, 0.4855, 0.4926, 0.4883, 0.4953, 0.4981,
         0.5052, 0.5035, 0.4998, 0.5033, 0.4915, 0.5028, 0.4889, 0.4862, 0.4985,
         0.5071, 0.4934],
        [0.5104, 0.4905, 0.5108, 0.4869, 0.4852, 0.4941, 0.4888, 0.4963, 0.5013,
         0.5069, 0.5048, 0.5002, 0.5060, 0.4919, 0.5039, 0.4910, 0.4875, 0.4981,
         0.5069, 0.4926],
        [0.5078, 0.4893, 0.5116, 0.4859, 0.4871, 0.4930, 0.4895, 0.4953, 0.4984,
         0.5086, 0.5038, 0.4998, 0.5050, 0.4913, 0.5038, 0.4906, 0.4862, 0.4987,
         0.5059, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
         0.5065, 0.5043, 0.4996, 0.5031, 0.4914, 0.5038, 0.4894, 0.4856, 0.4963,
         0.5054, 0.4936],
        [0.5112, 0.4916, 0.5101, 0.4865, 0.4863, 0.4944, 0.4908, 0.4938, 0.4969,
         0.5068, 0.5060, 0.5001, 0.5037, 0.4893, 0.5038, 0.4902, 0.4851, 0.4971,
         0.5062, 0.4932],
        [0.5095, 0.4912, 0.5125, 0.4883, 0.4857, 0.4936, 0.4889, 0.4942, 0.4976,
         0.5071, 0.5061, 0.4995, 0.5049, 0.4902, 0.5034, 0.4892, 0.4847, 0.4988,
         0.5072, 0.4932],
        [0.5101, 0.4911, 0.5111, 0.4853, 0.4865, 0.4927, 0.4891, 0.4950, 0.4998,
         0.5072, 0.5066, 0.5008, 0.5035, 0.4893, 0.5048, 0.4913, 0.4857, 0.4959,
         0.5076, 0.4931],
        [0.5109, 0.4924, 0.5109, 0.4851, 0.4884, 0.4929, 0.4896, 0.4941, 0.4977,
         0.5075, 0.5048, 0.4975, 0.5040, 0.4897, 0.5042, 0.4905, 0.4868, 0.4967,
         0.5084, 0.4936],
        [0.5113, 0.4918, 0.5118, 0.4873, 0.4878, 0.4928, 0.4892, 0.4933, 0.4999,
         0.5050, 0.5051, 0.4986, 0.5035, 0.4888, 0.5003, 0.4931, 0.4836, 0.4977,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5103, 0.4891, 0.5120, 0.4865, 0.4854, 0.4940, 0.4894, 0.4943, 0.4987,
        0.5074, 0.5026, 0.4999, 0.5024, 0.4916, 0.5044, 0.4914, 0.4858, 0.4987,
        0.5079, 0.4948], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4894, 0.5109, 0.4851, 0.4855, 0.4926, 0.4883, 0.4953, 0.4981,
        0.5052, 0.5035, 0.4998, 0.5033, 0.4915, 0.5028, 0.4889, 0.4862, 0.4985,
        0.5071, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5104, 0.4905, 0.5108, 0.4869, 0.4852, 0.4941, 0.4888, 0.4963, 0.5013,
        0.5069, 0.5048, 0.5002, 0.5060, 0.4919, 0.5039, 0.4910, 0.4875, 0.4981,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4893, 0.5116, 0.4859, 0.4871, 0.4930, 0.4895, 0.4953, 0.4984,
        0.5086, 0.5038, 0.4998, 0.5050, 0.4913, 0.5038, 0.4906, 0.4862, 0.4987,
        0.5059, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
        0.5065, 0.5043, 0.4996, 0.5031, 0.4914, 0.5038, 0.4894, 0.4856, 0.4963,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4916, 0.5101, 0.4865, 0.4863, 0.4944, 0.4908, 0.4938, 0.4969,
        0.5068, 0.5060, 0.5001, 0.5037, 0.4893, 0.5038, 0.4902, 0.4851, 0.4971,
        0.5062, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5125, 0.4883, 0.4857, 0.4936, 0.4889, 0.4942, 0.4976,
        0.5071, 0.5061, 0.4995, 0.5049, 0.4902, 0.5034, 0.4892, 0.4847, 0.4988,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4911, 0.5111, 0.4853, 0.4865, 0.4927, 0.4891, 0.4950, 0.4998,
        0.5072, 0.5066, 0.5008, 0.5035, 0.4893, 0.5048, 0.4913, 0.4857, 0.4959,
        0.5076, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4924, 0.5109, 0.4851, 0.4884, 0.4929, 0.4896, 0.4941, 0.4977,
        0.5075, 0.5048, 0.4975, 0.5040, 0.4897, 0.5042, 0.4905, 0.4868, 0.4967,
        0.5084, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4918, 0.5118, 0.4873, 0.4878, 0.4928, 0.4892, 0.4933, 0.4999,
        0.5050, 0.5051, 0.4986, 0.5035, 0.4888, 0.5003, 0.4931, 0.4836, 0.4977,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5126, 0.4963, 0.4966, 0.5026, 0.5066, 0.4926, 0.5091, 0.4952, 0.4969,
         0.5027, 0.4856, 0.4976, 0.4839, 0.5024, 0.5024, 0.4984, 0.4992, 0.5142,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4954, 0.5046, 0.5035, 0.4931, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4966, 0.4858, 0.5014, 0.5002, 0.4978, 0.4981, 0.5147,
         0.4947, 0.4971],
        [0.5131, 0.4973, 0.4974, 0.5024, 0.5047, 0.4922, 0.5102, 0.4979, 0.4964,
         0.5028, 0.4863, 0.4989, 0.4847, 0.5015, 0.4998, 0.4975, 0.4991, 0.5166,
         0.4947, 0.4969],
        [0.5126, 0.4974, 0.4968, 0.5032, 0.5041, 0.4915, 0.5109, 0.4967, 0.4963,
         0.5028, 0.4842, 0.4965, 0.4846, 0.5022, 0.5008, 0.4974, 0.5010, 0.5143,
         0.4953, 0.4969],
        [0.5126, 0.4982, 0.4962, 0.5038, 0.5033, 0.4935, 0.5087, 0.4953, 0.4969,
         0.5032, 0.4873, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
         0.4955, 0.4972],
        [0.5137, 0.4972, 0.4959, 0.5031, 0.5053, 0.4927, 0.5077, 0.4968, 0.4956,
         0.5025, 0.4856, 0.4962, 0.4854, 0.5032, 0.5018, 0.5002, 0.4994, 0.5146,
         0.4951, 0.4982],
        [0.5113, 0.4964, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4964, 0.4961,
         0.5028, 0.4880, 0.4971, 0.4850, 0.5027, 0.5009, 0.4988, 0.4999, 0.5138,
         0.4953, 0.4996],
        [0.5121, 0.4972, 0.4950, 0.5012, 0.5040, 0.4923, 0.5083, 0.4945, 0.4970,
         0.5023, 0.4858, 0.4986, 0.4865, 0.5011, 0.5008, 0.4985, 0.4996, 0.5153,
         0.4958, 0.4985],
        [0.5128, 0.4980, 0.4951, 0.5012, 0.5047, 0.4924, 0.5096, 0.4967, 0.4967,
         0.5019, 0.4885, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5007, 0.5140,
         0.4957, 0.4993],
        [0.5126, 0.4984, 0.4939, 0.5028, 0.5048, 0.4938, 0.5080, 0.4988, 0.4964,
         0.5034, 0.4885, 0.4981, 0.4857, 0.5001, 0.5009, 0.4982, 0.4993, 0.5155,
         0.4971, 0.5003]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5126, 0.4963, 0.4966, 0.5026, 0.5066, 0.4926, 0.5091, 0.4952, 0.4969,
        0.5027, 0.4856, 0.4976, 0.4839, 0.5024, 0.5024, 0.4984, 0.4992, 0.5142,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4954, 0.5046, 0.5035, 0.4931, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4966, 0.4858, 0.5014, 0.5002, 0.4978, 0.4981, 0.5147,
        0.4947, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4973, 0.4974, 0.5024, 0.5047, 0.4922, 0.5102, 0.4979, 0.4964,
        0.5028, 0.4863, 0.4989, 0.4847, 0.5015, 0.4998, 0.4975, 0.4991, 0.5166,
        0.4947, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4968, 0.5032, 0.5041, 0.4915, 0.5109, 0.4967, 0.4963,
        0.5028, 0.4842, 0.4965, 0.4846, 0.5022, 0.5008, 0.4974, 0.5010, 0.5143,
        0.4953, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4982, 0.4962, 0.5038, 0.5033, 0.4935, 0.5087, 0.4953, 0.4969,
        0.5032, 0.4873, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4972, 0.4959, 0.5031, 0.5053, 0.4927, 0.5077, 0.4968, 0.4956,
        0.5025, 0.4856, 0.4962, 0.4854, 0.5032, 0.5018, 0.5002, 0.4994, 0.5146,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4964, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4964, 0.4961,
        0.5028, 0.4880, 0.4971, 0.4850, 0.5027, 0.5009, 0.4988, 0.4999, 0.5138,
        0.4953, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5121, 0.4972, 0.4950, 0.5012, 0.5040, 0.4923, 0.5083, 0.4945, 0.4970,
        0.5023, 0.4858, 0.4986, 0.4865, 0.5011, 0.5008, 0.4985, 0.4996, 0.5153,
        0.4958, 0.4985], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4980, 0.4951, 0.5012, 0.5047, 0.4924, 0.5096, 0.4967, 0.4967,
        0.5019, 0.4885, 0.4950, 0.4851, 0.5017, 0.5011, 0.4971, 0.5007, 0.5140,
        0.4957, 0.4993], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4984, 0.4939, 0.5028, 0.5048, 0.4938, 0.5080, 0.4988, 0.4964,
        0.5034, 0.4885, 0.4981, 0.4857, 0.5001, 0.5009, 0.4982, 0.4993, 0.5155,
        0.4971, 0.5003], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [8, 2, 5, 4, 3, 9, 6, 7, 0, 1]
replay_buffer._size: [6000 6000 6000 6000 6000 6000 6000 6000 6000 6000]
2023-08-12 10:40:44,573 MainThread INFO: EPOCH:38
2023-08-12 10:40:44,574 MainThread INFO: Time Consumed:1.0662291049957275s
2023-08-12 10:40:44,574 MainThread INFO: Total Frames:58500s
 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 39/80 [00:38<00:39,  1.03it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1264.19418
Train_Epoch_Reward                    10936.67601
Running_Training_Average_Rewards      1430.55444
Explore_Time                          0.00731
Train___Time                          0.99965
Eval____Time                          0.00281
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.34479
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.11776
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.72237
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.31999
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.87462
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.52235
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.57038
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12890.74730
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.17698
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.15630
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.92838     1.12622    12.88803    9.38069
alpha_0                               0.94364      0.00040    0.94421     0.94308
alpha_1                               0.94363      0.00040    0.94420     0.94307
alpha_2                               0.94367      0.00040    0.94424     0.94310
alpha_3                               0.94364      0.00040    0.94421     0.94308
alpha_4                               0.94364      0.00040    0.94421     0.94307
alpha_5                               0.94364      0.00040    0.94421     0.94308
alpha_6                               0.94363      0.00040    0.94419     0.94306
alpha_7                               0.94363      0.00040    0.94420     0.94307
alpha_8                               0.94364      0.00040    0.94421     0.94307
alpha_9                               0.94365      0.00040    0.94422     0.94309
Alpha_loss                            -0.38845     0.00310    -0.38403    -0.39250
Training/policy_loss                  -2.72743     0.00619    -2.72031    -2.73611
Training/qf1_loss                     2338.43335   548.88199  3243.75439  1586.94910
Training/qf2_loss                     2337.44141   548.79036  3242.61328  1586.06091
Training/pf_norm                      0.11472      0.01052    0.13298     0.10275
Training/qf1_norm                     32.23972     2.68757    36.97092    28.65211
Training/qf2_norm                     35.57426     2.98929    40.78237    31.61767
log_std/mean                          -0.13214     0.00029    -0.13178    -0.13251
log_std/std                           0.01016      0.00004    0.01021     0.01010
log_std/max                           -0.11178     0.00033    -0.11148    -0.11220
log_std/min                           -0.15452     0.00046    -0.15394    -0.15518
log_probs/mean                        -2.73137     0.00593    -2.72491    -2.74052
log_probs/std                         0.23426      0.01206    0.25513     0.21791
log_probs/max                         -2.14958     0.03879    -2.08669    -2.19474
log_probs/min                         -5.05322     0.82401    -4.56120    -6.69601
mean/mean                             0.00150      0.00019    0.00179     0.00123
mean/std                              0.00350      0.00027    0.00393     0.00317
mean/max                              0.00799      0.00029    0.00848     0.00765
mean/min                              -0.00305     0.00034    -0.00257    -0.00357
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [4, 3, 9, 6, 5, 8, 1, 2, 0, 7]
replay_buffer._size: [6150 6150 6150 6150 6150 6150 6150 6150 6150 6150]
2023-08-12 10:40:45,172 MainThread INFO: EPOCH:39
2023-08-12 10:40:45,173 MainThread INFO: Time Consumed:0.42403483390808105s
2023-08-12 10:40:45,173 MainThread INFO: Total Frames:60000s
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 40/80 [00:38<00:34,  1.17it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1261.20366
Train_Epoch_Reward                    7361.80588
Running_Training_Average_Rewards      1273.99308
Explore_Time                          0.00314
Train___Time                          0.40550
Eval____Time                          0.00239
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -46.75230
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.15495
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.59737
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.25014
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.62500
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.32434
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.37702
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 12858.44055
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.21150
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -26.11138
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.51458     0.90558    13.18223    10.66765
alpha_0                               0.94223      0.00040    0.94279     0.94166
alpha_1                               0.94222      0.00040    0.94278     0.94165
alpha_2                               0.94225      0.00040    0.94282     0.94169
alpha_3                               0.94223      0.00040    0.94279     0.94166
alpha_4                               0.94222      0.00040    0.94279     0.94166
alpha_5                               0.94223      0.00040    0.94279     0.94166
alpha_6                               0.94221      0.00040    0.94278     0.94164
alpha_7                               0.94222      0.00040    0.94278     0.94165
alpha_8                               0.94222      0.00040    0.94279     0.94165
alpha_9                               0.94224      0.00040    0.94280     0.94167
Alpha_loss                            -0.39856     0.00273    -0.39446    -0.40248
Training/policy_loss                  -2.73086     0.00482    -2.72670    -2.74016
Training/qf1_loss                     2551.52881   435.74075  3284.82373  2020.32812
Training/qf2_loss                     2550.37717   435.58281  3283.27856  2019.33728
Training/pf_norm                      0.12388      0.02935    0.16589     0.08679
Training/qf1_norm                     34.07035     2.30439    38.38457    31.89532
Training/qf2_norm                     38.00518     3.11181    44.00771    35.15805
log_std/mean                          -0.13282     0.00015    -0.13260    -0.13296
log_std/std                           0.01021      0.00002    0.01025     0.01019
log_std/max                           -0.11248     0.00031    -0.11206    -0.11286
log_std/min                           -0.15521     0.00044    -0.15456    -0.15565
log_probs/mean                        -2.73126     0.00574    -2.72655    -2.74244
log_probs/std                         0.23221      0.01346    0.24837     0.21021
log_probs/max                         -2.14983     0.03255    -2.08685    -2.18137
log_probs/min                         -4.75869     0.56812    -4.02148    -5.76111
mean/mean                             0.00110      0.00004    0.00115     0.00104
mean/std                              0.00475      0.00034    0.00518     0.00424
mean/max                              0.00964      0.00047    0.01012     0.00891
mean/min                              -0.00423     0.00043    -0.00373    -0.00489
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4973, 0.5064, 0.4988, 0.5063, 0.5069, 0.4886, 0.4954, 0.5018, 0.5000,
         0.5024, 0.5099, 0.5008, 0.4961, 0.4888, 0.4926, 0.5077, 0.4926, 0.5094,
         0.4962, 0.5141],
        [0.4982, 0.5072, 0.4989, 0.5063, 0.5067, 0.4898, 0.4959, 0.5026, 0.5023,
         0.5028, 0.5083, 0.4992, 0.4963, 0.4882, 0.4905, 0.5077, 0.4919, 0.5087,
         0.4964, 0.5127],
        [0.4979, 0.5071, 0.4981, 0.5066, 0.5063, 0.4887, 0.4963, 0.5004, 0.5042,
         0.5012, 0.5065, 0.4990, 0.4965, 0.4882, 0.4903, 0.5098, 0.4935, 0.5088,
         0.4956, 0.5148],
        [0.4986, 0.5059, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.5000, 0.5025,
         0.5037, 0.5061, 0.4985, 0.4972, 0.4879, 0.4895, 0.5079, 0.4922, 0.5101,
         0.4931, 0.5142],
        [0.4986, 0.5075, 0.4981, 0.5051, 0.5055, 0.4886, 0.4957, 0.4990, 0.5020,
         0.5025, 0.5087, 0.4994, 0.4967, 0.4882, 0.4918, 0.5062, 0.4945, 0.5098,
         0.4953, 0.5132],
        [0.4975, 0.5045, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.5001, 0.5010,
         0.5030, 0.5073, 0.4998, 0.4953, 0.4880, 0.4903, 0.5086, 0.4953, 0.5081,
         0.4949, 0.5135],
        [0.4977, 0.5055, 0.4984, 0.5044, 0.5061, 0.4876, 0.4973, 0.5008, 0.5033,
         0.5017, 0.5103, 0.4996, 0.4960, 0.4870, 0.4909, 0.5082, 0.4955, 0.5077,
         0.4968, 0.5127],
        [0.4979, 0.5046, 0.4983, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5031,
         0.5019, 0.5096, 0.4997, 0.4962, 0.4888, 0.4919, 0.5079, 0.4934, 0.5062,
         0.4933, 0.5127],
        [0.4981, 0.5049, 0.4974, 0.5050, 0.5079, 0.4880, 0.4961, 0.5000, 0.5019,
         0.5008, 0.5065, 0.5012, 0.4976, 0.4886, 0.4923, 0.5083, 0.4928, 0.5088,
         0.4941, 0.5144],
        [0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4883, 0.4961, 0.5016, 0.5020,
         0.5039, 0.5075, 0.5003, 0.4945, 0.4906, 0.4927, 0.5093, 0.4940, 0.5051,
         0.4952, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4973, 0.5064, 0.4988, 0.5063, 0.5069, 0.4886, 0.4954, 0.5018, 0.5000,
        0.5024, 0.5099, 0.5008, 0.4961, 0.4888, 0.4926, 0.5077, 0.4926, 0.5094,
        0.4962, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5072, 0.4989, 0.5063, 0.5067, 0.4898, 0.4959, 0.5026, 0.5023,
        0.5028, 0.5083, 0.4992, 0.4963, 0.4882, 0.4905, 0.5077, 0.4919, 0.5087,
        0.4964, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5071, 0.4981, 0.5066, 0.5063, 0.4887, 0.4963, 0.5004, 0.5042,
        0.5012, 0.5065, 0.4990, 0.4965, 0.4882, 0.4903, 0.5098, 0.4935, 0.5088,
        0.4956, 0.5148], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5059, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.5000, 0.5025,
        0.5037, 0.5061, 0.4985, 0.4972, 0.4879, 0.4895, 0.5079, 0.4922, 0.5101,
        0.4931, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5075, 0.4981, 0.5051, 0.5055, 0.4886, 0.4957, 0.4990, 0.5020,
        0.5025, 0.5087, 0.4994, 0.4967, 0.4882, 0.4918, 0.5062, 0.4945, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4975, 0.5045, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.5001, 0.5010,
        0.5030, 0.5073, 0.4998, 0.4953, 0.4880, 0.4903, 0.5086, 0.4953, 0.5081,
        0.4949, 0.5135], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5055, 0.4984, 0.5044, 0.5061, 0.4876, 0.4973, 0.5008, 0.5033,
        0.5017, 0.5103, 0.4996, 0.4960, 0.4870, 0.4909, 0.5082, 0.4955, 0.5077,
        0.4968, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5046, 0.4983, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5031,
        0.5019, 0.5096, 0.4997, 0.4962, 0.4888, 0.4919, 0.5079, 0.4934, 0.5062,
        0.4933, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5049, 0.4974, 0.5050, 0.5079, 0.4880, 0.4961, 0.5000, 0.5019,
        0.5008, 0.5065, 0.5012, 0.4976, 0.4886, 0.4923, 0.5083, 0.4928, 0.5088,
        0.4941, 0.5144], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4883, 0.4961, 0.5016, 0.5020,
        0.5039, 0.5075, 0.5003, 0.4945, 0.4906, 0.4927, 0.5093, 0.4940, 0.5051,
        0.4952, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5104, 0.4891, 0.5119, 0.4865, 0.4854, 0.4940, 0.4893, 0.4942, 0.4987,
         0.5074, 0.5026, 0.5000, 0.5025, 0.4916, 0.5044, 0.4912, 0.4858, 0.4987,
         0.5078, 0.4948],
        [0.5087, 0.4894, 0.5108, 0.4851, 0.4855, 0.4926, 0.4882, 0.4952, 0.4981,
         0.5052, 0.5035, 0.4998, 0.5034, 0.4915, 0.5029, 0.4887, 0.4862, 0.4985,
         0.5071, 0.4934],
        [0.5104, 0.4905, 0.5108, 0.4869, 0.4852, 0.4942, 0.4888, 0.4963, 0.5013,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4919, 0.5040, 0.4909, 0.4875, 0.4980,
         0.5069, 0.4926],
        [0.5079, 0.4893, 0.5115, 0.4860, 0.4872, 0.4931, 0.4896, 0.4952, 0.4984,
         0.5086, 0.5038, 0.4999, 0.5050, 0.4913, 0.5039, 0.4906, 0.4863, 0.4986,
         0.5059, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
         0.5066, 0.5043, 0.4997, 0.5031, 0.4914, 0.5038, 0.4893, 0.4855, 0.4962,
         0.5054, 0.4936],
        [0.5112, 0.4917, 0.5101, 0.4866, 0.4862, 0.4945, 0.4907, 0.4938, 0.4970,
         0.5069, 0.5060, 0.5002, 0.5038, 0.4895, 0.5040, 0.4899, 0.4850, 0.4971,
         0.5062, 0.4931],
        [0.5095, 0.4912, 0.5124, 0.4882, 0.4856, 0.4936, 0.4888, 0.4942, 0.4976,
         0.5072, 0.5061, 0.4995, 0.5048, 0.4903, 0.5036, 0.4889, 0.4846, 0.4988,
         0.5072, 0.4932],
        [0.5101, 0.4912, 0.5111, 0.4853, 0.4865, 0.4927, 0.4891, 0.4949, 0.4998,
         0.5072, 0.5066, 0.5009, 0.5035, 0.4893, 0.5049, 0.4912, 0.4857, 0.4959,
         0.5076, 0.4932],
        [0.5109, 0.4925, 0.5109, 0.4852, 0.4884, 0.4929, 0.4895, 0.4940, 0.4978,
         0.5075, 0.5048, 0.4975, 0.5040, 0.4897, 0.5043, 0.4904, 0.4867, 0.4967,
         0.5083, 0.4936],
        [0.5113, 0.4919, 0.5118, 0.4873, 0.4879, 0.4928, 0.4892, 0.4932, 0.4999,
         0.5050, 0.5051, 0.4986, 0.5036, 0.4888, 0.5003, 0.4931, 0.4836, 0.4976,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5104, 0.4891, 0.5119, 0.4865, 0.4854, 0.4940, 0.4893, 0.4942, 0.4987,
        0.5074, 0.5026, 0.5000, 0.5025, 0.4916, 0.5044, 0.4912, 0.4858, 0.4987,
        0.5078, 0.4948], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4894, 0.5108, 0.4851, 0.4855, 0.4926, 0.4882, 0.4952, 0.4981,
        0.5052, 0.5035, 0.4998, 0.5034, 0.4915, 0.5029, 0.4887, 0.4862, 0.4985,
        0.5071, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5104, 0.4905, 0.5108, 0.4869, 0.4852, 0.4942, 0.4888, 0.4963, 0.5013,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4919, 0.5040, 0.4909, 0.4875, 0.4980,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4893, 0.5115, 0.4860, 0.4872, 0.4931, 0.4896, 0.4952, 0.4984,
        0.5086, 0.5038, 0.4999, 0.5050, 0.4913, 0.5039, 0.4906, 0.4863, 0.4986,
        0.5059, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4862, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
        0.5066, 0.5043, 0.4997, 0.5031, 0.4914, 0.5038, 0.4893, 0.4855, 0.4962,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4917, 0.5101, 0.4866, 0.4862, 0.4945, 0.4907, 0.4938, 0.4970,
        0.5069, 0.5060, 0.5002, 0.5038, 0.4895, 0.5040, 0.4899, 0.4850, 0.4971,
        0.5062, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5124, 0.4882, 0.4856, 0.4936, 0.4888, 0.4942, 0.4976,
        0.5072, 0.5061, 0.4995, 0.5048, 0.4903, 0.5036, 0.4889, 0.4846, 0.4988,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4912, 0.5111, 0.4853, 0.4865, 0.4927, 0.4891, 0.4949, 0.4998,
        0.5072, 0.5066, 0.5009, 0.5035, 0.4893, 0.5049, 0.4912, 0.4857, 0.4959,
        0.5076, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4925, 0.5109, 0.4852, 0.4884, 0.4929, 0.4895, 0.4940, 0.4978,
        0.5075, 0.5048, 0.4975, 0.5040, 0.4897, 0.5043, 0.4904, 0.4867, 0.4967,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4919, 0.5118, 0.4873, 0.4879, 0.4928, 0.4892, 0.4932, 0.4999,
        0.5050, 0.5051, 0.4986, 0.5036, 0.4888, 0.5003, 0.4931, 0.4836, 0.4976,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5126, 0.4963, 0.4966, 0.5026, 0.5066, 0.4926, 0.5092, 0.4952, 0.4969,
         0.5027, 0.4856, 0.4976, 0.4838, 0.5024, 0.5024, 0.4984, 0.4993, 0.5142,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4955, 0.5046, 0.5035, 0.4931, 0.5103, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4966, 0.4858, 0.5014, 0.5003, 0.4978, 0.4981, 0.5147,
         0.4946, 0.4971],
        [0.5131, 0.4974, 0.4974, 0.5024, 0.5047, 0.4922, 0.5103, 0.4979, 0.4963,
         0.5028, 0.4863, 0.4989, 0.4847, 0.5015, 0.4999, 0.4975, 0.4991, 0.5166,
         0.4947, 0.4970],
        [0.5125, 0.4974, 0.4969, 0.5033, 0.5041, 0.4914, 0.5110, 0.4966, 0.4963,
         0.5027, 0.4842, 0.4965, 0.4846, 0.5022, 0.5009, 0.4974, 0.5010, 0.5143,
         0.4953, 0.4969],
        [0.5126, 0.4982, 0.4962, 0.5038, 0.5032, 0.4935, 0.5088, 0.4953, 0.4969,
         0.5031, 0.4873, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
         0.4956, 0.4973],
        [0.5136, 0.4973, 0.4960, 0.5030, 0.5052, 0.4928, 0.5079, 0.4968, 0.4956,
         0.5025, 0.4856, 0.4962, 0.4853, 0.5032, 0.5019, 0.5002, 0.4994, 0.5145,
         0.4952, 0.4982],
        [0.5113, 0.4964, 0.4979, 0.5026, 0.5038, 0.4923, 0.5089, 0.4963, 0.4961,
         0.5027, 0.4880, 0.4971, 0.4850, 0.5027, 0.5009, 0.4987, 0.4999, 0.5138,
         0.4953, 0.4996],
        [0.5120, 0.4972, 0.4951, 0.5012, 0.5040, 0.4923, 0.5083, 0.4944, 0.4970,
         0.5023, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4985, 0.4996, 0.5153,
         0.4958, 0.4985],
        [0.5127, 0.4981, 0.4951, 0.5011, 0.5047, 0.4925, 0.5097, 0.4966, 0.4967,
         0.5019, 0.4885, 0.4949, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5140,
         0.4956, 0.4993],
        [0.5126, 0.4985, 0.4939, 0.5028, 0.5048, 0.4938, 0.5080, 0.4988, 0.4964,
         0.5034, 0.4885, 0.4981, 0.4857, 0.5001, 0.5010, 0.4982, 0.4993, 0.5155,
         0.4971, 0.5004]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5126, 0.4963, 0.4966, 0.5026, 0.5066, 0.4926, 0.5092, 0.4952, 0.4969,
        0.5027, 0.4856, 0.4976, 0.4838, 0.5024, 0.5024, 0.4984, 0.4993, 0.5142,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4955, 0.5046, 0.5035, 0.4931, 0.5103, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4966, 0.4858, 0.5014, 0.5003, 0.4978, 0.4981, 0.5147,
        0.4946, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4974, 0.4974, 0.5024, 0.5047, 0.4922, 0.5103, 0.4979, 0.4963,
        0.5028, 0.4863, 0.4989, 0.4847, 0.5015, 0.4999, 0.4975, 0.4991, 0.5166,
        0.4947, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4974, 0.4969, 0.5033, 0.5041, 0.4914, 0.5110, 0.4966, 0.4963,
        0.5027, 0.4842, 0.4965, 0.4846, 0.5022, 0.5009, 0.4974, 0.5010, 0.5143,
        0.4953, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4982, 0.4962, 0.5038, 0.5032, 0.4935, 0.5088, 0.4953, 0.4969,
        0.5031, 0.4873, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
        0.4956, 0.4973], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4973, 0.4960, 0.5030, 0.5052, 0.4928, 0.5079, 0.4968, 0.4956,
        0.5025, 0.4856, 0.4962, 0.4853, 0.5032, 0.5019, 0.5002, 0.4994, 0.5145,
        0.4952, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4964, 0.4979, 0.5026, 0.5038, 0.4923, 0.5089, 0.4963, 0.4961,
        0.5027, 0.4880, 0.4971, 0.4850, 0.5027, 0.5009, 0.4987, 0.4999, 0.5138,
        0.4953, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4972, 0.4951, 0.5012, 0.5040, 0.4923, 0.5083, 0.4944, 0.4970,
        0.5023, 0.4858, 0.4986, 0.4864, 0.5011, 0.5008, 0.4985, 0.4996, 0.5153,
        0.4958, 0.4985], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4981, 0.4951, 0.5011, 0.5047, 0.4925, 0.5097, 0.4966, 0.4967,
        0.5019, 0.4885, 0.4949, 0.4851, 0.5017, 0.5012, 0.4971, 0.5006, 0.5140,
        0.4956, 0.4993], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4985, 0.4939, 0.5028, 0.5048, 0.4938, 0.5080, 0.4988, 0.4964,
        0.5034, 0.4885, 0.4981, 0.4857, 0.5001, 0.5010, 0.4982, 0.4993, 0.5155,
        0.4971, 0.5004], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [3, 5, 0, 6, 7, 1, 4, 9, 2, 8]
replay_buffer._size: [6300 6300 6300 6300 6300 6300 6300 6300 6300 6300]
2023-08-12 10:40:46,551 MainThread INFO: EPOCH:40
2023-08-12 10:40:46,552 MainThread INFO: Time Consumed:0.8372206687927246s
2023-08-12 10:40:46,552 MainThread INFO: Total Frames:61500s
 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 41/80 [00:40<00:39,  1.01s/it]------------------------------------  -----------  ----------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1284.99970
Train_Epoch_Reward                    4971.34110
Running_Training_Average_Rewards      775.66077
Explore_Time                          0.00368
Train___Time                          0.82932
Eval____Time                          0.00362
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.92192
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.21858
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.67244
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.12069
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.57872
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.28570
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.33944
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 13099.29406
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.27308
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.88646
mean_success_rate                     0.00000

Name                                  Mean         Std         Max         Min
Reward_Mean                           11.11650     1.28584     13.49065    9.80094
alpha_0                               0.94081      0.00040     0.94138     0.94025
alpha_1                               0.94080      0.00040     0.94137     0.94024
alpha_2                               0.94084      0.00040     0.94140     0.94027
alpha_3                               0.94081      0.00040     0.94138     0.94024
alpha_4                               0.94081      0.00040     0.94137     0.94024
alpha_5                               0.94081      0.00040     0.94138     0.94025
alpha_6                               0.94080      0.00040     0.94136     0.94023
alpha_7                               0.94080      0.00040     0.94137     0.94024
alpha_8                               0.94081      0.00040     0.94137     0.94024
alpha_9                               0.94082      0.00040     0.94139     0.94026
Alpha_loss                            -0.40890     0.00257     -0.40530    -0.41246
Training/policy_loss                  -2.73729     0.00680     -2.73059    -2.74769
Training/qf1_loss                     3143.37180   1759.34612  6622.89453  1938.20251
Training/qf2_loss                     3142.24187   1759.21550  6621.50928  1937.13892
Training/pf_norm                      0.11199      0.03337     0.14018     0.06107
Training/qf1_norm                     33.22425     2.90074     38.50765    30.12951
Training/qf2_norm                     36.98129     3.38191     43.22311    33.37911
log_std/mean                          -0.13293     0.00010     -0.13277    -0.13304
log_std/std                           0.01011      0.00008     0.01022     0.01002
log_std/max                           -0.11297     0.00027     -0.11252    -0.11336
log_std/min                           -0.15503     0.00038     -0.15432    -0.15540
log_probs/mean                        -2.73498     0.00787     -2.72678    -2.74585
log_probs/std                         0.25218      0.01654     0.28291     0.23565
log_probs/max                         -2.11196     0.03279     -2.08109    -2.17419
log_probs/min                         -5.93467     0.63871     -5.27776    -7.13383
mean/mean                             0.00098      0.00004     0.00103     0.00094
mean/std                              0.00551      0.00016     0.00573     0.00529
mean/max                              0.01045      0.00011     0.01059     0.01034
mean/min                              -0.00601     0.00059     -0.00520    -0.00685
------------------------------------  -----------  ----------  ----------  ----------
self.update_end_epoch 3750
sample: [1, 7, 2, 5, 4, 3, 6, 0, 9, 8]
replay_buffer._size: [6450 6450 6450 6450 6450 6450 6450 6450 6450 6450]
snapshot at best
2023-08-12 10:40:47,963 MainThread INFO: EPOCH:41
2023-08-12 10:40:47,964 MainThread INFO: Time Consumed:1.2763965129852295s
2023-08-12 10:40:47,964 MainThread INFO: Total Frames:63000s
 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 42/80 [00:41<00:44,  1.16s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1311.84688
Train_Epoch_Reward                    18552.20893
Running_Training_Average_Rewards      1029.51186
Explore_Time                          0.00313
Train___Time                          0.71646
Eval____Time                          0.00239
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -55.34680
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.27317
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.82479
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.03697
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.68637
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.37111
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.42242
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 13373.43663
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.32794
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.67832
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.42285     0.66693    12.46830    10.72793
alpha_0                               0.93940      0.00040    0.93996     0.93883
alpha_1                               0.93939      0.00040    0.93995     0.93882
alpha_2                               0.93942      0.00040    0.93999     0.93886
alpha_3                               0.93939      0.00040    0.93996     0.93883
alpha_4                               0.93940      0.00040    0.93996     0.93883
alpha_5                               0.93940      0.00040    0.93996     0.93883
alpha_6                               0.93938      0.00040    0.93995     0.93882
alpha_7                               0.93939      0.00040    0.93995     0.93882
alpha_8                               0.93939      0.00040    0.93996     0.93883
alpha_9                               0.93941      0.00040    0.93997     0.93884
Alpha_loss                            -0.41887     0.00323    -0.41461    -0.42382
Training/policy_loss                  -2.73846     0.00849    -2.72969    -2.75401
Training/qf1_loss                     2564.46001   374.23481  3157.44849  2203.07886
Training/qf2_loss                     2563.14561   374.12268  3155.87744  2201.79590
Training/pf_norm                      0.12675      0.03505    0.17157     0.08540
Training/qf1_norm                     34.40919     1.67716    37.10083    32.76281
Training/qf2_norm                     38.84679     2.12668    42.55726    36.88368
log_std/mean                          -0.13305     0.00008    -0.13294    -0.13318
log_std/std                           0.00998      0.00004    0.01001     0.00991
log_std/max                           -0.11281     0.00015    -0.11258    -0.11299
log_std/min                           -0.15490     0.00030    -0.15436    -0.15527
log_probs/mean                        -2.73245     0.00815    -2.72353    -2.74680
log_probs/std                         0.23146      0.00696    0.24491     0.22650
log_probs/max                         -2.15713     0.03415    -2.11967    -2.20924
log_probs/min                         -4.97404     0.30279    -4.57108    -5.41615
mean/mean                             0.00077      0.00007    0.00088     0.00068
mean/std                              0.00624      0.00027    0.00663     0.00587
mean/max                              0.01111      0.00029    0.01155     0.01071
mean/min                              -0.00839     0.00074    -0.00736    -0.00942
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4973, 0.5064, 0.4989, 0.5063, 0.5069, 0.4885, 0.4954, 0.5018, 0.4999,
         0.5024, 0.5099, 0.5008, 0.4960, 0.4888, 0.4926, 0.5076, 0.4926, 0.5095,
         0.4963, 0.5141],
        [0.4982, 0.5072, 0.4990, 0.5063, 0.5067, 0.4898, 0.4959, 0.5027, 0.5023,
         0.5028, 0.5083, 0.4992, 0.4962, 0.4882, 0.4905, 0.5077, 0.4918, 0.5088,
         0.4965, 0.5126],
        [0.4979, 0.5072, 0.4981, 0.5066, 0.5063, 0.4888, 0.4963, 0.5004, 0.5042,
         0.5013, 0.5065, 0.4990, 0.4964, 0.4883, 0.4902, 0.5097, 0.4934, 0.5088,
         0.4956, 0.5147],
        [0.4987, 0.5060, 0.4986, 0.5069, 0.5062, 0.4896, 0.4945, 0.5000, 0.5025,
         0.5037, 0.5061, 0.4985, 0.4972, 0.4880, 0.4895, 0.5079, 0.4921, 0.5101,
         0.4931, 0.5142],
        [0.4986, 0.5076, 0.4981, 0.5051, 0.5055, 0.4886, 0.4956, 0.4990, 0.5020,
         0.5025, 0.5087, 0.4994, 0.4967, 0.4882, 0.4917, 0.5062, 0.4944, 0.5098,
         0.4953, 0.5132],
        [0.4975, 0.5046, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.5001, 0.5010,
         0.5030, 0.5074, 0.4998, 0.4953, 0.4880, 0.4902, 0.5086, 0.4952, 0.5081,
         0.4950, 0.5134],
        [0.4978, 0.5055, 0.4982, 0.5045, 0.5061, 0.4876, 0.4972, 0.5008, 0.5032,
         0.5018, 0.5102, 0.4998, 0.4960, 0.4872, 0.4908, 0.5081, 0.4954, 0.5077,
         0.4967, 0.5127],
        [0.4979, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5030,
         0.5018, 0.5096, 0.4998, 0.4962, 0.4889, 0.4919, 0.5079, 0.4934, 0.5063,
         0.4934, 0.5127],
        [0.4981, 0.5048, 0.4974, 0.5050, 0.5079, 0.4880, 0.4960, 0.5000, 0.5019,
         0.5008, 0.5065, 0.5012, 0.4976, 0.4886, 0.4922, 0.5082, 0.4928, 0.5088,
         0.4940, 0.5143],
        [0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4883, 0.4961, 0.5017, 0.5019,
         0.5039, 0.5076, 0.5003, 0.4944, 0.4906, 0.4927, 0.5094, 0.4940, 0.5050,
         0.4952, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4973, 0.5064, 0.4989, 0.5063, 0.5069, 0.4885, 0.4954, 0.5018, 0.4999,
        0.5024, 0.5099, 0.5008, 0.4960, 0.4888, 0.4926, 0.5076, 0.4926, 0.5095,
        0.4963, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5072, 0.4990, 0.5063, 0.5067, 0.4898, 0.4959, 0.5027, 0.5023,
        0.5028, 0.5083, 0.4992, 0.4962, 0.4882, 0.4905, 0.5077, 0.4918, 0.5088,
        0.4965, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5072, 0.4981, 0.5066, 0.5063, 0.4888, 0.4963, 0.5004, 0.5042,
        0.5013, 0.5065, 0.4990, 0.4964, 0.4883, 0.4902, 0.5097, 0.4934, 0.5088,
        0.4956, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4987, 0.5060, 0.4986, 0.5069, 0.5062, 0.4896, 0.4945, 0.5000, 0.5025,
        0.5037, 0.5061, 0.4985, 0.4972, 0.4880, 0.4895, 0.5079, 0.4921, 0.5101,
        0.4931, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5076, 0.4981, 0.5051, 0.5055, 0.4886, 0.4956, 0.4990, 0.5020,
        0.5025, 0.5087, 0.4994, 0.4967, 0.4882, 0.4917, 0.5062, 0.4944, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4975, 0.5046, 0.4976, 0.5060, 0.5053, 0.4883, 0.4947, 0.5001, 0.5010,
        0.5030, 0.5074, 0.4998, 0.4953, 0.4880, 0.4902, 0.5086, 0.4952, 0.5081,
        0.4950, 0.5134], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5055, 0.4982, 0.5045, 0.5061, 0.4876, 0.4972, 0.5008, 0.5032,
        0.5018, 0.5102, 0.4998, 0.4960, 0.4872, 0.4908, 0.5081, 0.4954, 0.5077,
        0.4967, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5030,
        0.5018, 0.5096, 0.4998, 0.4962, 0.4889, 0.4919, 0.5079, 0.4934, 0.5063,
        0.4934, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5048, 0.4974, 0.5050, 0.5079, 0.4880, 0.4960, 0.5000, 0.5019,
        0.5008, 0.5065, 0.5012, 0.4976, 0.4886, 0.4922, 0.5082, 0.4928, 0.5088,
        0.4940, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4883, 0.4961, 0.5017, 0.5019,
        0.5039, 0.5076, 0.5003, 0.4944, 0.4906, 0.4927, 0.5094, 0.4940, 0.5050,
        0.4952, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5103, 0.4891, 0.5120, 0.4865, 0.4853, 0.4940, 0.4894, 0.4943, 0.4987,
         0.5074, 0.5026, 0.5000, 0.5025, 0.4917, 0.5044, 0.4913, 0.4858, 0.4987,
         0.5079, 0.4947],
        [0.5087, 0.4894, 0.5109, 0.4851, 0.4855, 0.4927, 0.4883, 0.4953, 0.4981,
         0.5052, 0.5035, 0.4998, 0.5034, 0.4916, 0.5028, 0.4888, 0.4862, 0.4985,
         0.5071, 0.4934],
        [0.5104, 0.4904, 0.5108, 0.4869, 0.4852, 0.4942, 0.4888, 0.4963, 0.5012,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4919, 0.5040, 0.4909, 0.4875, 0.4981,
         0.5069, 0.4925],
        [0.5078, 0.4893, 0.5115, 0.4860, 0.4872, 0.4931, 0.4896, 0.4952, 0.4984,
         0.5086, 0.5038, 0.4999, 0.5050, 0.4914, 0.5039, 0.4905, 0.4862, 0.4986,
         0.5059, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4861, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
         0.5066, 0.5043, 0.4997, 0.5031, 0.4914, 0.5038, 0.4893, 0.4855, 0.4962,
         0.5054, 0.4936],
        [0.5111, 0.4917, 0.5102, 0.4865, 0.4863, 0.4945, 0.4907, 0.4938, 0.4970,
         0.5068, 0.5061, 0.5002, 0.5038, 0.4894, 0.5040, 0.4901, 0.4851, 0.4971,
         0.5062, 0.4931],
        [0.5095, 0.4912, 0.5125, 0.4882, 0.4858, 0.4936, 0.4888, 0.4942, 0.4976,
         0.5071, 0.5062, 0.4996, 0.5049, 0.4903, 0.5036, 0.4891, 0.4846, 0.4987,
         0.5072, 0.4932],
        [0.5101, 0.4912, 0.5112, 0.4853, 0.4865, 0.4927, 0.4891, 0.4949, 0.4998,
         0.5072, 0.5067, 0.5008, 0.5035, 0.4893, 0.5049, 0.4913, 0.4856, 0.4959,
         0.5076, 0.4932],
        [0.5110, 0.4925, 0.5109, 0.4851, 0.4885, 0.4929, 0.4895, 0.4940, 0.4978,
         0.5075, 0.5049, 0.4975, 0.5041, 0.4897, 0.5042, 0.4905, 0.4867, 0.4966,
         0.5083, 0.4936],
        [0.5113, 0.4919, 0.5118, 0.4872, 0.4879, 0.4928, 0.4893, 0.4933, 0.4998,
         0.5051, 0.5052, 0.4986, 0.5035, 0.4888, 0.5003, 0.4932, 0.4836, 0.4976,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5103, 0.4891, 0.5120, 0.4865, 0.4853, 0.4940, 0.4894, 0.4943, 0.4987,
        0.5074, 0.5026, 0.5000, 0.5025, 0.4917, 0.5044, 0.4913, 0.4858, 0.4987,
        0.5079, 0.4947], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4894, 0.5109, 0.4851, 0.4855, 0.4927, 0.4883, 0.4953, 0.4981,
        0.5052, 0.5035, 0.4998, 0.5034, 0.4916, 0.5028, 0.4888, 0.4862, 0.4985,
        0.5071, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5104, 0.4904, 0.5108, 0.4869, 0.4852, 0.4942, 0.4888, 0.4963, 0.5012,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4919, 0.5040, 0.4909, 0.4875, 0.4981,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4893, 0.5115, 0.4860, 0.4872, 0.4931, 0.4896, 0.4952, 0.4984,
        0.5086, 0.5038, 0.4999, 0.5050, 0.4914, 0.5039, 0.4905, 0.4862, 0.4986,
        0.5059, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4861, 0.4871, 0.4927, 0.4909, 0.4923, 0.4956,
        0.5066, 0.5043, 0.4997, 0.5031, 0.4914, 0.5038, 0.4893, 0.4855, 0.4962,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4917, 0.5102, 0.4865, 0.4863, 0.4945, 0.4907, 0.4938, 0.4970,
        0.5068, 0.5061, 0.5002, 0.5038, 0.4894, 0.5040, 0.4901, 0.4851, 0.4971,
        0.5062, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5125, 0.4882, 0.4858, 0.4936, 0.4888, 0.4942, 0.4976,
        0.5071, 0.5062, 0.4996, 0.5049, 0.4903, 0.5036, 0.4891, 0.4846, 0.4987,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4912, 0.5112, 0.4853, 0.4865, 0.4927, 0.4891, 0.4949, 0.4998,
        0.5072, 0.5067, 0.5008, 0.5035, 0.4893, 0.5049, 0.4913, 0.4856, 0.4959,
        0.5076, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4925, 0.5109, 0.4851, 0.4885, 0.4929, 0.4895, 0.4940, 0.4978,
        0.5075, 0.5049, 0.4975, 0.5041, 0.4897, 0.5042, 0.4905, 0.4867, 0.4966,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4919, 0.5118, 0.4872, 0.4879, 0.4928, 0.4893, 0.4933, 0.4998,
        0.5051, 0.5052, 0.4986, 0.5035, 0.4888, 0.5003, 0.4932, 0.4836, 0.4976,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5126, 0.4963, 0.4966, 0.5026, 0.5066, 0.4926, 0.5092, 0.4953, 0.4968,
         0.5027, 0.4856, 0.4976, 0.4839, 0.5023, 0.5024, 0.4984, 0.4992, 0.5142,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4955, 0.5045, 0.5035, 0.4931, 0.5102, 0.4966, 0.4960,
         0.5026, 0.4848, 0.4967, 0.4858, 0.5014, 0.5002, 0.4978, 0.4980, 0.5148,
         0.4947, 0.4971],
        [0.5131, 0.4973, 0.4975, 0.5024, 0.5046, 0.4922, 0.5103, 0.4979, 0.4963,
         0.5028, 0.4862, 0.4989, 0.4847, 0.5015, 0.4999, 0.4975, 0.4991, 0.5166,
         0.4947, 0.4969],
        [0.5125, 0.4974, 0.4969, 0.5033, 0.5041, 0.4915, 0.5109, 0.4966, 0.4963,
         0.5028, 0.4842, 0.4965, 0.4846, 0.5022, 0.5009, 0.4974, 0.5010, 0.5143,
         0.4953, 0.4969],
        [0.5126, 0.4982, 0.4962, 0.5038, 0.5033, 0.4935, 0.5088, 0.4953, 0.4969,
         0.5032, 0.4872, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
         0.4955, 0.4972],
        [0.5136, 0.4973, 0.4960, 0.5031, 0.5052, 0.4927, 0.5078, 0.4968, 0.4956,
         0.5025, 0.4856, 0.4963, 0.4853, 0.5032, 0.5019, 0.5002, 0.4994, 0.5145,
         0.4951, 0.4982],
        [0.5114, 0.4965, 0.4980, 0.5025, 0.5039, 0.4922, 0.5088, 0.4963, 0.4961,
         0.5026, 0.4880, 0.4971, 0.4849, 0.5028, 0.5009, 0.4988, 0.4998, 0.5138,
         0.4952, 0.4996],
        [0.5120, 0.4972, 0.4951, 0.5012, 0.5040, 0.4922, 0.5083, 0.4944, 0.4970,
         0.5023, 0.4858, 0.4985, 0.4864, 0.5012, 0.5008, 0.4986, 0.4997, 0.5153,
         0.4958, 0.4986],
        [0.5128, 0.4981, 0.4951, 0.5011, 0.5048, 0.4924, 0.5096, 0.4966, 0.4967,
         0.5019, 0.4885, 0.4949, 0.4851, 0.5017, 0.5013, 0.4971, 0.5007, 0.5140,
         0.4957, 0.4993],
        [0.5126, 0.4984, 0.4939, 0.5028, 0.5048, 0.4938, 0.5079, 0.4988, 0.4964,
         0.5034, 0.4885, 0.4981, 0.4857, 0.5001, 0.5009, 0.4982, 0.4994, 0.5155,
         0.4971, 0.5004]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5126, 0.4963, 0.4966, 0.5026, 0.5066, 0.4926, 0.5092, 0.4953, 0.4968,
        0.5027, 0.4856, 0.4976, 0.4839, 0.5023, 0.5024, 0.4984, 0.4992, 0.5142,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4955, 0.5045, 0.5035, 0.4931, 0.5102, 0.4966, 0.4960,
        0.5026, 0.4848, 0.4967, 0.4858, 0.5014, 0.5002, 0.4978, 0.4980, 0.5148,
        0.4947, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4973, 0.4975, 0.5024, 0.5046, 0.4922, 0.5103, 0.4979, 0.4963,
        0.5028, 0.4862, 0.4989, 0.4847, 0.5015, 0.4999, 0.4975, 0.4991, 0.5166,
        0.4947, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4974, 0.4969, 0.5033, 0.5041, 0.4915, 0.5109, 0.4966, 0.4963,
        0.5028, 0.4842, 0.4965, 0.4846, 0.5022, 0.5009, 0.4974, 0.5010, 0.5143,
        0.4953, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4982, 0.4962, 0.5038, 0.5033, 0.4935, 0.5088, 0.4953, 0.4969,
        0.5032, 0.4872, 0.4980, 0.4826, 0.5026, 0.5003, 0.4992, 0.5000, 0.5151,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4973, 0.4960, 0.5031, 0.5052, 0.4927, 0.5078, 0.4968, 0.4956,
        0.5025, 0.4856, 0.4963, 0.4853, 0.5032, 0.5019, 0.5002, 0.4994, 0.5145,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4965, 0.4980, 0.5025, 0.5039, 0.4922, 0.5088, 0.4963, 0.4961,
        0.5026, 0.4880, 0.4971, 0.4849, 0.5028, 0.5009, 0.4988, 0.4998, 0.5138,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4972, 0.4951, 0.5012, 0.5040, 0.4922, 0.5083, 0.4944, 0.4970,
        0.5023, 0.4858, 0.4985, 0.4864, 0.5012, 0.5008, 0.4986, 0.4997, 0.5153,
        0.4958, 0.4986], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4951, 0.5011, 0.5048, 0.4924, 0.5096, 0.4966, 0.4967,
        0.5019, 0.4885, 0.4949, 0.4851, 0.5017, 0.5013, 0.4971, 0.5007, 0.5140,
        0.4957, 0.4993], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4984, 0.4939, 0.5028, 0.5048, 0.4938, 0.5079, 0.4988, 0.4964,
        0.5034, 0.4885, 0.4981, 0.4857, 0.5001, 0.5009, 0.4982, 0.4994, 0.5155,
        0.4971, 0.5004], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [0, 3, 9, 2, 7, 4, 8, 1, 5, 6]
replay_buffer._size: [6600 6600 6600 6600 6600 6600 6600 6600 6600 6600]
snapshot at best
2023-08-12 10:40:49,770 MainThread INFO: EPOCH:42
2023-08-12 10:40:49,771 MainThread INFO: Time Consumed:1.0932707786560059s
2023-08-12 10:40:49,771 MainThread INFO: Total Frames:64500s
 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 43/80 [00:43<00:48,  1.32s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1330.18039
Train_Epoch_Reward                    35204.84768
Running_Training_Average_Rewards      1957.61326
Explore_Time                          0.00354
Train___Time                          0.38791
Eval____Time                          0.00295
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.03567
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.39462
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.88107
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.01527
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.72922
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.40574
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.45775
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 13549.69317
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.45438
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.51556
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.81004     0.56414    11.63677    10.08983
alpha_0                               0.93799      0.00040    0.93855     0.93742
alpha_1                               0.93798      0.00040    0.93854     0.93741
alpha_2                               0.93801      0.00040    0.93858     0.93745
alpha_3                               0.93798      0.00040    0.93855     0.93742
alpha_4                               0.93798      0.00040    0.93855     0.93742
alpha_5                               0.93799      0.00040    0.93855     0.93742
alpha_6                               0.93797      0.00040    0.93854     0.93741
alpha_7                               0.93798      0.00040    0.93854     0.93741
alpha_8                               0.93798      0.00040    0.93855     0.93742
alpha_9                               0.93800      0.00040    0.93856     0.93743
Alpha_loss                            -0.42923     0.00260    -0.42531    -0.43268
Training/policy_loss                  -2.74668     0.00489    -2.73875    -2.75407
Training/qf1_loss                     2393.99734   412.22213  2812.95654  1769.65955
Training/qf2_loss                     2392.63738   412.16894  2811.38086  1768.20862
Training/pf_norm                      0.13939      0.02593    0.18985     0.11604
Training/qf1_norm                     33.22309     1.45959    35.37383    31.53099
Training/qf2_norm                     37.80656     2.06719    40.91111    34.75975
log_std/mean                          -0.13283     0.00010    -0.13270    -0.13297
log_std/std                           0.00983      0.00005    0.00990     0.00976
log_std/max                           -0.11327     0.00022    -0.11293    -0.11351
log_std/min                           -0.15403     0.00067    -0.15272    -0.15449
log_probs/mean                        -2.73622     0.00572    -2.72693    -2.74457
log_probs/std                         0.22832      0.00988    0.24540     0.21820
log_probs/max                         -2.16218     0.06603    -2.08590    -2.23896
log_probs/min                         -4.44329     0.22570    -4.25624    -4.82912
mean/mean                             0.00045      0.00010    0.00059     0.00030
mean/std                              0.00682      0.00004    0.00686     0.00677
mean/max                              0.01136      0.00028    0.01164     0.01087
mean/min                              -0.01018     0.00019    -0.00984    -0.01039
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [3, 9, 0, 1, 2, 5, 7, 4, 8, 6]
replay_buffer._size: [6750 6750 6750 6750 6750 6750 6750 6750 6750 6750]
snapshot at best
2023-08-12 10:40:50,913 MainThread INFO: EPOCH:43
2023-08-12 10:40:50,913 MainThread INFO: Time Consumed:1.0118317604064941s
2023-08-12 10:40:50,913 MainThread INFO: Total Frames:66000s
 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 44/80 [00:44<00:45,  1.28s/it]------------------------------------  -----------  ----------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1353.44338
Train_Epoch_Reward                    8272.36052
Running_Training_Average_Rewards      2067.64724
Explore_Time                          0.00374
Train___Time                          0.40668
Eval____Time                          0.00283
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.43660
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.51097
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.95239
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.94197
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.73124
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.40697
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.46069
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 13783.74865
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.57496
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.29907
mean_success_rate                     0.00000

Name                                  Mean         Std         Max         Min
Reward_Mean                           10.16943     0.99382     11.13820    8.58276
alpha_0                               0.93658      0.00040     0.93714     0.93602
alpha_1                               0.93657      0.00040     0.93713     0.93601
alpha_2                               0.93660      0.00040     0.93716     0.93604
alpha_3                               0.93657      0.00040     0.93714     0.93601
alpha_4                               0.93658      0.00040     0.93714     0.93601
alpha_5                               0.93658      0.00040     0.93714     0.93601
alpha_6                               0.93657      0.00040     0.93713     0.93600
alpha_7                               0.93657      0.00040     0.93713     0.93600
alpha_8                               0.93657      0.00040     0.93713     0.93601
alpha_9                               0.93659      0.00040     0.93715     0.93603
Alpha_loss                            -0.43905     0.00275     -0.43535    -0.44283
Training/policy_loss                  -2.74645     0.00315     -2.74153    -2.74957
Training/qf1_loss                     2671.51267   1460.56289  5508.14893  1392.50745
Training/qf2_loss                     2670.34856   1460.52821  5506.94922  1391.45142
Training/pf_norm                      0.10406      0.02792     0.14555     0.07248
Training/qf1_norm                     31.80505     2.42212     34.06804    27.79518
Training/qf2_norm                     35.47390     2.70340     38.02480    31.14274
log_std/mean                          -0.13269     0.00002     -0.13266    -0.13272
log_std/std                           0.00962      0.00005     0.00968     0.00953
log_std/max                           -0.11339     0.00040     -0.11311    -0.11418
log_std/min                           -0.15330     0.00067     -0.15211    -0.15403
log_probs/mean                        -2.73162     0.00383     -2.72710    -2.73685
log_probs/std                         0.22447      0.00755     0.23390     0.21541
log_probs/max                         -2.11519     0.03304     -2.07176    -2.15816
log_probs/min                         -4.59757     0.28267     -4.21655    -5.06289
mean/mean                             -0.00005     0.00016     0.00018     -0.00030
mean/std                              0.00710      0.00011     0.00725     0.00692
mean/max                              0.01104      0.00012     0.01119     0.01085
mean/min                              -0.01108     0.00031     -0.01068    -0.01158
------------------------------------  -----------  ----------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4973, 0.5065, 0.4988, 0.5064, 0.5069, 0.4885, 0.4953, 0.5018, 0.5000,
         0.5024, 0.5099, 0.5007, 0.4961, 0.4887, 0.4925, 0.5076, 0.4925, 0.5095,
         0.4962, 0.5141],
        [0.4982, 0.5073, 0.4990, 0.5063, 0.5067, 0.4898, 0.4959, 0.5027, 0.5023,
         0.5028, 0.5083, 0.4992, 0.4962, 0.4882, 0.4904, 0.5077, 0.4918, 0.5088,
         0.4965, 0.5126],
        [0.4979, 0.5072, 0.4982, 0.5066, 0.5063, 0.4888, 0.4963, 0.5005, 0.5041,
         0.5013, 0.5066, 0.4991, 0.4964, 0.4882, 0.4902, 0.5097, 0.4934, 0.5089,
         0.4957, 0.5147],
        [0.4986, 0.5060, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.5000, 0.5024,
         0.5037, 0.5062, 0.4985, 0.4971, 0.4880, 0.4895, 0.5078, 0.4922, 0.5101,
         0.4932, 0.5141],
        [0.4986, 0.5076, 0.4981, 0.5051, 0.5055, 0.4887, 0.4956, 0.4990, 0.5020,
         0.5026, 0.5087, 0.4994, 0.4967, 0.4882, 0.4917, 0.5062, 0.4944, 0.5098,
         0.4953, 0.5132],
        [0.4975, 0.5046, 0.4976, 0.5060, 0.5053, 0.4883, 0.4946, 0.5001, 0.5010,
         0.5030, 0.5074, 0.4998, 0.4953, 0.4880, 0.4902, 0.5085, 0.4952, 0.5081,
         0.4950, 0.5133],
        [0.4978, 0.5056, 0.4982, 0.5045, 0.5061, 0.4876, 0.4972, 0.5008, 0.5031,
         0.5018, 0.5103, 0.4997, 0.4959, 0.4872, 0.4908, 0.5081, 0.4954, 0.5077,
         0.4967, 0.5127],
        [0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5030,
         0.5018, 0.5096, 0.4997, 0.4963, 0.4889, 0.4919, 0.5079, 0.4934, 0.5063,
         0.4933, 0.5127],
        [0.4981, 0.5048, 0.4974, 0.5050, 0.5079, 0.4880, 0.4960, 0.5000, 0.5018,
         0.5008, 0.5065, 0.5012, 0.4975, 0.4886, 0.4923, 0.5082, 0.4928, 0.5087,
         0.4941, 0.5143],
        [0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4882, 0.4961, 0.5017, 0.5019,
         0.5040, 0.5075, 0.5002, 0.4944, 0.4907, 0.4927, 0.5094, 0.4941, 0.5050,
         0.4952, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4973, 0.5065, 0.4988, 0.5064, 0.5069, 0.4885, 0.4953, 0.5018, 0.5000,
        0.5024, 0.5099, 0.5007, 0.4961, 0.4887, 0.4925, 0.5076, 0.4925, 0.5095,
        0.4962, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5073, 0.4990, 0.5063, 0.5067, 0.4898, 0.4959, 0.5027, 0.5023,
        0.5028, 0.5083, 0.4992, 0.4962, 0.4882, 0.4904, 0.5077, 0.4918, 0.5088,
        0.4965, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5072, 0.4982, 0.5066, 0.5063, 0.4888, 0.4963, 0.5005, 0.5041,
        0.5013, 0.5066, 0.4991, 0.4964, 0.4882, 0.4902, 0.5097, 0.4934, 0.5089,
        0.4957, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5060, 0.4986, 0.5069, 0.5062, 0.4896, 0.4946, 0.5000, 0.5024,
        0.5037, 0.5062, 0.4985, 0.4971, 0.4880, 0.4895, 0.5078, 0.4922, 0.5101,
        0.4932, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5076, 0.4981, 0.5051, 0.5055, 0.4887, 0.4956, 0.4990, 0.5020,
        0.5026, 0.5087, 0.4994, 0.4967, 0.4882, 0.4917, 0.5062, 0.4944, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4975, 0.5046, 0.4976, 0.5060, 0.5053, 0.4883, 0.4946, 0.5001, 0.5010,
        0.5030, 0.5074, 0.4998, 0.4953, 0.4880, 0.4902, 0.5085, 0.4952, 0.5081,
        0.4950, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5056, 0.4982, 0.5045, 0.5061, 0.4876, 0.4972, 0.5008, 0.5031,
        0.5018, 0.5103, 0.4997, 0.4959, 0.4872, 0.4908, 0.5081, 0.4954, 0.5077,
        0.4967, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4953, 0.5011, 0.5030,
        0.5018, 0.5096, 0.4997, 0.4963, 0.4889, 0.4919, 0.5079, 0.4934, 0.5063,
        0.4933, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5048, 0.4974, 0.5050, 0.5079, 0.4880, 0.4960, 0.5000, 0.5018,
        0.5008, 0.5065, 0.5012, 0.4975, 0.4886, 0.4923, 0.5082, 0.4928, 0.5087,
        0.4941, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5049, 0.5073, 0.4882, 0.4961, 0.5017, 0.5019,
        0.5040, 0.5075, 0.5002, 0.4944, 0.4907, 0.4927, 0.5094, 0.4941, 0.5050,
        0.4952, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5103, 0.4891, 0.5119, 0.4865, 0.4853, 0.4941, 0.4894, 0.4942, 0.4987,
         0.5074, 0.5026, 0.5000, 0.5025, 0.4917, 0.5045, 0.4912, 0.4858, 0.4986,
         0.5078, 0.4947],
        [0.5087, 0.4895, 0.5109, 0.4851, 0.4855, 0.4927, 0.4883, 0.4953, 0.4981,
         0.5052, 0.5036, 0.4999, 0.5034, 0.4916, 0.5029, 0.4887, 0.4862, 0.4984,
         0.5071, 0.4933],
        [0.5103, 0.4904, 0.5108, 0.4869, 0.4852, 0.4942, 0.4888, 0.4964, 0.5012,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4919, 0.5040, 0.4909, 0.4874, 0.4981,
         0.5069, 0.4925],
        [0.5078, 0.4893, 0.5116, 0.4860, 0.4872, 0.4932, 0.4897, 0.4953, 0.4984,
         0.5086, 0.5039, 0.5000, 0.5050, 0.4914, 0.5039, 0.4906, 0.4862, 0.4986,
         0.5058, 0.4923],
        [0.5095, 0.4912, 0.5131, 0.4861, 0.4872, 0.4928, 0.4909, 0.4923, 0.4955,
         0.5066, 0.5044, 0.4998, 0.5031, 0.4914, 0.5038, 0.4893, 0.4855, 0.4962,
         0.5054, 0.4936],
        [0.5111, 0.4917, 0.5102, 0.4865, 0.4863, 0.4945, 0.4907, 0.4938, 0.4970,
         0.5069, 0.5061, 0.5002, 0.5038, 0.4895, 0.5040, 0.4900, 0.4851, 0.4971,
         0.5062, 0.4931],
        [0.5095, 0.4913, 0.5125, 0.4881, 0.4858, 0.4937, 0.4888, 0.4942, 0.4976,
         0.5071, 0.5063, 0.4996, 0.5049, 0.4903, 0.5036, 0.4892, 0.4846, 0.4987,
         0.5072, 0.4932],
        [0.5101, 0.4913, 0.5112, 0.4853, 0.4866, 0.4927, 0.4892, 0.4949, 0.4998,
         0.5072, 0.5067, 0.5008, 0.5035, 0.4893, 0.5049, 0.4913, 0.4856, 0.4958,
         0.5076, 0.4932],
        [0.5110, 0.4926, 0.5109, 0.4851, 0.4885, 0.4929, 0.4895, 0.4941, 0.4979,
         0.5075, 0.5048, 0.4975, 0.5041, 0.4897, 0.5042, 0.4905, 0.4867, 0.4966,
         0.5083, 0.4936],
        [0.5113, 0.4920, 0.5118, 0.4872, 0.4880, 0.4928, 0.4893, 0.4933, 0.4999,
         0.5051, 0.5052, 0.4986, 0.5036, 0.4888, 0.5004, 0.4931, 0.4836, 0.4975,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5103, 0.4891, 0.5119, 0.4865, 0.4853, 0.4941, 0.4894, 0.4942, 0.4987,
        0.5074, 0.5026, 0.5000, 0.5025, 0.4917, 0.5045, 0.4912, 0.4858, 0.4986,
        0.5078, 0.4947], grad_fn=<UnbindBackward>), tensor([0.5087, 0.4895, 0.5109, 0.4851, 0.4855, 0.4927, 0.4883, 0.4953, 0.4981,
        0.5052, 0.5036, 0.4999, 0.5034, 0.4916, 0.5029, 0.4887, 0.4862, 0.4984,
        0.5071, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5103, 0.4904, 0.5108, 0.4869, 0.4852, 0.4942, 0.4888, 0.4964, 0.5012,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4919, 0.5040, 0.4909, 0.4874, 0.4981,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4893, 0.5116, 0.4860, 0.4872, 0.4932, 0.4897, 0.4953, 0.4984,
        0.5086, 0.5039, 0.5000, 0.5050, 0.4914, 0.5039, 0.4906, 0.4862, 0.4986,
        0.5058, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5131, 0.4861, 0.4872, 0.4928, 0.4909, 0.4923, 0.4955,
        0.5066, 0.5044, 0.4998, 0.5031, 0.4914, 0.5038, 0.4893, 0.4855, 0.4962,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4917, 0.5102, 0.4865, 0.4863, 0.4945, 0.4907, 0.4938, 0.4970,
        0.5069, 0.5061, 0.5002, 0.5038, 0.4895, 0.5040, 0.4900, 0.4851, 0.4971,
        0.5062, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4913, 0.5125, 0.4881, 0.4858, 0.4937, 0.4888, 0.4942, 0.4976,
        0.5071, 0.5063, 0.4996, 0.5049, 0.4903, 0.5036, 0.4892, 0.4846, 0.4987,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4913, 0.5112, 0.4853, 0.4866, 0.4927, 0.4892, 0.4949, 0.4998,
        0.5072, 0.5067, 0.5008, 0.5035, 0.4893, 0.5049, 0.4913, 0.4856, 0.4958,
        0.5076, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4926, 0.5109, 0.4851, 0.4885, 0.4929, 0.4895, 0.4941, 0.4979,
        0.5075, 0.5048, 0.4975, 0.5041, 0.4897, 0.5042, 0.4905, 0.4867, 0.4966,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4920, 0.5118, 0.4872, 0.4880, 0.4928, 0.4893, 0.4933, 0.4999,
        0.5051, 0.5052, 0.4986, 0.5036, 0.4888, 0.5004, 0.4931, 0.4836, 0.4975,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5125, 0.4963, 0.4966, 0.5026, 0.5066, 0.4926, 0.5092, 0.4953, 0.4968,
         0.5027, 0.4855, 0.4976, 0.4839, 0.5023, 0.5024, 0.4983, 0.4992, 0.5142,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4955, 0.5045, 0.5035, 0.4932, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4966, 0.4857, 0.5014, 0.5002, 0.4978, 0.4981, 0.5148,
         0.4946, 0.4971],
        [0.5131, 0.4973, 0.4974, 0.5024, 0.5046, 0.4922, 0.5103, 0.4979, 0.4963,
         0.5028, 0.4861, 0.4989, 0.4847, 0.5015, 0.4998, 0.4975, 0.4991, 0.5166,
         0.4946, 0.4968],
        [0.5126, 0.4974, 0.4969, 0.5032, 0.5041, 0.4914, 0.5109, 0.4966, 0.4963,
         0.5027, 0.4842, 0.4966, 0.4846, 0.5022, 0.5007, 0.4974, 0.5010, 0.5144,
         0.4953, 0.4969],
        [0.5127, 0.4982, 0.4963, 0.5038, 0.5033, 0.4935, 0.5088, 0.4953, 0.4969,
         0.5031, 0.4872, 0.4980, 0.4826, 0.5027, 0.5003, 0.4993, 0.5000, 0.5151,
         0.4955, 0.4972],
        [0.5136, 0.4973, 0.4961, 0.5031, 0.5052, 0.4927, 0.5078, 0.4968, 0.4956,
         0.5025, 0.4856, 0.4963, 0.4853, 0.5033, 0.5019, 0.5003, 0.4994, 0.5146,
         0.4951, 0.4982],
        [0.5114, 0.4964, 0.4980, 0.5025, 0.5039, 0.4922, 0.5088, 0.4963, 0.4960,
         0.5026, 0.4879, 0.4971, 0.4849, 0.5027, 0.5009, 0.4988, 0.4998, 0.5138,
         0.4952, 0.4996],
        [0.5120, 0.4972, 0.4951, 0.5012, 0.5040, 0.4922, 0.5083, 0.4944, 0.4970,
         0.5023, 0.4859, 0.4985, 0.4865, 0.5012, 0.5009, 0.4986, 0.4997, 0.5153,
         0.4959, 0.4986],
        [0.5128, 0.4981, 0.4951, 0.5011, 0.5048, 0.4925, 0.5096, 0.4966, 0.4968,
         0.5019, 0.4885, 0.4950, 0.4851, 0.5017, 0.5012, 0.4971, 0.5007, 0.5140,
         0.4957, 0.4994],
        [0.5126, 0.4984, 0.4939, 0.5028, 0.5049, 0.4938, 0.5079, 0.4988, 0.4964,
         0.5033, 0.4885, 0.4980, 0.4857, 0.5001, 0.5010, 0.4982, 0.4994, 0.5155,
         0.4971, 0.5004]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5125, 0.4963, 0.4966, 0.5026, 0.5066, 0.4926, 0.5092, 0.4953, 0.4968,
        0.5027, 0.4855, 0.4976, 0.4839, 0.5023, 0.5024, 0.4983, 0.4992, 0.5142,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4955, 0.5045, 0.5035, 0.4932, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4966, 0.4857, 0.5014, 0.5002, 0.4978, 0.4981, 0.5148,
        0.4946, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4973, 0.4974, 0.5024, 0.5046, 0.4922, 0.5103, 0.4979, 0.4963,
        0.5028, 0.4861, 0.4989, 0.4847, 0.5015, 0.4998, 0.4975, 0.4991, 0.5166,
        0.4946, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4969, 0.5032, 0.5041, 0.4914, 0.5109, 0.4966, 0.4963,
        0.5027, 0.4842, 0.4966, 0.4846, 0.5022, 0.5007, 0.4974, 0.5010, 0.5144,
        0.4953, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4982, 0.4963, 0.5038, 0.5033, 0.4935, 0.5088, 0.4953, 0.4969,
        0.5031, 0.4872, 0.4980, 0.4826, 0.5027, 0.5003, 0.4993, 0.5000, 0.5151,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4973, 0.4961, 0.5031, 0.5052, 0.4927, 0.5078, 0.4968, 0.4956,
        0.5025, 0.4856, 0.4963, 0.4853, 0.5033, 0.5019, 0.5003, 0.4994, 0.5146,
        0.4951, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4964, 0.4980, 0.5025, 0.5039, 0.4922, 0.5088, 0.4963, 0.4960,
        0.5026, 0.4879, 0.4971, 0.4849, 0.5027, 0.5009, 0.4988, 0.4998, 0.5138,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4972, 0.4951, 0.5012, 0.5040, 0.4922, 0.5083, 0.4944, 0.4970,
        0.5023, 0.4859, 0.4985, 0.4865, 0.5012, 0.5009, 0.4986, 0.4997, 0.5153,
        0.4959, 0.4986], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4951, 0.5011, 0.5048, 0.4925, 0.5096, 0.4966, 0.4968,
        0.5019, 0.4885, 0.4950, 0.4851, 0.5017, 0.5012, 0.4971, 0.5007, 0.5140,
        0.4957, 0.4994], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4984, 0.4939, 0.5028, 0.5049, 0.4938, 0.5079, 0.4988, 0.4964,
        0.5033, 0.4885, 0.4980, 0.4857, 0.5001, 0.5010, 0.4982, 0.4994, 0.5155,
        0.4971, 0.5004], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [8, 3, 9, 1, 5, 0, 2, 7, 6, 4]
replay_buffer._size: [6900 6900 6900 6900 6900 6900 6900 6900 6900 6900]
2023-08-12 10:40:51,996 MainThread INFO: EPOCH:44
2023-08-12 10:40:51,996 MainThread INFO: Time Consumed:0.34525084495544434s
2023-08-12 10:40:51,996 MainThread INFO: Total Frames:67500s
 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 45/80 [00:45<00:42,  1.21s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1352.99073
Train_Epoch_Reward                    12816.92688
Running_Training_Average_Rewards      1876.47117
Explore_Time                          0.00407
Train___Time                          0.33171
Eval____Time                          0.00322
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -44.95386
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.60096
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.91562
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.99072
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.73016
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.41066
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.46629
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 13774.92251
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.66925
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.27774
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           11.18287     0.58831    11.99367    10.51862
alpha_0                               0.93517      0.00040    0.93574     0.93461
alpha_1                               0.93516      0.00040    0.93573     0.93460
alpha_2                               0.93519      0.00040    0.93576     0.93463
alpha_3                               0.93517      0.00040    0.93573     0.93461
alpha_4                               0.93517      0.00040    0.93573     0.93461
alpha_5                               0.93517      0.00040    0.93573     0.93461
alpha_6                               0.93516      0.00040    0.93572     0.93460
alpha_7                               0.93516      0.00040    0.93572     0.93460
alpha_8                               0.93516      0.00040    0.93573     0.93460
alpha_9                               0.93518      0.00040    0.93575     0.93462
Alpha_loss                            -0.44934     0.00270    -0.44512    -0.45289
Training/policy_loss                  -2.75278     0.00438    -2.74781    -2.75814
Training/qf1_loss                     2496.45444   375.78979  2821.84644  2004.38867
Training/qf2_loss                     2494.79102   375.72512  2819.99268  2002.81421
Training/pf_norm                      0.11044      0.03103    0.14933     0.06742
Training/qf1_norm                     34.89787     1.43326    36.63306    33.11433
Training/qf2_norm                     40.53321     1.69445    42.68261    38.55827
log_std/mean                          -0.13259     0.00005    -0.13252    -0.13266
log_std/std                           0.00951      0.00002    0.00954     0.00948
log_std/max                           -0.11362     0.00016    -0.11350    -0.11394
log_std/min                           -0.15344     0.00039    -0.15282    -0.15383
log_probs/mean                        -2.73410     0.00536    -2.72675    -2.74117
log_probs/std                         0.23637      0.01806    0.26394     0.21236
log_probs/max                         -2.15155     0.03295    -2.11131    -2.19921
log_probs/min                         -5.14945     0.90586    -4.23556    -6.81048
mean/mean                             -0.00080     0.00025    -0.00045    -0.00113
mean/std                              0.00746      0.00009    0.00760     0.00734
mean/max                              0.01044      0.00012    0.01064     0.01031
mean/min                              -0.01253     0.00042    -0.01199    -0.01313
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [8, 1, 6, 3, 0, 9, 5, 2, 4, 7]
replay_buffer._size: [7050 7050 7050 7050 7050 7050 7050 7050 7050 7050]
snapshot at best
2023-08-12 10:40:53,947 MainThread INFO: EPOCH:45
2023-08-12 10:40:53,947 MainThread INFO: Time Consumed:1.8211982250213623s
2023-08-12 10:40:53,947 MainThread INFO: Total Frames:69000s
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 46/80 [00:47<00:48,  1.43s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1354.37486
Train_Epoch_Reward                    12278.21971
Running_Training_Average_Rewards      1112.25024
Explore_Time                          0.00285
Train___Time                          1.18960
Eval____Time                          0.00295
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.39068
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.72505
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.90065
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.06225
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.77906
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.45250
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.50934
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 13793.60410
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.79989
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.23612
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.75501      0.37978    10.23375    9.17118
alpha_0                               0.93377      0.00040    0.93433     0.93321
alpha_1                               0.93376      0.00040    0.93432     0.93320
alpha_2                               0.93379      0.00040    0.93435     0.93323
alpha_3                               0.93376      0.00040    0.93432     0.93320
alpha_4                               0.93377      0.00040    0.93433     0.93320
alpha_5                               0.93376      0.00040    0.93432     0.93320
alpha_6                               0.93376      0.00040    0.93432     0.93320
alpha_7                               0.93376      0.00040    0.93432     0.93319
alpha_8                               0.93376      0.00040    0.93432     0.93320
alpha_9                               0.93378      0.00040    0.93434     0.93322
Alpha_loss                            -0.45957     0.00273    -0.45524    -0.46329
Training/policy_loss                  -2.75859     0.00527    -2.75225    -2.76798
Training/qf1_loss                     1789.09272   224.80611  2133.13574  1500.59338
Training/qf2_loss                     1787.51340   224.71240  2131.36475  1498.99036
Training/pf_norm                      0.11905      0.01535    0.14512     0.10324
Training/qf1_norm                     31.66600     1.00332    32.79718    30.01486
Training/qf2_norm                     36.94491     1.45992    38.51775    34.28132
log_std/mean                          -0.13254     0.00003    -0.13250    -0.13258
log_std/std                           0.00941      0.00004    0.00946     0.00934
log_std/max                           -0.11353     0.00034    -0.11303    -0.11403
log_std/min                           -0.15349     0.00024    -0.15313    -0.15375
log_probs/mean                        -2.73571     0.00607    -2.73087    -2.74723
log_probs/std                         0.23369      0.01283    0.25135     0.21744
log_probs/max                         -2.14735     0.06689    -2.02232    -2.20435
log_probs/min                         -4.66167     0.33215    -4.26691    -5.15863
mean/mean                             -0.00129     0.00008    -0.00121    -0.00142
mean/std                              0.00784      0.00016    0.00809     0.00763
mean/max                              0.01105      0.00035    0.01158     0.01055
mean/min                              -0.01380     0.00034    -0.01337    -0.01433
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4973, 0.5065, 0.4989, 0.5064, 0.5068, 0.4885, 0.4953, 0.5018, 0.4999,
         0.5024, 0.5099, 0.5007, 0.4961, 0.4887, 0.4924, 0.5076, 0.4925, 0.5095,
         0.4963, 0.5141],
        [0.4982, 0.5073, 0.4989, 0.5063, 0.5067, 0.4898, 0.4958, 0.5027, 0.5023,
         0.5029, 0.5083, 0.4991, 0.4962, 0.4882, 0.4904, 0.5077, 0.4918, 0.5088,
         0.4964, 0.5126],
        [0.4979, 0.5073, 0.4982, 0.5066, 0.5063, 0.4888, 0.4963, 0.5004, 0.5041,
         0.5014, 0.5067, 0.4989, 0.4964, 0.4882, 0.4901, 0.5096, 0.4934, 0.5089,
         0.4957, 0.5147],
        [0.4985, 0.5060, 0.4986, 0.5069, 0.5061, 0.4896, 0.4946, 0.5000, 0.5024,
         0.5037, 0.5063, 0.4985, 0.4971, 0.4880, 0.4895, 0.5078, 0.4921, 0.5102,
         0.4932, 0.5141],
        [0.4985, 0.5076, 0.4981, 0.5051, 0.5054, 0.4887, 0.4956, 0.4989, 0.5020,
         0.5026, 0.5087, 0.4993, 0.4967, 0.4882, 0.4917, 0.5062, 0.4944, 0.5098,
         0.4953, 0.5132],
        [0.4974, 0.5047, 0.4976, 0.5060, 0.5052, 0.4883, 0.4947, 0.5000, 0.5010,
         0.5029, 0.5075, 0.4998, 0.4953, 0.4879, 0.4902, 0.5085, 0.4952, 0.5082,
         0.4951, 0.5134],
        [0.4976, 0.5056, 0.4984, 0.5045, 0.5061, 0.4877, 0.4972, 0.5007, 0.5032,
         0.5017, 0.5102, 0.4996, 0.4960, 0.4871, 0.4908, 0.5082, 0.4954, 0.5077,
         0.4967, 0.5127],
        [0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4952, 0.5010, 0.5030,
         0.5018, 0.5096, 0.4997, 0.4963, 0.4889, 0.4919, 0.5079, 0.4933, 0.5063,
         0.4933, 0.5127],
        [0.4980, 0.5048, 0.4974, 0.5049, 0.5079, 0.4880, 0.4960, 0.5000, 0.5018,
         0.5008, 0.5065, 0.5012, 0.4975, 0.4886, 0.4922, 0.5082, 0.4928, 0.5087,
         0.4941, 0.5143],
        [0.4984, 0.5057, 0.4995, 0.5048, 0.5073, 0.4882, 0.4962, 0.5017, 0.5019,
         0.5039, 0.5076, 0.5003, 0.4943, 0.4907, 0.4928, 0.5094, 0.4941, 0.5049,
         0.4952, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4973, 0.5065, 0.4989, 0.5064, 0.5068, 0.4885, 0.4953, 0.5018, 0.4999,
        0.5024, 0.5099, 0.5007, 0.4961, 0.4887, 0.4924, 0.5076, 0.4925, 0.5095,
        0.4963, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5073, 0.4989, 0.5063, 0.5067, 0.4898, 0.4958, 0.5027, 0.5023,
        0.5029, 0.5083, 0.4991, 0.4962, 0.4882, 0.4904, 0.5077, 0.4918, 0.5088,
        0.4964, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5073, 0.4982, 0.5066, 0.5063, 0.4888, 0.4963, 0.5004, 0.5041,
        0.5014, 0.5067, 0.4989, 0.4964, 0.4882, 0.4901, 0.5096, 0.4934, 0.5089,
        0.4957, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5060, 0.4986, 0.5069, 0.5061, 0.4896, 0.4946, 0.5000, 0.5024,
        0.5037, 0.5063, 0.4985, 0.4971, 0.4880, 0.4895, 0.5078, 0.4921, 0.5102,
        0.4932, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5076, 0.4981, 0.5051, 0.5054, 0.4887, 0.4956, 0.4989, 0.5020,
        0.5026, 0.5087, 0.4993, 0.4967, 0.4882, 0.4917, 0.5062, 0.4944, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4974, 0.5047, 0.4976, 0.5060, 0.5052, 0.4883, 0.4947, 0.5000, 0.5010,
        0.5029, 0.5075, 0.4998, 0.4953, 0.4879, 0.4902, 0.5085, 0.4952, 0.5082,
        0.4951, 0.5134], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5056, 0.4984, 0.5045, 0.5061, 0.4877, 0.4972, 0.5007, 0.5032,
        0.5017, 0.5102, 0.4996, 0.4960, 0.4871, 0.4908, 0.5082, 0.4954, 0.5077,
        0.4967, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4952, 0.5010, 0.5030,
        0.5018, 0.5096, 0.4997, 0.4963, 0.4889, 0.4919, 0.5079, 0.4933, 0.5063,
        0.4933, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5048, 0.4974, 0.5049, 0.5079, 0.4880, 0.4960, 0.5000, 0.5018,
        0.5008, 0.5065, 0.5012, 0.4975, 0.4886, 0.4922, 0.5082, 0.4928, 0.5087,
        0.4941, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5048, 0.5073, 0.4882, 0.4962, 0.5017, 0.5019,
        0.5039, 0.5076, 0.5003, 0.4943, 0.4907, 0.4928, 0.5094, 0.4941, 0.5049,
        0.4952, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5103, 0.4892, 0.5120, 0.4865, 0.4853, 0.4941, 0.4895, 0.4943, 0.4987,
         0.5074, 0.5027, 0.5001, 0.5024, 0.4917, 0.5045, 0.4912, 0.4858, 0.4986,
         0.5078, 0.4947],
        [0.5088, 0.4895, 0.5108, 0.4851, 0.4856, 0.4928, 0.4884, 0.4953, 0.4981,
         0.5052, 0.5037, 0.4999, 0.5034, 0.4916, 0.5029, 0.4889, 0.4862, 0.4984,
         0.5070, 0.4933],
        [0.5104, 0.4904, 0.5107, 0.4869, 0.4852, 0.4943, 0.4888, 0.4964, 0.5012,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4919, 0.5040, 0.4910, 0.4875, 0.4981,
         0.5069, 0.4925],
        [0.5079, 0.4893, 0.5116, 0.4861, 0.4872, 0.4933, 0.4897, 0.4952, 0.4984,
         0.5086, 0.5039, 0.5000, 0.5050, 0.4914, 0.5039, 0.4906, 0.4862, 0.4985,
         0.5057, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4861, 0.4872, 0.4928, 0.4909, 0.4923, 0.4956,
         0.5066, 0.5043, 0.4998, 0.5031, 0.4914, 0.5039, 0.4893, 0.4854, 0.4962,
         0.5053, 0.4936],
        [0.5111, 0.4916, 0.5101, 0.4866, 0.4862, 0.4945, 0.4908, 0.4939, 0.4970,
         0.5068, 0.5060, 0.5002, 0.5038, 0.4896, 0.5040, 0.4900, 0.4850, 0.4971,
         0.5061, 0.4931],
        [0.5096, 0.4913, 0.5124, 0.4881, 0.4858, 0.4937, 0.4889, 0.4942, 0.4976,
         0.5071, 0.5062, 0.4995, 0.5048, 0.4903, 0.5036, 0.4892, 0.4847, 0.4986,
         0.5071, 0.4932],
        [0.5102, 0.4913, 0.5112, 0.4853, 0.4867, 0.4927, 0.4891, 0.4949, 0.4998,
         0.5072, 0.5067, 0.5008, 0.5035, 0.4893, 0.5049, 0.4913, 0.4856, 0.4958,
         0.5076, 0.4932],
        [0.5110, 0.4926, 0.5109, 0.4851, 0.4885, 0.4929, 0.4895, 0.4941, 0.4979,
         0.5075, 0.5049, 0.4975, 0.5041, 0.4897, 0.5042, 0.4905, 0.4866, 0.4966,
         0.5083, 0.4936],
        [0.5113, 0.4919, 0.5118, 0.4871, 0.4880, 0.4928, 0.4893, 0.4933, 0.4998,
         0.5051, 0.5052, 0.4986, 0.5036, 0.4888, 0.5004, 0.4931, 0.4836, 0.4975,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5103, 0.4892, 0.5120, 0.4865, 0.4853, 0.4941, 0.4895, 0.4943, 0.4987,
        0.5074, 0.5027, 0.5001, 0.5024, 0.4917, 0.5045, 0.4912, 0.4858, 0.4986,
        0.5078, 0.4947], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4895, 0.5108, 0.4851, 0.4856, 0.4928, 0.4884, 0.4953, 0.4981,
        0.5052, 0.5037, 0.4999, 0.5034, 0.4916, 0.5029, 0.4889, 0.4862, 0.4984,
        0.5070, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5104, 0.4904, 0.5107, 0.4869, 0.4852, 0.4943, 0.4888, 0.4964, 0.5012,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4919, 0.5040, 0.4910, 0.4875, 0.4981,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4893, 0.5116, 0.4861, 0.4872, 0.4933, 0.4897, 0.4952, 0.4984,
        0.5086, 0.5039, 0.5000, 0.5050, 0.4914, 0.5039, 0.4906, 0.4862, 0.4985,
        0.5057, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4861, 0.4872, 0.4928, 0.4909, 0.4923, 0.4956,
        0.5066, 0.5043, 0.4998, 0.5031, 0.4914, 0.5039, 0.4893, 0.4854, 0.4962,
        0.5053, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4916, 0.5101, 0.4866, 0.4862, 0.4945, 0.4908, 0.4939, 0.4970,
        0.5068, 0.5060, 0.5002, 0.5038, 0.4896, 0.5040, 0.4900, 0.4850, 0.4971,
        0.5061, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4913, 0.5124, 0.4881, 0.4858, 0.4937, 0.4889, 0.4942, 0.4976,
        0.5071, 0.5062, 0.4995, 0.5048, 0.4903, 0.5036, 0.4892, 0.4847, 0.4986,
        0.5071, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5102, 0.4913, 0.5112, 0.4853, 0.4867, 0.4927, 0.4891, 0.4949, 0.4998,
        0.5072, 0.5067, 0.5008, 0.5035, 0.4893, 0.5049, 0.4913, 0.4856, 0.4958,
        0.5076, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4926, 0.5109, 0.4851, 0.4885, 0.4929, 0.4895, 0.4941, 0.4979,
        0.5075, 0.5049, 0.4975, 0.5041, 0.4897, 0.5042, 0.4905, 0.4866, 0.4966,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4919, 0.5118, 0.4871, 0.4880, 0.4928, 0.4893, 0.4933, 0.4998,
        0.5051, 0.5052, 0.4986, 0.5036, 0.4888, 0.5004, 0.4931, 0.4836, 0.4975,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5125, 0.4963, 0.4966, 0.5026, 0.5067, 0.4927, 0.5092, 0.4953, 0.4968,
         0.5027, 0.4855, 0.4976, 0.4839, 0.5023, 0.5023, 0.4983, 0.4992, 0.5143,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4956, 0.5045, 0.5036, 0.4931, 0.5103, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4966, 0.4857, 0.5015, 0.5002, 0.4978, 0.4981, 0.5148,
         0.4946, 0.4970],
        [0.5131, 0.4972, 0.4975, 0.5024, 0.5047, 0.4921, 0.5103, 0.4978, 0.4962,
         0.5027, 0.4861, 0.4988, 0.4846, 0.5016, 0.4999, 0.4976, 0.4992, 0.5166,
         0.4947, 0.4969],
        [0.5126, 0.4974, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4963,
         0.5027, 0.4842, 0.4966, 0.4845, 0.5023, 0.5008, 0.4974, 0.5011, 0.5144,
         0.4954, 0.4970],
        [0.5127, 0.4982, 0.4963, 0.5038, 0.5033, 0.4935, 0.5088, 0.4953, 0.4968,
         0.5031, 0.4871, 0.4980, 0.4826, 0.5027, 0.5003, 0.4993, 0.5001, 0.5152,
         0.4956, 0.4972],
        [0.5136, 0.4973, 0.4961, 0.5031, 0.5052, 0.4927, 0.5078, 0.4968, 0.4956,
         0.5026, 0.4856, 0.4962, 0.4853, 0.5032, 0.5018, 0.5003, 0.4995, 0.5146,
         0.4952, 0.4983],
        [0.5113, 0.4964, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4963, 0.4960,
         0.5027, 0.4878, 0.4971, 0.4850, 0.5027, 0.5009, 0.4989, 0.5001, 0.5139,
         0.4954, 0.4997],
        [0.5120, 0.4972, 0.4951, 0.5011, 0.5040, 0.4922, 0.5083, 0.4944, 0.4969,
         0.5023, 0.4859, 0.4985, 0.4864, 0.5012, 0.5009, 0.4986, 0.4997, 0.5153,
         0.4959, 0.4987],
        [0.5128, 0.4981, 0.4951, 0.5011, 0.5048, 0.4925, 0.5096, 0.4966, 0.4967,
         0.5019, 0.4885, 0.4950, 0.4852, 0.5016, 0.5012, 0.4971, 0.5007, 0.5141,
         0.4957, 0.4994],
        [0.5127, 0.4985, 0.4939, 0.5027, 0.5049, 0.4938, 0.5079, 0.4988, 0.4964,
         0.5033, 0.4885, 0.4980, 0.4857, 0.5001, 0.5009, 0.4982, 0.4994, 0.5155,
         0.4972, 0.5004]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5125, 0.4963, 0.4966, 0.5026, 0.5067, 0.4927, 0.5092, 0.4953, 0.4968,
        0.5027, 0.4855, 0.4976, 0.4839, 0.5023, 0.5023, 0.4983, 0.4992, 0.5143,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4956, 0.5045, 0.5036, 0.4931, 0.5103, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4966, 0.4857, 0.5015, 0.5002, 0.4978, 0.4981, 0.5148,
        0.4946, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4972, 0.4975, 0.5024, 0.5047, 0.4921, 0.5103, 0.4978, 0.4962,
        0.5027, 0.4861, 0.4988, 0.4846, 0.5016, 0.4999, 0.4976, 0.4992, 0.5166,
        0.4947, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4974, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4963,
        0.5027, 0.4842, 0.4966, 0.4845, 0.5023, 0.5008, 0.4974, 0.5011, 0.5144,
        0.4954, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4982, 0.4963, 0.5038, 0.5033, 0.4935, 0.5088, 0.4953, 0.4968,
        0.5031, 0.4871, 0.4980, 0.4826, 0.5027, 0.5003, 0.4993, 0.5001, 0.5152,
        0.4956, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4973, 0.4961, 0.5031, 0.5052, 0.4927, 0.5078, 0.4968, 0.4956,
        0.5026, 0.4856, 0.4962, 0.4853, 0.5032, 0.5018, 0.5003, 0.4995, 0.5146,
        0.4952, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4964, 0.4979, 0.5026, 0.5038, 0.4923, 0.5088, 0.4963, 0.4960,
        0.5027, 0.4878, 0.4971, 0.4850, 0.5027, 0.5009, 0.4989, 0.5001, 0.5139,
        0.4954, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4972, 0.4951, 0.5011, 0.5040, 0.4922, 0.5083, 0.4944, 0.4969,
        0.5023, 0.4859, 0.4985, 0.4864, 0.5012, 0.5009, 0.4986, 0.4997, 0.5153,
        0.4959, 0.4987], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4951, 0.5011, 0.5048, 0.4925, 0.5096, 0.4966, 0.4967,
        0.5019, 0.4885, 0.4950, 0.4852, 0.5016, 0.5012, 0.4971, 0.5007, 0.5141,
        0.4957, 0.4994], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4939, 0.5027, 0.5049, 0.4938, 0.5079, 0.4988, 0.4964,
        0.5033, 0.4885, 0.4980, 0.4857, 0.5001, 0.5009, 0.4982, 0.4994, 0.5155,
        0.4972, 0.5004], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [7, 9, 1, 6, 4, 8, 0, 3, 2, 5]
replay_buffer._size: [7200 7200 7200 7200 7200 7200 7200 7200 7200 7200]
snapshot at best
2023-08-12 10:40:56,041 MainThread INFO: EPOCH:46
2023-08-12 10:40:56,042 MainThread INFO: Time Consumed:1.6682307720184326s
2023-08-12 10:40:56,042 MainThread INFO: Total Frames:70500s
 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 47/80 [00:49<00:53,  1.63s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1358.45472
Train_Epoch_Reward                    1423.15533
Running_Training_Average_Rewards      883.94340
Explore_Time                          0.00300
Train___Time                          1.05350
Eval____Time                          0.01641
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -43.57711
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.81798
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.83734
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -23.02636
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.63827
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.34014
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.40162
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 13828.22327
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.89285
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -25.14441
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.04813     0.67342    11.30724    9.37257
alpha_0                               0.93237      0.00040    0.93293     0.93181
alpha_1                               0.93236      0.00040    0.93292     0.93180
alpha_2                               0.93238      0.00040    0.93295     0.93182
alpha_3                               0.93236      0.00040    0.93292     0.93180
alpha_4                               0.93236      0.00040    0.93292     0.93180
alpha_5                               0.93236      0.00040    0.93292     0.93180
alpha_6                               0.93236      0.00040    0.93292     0.93180
alpha_7                               0.93235      0.00040    0.93291     0.93179
alpha_8                               0.93235      0.00040    0.93292     0.93179
alpha_9                               0.93238      0.00040    0.93294     0.93181
Alpha_loss                            -0.46969     0.00322    -0.46511    -0.47433
Training/policy_loss                  -2.76369     0.00631    -2.75479    -2.77347
Training/qf1_loss                     1948.15935   264.59106  2435.04443  1644.18201
Training/qf2_loss                     1946.56960   264.50254  2433.28149  1642.69116
Training/pf_norm                      0.09965      0.02230    0.14246     0.08209
Training/qf1_norm                     32.61211     1.78508    36.03648    30.90504
Training/qf2_norm                     37.77114     2.08666    41.74900    35.69471
log_std/mean                          -0.13265     0.00009    -0.13256    -0.13278
log_std/std                           0.00940      0.00004    0.00947     0.00936
log_std/max                           -0.11350     0.00025    -0.11314    -0.11393
log_std/min                           -0.15421     0.00037    -0.15371    -0.15479
log_probs/mean                        -2.73554     0.00546    -2.72795    -2.74408
log_probs/std                         0.23587      0.00623    0.24390     0.22789
log_probs/max                         -2.12367     0.02011    -2.09805    -2.15760
log_probs/min                         -4.95466     0.36185    -4.59711    -5.59098
mean/mean                             -0.00144     0.00004    -0.00141    -0.00152
mean/std                              0.00841      0.00012    0.00856     0.00824
mean/max                              0.01226      0.00030    0.01268     0.01193
mean/min                              -0.01454     0.00010    -0.01438    -0.01466
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [8, 5, 3, 2, 1, 9, 4, 7, 6, 0]
replay_buffer._size: [7350 7350 7350 7350 7350 7350 7350 7350 7350 7350]
snapshot at best
2023-08-12 10:40:57,191 MainThread INFO: EPOCH:47
2023-08-12 10:40:57,191 MainThread INFO: Time Consumed:1.0054254531860352s
2023-08-12 10:40:57,191 MainThread INFO: Total Frames:72000s
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 48/80 [00:50<00:47,  1.50s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1376.48911
Train_Epoch_Reward                    8496.76747
Running_Training_Average_Rewards      739.93808
Explore_Time                          0.00428
Train___Time                          0.44224
Eval____Time                          0.00331
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.54376
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.81101
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.89451
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.88014
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.54191
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.26121
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.32575
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14013.00998
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.88150
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.97909
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.38975     0.56403    11.05163    9.47917
alpha_0                               0.93097      0.00040    0.93153     0.93041
alpha_1                               0.93096      0.00040    0.93152     0.93040
alpha_2                               0.93098      0.00040    0.93154     0.93042
alpha_3                               0.93096      0.00040    0.93152     0.93040
alpha_4                               0.93096      0.00040    0.93152     0.93040
alpha_5                               0.93096      0.00040    0.93152     0.93040
alpha_6                               0.93096      0.00040    0.93152     0.93040
alpha_7                               0.93095      0.00040    0.93151     0.93039
alpha_8                               0.93095      0.00040    0.93151     0.93039
alpha_9                               0.93097      0.00040    0.93153     0.93041
Alpha_loss                            -0.47974     0.00320    -0.47515    -0.48407
Training/policy_loss                  -2.76768     0.00648    -2.75844    -2.77445
Training/qf1_loss                     2188.79504   282.87261  2518.28247  1833.89331
Training/qf2_loss                     2187.03254   282.86510  2516.43433  1832.06030
Training/pf_norm                      0.09943      0.02585    0.14045     0.06067
Training/qf1_norm                     33.78431     1.42301    35.69169    31.59548
Training/qf2_norm                     39.51169     1.51101    41.63854    37.74282
log_std/mean                          -0.13296     0.00012    -0.13285    -0.13316
log_std/std                           0.00934      0.00007    0.00943     0.00924
log_std/max                           -0.11343     0.00028    -0.11305    -0.11380
log_std/min                           -0.15467     0.00057    -0.15411    -0.15561
log_probs/mean                        -2.73453     0.00520    -2.72690    -2.74028
log_probs/std                         0.23925      0.01241    0.25959     0.22422
log_probs/max                         -2.15035     0.01783    -2.13294    -2.18242
log_probs/min                         -5.05569     0.35239    -4.75318    -5.72549
mean/mean                             -0.00148     0.00007    -0.00136    -0.00155
mean/std                              0.00881      0.00013    0.00897     0.00863
mean/max                              0.01293      0.00009    0.01307     0.01284
mean/min                              -0.01490     0.00013    -0.01472    -0.01507
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4973, 0.5065, 0.4989, 0.5064, 0.5068, 0.4886, 0.4953, 0.5018, 0.4999,
         0.5024, 0.5100, 0.5007, 0.4961, 0.4887, 0.4924, 0.5076, 0.4925, 0.5096,
         0.4962, 0.5141],
        [0.4982, 0.5073, 0.4990, 0.5063, 0.5067, 0.4898, 0.4958, 0.5027, 0.5023,
         0.5029, 0.5083, 0.4991, 0.4962, 0.4882, 0.4903, 0.5077, 0.4918, 0.5088,
         0.4964, 0.5126],
        [0.4978, 0.5073, 0.4982, 0.5066, 0.5062, 0.4888, 0.4962, 0.5005, 0.5041,
         0.5014, 0.5067, 0.4988, 0.4964, 0.4882, 0.4901, 0.5096, 0.4934, 0.5089,
         0.4957, 0.5147],
        [0.4986, 0.5061, 0.4986, 0.5069, 0.5061, 0.4896, 0.4945, 0.5000, 0.5024,
         0.5037, 0.5063, 0.4985, 0.4971, 0.4879, 0.4896, 0.5078, 0.4922, 0.5101,
         0.4932, 0.5141],
        [0.4985, 0.5076, 0.4981, 0.5052, 0.5054, 0.4886, 0.4955, 0.4989, 0.5020,
         0.5026, 0.5087, 0.4992, 0.4967, 0.4882, 0.4916, 0.5062, 0.4945, 0.5098,
         0.4953, 0.5132],
        [0.4974, 0.5047, 0.4977, 0.5060, 0.5052, 0.4882, 0.4947, 0.5001, 0.5010,
         0.5030, 0.5075, 0.4997, 0.4954, 0.4879, 0.4902, 0.5085, 0.4952, 0.5082,
         0.4950, 0.5133],
        [0.4977, 0.5056, 0.4983, 0.5046, 0.5061, 0.4877, 0.4971, 0.5008, 0.5031,
         0.5018, 0.5102, 0.4997, 0.4959, 0.4872, 0.4908, 0.5082, 0.4953, 0.5078,
         0.4967, 0.5127],
        [0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4952, 0.5010, 0.5030,
         0.5018, 0.5096, 0.4997, 0.4963, 0.4888, 0.4919, 0.5079, 0.4933, 0.5063,
         0.4933, 0.5127],
        [0.4980, 0.5048, 0.4974, 0.5049, 0.5079, 0.4880, 0.4960, 0.5001, 0.5018,
         0.5008, 0.5066, 0.5012, 0.4975, 0.4886, 0.4923, 0.5083, 0.4928, 0.5087,
         0.4941, 0.5143],
        [0.4984, 0.5057, 0.4995, 0.5048, 0.5073, 0.4882, 0.4962, 0.5016, 0.5019,
         0.5040, 0.5076, 0.5003, 0.4943, 0.4907, 0.4928, 0.5094, 0.4941, 0.5049,
         0.4952, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4973, 0.5065, 0.4989, 0.5064, 0.5068, 0.4886, 0.4953, 0.5018, 0.4999,
        0.5024, 0.5100, 0.5007, 0.4961, 0.4887, 0.4924, 0.5076, 0.4925, 0.5096,
        0.4962, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5073, 0.4990, 0.5063, 0.5067, 0.4898, 0.4958, 0.5027, 0.5023,
        0.5029, 0.5083, 0.4991, 0.4962, 0.4882, 0.4903, 0.5077, 0.4918, 0.5088,
        0.4964, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5073, 0.4982, 0.5066, 0.5062, 0.4888, 0.4962, 0.5005, 0.5041,
        0.5014, 0.5067, 0.4988, 0.4964, 0.4882, 0.4901, 0.5096, 0.4934, 0.5089,
        0.4957, 0.5147], grad_fn=<UnbindBackward>), tensor([0.4986, 0.5061, 0.4986, 0.5069, 0.5061, 0.4896, 0.4945, 0.5000, 0.5024,
        0.5037, 0.5063, 0.4985, 0.4971, 0.4879, 0.4896, 0.5078, 0.4922, 0.5101,
        0.4932, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5076, 0.4981, 0.5052, 0.5054, 0.4886, 0.4955, 0.4989, 0.5020,
        0.5026, 0.5087, 0.4992, 0.4967, 0.4882, 0.4916, 0.5062, 0.4945, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4974, 0.5047, 0.4977, 0.5060, 0.5052, 0.4882, 0.4947, 0.5001, 0.5010,
        0.5030, 0.5075, 0.4997, 0.4954, 0.4879, 0.4902, 0.5085, 0.4952, 0.5082,
        0.4950, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5056, 0.4983, 0.5046, 0.5061, 0.4877, 0.4971, 0.5008, 0.5031,
        0.5018, 0.5102, 0.4997, 0.4959, 0.4872, 0.4908, 0.5082, 0.4953, 0.5078,
        0.4967, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4952, 0.5010, 0.5030,
        0.5018, 0.5096, 0.4997, 0.4963, 0.4888, 0.4919, 0.5079, 0.4933, 0.5063,
        0.4933, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5048, 0.4974, 0.5049, 0.5079, 0.4880, 0.4960, 0.5001, 0.5018,
        0.5008, 0.5066, 0.5012, 0.4975, 0.4886, 0.4923, 0.5083, 0.4928, 0.5087,
        0.4941, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5048, 0.5073, 0.4882, 0.4962, 0.5016, 0.5019,
        0.5040, 0.5076, 0.5003, 0.4943, 0.4907, 0.4928, 0.5094, 0.4941, 0.5049,
        0.4952, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5103, 0.4892, 0.5120, 0.4865, 0.4853, 0.4941, 0.4895, 0.4942, 0.4987,
         0.5074, 0.5027, 0.5001, 0.5025, 0.4918, 0.5045, 0.4913, 0.4858, 0.4986,
         0.5078, 0.4947],
        [0.5088, 0.4895, 0.5108, 0.4851, 0.4856, 0.4928, 0.4884, 0.4953, 0.4981,
         0.5052, 0.5037, 0.4999, 0.5034, 0.4916, 0.5029, 0.4888, 0.4862, 0.4984,
         0.5070, 0.4933],
        [0.5103, 0.4904, 0.5107, 0.4869, 0.4852, 0.4943, 0.4888, 0.4963, 0.5012,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4875, 0.4980,
         0.5069, 0.4926],
        [0.5079, 0.4894, 0.5115, 0.4861, 0.4872, 0.4932, 0.4897, 0.4952, 0.4983,
         0.5086, 0.5039, 0.5000, 0.5050, 0.4914, 0.5040, 0.4905, 0.4863, 0.4985,
         0.5058, 0.4923],
        [0.5095, 0.4913, 0.5131, 0.4861, 0.4872, 0.4928, 0.4910, 0.4922, 0.4956,
         0.5067, 0.5044, 0.4998, 0.5030, 0.4914, 0.5039, 0.4894, 0.4854, 0.4961,
         0.5054, 0.4936],
        [0.5111, 0.4917, 0.5101, 0.4866, 0.4862, 0.4945, 0.4908, 0.4939, 0.4970,
         0.5068, 0.5061, 0.5003, 0.5038, 0.4896, 0.5040, 0.4900, 0.4851, 0.4971,
         0.5061, 0.4930],
        [0.5096, 0.4914, 0.5125, 0.4881, 0.4858, 0.4937, 0.4889, 0.4942, 0.4976,
         0.5072, 0.5062, 0.4996, 0.5049, 0.4903, 0.5036, 0.4892, 0.4846, 0.4986,
         0.5071, 0.4932],
        [0.5101, 0.4914, 0.5113, 0.4853, 0.4866, 0.4926, 0.4891, 0.4948, 0.4998,
         0.5072, 0.5067, 0.5009, 0.5035, 0.4893, 0.5049, 0.4912, 0.4855, 0.4958,
         0.5076, 0.4932],
        [0.5111, 0.4926, 0.5110, 0.4851, 0.4886, 0.4929, 0.4895, 0.4941, 0.4980,
         0.5074, 0.5049, 0.4974, 0.5041, 0.4896, 0.5041, 0.4906, 0.4866, 0.4966,
         0.5082, 0.4936],
        [0.5114, 0.4920, 0.5118, 0.4871, 0.4881, 0.4928, 0.4893, 0.4933, 0.4998,
         0.5051, 0.5052, 0.4985, 0.5036, 0.4888, 0.5004, 0.4931, 0.4836, 0.4975,
         0.5057, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5103, 0.4892, 0.5120, 0.4865, 0.4853, 0.4941, 0.4895, 0.4942, 0.4987,
        0.5074, 0.5027, 0.5001, 0.5025, 0.4918, 0.5045, 0.4913, 0.4858, 0.4986,
        0.5078, 0.4947], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4895, 0.5108, 0.4851, 0.4856, 0.4928, 0.4884, 0.4953, 0.4981,
        0.5052, 0.5037, 0.4999, 0.5034, 0.4916, 0.5029, 0.4888, 0.4862, 0.4984,
        0.5070, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5103, 0.4904, 0.5107, 0.4869, 0.4852, 0.4943, 0.4888, 0.4963, 0.5012,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4875, 0.4980,
        0.5069, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4894, 0.5115, 0.4861, 0.4872, 0.4932, 0.4897, 0.4952, 0.4983,
        0.5086, 0.5039, 0.5000, 0.5050, 0.4914, 0.5040, 0.4905, 0.4863, 0.4985,
        0.5058, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4913, 0.5131, 0.4861, 0.4872, 0.4928, 0.4910, 0.4922, 0.4956,
        0.5067, 0.5044, 0.4998, 0.5030, 0.4914, 0.5039, 0.4894, 0.4854, 0.4961,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4917, 0.5101, 0.4866, 0.4862, 0.4945, 0.4908, 0.4939, 0.4970,
        0.5068, 0.5061, 0.5003, 0.5038, 0.4896, 0.5040, 0.4900, 0.4851, 0.4971,
        0.5061, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4914, 0.5125, 0.4881, 0.4858, 0.4937, 0.4889, 0.4942, 0.4976,
        0.5072, 0.5062, 0.4996, 0.5049, 0.4903, 0.5036, 0.4892, 0.4846, 0.4986,
        0.5071, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4914, 0.5113, 0.4853, 0.4866, 0.4926, 0.4891, 0.4948, 0.4998,
        0.5072, 0.5067, 0.5009, 0.5035, 0.4893, 0.5049, 0.4912, 0.4855, 0.4958,
        0.5076, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4926, 0.5110, 0.4851, 0.4886, 0.4929, 0.4895, 0.4941, 0.4980,
        0.5074, 0.5049, 0.4974, 0.5041, 0.4896, 0.5041, 0.4906, 0.4866, 0.4966,
        0.5082, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4920, 0.5118, 0.4871, 0.4881, 0.4928, 0.4893, 0.4933, 0.4998,
        0.5051, 0.5052, 0.4985, 0.5036, 0.4888, 0.5004, 0.4931, 0.4836, 0.4975,
        0.5057, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5125, 0.4963, 0.4967, 0.5026, 0.5067, 0.4926, 0.5092, 0.4953, 0.4968,
         0.5027, 0.4855, 0.4977, 0.4839, 0.5023, 0.5023, 0.4983, 0.4992, 0.5143,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4956, 0.5045, 0.5035, 0.4931, 0.5103, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4967, 0.4857, 0.5014, 0.5002, 0.4978, 0.4981, 0.5148,
         0.4946, 0.4970],
        [0.5130, 0.4972, 0.4976, 0.5025, 0.5047, 0.4921, 0.5104, 0.4978, 0.4962,
         0.5027, 0.4861, 0.4989, 0.4846, 0.5016, 0.5000, 0.4975, 0.4992, 0.5166,
         0.4947, 0.4969],
        [0.5125, 0.4975, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4963,
         0.5027, 0.4842, 0.4966, 0.4845, 0.5022, 0.5009, 0.4975, 0.5011, 0.5144,
         0.4953, 0.4970],
        [0.5126, 0.4981, 0.4963, 0.5038, 0.5033, 0.4935, 0.5089, 0.4953, 0.4967,
         0.5031, 0.4871, 0.4979, 0.4826, 0.5027, 0.5003, 0.4993, 0.5001, 0.5151,
         0.4956, 0.4972],
        [0.5136, 0.4973, 0.4961, 0.5031, 0.5052, 0.4927, 0.5079, 0.4968, 0.4956,
         0.5025, 0.4856, 0.4962, 0.4853, 0.5032, 0.5019, 0.5003, 0.4995, 0.5146,
         0.4952, 0.4982],
        [0.5114, 0.4964, 0.4979, 0.5026, 0.5038, 0.4922, 0.5088, 0.4963, 0.4960,
         0.5026, 0.4878, 0.4971, 0.4850, 0.5027, 0.5009, 0.4989, 0.5000, 0.5139,
         0.4953, 0.4996],
        [0.5120, 0.4973, 0.4951, 0.5011, 0.5040, 0.4922, 0.5083, 0.4944, 0.4970,
         0.5022, 0.4859, 0.4985, 0.4865, 0.5012, 0.5009, 0.4986, 0.4998, 0.5153,
         0.4959, 0.4987],
        [0.5127, 0.4981, 0.4951, 0.5011, 0.5048, 0.4925, 0.5096, 0.4966, 0.4967,
         0.5019, 0.4885, 0.4950, 0.4852, 0.5016, 0.5012, 0.4972, 0.5007, 0.5141,
         0.4957, 0.4994],
        [0.5126, 0.4985, 0.4939, 0.5027, 0.5049, 0.4937, 0.5079, 0.4988, 0.4964,
         0.5033, 0.4885, 0.4980, 0.4857, 0.5001, 0.5011, 0.4982, 0.4995, 0.5155,
         0.4972, 0.5005]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5125, 0.4963, 0.4967, 0.5026, 0.5067, 0.4926, 0.5092, 0.4953, 0.4968,
        0.5027, 0.4855, 0.4977, 0.4839, 0.5023, 0.5023, 0.4983, 0.4992, 0.5143,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4956, 0.5045, 0.5035, 0.4931, 0.5103, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4967, 0.4857, 0.5014, 0.5002, 0.4978, 0.4981, 0.5148,
        0.4946, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5130, 0.4972, 0.4976, 0.5025, 0.5047, 0.4921, 0.5104, 0.4978, 0.4962,
        0.5027, 0.4861, 0.4989, 0.4846, 0.5016, 0.5000, 0.4975, 0.4992, 0.5166,
        0.4947, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5125, 0.4975, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4963,
        0.5027, 0.4842, 0.4966, 0.4845, 0.5022, 0.5009, 0.4975, 0.5011, 0.5144,
        0.4953, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4981, 0.4963, 0.5038, 0.5033, 0.4935, 0.5089, 0.4953, 0.4967,
        0.5031, 0.4871, 0.4979, 0.4826, 0.5027, 0.5003, 0.4993, 0.5001, 0.5151,
        0.4956, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4973, 0.4961, 0.5031, 0.5052, 0.4927, 0.5079, 0.4968, 0.4956,
        0.5025, 0.4856, 0.4962, 0.4853, 0.5032, 0.5019, 0.5003, 0.4995, 0.5146,
        0.4952, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4964, 0.4979, 0.5026, 0.5038, 0.4922, 0.5088, 0.4963, 0.4960,
        0.5026, 0.4878, 0.4971, 0.4850, 0.5027, 0.5009, 0.4989, 0.5000, 0.5139,
        0.4953, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4973, 0.4951, 0.5011, 0.5040, 0.4922, 0.5083, 0.4944, 0.4970,
        0.5022, 0.4859, 0.4985, 0.4865, 0.5012, 0.5009, 0.4986, 0.4998, 0.5153,
        0.4959, 0.4987], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4981, 0.4951, 0.5011, 0.5048, 0.4925, 0.5096, 0.4966, 0.4967,
        0.5019, 0.4885, 0.4950, 0.4852, 0.5016, 0.5012, 0.4972, 0.5007, 0.5141,
        0.4957, 0.4994], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4985, 0.4939, 0.5027, 0.5049, 0.4937, 0.5079, 0.4988, 0.4964,
        0.5033, 0.4885, 0.4980, 0.4857, 0.5001, 0.5011, 0.4982, 0.4995, 0.5155,
        0.4972, 0.5005], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [2, 5, 4, 0, 3, 7, 1, 6, 9, 8]
replay_buffer._size: [7500 7500 7500 7500 7500 7500 7500 7500 7500 7500]
snapshot at best
2023-08-12 10:40:59,710 MainThread INFO: EPOCH:48
2023-08-12 10:40:59,711 MainThread INFO: Time Consumed:1.3177037239074707s
2023-08-12 10:40:59,711 MainThread INFO: Total Frames:73500s
 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 49/80 [00:53<00:55,  1.79s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1388.26899
Train_Epoch_Reward                    9166.63614
Running_Training_Average_Rewards      636.21863
Explore_Time                          0.00300
Train___Time                          0.75735
Eval____Time                          0.00736
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.62929
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.76585
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.94759
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.77954
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.50011
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.22521
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.29138
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14129.55846
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.83269
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.89696
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.43758     0.90857    11.43596    9.21837
alpha_0                               0.92957      0.00040    0.93013     0.92901
alpha_1                               0.92956      0.00040    0.93012     0.92900
alpha_2                               0.92958      0.00040    0.93014     0.92902
alpha_3                               0.92956      0.00040    0.93012     0.92900
alpha_4                               0.92957      0.00040    0.93013     0.92901
alpha_5                               0.92956      0.00040    0.93012     0.92900
alpha_6                               0.92956      0.00039    0.93012     0.92900
alpha_7                               0.92955      0.00040    0.93011     0.92900
alpha_8                               0.92956      0.00040    0.93011     0.92900
alpha_9                               0.92957      0.00040    0.93013     0.92901
Alpha_loss                            -0.49020     0.00287    -0.48601    -0.49411
Training/policy_loss                  -2.77768     0.00254    -2.77407    -2.78132
Training/qf1_loss                     2248.07979   567.29542  3144.50366  1599.94824
Training/qf2_loss                     2246.24519   567.18478  3142.50122  1598.19189
Training/pf_norm                      0.08874      0.01201    0.10698     0.07467
Training/qf1_norm                     34.49791     2.39478    37.04728    31.41319
Training/qf2_norm                     40.39977     2.76783    43.48545    37.10073
log_std/mean                          -0.13313     0.00004    -0.13308    -0.13319
log_std/std                           0.00933      0.00004    0.00939     0.00927
log_std/max                           -0.11360     0.00024    -0.11327    -0.11386
log_std/min                           -0.15577     0.00029    -0.15529    -0.15610
log_probs/mean                        -2.73928     0.00229    -2.73722    -2.74354
log_probs/std                         0.24649      0.00634    0.25605     0.23673
log_probs/max                         -2.15868     0.02179    -2.12378    -2.19202
log_probs/min                         -5.15659     0.36711    -4.64685    -5.70947
mean/mean                             -0.00130     0.00006    -0.00120    -0.00137
mean/std                              0.00886      0.00008    0.00897     0.00873
mean/max                              0.01326      0.00007    0.01334     0.01313
mean/min                              -0.01497     0.00008    -0.01482    -0.01507
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [6, 4, 5, 8, 9, 1, 0, 7, 3, 2]
replay_buffer._size: [7650 7650 7650 7650 7650 7650 7650 7650 7650 7650]
snapshot at best
2023-08-12 10:41:00,880 MainThread INFO: EPOCH:49
2023-08-12 10:41:00,881 MainThread INFO: Time Consumed:1.0082731246948242s
2023-08-12 10:41:00,881 MainThread INFO: Total Frames:75000s
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 50/80 [00:54<00:48,  1.63s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1399.00976
Train_Epoch_Reward                    8240.00547
Running_Training_Average_Rewards      863.44697
Explore_Time                          0.00358
Train___Time                          0.43313
Eval____Time                          0.00297
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.30319
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.76445
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.03341
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.76787
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.60256
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.30687
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.37135
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14240.92050
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.83371
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.83951
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.72918      0.74802    10.52091    8.53862
alpha_0                               0.92817      0.00039    0.92873     0.92761
alpha_1                               0.92816      0.00039    0.92872     0.92761
alpha_2                               0.92819      0.00039    0.92875     0.92763
alpha_3                               0.92816      0.00039    0.92872     0.92761
alpha_4                               0.92817      0.00039    0.92873     0.92761
alpha_5                               0.92816      0.00039    0.92872     0.92761
alpha_6                               0.92817      0.00039    0.92873     0.92761
alpha_7                               0.92816      0.00039    0.92872     0.92760
alpha_8                               0.92816      0.00039    0.92872     0.92760
alpha_9                               0.92818      0.00039    0.92873     0.92762
Alpha_loss                            -0.49978     0.00287    -0.49564    -0.50398
Training/policy_loss                  -2.77494     0.00350    -2.77011    -2.77879
Training/qf1_loss                     1803.44004   281.07038  2223.59229  1397.14307
Training/qf2_loss                     1801.61938   280.91343  2221.62085  1395.63232
Training/pf_norm                      0.12600      0.01820    0.14236     0.09255
Training/qf1_norm                     32.79603     1.97534    34.72316    29.53071
Training/qf2_norm                     38.55618     2.58066    41.06851    34.14680
log_std/mean                          -0.13316     0.00006    -0.13308    -0.13326
log_std/std                           0.00924      0.00005    0.00931     0.00916
log_std/max                           -0.11360     0.00022    -0.11338    -0.11388
log_std/min                           -0.15441     0.00048    -0.15394    -0.15516
log_probs/mean                        -2.73180     0.00371    -2.72535    -2.73631
log_probs/std                         0.23525      0.01690    0.25202     0.20431
log_probs/max                         -2.14386     0.05231    -2.09244    -2.24207
log_probs/min                         -4.91887     0.61388    -3.81697    -5.47722
mean/mean                             -0.00116     0.00004    -0.00112    -0.00122
mean/std                              0.00858      0.00008    0.00873     0.00849
mean/max                              0.01274      0.00012    0.01295     0.01258
mean/min                              -0.01470     0.00023    -0.01446    -0.01504
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4973, 0.5066, 0.4989, 0.5065, 0.5068, 0.4886, 0.4953, 0.5018, 0.4999,
         0.5024, 0.5100, 0.5006, 0.4960, 0.4887, 0.4924, 0.5076, 0.4925, 0.5096,
         0.4963, 0.5141],
        [0.4982, 0.5073, 0.4990, 0.5063, 0.5066, 0.4899, 0.4958, 0.5027, 0.5022,
         0.5029, 0.5083, 0.4992, 0.4962, 0.4882, 0.4903, 0.5077, 0.4918, 0.5088,
         0.4965, 0.5126],
        [0.4979, 0.5074, 0.4982, 0.5067, 0.5062, 0.4888, 0.4963, 0.5005, 0.5040,
         0.5014, 0.5068, 0.4989, 0.4963, 0.4882, 0.4901, 0.5096, 0.4934, 0.5090,
         0.4958, 0.5146],
        [0.4985, 0.5061, 0.4986, 0.5069, 0.5061, 0.4896, 0.4946, 0.5001, 0.5024,
         0.5037, 0.5065, 0.4985, 0.4970, 0.4879, 0.4896, 0.5078, 0.4922, 0.5101,
         0.4934, 0.5141],
        [0.4985, 0.5076, 0.4981, 0.5052, 0.5054, 0.4887, 0.4955, 0.4989, 0.5020,
         0.5026, 0.5088, 0.4992, 0.4966, 0.4882, 0.4916, 0.5062, 0.4945, 0.5098,
         0.4953, 0.5132],
        [0.4974, 0.5047, 0.4976, 0.5060, 0.5052, 0.4883, 0.4946, 0.5001, 0.5009,
         0.5030, 0.5075, 0.4997, 0.4953, 0.4879, 0.4902, 0.5085, 0.4952, 0.5082,
         0.4950, 0.5133],
        [0.4978, 0.5057, 0.4983, 0.5046, 0.5061, 0.4877, 0.4970, 0.5008, 0.5030,
         0.5018, 0.5102, 0.4997, 0.4959, 0.4872, 0.4907, 0.5081, 0.4953, 0.5078,
         0.4966, 0.5127],
        [0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4952, 0.5010, 0.5030,
         0.5018, 0.5097, 0.4997, 0.4963, 0.4888, 0.4919, 0.5079, 0.4933, 0.5063,
         0.4933, 0.5127],
        [0.4980, 0.5048, 0.4975, 0.5049, 0.5080, 0.4880, 0.4960, 0.5001, 0.5018,
         0.5008, 0.5066, 0.5013, 0.4975, 0.4886, 0.4923, 0.5083, 0.4928, 0.5086,
         0.4941, 0.5142],
        [0.4984, 0.5057, 0.4995, 0.5048, 0.5073, 0.4882, 0.4962, 0.5017, 0.5019,
         0.5040, 0.5076, 0.5003, 0.4943, 0.4907, 0.4928, 0.5095, 0.4941, 0.5049,
         0.4952, 0.5155]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4973, 0.5066, 0.4989, 0.5065, 0.5068, 0.4886, 0.4953, 0.5018, 0.4999,
        0.5024, 0.5100, 0.5006, 0.4960, 0.4887, 0.4924, 0.5076, 0.4925, 0.5096,
        0.4963, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5073, 0.4990, 0.5063, 0.5066, 0.4899, 0.4958, 0.5027, 0.5022,
        0.5029, 0.5083, 0.4992, 0.4962, 0.4882, 0.4903, 0.5077, 0.4918, 0.5088,
        0.4965, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5074, 0.4982, 0.5067, 0.5062, 0.4888, 0.4963, 0.5005, 0.5040,
        0.5014, 0.5068, 0.4989, 0.4963, 0.4882, 0.4901, 0.5096, 0.4934, 0.5090,
        0.4958, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5061, 0.4986, 0.5069, 0.5061, 0.4896, 0.4946, 0.5001, 0.5024,
        0.5037, 0.5065, 0.4985, 0.4970, 0.4879, 0.4896, 0.5078, 0.4922, 0.5101,
        0.4934, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5076, 0.4981, 0.5052, 0.5054, 0.4887, 0.4955, 0.4989, 0.5020,
        0.5026, 0.5088, 0.4992, 0.4966, 0.4882, 0.4916, 0.5062, 0.4945, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4974, 0.5047, 0.4976, 0.5060, 0.5052, 0.4883, 0.4946, 0.5001, 0.5009,
        0.5030, 0.5075, 0.4997, 0.4953, 0.4879, 0.4902, 0.5085, 0.4952, 0.5082,
        0.4950, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5057, 0.4983, 0.5046, 0.5061, 0.4877, 0.4970, 0.5008, 0.5030,
        0.5018, 0.5102, 0.4997, 0.4959, 0.4872, 0.4907, 0.5081, 0.4953, 0.5078,
        0.4966, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4880, 0.4952, 0.5010, 0.5030,
        0.5018, 0.5097, 0.4997, 0.4963, 0.4888, 0.4919, 0.5079, 0.4933, 0.5063,
        0.4933, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5048, 0.4975, 0.5049, 0.5080, 0.4880, 0.4960, 0.5001, 0.5018,
        0.5008, 0.5066, 0.5013, 0.4975, 0.4886, 0.4923, 0.5083, 0.4928, 0.5086,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5048, 0.5073, 0.4882, 0.4962, 0.5017, 0.5019,
        0.5040, 0.5076, 0.5003, 0.4943, 0.4907, 0.4928, 0.5095, 0.4941, 0.5049,
        0.4952, 0.5155], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5103, 0.4891, 0.5120, 0.4864, 0.4853, 0.4941, 0.4895, 0.4942, 0.4987,
         0.5074, 0.5027, 0.5002, 0.5025, 0.4918, 0.5045, 0.4913, 0.4858, 0.4985,
         0.5078, 0.4947],
        [0.5088, 0.4895, 0.5109, 0.4850, 0.4856, 0.4928, 0.4884, 0.4953, 0.4981,
         0.5052, 0.5037, 0.5000, 0.5034, 0.4917, 0.5029, 0.4889, 0.4862, 0.4984,
         0.5071, 0.4933],
        [0.5103, 0.4904, 0.5108, 0.4868, 0.4853, 0.4943, 0.4889, 0.4964, 0.5011,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4874, 0.4981,
         0.5068, 0.4925],
        [0.5079, 0.4894, 0.5116, 0.4860, 0.4872, 0.4933, 0.4898, 0.4952, 0.4983,
         0.5086, 0.5039, 0.5000, 0.5050, 0.4915, 0.5040, 0.4906, 0.4862, 0.4985,
         0.5057, 0.4923],
        [0.5095, 0.4912, 0.5131, 0.4861, 0.4872, 0.4929, 0.4910, 0.4923, 0.4955,
         0.5067, 0.5045, 0.4999, 0.5031, 0.4914, 0.5039, 0.4894, 0.4854, 0.4961,
         0.5054, 0.4936],
        [0.5111, 0.4917, 0.5102, 0.4866, 0.4863, 0.4946, 0.4908, 0.4939, 0.4970,
         0.5068, 0.5061, 0.5003, 0.5038, 0.4896, 0.5040, 0.4900, 0.4851, 0.4971,
         0.5062, 0.4930],
        [0.5095, 0.4914, 0.5125, 0.4880, 0.4858, 0.4937, 0.4889, 0.4942, 0.4976,
         0.5072, 0.5063, 0.4996, 0.5049, 0.4903, 0.5037, 0.4893, 0.4846, 0.4986,
         0.5072, 0.4932],
        [0.5101, 0.4914, 0.5113, 0.4853, 0.4867, 0.4926, 0.4891, 0.4948, 0.4998,
         0.5072, 0.5067, 0.5009, 0.5035, 0.4893, 0.5049, 0.4912, 0.4855, 0.4958,
         0.5076, 0.4932],
        [0.5111, 0.4926, 0.5110, 0.4851, 0.4886, 0.4928, 0.4894, 0.4941, 0.4980,
         0.5074, 0.5049, 0.4974, 0.5041, 0.4897, 0.5041, 0.4906, 0.4865, 0.4966,
         0.5083, 0.4936],
        [0.5113, 0.4920, 0.5118, 0.4871, 0.4881, 0.4928, 0.4893, 0.4933, 0.4998,
         0.5052, 0.5053, 0.4985, 0.5036, 0.4888, 0.5005, 0.4931, 0.4836, 0.4975,
         0.5058, 0.4940]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5103, 0.4891, 0.5120, 0.4864, 0.4853, 0.4941, 0.4895, 0.4942, 0.4987,
        0.5074, 0.5027, 0.5002, 0.5025, 0.4918, 0.5045, 0.4913, 0.4858, 0.4985,
        0.5078, 0.4947], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4895, 0.5109, 0.4850, 0.4856, 0.4928, 0.4884, 0.4953, 0.4981,
        0.5052, 0.5037, 0.5000, 0.5034, 0.4917, 0.5029, 0.4889, 0.4862, 0.4984,
        0.5071, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5103, 0.4904, 0.5108, 0.4868, 0.4853, 0.4943, 0.4889, 0.4964, 0.5011,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4874, 0.4981,
        0.5068, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4894, 0.5116, 0.4860, 0.4872, 0.4933, 0.4898, 0.4952, 0.4983,
        0.5086, 0.5039, 0.5000, 0.5050, 0.4915, 0.5040, 0.4906, 0.4862, 0.4985,
        0.5057, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5131, 0.4861, 0.4872, 0.4929, 0.4910, 0.4923, 0.4955,
        0.5067, 0.5045, 0.4999, 0.5031, 0.4914, 0.5039, 0.4894, 0.4854, 0.4961,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4917, 0.5102, 0.4866, 0.4863, 0.4946, 0.4908, 0.4939, 0.4970,
        0.5068, 0.5061, 0.5003, 0.5038, 0.4896, 0.5040, 0.4900, 0.4851, 0.4971,
        0.5062, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4914, 0.5125, 0.4880, 0.4858, 0.4937, 0.4889, 0.4942, 0.4976,
        0.5072, 0.5063, 0.4996, 0.5049, 0.4903, 0.5037, 0.4893, 0.4846, 0.4986,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4914, 0.5113, 0.4853, 0.4867, 0.4926, 0.4891, 0.4948, 0.4998,
        0.5072, 0.5067, 0.5009, 0.5035, 0.4893, 0.5049, 0.4912, 0.4855, 0.4958,
        0.5076, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4926, 0.5110, 0.4851, 0.4886, 0.4928, 0.4894, 0.4941, 0.4980,
        0.5074, 0.5049, 0.4974, 0.5041, 0.4897, 0.5041, 0.4906, 0.4865, 0.4966,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4920, 0.5118, 0.4871, 0.4881, 0.4928, 0.4893, 0.4933, 0.4998,
        0.5052, 0.5053, 0.4985, 0.5036, 0.4888, 0.5005, 0.4931, 0.4836, 0.4975,
        0.5058, 0.4940], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5125, 0.4963, 0.4967, 0.5027, 0.5067, 0.4926, 0.5093, 0.4953, 0.4968,
         0.5027, 0.4855, 0.4977, 0.4838, 0.5023, 0.5023, 0.4983, 0.4992, 0.5143,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4956, 0.5045, 0.5036, 0.4932, 0.5102, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4967, 0.4856, 0.5015, 0.5001, 0.4979, 0.4980, 0.5149,
         0.4946, 0.4970],
        [0.5131, 0.4972, 0.4976, 0.5025, 0.5046, 0.4921, 0.5103, 0.4978, 0.4962,
         0.5027, 0.4860, 0.4989, 0.4846, 0.5016, 0.4998, 0.4976, 0.4992, 0.5166,
         0.4946, 0.4968],
        [0.5126, 0.4975, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4963,
         0.5027, 0.4842, 0.4967, 0.4845, 0.5023, 0.5008, 0.4975, 0.5011, 0.5144,
         0.4953, 0.4970],
        [0.5127, 0.4981, 0.4964, 0.5039, 0.5033, 0.4934, 0.5088, 0.4953, 0.4967,
         0.5031, 0.4870, 0.4979, 0.4826, 0.5028, 0.5003, 0.4993, 0.5001, 0.5151,
         0.4956, 0.4972],
        [0.5136, 0.4973, 0.4962, 0.5032, 0.5051, 0.4927, 0.5078, 0.4968, 0.4956,
         0.5025, 0.4856, 0.4963, 0.4853, 0.5033, 0.5018, 0.5004, 0.4995, 0.5146,
         0.4952, 0.4982],
        [0.5114, 0.4964, 0.4980, 0.5025, 0.5039, 0.4922, 0.5088, 0.4963, 0.4960,
         0.5026, 0.4878, 0.4971, 0.4850, 0.5027, 0.5009, 0.4989, 0.4999, 0.5138,
         0.4952, 0.4996],
        [0.5120, 0.4973, 0.4951, 0.5011, 0.5040, 0.4922, 0.5083, 0.4944, 0.4970,
         0.5022, 0.4859, 0.4985, 0.4865, 0.5011, 0.5009, 0.4986, 0.4998, 0.5153,
         0.4959, 0.4987],
        [0.5128, 0.4981, 0.4950, 0.5010, 0.5048, 0.4925, 0.5095, 0.4967, 0.4967,
         0.5019, 0.4884, 0.4951, 0.4852, 0.5015, 0.5012, 0.4971, 0.5007, 0.5141,
         0.4957, 0.4994],
        [0.5126, 0.4984, 0.4938, 0.5027, 0.5049, 0.4937, 0.5079, 0.4988, 0.4964,
         0.5033, 0.4885, 0.4979, 0.4857, 0.5000, 0.5010, 0.4982, 0.4995, 0.5154,
         0.4971, 0.5005]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5125, 0.4963, 0.4967, 0.5027, 0.5067, 0.4926, 0.5093, 0.4953, 0.4968,
        0.5027, 0.4855, 0.4977, 0.4838, 0.5023, 0.5023, 0.4983, 0.4992, 0.5143,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4956, 0.5045, 0.5036, 0.4932, 0.5102, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4967, 0.4856, 0.5015, 0.5001, 0.4979, 0.4980, 0.5149,
        0.4946, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4972, 0.4976, 0.5025, 0.5046, 0.4921, 0.5103, 0.4978, 0.4962,
        0.5027, 0.4860, 0.4989, 0.4846, 0.5016, 0.4998, 0.4976, 0.4992, 0.5166,
        0.4946, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4975, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4963,
        0.5027, 0.4842, 0.4967, 0.4845, 0.5023, 0.5008, 0.4975, 0.5011, 0.5144,
        0.4953, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4981, 0.4964, 0.5039, 0.5033, 0.4934, 0.5088, 0.4953, 0.4967,
        0.5031, 0.4870, 0.4979, 0.4826, 0.5028, 0.5003, 0.4993, 0.5001, 0.5151,
        0.4956, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4973, 0.4962, 0.5032, 0.5051, 0.4927, 0.5078, 0.4968, 0.4956,
        0.5025, 0.4856, 0.4963, 0.4853, 0.5033, 0.5018, 0.5004, 0.4995, 0.5146,
        0.4952, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4964, 0.4980, 0.5025, 0.5039, 0.4922, 0.5088, 0.4963, 0.4960,
        0.5026, 0.4878, 0.4971, 0.4850, 0.5027, 0.5009, 0.4989, 0.4999, 0.5138,
        0.4952, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4973, 0.4951, 0.5011, 0.5040, 0.4922, 0.5083, 0.4944, 0.4970,
        0.5022, 0.4859, 0.4985, 0.4865, 0.5011, 0.5009, 0.4986, 0.4998, 0.5153,
        0.4959, 0.4987], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4950, 0.5010, 0.5048, 0.4925, 0.5095, 0.4967, 0.4967,
        0.5019, 0.4884, 0.4951, 0.4852, 0.5015, 0.5012, 0.4971, 0.5007, 0.5141,
        0.4957, 0.4994], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4984, 0.4938, 0.5027, 0.5049, 0.4937, 0.5079, 0.4988, 0.4964,
        0.5033, 0.4885, 0.4979, 0.4857, 0.5000, 0.5010, 0.4982, 0.4995, 0.5154,
        0.4971, 0.5005], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [7, 8, 6, 4, 1, 0, 3, 2, 9, 5]
replay_buffer._size: [7800 7800 7800 7800 7800 7800 7800 7800 7800 7800]
2023-08-12 10:41:01,903 MainThread INFO: EPOCH:50
2023-08-12 10:41:01,903 MainThread INFO: Time Consumed:0.3805258274078369s
2023-08-12 10:41:01,903 MainThread INFO: Total Frames:76500s
 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 51/80 [00:55<00:41,  1.42s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1391.73883
Train_Epoch_Reward                    12877.40792
Running_Training_Average_Rewards      1009.46832
Explore_Time                          0.00389
Train___Time                          0.36836
Eval____Time                          0.00268
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.68682
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.74251
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -18.98066
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.77606
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.54465
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.25985
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.32623
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14165.40907
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.81181
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.89220
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.57749      0.65684    10.45161    8.76616
alpha_0                               0.92678      0.00039    0.92733     0.92622
alpha_1                               0.92677      0.00039    0.92733     0.92621
alpha_2                               0.92679      0.00039    0.92735     0.92624
alpha_3                               0.92677      0.00039    0.92733     0.92621
alpha_4                               0.92678      0.00039    0.92733     0.92622
alpha_5                               0.92677      0.00039    0.92733     0.92621
alpha_6                               0.92678      0.00039    0.92733     0.92622
alpha_7                               0.92676      0.00039    0.92732     0.92621
alpha_8                               0.92677      0.00039    0.92732     0.92621
alpha_9                               0.92678      0.00039    0.92734     0.92622
Alpha_loss                            -0.50995     0.00261    -0.50634    -0.51403
Training/policy_loss                  -2.78178     0.00563    -2.77059    -2.78573
Training/qf1_loss                     1817.51145   340.54472  2213.31470  1360.64929
Training/qf2_loss                     1815.54072   340.45412  2211.07593  1358.72827
Training/pf_norm                      0.13454      0.02777    0.17617     0.09640
Training/qf1_norm                     32.86460     1.76853    35.37607    30.76214
Training/qf2_norm                     39.14589     2.13231    42.65237    36.82762
log_std/mean                          -0.13330     0.00005    -0.13323    -0.13338
log_std/std                           0.00913      0.00004    0.00919     0.00908
log_std/max                           -0.11462     0.00039    -0.11405    -0.11511
log_std/min                           -0.15423     0.00058    -0.15343    -0.15504
log_probs/mean                        -2.73256     0.00618    -2.72067    -2.73830
log_probs/std                         0.24052      0.01080    0.25128     0.22626
log_probs/max                         -2.13211     0.02268    -2.09383    -2.16471
log_probs/min                         -4.96741     0.53931    -4.31402    -5.64487
mean/mean                             -0.00134     0.00009    -0.00122    -0.00147
mean/std                              0.00939      0.00030    0.00984     0.00899
mean/max                              0.01392      0.00034    0.01431     0.01332
mean/min                              -0.01615     0.00055    -0.01544    -0.01703
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [1, 7, 3, 8, 0, 5, 6, 4, 2, 9]
replay_buffer._size: [7950 7950 7950 7950 7950 7950 7950 7950 7950 7950]
snapshot at best
2023-08-12 10:41:02,932 MainThread INFO: EPOCH:51
2023-08-12 10:41:02,932 MainThread INFO: Time Consumed:0.927586555480957s
2023-08-12 10:41:02,932 MainThread INFO: Total Frames:78000s
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 52/80 [00:56<00:36,  1.30s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1402.34203
Train_Epoch_Reward                    6807.33064
Running_Training_Average_Rewards      930.82480
Explore_Time                          0.00227
Train___Time                          0.39187
Eval____Time                          0.00336
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -54.87390
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.77211
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.02017
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.73055
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.53999
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.25183
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.31970
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14277.56955
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.84473
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.79623
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.01089     0.78487    11.42340    9.06404
alpha_0                               0.92538      0.00039    0.92594     0.92483
alpha_1                               0.92538      0.00039    0.92593     0.92482
alpha_2                               0.92540      0.00039    0.92596     0.92484
alpha_3                               0.92538      0.00039    0.92593     0.92482
alpha_4                               0.92539      0.00039    0.92594     0.92483
alpha_5                               0.92538      0.00039    0.92593     0.92482
alpha_6                               0.92539      0.00039    0.92594     0.92483
alpha_7                               0.92537      0.00039    0.92593     0.92482
alpha_8                               0.92537      0.00039    0.92593     0.92482
alpha_9                               0.92539      0.00039    0.92595     0.92483
Alpha_loss                            -0.51994     0.00296    -0.51583    -0.52393
Training/policy_loss                  -2.78681     0.00367    -2.78131    -2.79077
Training/qf1_loss                     2102.63762   431.54296  2835.98193  1677.59314
Training/qf2_loss                     2100.50852   431.39548  2833.45190  1675.51831
Training/pf_norm                      0.11597      0.02777    0.13802     0.06182
Training/qf1_norm                     34.23153     2.15563    38.24020    31.77212
Training/qf2_norm                     40.96164     2.86038    46.43077    37.97570
log_std/mean                          -0.13349     0.00004    -0.13344    -0.13355
log_std/std                           0.00907      0.00002    0.00909     0.00904
log_std/max                           -0.11478     0.00023    -0.11443    -0.11508
log_std/min                           -0.15410     0.00068    -0.15275    -0.15461
log_probs/mean                        -2.73089     0.00302    -2.72604    -2.73426
log_probs/std                         0.23859      0.01560    0.26816     0.22349
log_probs/max                         -2.15949     0.03329    -2.10902    -2.20733
log_probs/min                         -5.05704     0.48081    -4.55517    -5.83806
mean/mean                             -0.00152     0.00002    -0.00149    -0.00155
mean/std                              0.01036      0.00016    0.01052     0.01010
mean/max                              0.01539      0.00036    0.01579     0.01475
mean/min                              -0.01783     0.00029    -0.01730    -0.01809
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4972, 0.5066, 0.4989, 0.5065, 0.5068, 0.4887, 0.4953, 0.5018, 0.4999,
         0.5024, 0.5100, 0.5006, 0.4960, 0.4887, 0.4924, 0.5076, 0.4924, 0.5097,
         0.4963, 0.5141],
        [0.4982, 0.5074, 0.4990, 0.5063, 0.5066, 0.4899, 0.4958, 0.5027, 0.5023,
         0.5029, 0.5084, 0.4991, 0.4962, 0.4882, 0.4903, 0.5077, 0.4917, 0.5089,
         0.4965, 0.5126],
        [0.4979, 0.5074, 0.4982, 0.5067, 0.5062, 0.4889, 0.4962, 0.5005, 0.5040,
         0.5014, 0.5068, 0.4989, 0.4963, 0.4882, 0.4900, 0.5095, 0.4933, 0.5090,
         0.4958, 0.5146],
        [0.4985, 0.5061, 0.4987, 0.5069, 0.5061, 0.4896, 0.4946, 0.5001, 0.5023,
         0.5036, 0.5066, 0.4986, 0.4970, 0.4879, 0.4896, 0.5077, 0.4922, 0.5102,
         0.4935, 0.5140],
        [0.4985, 0.5077, 0.4982, 0.5052, 0.5054, 0.4887, 0.4954, 0.4989, 0.5020,
         0.5027, 0.5088, 0.4992, 0.4966, 0.4882, 0.4916, 0.5062, 0.4945, 0.5098,
         0.4953, 0.5132],
        [0.4974, 0.5048, 0.4977, 0.5060, 0.5052, 0.4883, 0.4946, 0.5000, 0.5009,
         0.5030, 0.5076, 0.4997, 0.4953, 0.4879, 0.4902, 0.5084, 0.4952, 0.5082,
         0.4951, 0.5133],
        [0.4978, 0.5057, 0.4983, 0.5046, 0.5061, 0.4878, 0.4970, 0.5008, 0.5030,
         0.5018, 0.5101, 0.4997, 0.4959, 0.4873, 0.4907, 0.5082, 0.4952, 0.5079,
         0.4966, 0.5127],
        [0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4881, 0.4953, 0.5011, 0.5029,
         0.5018, 0.5097, 0.4998, 0.4962, 0.4888, 0.4919, 0.5078, 0.4933, 0.5063,
         0.4934, 0.5126],
        [0.4980, 0.5048, 0.4975, 0.5049, 0.5080, 0.4880, 0.4961, 0.5001, 0.5018,
         0.5008, 0.5066, 0.5012, 0.4975, 0.4886, 0.4923, 0.5083, 0.4928, 0.5086,
         0.4941, 0.5142],
        [0.4984, 0.5057, 0.4995, 0.5048, 0.5074, 0.4882, 0.4962, 0.5017, 0.5018,
         0.5040, 0.5076, 0.5003, 0.4943, 0.4907, 0.4928, 0.5095, 0.4941, 0.5049,
         0.4952, 0.5155]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4972, 0.5066, 0.4989, 0.5065, 0.5068, 0.4887, 0.4953, 0.5018, 0.4999,
        0.5024, 0.5100, 0.5006, 0.4960, 0.4887, 0.4924, 0.5076, 0.4924, 0.5097,
        0.4963, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5074, 0.4990, 0.5063, 0.5066, 0.4899, 0.4958, 0.5027, 0.5023,
        0.5029, 0.5084, 0.4991, 0.4962, 0.4882, 0.4903, 0.5077, 0.4917, 0.5089,
        0.4965, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4979, 0.5074, 0.4982, 0.5067, 0.5062, 0.4889, 0.4962, 0.5005, 0.5040,
        0.5014, 0.5068, 0.4989, 0.4963, 0.4882, 0.4900, 0.5095, 0.4933, 0.5090,
        0.4958, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5061, 0.4987, 0.5069, 0.5061, 0.4896, 0.4946, 0.5001, 0.5023,
        0.5036, 0.5066, 0.4986, 0.4970, 0.4879, 0.4896, 0.5077, 0.4922, 0.5102,
        0.4935, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5077, 0.4982, 0.5052, 0.5054, 0.4887, 0.4954, 0.4989, 0.5020,
        0.5027, 0.5088, 0.4992, 0.4966, 0.4882, 0.4916, 0.5062, 0.4945, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4974, 0.5048, 0.4977, 0.5060, 0.5052, 0.4883, 0.4946, 0.5000, 0.5009,
        0.5030, 0.5076, 0.4997, 0.4953, 0.4879, 0.4902, 0.5084, 0.4952, 0.5082,
        0.4951, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5057, 0.4983, 0.5046, 0.5061, 0.4878, 0.4970, 0.5008, 0.5030,
        0.5018, 0.5101, 0.4997, 0.4959, 0.4873, 0.4907, 0.5082, 0.4952, 0.5079,
        0.4966, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5047, 0.4983, 0.5050, 0.5065, 0.4881, 0.4953, 0.5011, 0.5029,
        0.5018, 0.5097, 0.4998, 0.4962, 0.4888, 0.4919, 0.5078, 0.4933, 0.5063,
        0.4934, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5048, 0.4975, 0.5049, 0.5080, 0.4880, 0.4961, 0.5001, 0.5018,
        0.5008, 0.5066, 0.5012, 0.4975, 0.4886, 0.4923, 0.5083, 0.4928, 0.5086,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5048, 0.5074, 0.4882, 0.4962, 0.5017, 0.5018,
        0.5040, 0.5076, 0.5003, 0.4943, 0.4907, 0.4928, 0.5095, 0.4941, 0.5049,
        0.4952, 0.5155], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4891, 0.5120, 0.4864, 0.4853, 0.4941, 0.4896, 0.4942, 0.4987,
         0.5074, 0.5027, 0.5002, 0.5025, 0.4918, 0.5045, 0.4913, 0.4858, 0.4985,
         0.5078, 0.4947],
        [0.5088, 0.4895, 0.5108, 0.4850, 0.4856, 0.4928, 0.4884, 0.4953, 0.4981,
         0.5052, 0.5037, 0.4999, 0.5034, 0.4917, 0.5030, 0.4888, 0.4862, 0.4983,
         0.5070, 0.4933],
        [0.5103, 0.4904, 0.5108, 0.4868, 0.4853, 0.4943, 0.4889, 0.4963, 0.5010,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4874, 0.4981,
         0.5069, 0.4925],
        [0.5078, 0.4894, 0.5117, 0.4860, 0.4871, 0.4933, 0.4899, 0.4952, 0.4983,
         0.5086, 0.5039, 0.5001, 0.5050, 0.4915, 0.5041, 0.4906, 0.4862, 0.4985,
         0.5057, 0.4923],
        [0.5094, 0.4912, 0.5131, 0.4861, 0.4872, 0.4929, 0.4910, 0.4923, 0.4956,
         0.5067, 0.5045, 0.4999, 0.5031, 0.4914, 0.5039, 0.4894, 0.4854, 0.4961,
         0.5054, 0.4936],
        [0.5111, 0.4917, 0.5102, 0.4866, 0.4863, 0.4946, 0.4908, 0.4939, 0.4970,
         0.5068, 0.5061, 0.5003, 0.5038, 0.4896, 0.5040, 0.4900, 0.4851, 0.4971,
         0.5061, 0.4930],
        [0.5096, 0.4914, 0.5125, 0.4880, 0.4859, 0.4937, 0.4889, 0.4942, 0.4976,
         0.5071, 0.5063, 0.4996, 0.5049, 0.4904, 0.5036, 0.4893, 0.4846, 0.4986,
         0.5072, 0.4932],
        [0.5101, 0.4913, 0.5113, 0.4852, 0.4868, 0.4926, 0.4891, 0.4948, 0.4997,
         0.5072, 0.5066, 0.5008, 0.5035, 0.4893, 0.5048, 0.4912, 0.4855, 0.4958,
         0.5076, 0.4933],
        [0.5111, 0.4926, 0.5110, 0.4851, 0.4886, 0.4928, 0.4894, 0.4941, 0.4980,
         0.5074, 0.5049, 0.4974, 0.5041, 0.4896, 0.5040, 0.4907, 0.4865, 0.4966,
         0.5083, 0.4936],
        [0.5113, 0.4921, 0.5119, 0.4870, 0.4881, 0.4928, 0.4893, 0.4933, 0.4998,
         0.5052, 0.5053, 0.4985, 0.5036, 0.4888, 0.5005, 0.4932, 0.4836, 0.4974,
         0.5058, 0.4940]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4891, 0.5120, 0.4864, 0.4853, 0.4941, 0.4896, 0.4942, 0.4987,
        0.5074, 0.5027, 0.5002, 0.5025, 0.4918, 0.5045, 0.4913, 0.4858, 0.4985,
        0.5078, 0.4947], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4895, 0.5108, 0.4850, 0.4856, 0.4928, 0.4884, 0.4953, 0.4981,
        0.5052, 0.5037, 0.4999, 0.5034, 0.4917, 0.5030, 0.4888, 0.4862, 0.4983,
        0.5070, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5103, 0.4904, 0.5108, 0.4868, 0.4853, 0.4943, 0.4889, 0.4963, 0.5010,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4874, 0.4981,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4894, 0.5117, 0.4860, 0.4871, 0.4933, 0.4899, 0.4952, 0.4983,
        0.5086, 0.5039, 0.5001, 0.5050, 0.4915, 0.5041, 0.4906, 0.4862, 0.4985,
        0.5057, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4912, 0.5131, 0.4861, 0.4872, 0.4929, 0.4910, 0.4923, 0.4956,
        0.5067, 0.5045, 0.4999, 0.5031, 0.4914, 0.5039, 0.4894, 0.4854, 0.4961,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4917, 0.5102, 0.4866, 0.4863, 0.4946, 0.4908, 0.4939, 0.4970,
        0.5068, 0.5061, 0.5003, 0.5038, 0.4896, 0.5040, 0.4900, 0.4851, 0.4971,
        0.5061, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4914, 0.5125, 0.4880, 0.4859, 0.4937, 0.4889, 0.4942, 0.4976,
        0.5071, 0.5063, 0.4996, 0.5049, 0.4904, 0.5036, 0.4893, 0.4846, 0.4986,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4913, 0.5113, 0.4852, 0.4868, 0.4926, 0.4891, 0.4948, 0.4997,
        0.5072, 0.5066, 0.5008, 0.5035, 0.4893, 0.5048, 0.4912, 0.4855, 0.4958,
        0.5076, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4926, 0.5110, 0.4851, 0.4886, 0.4928, 0.4894, 0.4941, 0.4980,
        0.5074, 0.5049, 0.4974, 0.5041, 0.4896, 0.5040, 0.4907, 0.4865, 0.4966,
        0.5083, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4921, 0.5119, 0.4870, 0.4881, 0.4928, 0.4893, 0.4933, 0.4998,
        0.5052, 0.5053, 0.4985, 0.5036, 0.4888, 0.5005, 0.4932, 0.4836, 0.4974,
        0.5058, 0.4940], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5125, 0.4963, 0.4967, 0.5026, 0.5067, 0.4927, 0.5093, 0.4953, 0.4968,
         0.5027, 0.4855, 0.4977, 0.4838, 0.5022, 0.5023, 0.4983, 0.4992, 0.5144,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4956, 0.5045, 0.5036, 0.4931, 0.5103, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4967, 0.4856, 0.5015, 0.5001, 0.4979, 0.4981, 0.5149,
         0.4946, 0.4970],
        [0.5131, 0.4972, 0.4976, 0.5025, 0.5046, 0.4921, 0.5104, 0.4978, 0.4962,
         0.5027, 0.4859, 0.4988, 0.4846, 0.5017, 0.4998, 0.4976, 0.4992, 0.5166,
         0.4946, 0.4968],
        [0.5126, 0.4975, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
         0.5027, 0.4841, 0.4967, 0.4845, 0.5022, 0.5007, 0.4975, 0.5011, 0.5145,
         0.4953, 0.4970],
        [0.5127, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5088, 0.4953, 0.4967,
         0.5031, 0.4870, 0.4979, 0.4826, 0.5028, 0.5003, 0.4993, 0.5001, 0.5152,
         0.4956, 0.4971],
        [0.5137, 0.4973, 0.4962, 0.5032, 0.5051, 0.4927, 0.5078, 0.4968, 0.4955,
         0.5025, 0.4856, 0.4962, 0.4852, 0.5033, 0.5018, 0.5004, 0.4995, 0.5147,
         0.4952, 0.4982],
        [0.5115, 0.4965, 0.4980, 0.5026, 0.5038, 0.4922, 0.5087, 0.4963, 0.4960,
         0.5026, 0.4877, 0.4971, 0.4850, 0.5027, 0.5008, 0.4989, 0.4999, 0.5139,
         0.4953, 0.4996],
        [0.5121, 0.4973, 0.4951, 0.5010, 0.5040, 0.4923, 0.5082, 0.4945, 0.4970,
         0.5022, 0.4859, 0.4985, 0.4865, 0.5012, 0.5008, 0.4987, 0.4998, 0.5153,
         0.4959, 0.4987],
        [0.5128, 0.4981, 0.4950, 0.5010, 0.5048, 0.4925, 0.5095, 0.4967, 0.4967,
         0.5019, 0.4884, 0.4951, 0.4852, 0.5015, 0.5012, 0.4972, 0.5007, 0.5141,
         0.4958, 0.4995],
        [0.5127, 0.4984, 0.4938, 0.5027, 0.5050, 0.4937, 0.5078, 0.4988, 0.4964,
         0.5033, 0.4885, 0.4979, 0.4857, 0.5000, 0.5009, 0.4982, 0.4995, 0.5154,
         0.4971, 0.5005]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5125, 0.4963, 0.4967, 0.5026, 0.5067, 0.4927, 0.5093, 0.4953, 0.4968,
        0.5027, 0.4855, 0.4977, 0.4838, 0.5022, 0.5023, 0.4983, 0.4992, 0.5144,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4956, 0.5045, 0.5036, 0.4931, 0.5103, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4967, 0.4856, 0.5015, 0.5001, 0.4979, 0.4981, 0.5149,
        0.4946, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4972, 0.4976, 0.5025, 0.5046, 0.4921, 0.5104, 0.4978, 0.4962,
        0.5027, 0.4859, 0.4988, 0.4846, 0.5017, 0.4998, 0.4976, 0.4992, 0.5166,
        0.4946, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4975, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
        0.5027, 0.4841, 0.4967, 0.4845, 0.5022, 0.5007, 0.4975, 0.5011, 0.5145,
        0.4953, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5088, 0.4953, 0.4967,
        0.5031, 0.4870, 0.4979, 0.4826, 0.5028, 0.5003, 0.4993, 0.5001, 0.5152,
        0.4956, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4973, 0.4962, 0.5032, 0.5051, 0.4927, 0.5078, 0.4968, 0.4955,
        0.5025, 0.4856, 0.4962, 0.4852, 0.5033, 0.5018, 0.5004, 0.4995, 0.5147,
        0.4952, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4965, 0.4980, 0.5026, 0.5038, 0.4922, 0.5087, 0.4963, 0.4960,
        0.5026, 0.4877, 0.4971, 0.4850, 0.5027, 0.5008, 0.4989, 0.4999, 0.5139,
        0.4953, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5121, 0.4973, 0.4951, 0.5010, 0.5040, 0.4923, 0.5082, 0.4945, 0.4970,
        0.5022, 0.4859, 0.4985, 0.4865, 0.5012, 0.5008, 0.4987, 0.4998, 0.5153,
        0.4959, 0.4987], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4950, 0.5010, 0.5048, 0.4925, 0.5095, 0.4967, 0.4967,
        0.5019, 0.4884, 0.4951, 0.4852, 0.5015, 0.5012, 0.4972, 0.5007, 0.5141,
        0.4958, 0.4995], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4984, 0.4938, 0.5027, 0.5050, 0.4937, 0.5078, 0.4988, 0.4964,
        0.5033, 0.4885, 0.4979, 0.4857, 0.5000, 0.5009, 0.4982, 0.4995, 0.5154,
        0.4971, 0.5005], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [9, 0, 3, 7, 6, 5, 2, 8, 1, 4]
replay_buffer._size: [8100 8100 8100 8100 8100 8100 8100 8100 8100 8100]
snapshot at best
2023-08-12 10:41:04,393 MainThread INFO: EPOCH:52
2023-08-12 10:41:04,394 MainThread INFO: Time Consumed:1.062441110610962s
2023-08-12 10:41:04,394 MainThread INFO: Total Frames:79500s
 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 53/80 [00:57<00:36,  1.35s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1432.27576
Train_Epoch_Reward                    2937.83187
Running_Training_Average_Rewards      754.08568
Explore_Time                          0.00276
Train___Time                          0.36223
Eval____Time                          0.00364
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.20661
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.82125
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.16594
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.62871
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.60417
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.29739
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.36495
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14572.32012
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.89946
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.57407
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.33442      0.62436    10.06609    8.44348
alpha_0                               0.92399      0.00039    0.92455     0.92344
alpha_1                               0.92399      0.00039    0.92454     0.92343
alpha_2                               0.92401      0.00039    0.92457     0.92346
alpha_3                               0.92399      0.00039    0.92454     0.92343
alpha_4                               0.92400      0.00039    0.92455     0.92344
alpha_5                               0.92399      0.00039    0.92454     0.92343
alpha_6                               0.92400      0.00039    0.92455     0.92344
alpha_7                               0.92398      0.00039    0.92454     0.92343
alpha_8                               0.92399      0.00039    0.92454     0.92343
alpha_9                               0.92400      0.00039    0.92456     0.92345
Alpha_loss                            -0.53047     0.00320    -0.52602    -0.53518
Training/policy_loss                  -2.79756     0.00575    -2.79075    -2.80765
Training/qf1_loss                     1804.35337   320.36310  2293.71753  1473.10962
Training/qf2_loss                     1802.12029   320.20887  2291.09912  1470.86060
Training/pf_norm                      0.10360      0.03212    0.15622     0.07479
Training/qf1_norm                     32.82763     1.88151    35.15651    30.29553
Training/qf2_norm                     39.85965     2.48911    43.68073    36.77729
log_std/mean                          -0.13321     0.00011    -0.13309    -0.13339
log_std/std                           0.00899      0.00002    0.00902     0.00896
log_std/max                           -0.11504     0.00030    -0.11457    -0.11547
log_std/min                           -0.15389     0.00021    -0.15361    -0.15418
log_probs/mean                        -2.73608     0.00463    -2.73097    -2.74454
log_probs/std                         0.23886      0.01165    0.25106     0.22414
log_probs/max                         -2.12565     0.03908    -2.07357    -2.17654
log_probs/min                         -5.21478     0.30131    -4.65913    -5.57458
mean/mean                             -0.00147     0.00005    -0.00141    -0.00154
mean/std                              0.01046      0.00004    0.01052     0.01039
mean/max                              0.01557      0.00008    0.01573     0.01548
mean/min                              -0.01841     0.00006    -0.01831    -0.01846
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [0, 5, 6, 8, 1, 7, 2, 9, 4, 3]
replay_buffer._size: [8250 8250 8250 8250 8250 8250 8250 8250 8250 8250]
snapshot at best
2023-08-12 10:41:05,646 MainThread INFO: EPOCH:53
2023-08-12 10:41:05,646 MainThread INFO: Time Consumed:1.1091978549957275s
2023-08-12 10:41:05,646 MainThread INFO: Total Frames:81000s
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 54/80 [00:59<00:34,  1.34s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1444.88996
Train_Epoch_Reward                    7161.26364
Running_Training_Average_Rewards      563.54754
Explore_Time                          0.00430
Train___Time                          0.48192
Eval____Time                          0.00242
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.01347
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.82727
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.20866
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.54492
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.55533
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.25180
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.32010
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14698.99299
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.90706
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.46474
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.56528      0.78690    10.94608    8.62345
alpha_0                               0.92261      0.00039    0.92316     0.92205
alpha_1                               0.92260      0.00039    0.92316     0.92205
alpha_2                               0.92262      0.00039    0.92318     0.92207
alpha_3                               0.92260      0.00039    0.92315     0.92205
alpha_4                               0.92261      0.00039    0.92316     0.92205
alpha_5                               0.92260      0.00039    0.92316     0.92205
alpha_6                               0.92261      0.00039    0.92317     0.92206
alpha_7                               0.92259      0.00039    0.92315     0.92204
alpha_8                               0.92260      0.00039    0.92315     0.92204
alpha_9                               0.92262      0.00039    0.92317     0.92206
Alpha_loss                            -0.54039     0.00235    -0.53711    -0.54413
Training/policy_loss                  -2.80165     0.00794    -2.78659    -2.80782
Training/qf1_loss                     1894.25142   482.91142  2773.47388  1426.01587
Training/qf2_loss                     1892.05200   482.79492  2771.19849  1423.98999
Training/pf_norm                      0.09721      0.02698    0.13408     0.06423
Training/qf1_norm                     33.75525     2.33438    37.82160    30.92146
Training/qf2_norm                     40.51788     2.83600    44.79829    37.14167
log_std/mean                          -0.13284     0.00013    -0.13268    -0.13298
log_std/std                           0.00892      0.00006    0.00898     0.00883
log_std/max                           -0.11476     0.00018    -0.11454    -0.11500
log_std/min                           -0.15322     0.00054    -0.15253    -0.15416
log_probs/mean                        -2.73370     0.01006    -2.71569    -2.74330
log_probs/std                         0.23052      0.01081    0.24140     0.21022
log_probs/max                         -2.15065     0.02074    -2.12598    -2.18523
log_probs/min                         -4.89027     0.46734    -4.18452    -5.57416
mean/mean                             -0.00085     0.00030    -0.00044    -0.00126
mean/std                              0.01036      0.00002    0.01038     0.01034
mean/max                              0.01562      0.00030    0.01604     0.01523
mean/min                              -0.01805     0.00025    -0.01763    -0.01834
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4972, 0.5066, 0.4989, 0.5066, 0.5067, 0.4887, 0.4952, 0.5018, 0.4998,
         0.5024, 0.5101, 0.5006, 0.4960, 0.4887, 0.4924, 0.5076, 0.4924, 0.5097,
         0.4963, 0.5141],
        [0.4982, 0.5074, 0.4991, 0.5064, 0.5066, 0.4899, 0.4958, 0.5028, 0.5022,
         0.5029, 0.5084, 0.4991, 0.4961, 0.4882, 0.4903, 0.5077, 0.4917, 0.5089,
         0.4965, 0.5125],
        [0.4978, 0.5075, 0.4982, 0.5067, 0.5061, 0.4889, 0.4962, 0.5005, 0.5039,
         0.5015, 0.5069, 0.4989, 0.4962, 0.4882, 0.4900, 0.5095, 0.4933, 0.5091,
         0.4959, 0.5146],
        [0.4984, 0.5061, 0.4987, 0.5069, 0.5061, 0.4896, 0.4947, 0.5002, 0.5023,
         0.5036, 0.5067, 0.4985, 0.4969, 0.4880, 0.4896, 0.5077, 0.4922, 0.5102,
         0.4935, 0.5140],
        [0.4985, 0.5077, 0.4982, 0.5052, 0.5054, 0.4887, 0.4954, 0.4989, 0.5020,
         0.5027, 0.5088, 0.4992, 0.4965, 0.4882, 0.4916, 0.5062, 0.4945, 0.5098,
         0.4953, 0.5132],
        [0.4973, 0.5048, 0.4977, 0.5060, 0.5052, 0.4884, 0.4946, 0.5000, 0.5009,
         0.5030, 0.5077, 0.4998, 0.4952, 0.4879, 0.4902, 0.5084, 0.4952, 0.5083,
         0.4952, 0.5133],
        [0.4976, 0.5057, 0.4983, 0.5046, 0.5061, 0.4878, 0.4970, 0.5008, 0.5030,
         0.5018, 0.5101, 0.4996, 0.4959, 0.4872, 0.4907, 0.5082, 0.4952, 0.5078,
         0.4965, 0.5128],
        [0.4978, 0.5048, 0.4984, 0.5050, 0.5065, 0.4881, 0.4952, 0.5011, 0.5029,
         0.5018, 0.5097, 0.4997, 0.4963, 0.4889, 0.4919, 0.5079, 0.4933, 0.5063,
         0.4934, 0.5127],
        [0.4980, 0.5048, 0.4975, 0.5048, 0.5080, 0.4880, 0.4960, 0.5001, 0.5018,
         0.5008, 0.5066, 0.5012, 0.4974, 0.4887, 0.4923, 0.5083, 0.4928, 0.5085,
         0.4941, 0.5142],
        [0.4984, 0.5057, 0.4995, 0.5048, 0.5073, 0.4882, 0.4962, 0.5016, 0.5019,
         0.5040, 0.5076, 0.5003, 0.4943, 0.4908, 0.4928, 0.5095, 0.4941, 0.5049,
         0.4951, 0.5155]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4972, 0.5066, 0.4989, 0.5066, 0.5067, 0.4887, 0.4952, 0.5018, 0.4998,
        0.5024, 0.5101, 0.5006, 0.4960, 0.4887, 0.4924, 0.5076, 0.4924, 0.5097,
        0.4963, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5074, 0.4991, 0.5064, 0.5066, 0.4899, 0.4958, 0.5028, 0.5022,
        0.5029, 0.5084, 0.4991, 0.4961, 0.4882, 0.4903, 0.5077, 0.4917, 0.5089,
        0.4965, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5075, 0.4982, 0.5067, 0.5061, 0.4889, 0.4962, 0.5005, 0.5039,
        0.5015, 0.5069, 0.4989, 0.4962, 0.4882, 0.4900, 0.5095, 0.4933, 0.5091,
        0.4959, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5061, 0.4987, 0.5069, 0.5061, 0.4896, 0.4947, 0.5002, 0.5023,
        0.5036, 0.5067, 0.4985, 0.4969, 0.4880, 0.4896, 0.5077, 0.4922, 0.5102,
        0.4935, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5077, 0.4982, 0.5052, 0.5054, 0.4887, 0.4954, 0.4989, 0.5020,
        0.5027, 0.5088, 0.4992, 0.4965, 0.4882, 0.4916, 0.5062, 0.4945, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4973, 0.5048, 0.4977, 0.5060, 0.5052, 0.4884, 0.4946, 0.5000, 0.5009,
        0.5030, 0.5077, 0.4998, 0.4952, 0.4879, 0.4902, 0.5084, 0.4952, 0.5083,
        0.4952, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5057, 0.4983, 0.5046, 0.5061, 0.4878, 0.4970, 0.5008, 0.5030,
        0.5018, 0.5101, 0.4996, 0.4959, 0.4872, 0.4907, 0.5082, 0.4952, 0.5078,
        0.4965, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5048, 0.4984, 0.5050, 0.5065, 0.4881, 0.4952, 0.5011, 0.5029,
        0.5018, 0.5097, 0.4997, 0.4963, 0.4889, 0.4919, 0.5079, 0.4933, 0.5063,
        0.4934, 0.5127], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5048, 0.4975, 0.5048, 0.5080, 0.4880, 0.4960, 0.5001, 0.5018,
        0.5008, 0.5066, 0.5012, 0.4974, 0.4887, 0.4923, 0.5083, 0.4928, 0.5085,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5048, 0.5073, 0.4882, 0.4962, 0.5016, 0.5019,
        0.5040, 0.5076, 0.5003, 0.4943, 0.4908, 0.4928, 0.5095, 0.4941, 0.5049,
        0.4951, 0.5155], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4891, 0.5121, 0.4864, 0.4853, 0.4942, 0.4896, 0.4942, 0.4987,
         0.5074, 0.5028, 0.5003, 0.5025, 0.4918, 0.5045, 0.4914, 0.4858, 0.4985,
         0.5077, 0.4947],
        [0.5088, 0.4895, 0.5108, 0.4850, 0.4855, 0.4928, 0.4884, 0.4953, 0.4981,
         0.5052, 0.5037, 0.5000, 0.5034, 0.4917, 0.5030, 0.4889, 0.4862, 0.4983,
         0.5071, 0.4933],
        [0.5103, 0.4904, 0.5108, 0.4868, 0.4853, 0.4943, 0.4889, 0.4964, 0.5010,
         0.5069, 0.5046, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4874, 0.4981,
         0.5068, 0.4925],
        [0.5079, 0.4894, 0.5117, 0.4860, 0.4871, 0.4934, 0.4899, 0.4952, 0.4983,
         0.5086, 0.5038, 0.5001, 0.5050, 0.4915, 0.5041, 0.4906, 0.4862, 0.4985,
         0.5057, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4861, 0.4872, 0.4929, 0.4910, 0.4923, 0.4956,
         0.5067, 0.5044, 0.4999, 0.5030, 0.4914, 0.5040, 0.4893, 0.4853, 0.4961,
         0.5054, 0.4936],
        [0.5111, 0.4916, 0.5102, 0.4866, 0.4862, 0.4946, 0.4908, 0.4939, 0.4969,
         0.5069, 0.5060, 0.5003, 0.5039, 0.4897, 0.5040, 0.4899, 0.4851, 0.4970,
         0.5061, 0.4930],
        [0.5096, 0.4914, 0.5125, 0.4880, 0.4859, 0.4937, 0.4890, 0.4942, 0.4976,
         0.5072, 0.5063, 0.4996, 0.5049, 0.4904, 0.5037, 0.4893, 0.4846, 0.4985,
         0.5071, 0.4932],
        [0.5101, 0.4914, 0.5114, 0.4852, 0.4868, 0.4926, 0.4892, 0.4947, 0.4998,
         0.5072, 0.5068, 0.5008, 0.5035, 0.4893, 0.5049, 0.4913, 0.4854, 0.4957,
         0.5075, 0.4933],
        [0.5111, 0.4926, 0.5110, 0.4851, 0.4886, 0.4928, 0.4894, 0.4941, 0.4981,
         0.5074, 0.5049, 0.4974, 0.5041, 0.4896, 0.5040, 0.4907, 0.4864, 0.4966,
         0.5082, 0.4937],
        [0.5113, 0.4921, 0.5118, 0.4870, 0.4882, 0.4928, 0.4893, 0.4932, 0.4999,
         0.5052, 0.5052, 0.4985, 0.5036, 0.4888, 0.5005, 0.4931, 0.4836, 0.4974,
         0.5058, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4891, 0.5121, 0.4864, 0.4853, 0.4942, 0.4896, 0.4942, 0.4987,
        0.5074, 0.5028, 0.5003, 0.5025, 0.4918, 0.5045, 0.4914, 0.4858, 0.4985,
        0.5077, 0.4947], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4895, 0.5108, 0.4850, 0.4855, 0.4928, 0.4884, 0.4953, 0.4981,
        0.5052, 0.5037, 0.5000, 0.5034, 0.4917, 0.5030, 0.4889, 0.4862, 0.4983,
        0.5071, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5103, 0.4904, 0.5108, 0.4868, 0.4853, 0.4943, 0.4889, 0.4964, 0.5010,
        0.5069, 0.5046, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4874, 0.4981,
        0.5068, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4894, 0.5117, 0.4860, 0.4871, 0.4934, 0.4899, 0.4952, 0.4983,
        0.5086, 0.5038, 0.5001, 0.5050, 0.4915, 0.5041, 0.4906, 0.4862, 0.4985,
        0.5057, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4861, 0.4872, 0.4929, 0.4910, 0.4923, 0.4956,
        0.5067, 0.5044, 0.4999, 0.5030, 0.4914, 0.5040, 0.4893, 0.4853, 0.4961,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4916, 0.5102, 0.4866, 0.4862, 0.4946, 0.4908, 0.4939, 0.4969,
        0.5069, 0.5060, 0.5003, 0.5039, 0.4897, 0.5040, 0.4899, 0.4851, 0.4970,
        0.5061, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4914, 0.5125, 0.4880, 0.4859, 0.4937, 0.4890, 0.4942, 0.4976,
        0.5072, 0.5063, 0.4996, 0.5049, 0.4904, 0.5037, 0.4893, 0.4846, 0.4985,
        0.5071, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4914, 0.5114, 0.4852, 0.4868, 0.4926, 0.4892, 0.4947, 0.4998,
        0.5072, 0.5068, 0.5008, 0.5035, 0.4893, 0.5049, 0.4913, 0.4854, 0.4957,
        0.5075, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4926, 0.5110, 0.4851, 0.4886, 0.4928, 0.4894, 0.4941, 0.4981,
        0.5074, 0.5049, 0.4974, 0.5041, 0.4896, 0.5040, 0.4907, 0.4864, 0.4966,
        0.5082, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4921, 0.5118, 0.4870, 0.4882, 0.4928, 0.4893, 0.4932, 0.4999,
        0.5052, 0.5052, 0.4985, 0.5036, 0.4888, 0.5005, 0.4931, 0.4836, 0.4974,
        0.5058, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5125, 0.4963, 0.4967, 0.5027, 0.5067, 0.4927, 0.5093, 0.4954, 0.4967,
         0.5027, 0.4855, 0.4978, 0.4838, 0.5022, 0.5023, 0.4983, 0.4992, 0.5144,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4956, 0.5044, 0.5036, 0.4932, 0.5103, 0.4966, 0.4959,
         0.5026, 0.4848, 0.4967, 0.4855, 0.5015, 0.5000, 0.4979, 0.4980, 0.5149,
         0.4946, 0.4970],
        [0.5131, 0.4971, 0.4976, 0.5025, 0.5046, 0.4921, 0.5103, 0.4978, 0.4961,
         0.5027, 0.4859, 0.4988, 0.4846, 0.5017, 0.4998, 0.4976, 0.4992, 0.5166,
         0.4946, 0.4967],
        [0.5126, 0.4975, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
         0.5027, 0.4841, 0.4967, 0.4845, 0.5022, 0.5007, 0.4975, 0.5011, 0.5145,
         0.4954, 0.4970],
        [0.5128, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5088, 0.4953, 0.4966,
         0.5031, 0.4869, 0.4979, 0.4826, 0.5028, 0.5003, 0.4993, 0.5001, 0.5152,
         0.4956, 0.4971],
        [0.5137, 0.4973, 0.4963, 0.5031, 0.5050, 0.4927, 0.5078, 0.4967, 0.4955,
         0.5025, 0.4856, 0.4962, 0.4852, 0.5033, 0.5017, 0.5004, 0.4995, 0.5147,
         0.4953, 0.4983],
        [0.5114, 0.4964, 0.4980, 0.5026, 0.5038, 0.4922, 0.5087, 0.4963, 0.4960,
         0.5026, 0.4877, 0.4971, 0.4851, 0.5027, 0.5009, 0.4989, 0.5001, 0.5140,
         0.4955, 0.4997],
        [0.5120, 0.4973, 0.4951, 0.5010, 0.5040, 0.4922, 0.5082, 0.4944, 0.4969,
         0.5022, 0.4859, 0.4984, 0.4865, 0.5012, 0.5009, 0.4987, 0.4998, 0.5153,
         0.4959, 0.4988],
        [0.5128, 0.4981, 0.4950, 0.5010, 0.5049, 0.4925, 0.5095, 0.4966, 0.4967,
         0.5019, 0.4884, 0.4951, 0.4853, 0.5015, 0.5012, 0.4972, 0.5007, 0.5141,
         0.4957, 0.4995],
        [0.5127, 0.4985, 0.4938, 0.5027, 0.5050, 0.4937, 0.5079, 0.4988, 0.4964,
         0.5032, 0.4885, 0.4979, 0.4857, 0.5001, 0.5011, 0.4982, 0.4995, 0.5154,
         0.4972, 0.5005]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5125, 0.4963, 0.4967, 0.5027, 0.5067, 0.4927, 0.5093, 0.4954, 0.4967,
        0.5027, 0.4855, 0.4978, 0.4838, 0.5022, 0.5023, 0.4983, 0.4992, 0.5144,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4956, 0.5044, 0.5036, 0.4932, 0.5103, 0.4966, 0.4959,
        0.5026, 0.4848, 0.4967, 0.4855, 0.5015, 0.5000, 0.4979, 0.4980, 0.5149,
        0.4946, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4971, 0.4976, 0.5025, 0.5046, 0.4921, 0.5103, 0.4978, 0.4961,
        0.5027, 0.4859, 0.4988, 0.4846, 0.5017, 0.4998, 0.4976, 0.4992, 0.5166,
        0.4946, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4975, 0.4970, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
        0.5027, 0.4841, 0.4967, 0.4845, 0.5022, 0.5007, 0.4975, 0.5011, 0.5145,
        0.4954, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5088, 0.4953, 0.4966,
        0.5031, 0.4869, 0.4979, 0.4826, 0.5028, 0.5003, 0.4993, 0.5001, 0.5152,
        0.4956, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4973, 0.4963, 0.5031, 0.5050, 0.4927, 0.5078, 0.4967, 0.4955,
        0.5025, 0.4856, 0.4962, 0.4852, 0.5033, 0.5017, 0.5004, 0.4995, 0.5147,
        0.4953, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4964, 0.4980, 0.5026, 0.5038, 0.4922, 0.5087, 0.4963, 0.4960,
        0.5026, 0.4877, 0.4971, 0.4851, 0.5027, 0.5009, 0.4989, 0.5001, 0.5140,
        0.4955, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4973, 0.4951, 0.5010, 0.5040, 0.4922, 0.5082, 0.4944, 0.4969,
        0.5022, 0.4859, 0.4984, 0.4865, 0.5012, 0.5009, 0.4987, 0.4998, 0.5153,
        0.4959, 0.4988], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4950, 0.5010, 0.5049, 0.4925, 0.5095, 0.4966, 0.4967,
        0.5019, 0.4884, 0.4951, 0.4853, 0.5015, 0.5012, 0.4972, 0.5007, 0.5141,
        0.4957, 0.4995], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4938, 0.5027, 0.5050, 0.4937, 0.5079, 0.4988, 0.4964,
        0.5032, 0.4885, 0.4979, 0.4857, 0.5001, 0.5011, 0.4982, 0.4995, 0.5154,
        0.4972, 0.5005], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [7, 0, 2, 9, 4, 3, 8, 5, 6, 1]
replay_buffer._size: [8400 8400 8400 8400 8400 8400 8400 8400 8400 8400]
2023-08-12 10:41:06,835 MainThread INFO: EPOCH:54
2023-08-12 10:41:06,836 MainThread INFO: Time Consumed:0.38167238235473633s
2023-08-12 10:41:06,836 MainThread INFO: Total Frames:82500s
 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 55/80 [01:00<00:32,  1.29s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1441.45974
Train_Epoch_Reward                    2804.39531
Running_Training_Average_Rewards      430.11636
Explore_Time                          0.00357
Train___Time                          0.36999
Eval____Time                          0.00322
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.65618
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.85216
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.17888
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.57219
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.54370
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.24093
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.31040
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14663.36414
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.93284
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.47947
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.57948      1.17805    11.42890    7.97297
alpha_0                               0.92122      0.00039    0.92178     0.92067
alpha_1                               0.92122      0.00039    0.92177     0.92067
alpha_2                               0.92124      0.00039    0.92179     0.92068
alpha_3                               0.92121      0.00039    0.92177     0.92066
alpha_4                               0.92122      0.00039    0.92178     0.92067
alpha_5                               0.92121      0.00039    0.92177     0.92066
alpha_6                               0.92123      0.00039    0.92178     0.92067
alpha_7                               0.92121      0.00039    0.92176     0.92066
alpha_8                               0.92121      0.00039    0.92177     0.92066
alpha_9                               0.92123      0.00039    0.92178     0.92068
Alpha_loss                            -0.55053     0.00287    -0.54645    -0.55454
Training/policy_loss                  -2.80841     0.00386    -2.80473    -2.81512
Training/qf1_loss                     1838.04443   509.67476  2546.70654  1276.30542
Training/qf2_loss                     1835.48181   509.47084  2543.72510  1273.80701
Training/pf_norm                      0.10413      0.02322    0.14173     0.07830
Training/qf1_norm                     34.36013     3.26317    39.42081    30.07108
Training/qf2_norm                     42.35730     3.93179    48.79442    37.99456
log_std/mean                          -0.13281     0.00003    -0.13277    -0.13286
log_std/std                           0.00883      0.00002    0.00886     0.00879
log_std/max                           -0.11518     0.00015    -0.11495    -0.11533
log_std/min                           -0.15329     0.00037    -0.15272    -0.15369
log_probs/mean                        -2.73396     0.00325    -2.73106    -2.74017
log_probs/std                         0.22745      0.00764    0.24225     0.22173
log_probs/max                         -2.15618     0.04114    -2.10060    -2.22641
log_probs/min                         -4.91756     0.47770    -4.31729    -5.71032
mean/mean                             0.00004      0.00024    0.00038     -0.00029
mean/std                              0.01061      0.00012    0.01078     0.01044
mean/max                              0.01676      0.00026    0.01713     0.01640
mean/min                              -0.01776     0.00012    -0.01760    -0.01791
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [1, 7, 4, 9, 0, 2, 8, 5, 6, 3]
replay_buffer._size: [8550 8550 8550 8550 8550 8550 8550 8550 8550 8550]
snapshot at best
2023-08-12 10:41:07,845 MainThread INFO: EPOCH:55
2023-08-12 10:41:07,846 MainThread INFO: Time Consumed:0.8706328868865967s
2023-08-12 10:41:07,846 MainThread INFO: Total Frames:84000s
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 56/80 [01:01<00:28,  1.20s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1453.58958
Train_Epoch_Reward                    3456.61250
Running_Training_Average_Rewards      447.40905
Explore_Time                          0.00417
Train___Time                          0.33852
Eval____Time                          0.00251
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.70563
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.73630
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.30064
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.50466
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.64915
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.32464
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.39276
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14782.77965
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.81193
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.45816
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.62140      1.09738    11.61900    8.57746
alpha_0                               0.91984      0.00039    0.92039     0.91929
alpha_1                               0.91984      0.00039    0.92039     0.91928
alpha_2                               0.91985      0.00039    0.92041     0.91930
alpha_3                               0.91983      0.00039    0.92038     0.91928
alpha_4                               0.91984      0.00039    0.92039     0.91929
alpha_5                               0.91983      0.00039    0.92038     0.91928
alpha_6                               0.91985      0.00039    0.92040     0.91929
alpha_7                               0.91983      0.00039    0.92038     0.91927
alpha_8                               0.91983      0.00039    0.92038     0.91928
alpha_9                               0.91985      0.00039    0.92040     0.91929
Alpha_loss                            -0.56042     0.00302    -0.55595    -0.56435
Training/policy_loss                  -2.81200     0.00453    -2.80383    -2.81740
Training/qf1_loss                     1971.79402   549.71146  2986.96606  1376.08765
Training/qf2_loss                     1969.21563   549.50775  2983.94019  1373.58008
Training/pf_norm                      0.14767      0.02553    0.17905     0.11497
Training/qf1_norm                     34.75342     3.22588    40.70068    31.99195
Training/qf2_norm                     42.64269     4.04154    49.99188    38.87243
log_std/mean                          -0.13305     0.00013    -0.13281    -0.13316
log_std/std                           0.00881      0.00004    0.00886     0.00876
log_std/max                           -0.11567     0.00022    -0.11526    -0.11584
log_std/min                           -0.15372     0.00021    -0.15344    -0.15406
log_probs/mean                        -2.73114     0.00346    -2.72602    -2.73673
log_probs/std                         0.22024      0.00917    0.23674     0.21098
log_probs/max                         -2.15072     0.03159    -2.10243    -2.19929
log_probs/min                         -4.59687     0.40553    -3.94723    -5.03475
mean/mean                             0.00123      0.00039    0.00171     0.00064
mean/std                              0.01130      0.00027    0.01169     0.01090
mean/max                              0.01802      0.00065    0.01878     0.01710
mean/min                              -0.01841     0.00015    -0.01815    -0.01860
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4972, 0.5067, 0.4989, 0.5066, 0.5067, 0.4887, 0.4952, 0.5018, 0.4998,
         0.5025, 0.5101, 0.5006, 0.4960, 0.4887, 0.4923, 0.5076, 0.4924, 0.5098,
         0.4963, 0.5141],
        [0.4981, 0.5075, 0.4990, 0.5063, 0.5066, 0.4899, 0.4958, 0.5028, 0.5023,
         0.5030, 0.5084, 0.4990, 0.4961, 0.4882, 0.4902, 0.5077, 0.4917, 0.5089,
         0.4965, 0.5125],
        [0.4978, 0.5075, 0.4983, 0.5067, 0.5061, 0.4889, 0.4962, 0.5006, 0.5040,
         0.5015, 0.5069, 0.4987, 0.4963, 0.4882, 0.4900, 0.5095, 0.4933, 0.5091,
         0.4959, 0.5146],
        [0.4985, 0.5062, 0.4987, 0.5068, 0.5061, 0.4896, 0.4947, 0.5002, 0.5023,
         0.5036, 0.5067, 0.4985, 0.4969, 0.4880, 0.4897, 0.5077, 0.4922, 0.5102,
         0.4936, 0.5140],
        [0.4985, 0.5077, 0.4982, 0.5052, 0.5054, 0.4887, 0.4954, 0.4989, 0.5020,
         0.5027, 0.5088, 0.4991, 0.4965, 0.4882, 0.4915, 0.5062, 0.4945, 0.5098,
         0.4953, 0.5132],
        [0.4974, 0.5048, 0.4977, 0.5060, 0.5051, 0.4883, 0.4946, 0.5000, 0.5009,
         0.5030, 0.5077, 0.4997, 0.4952, 0.4879, 0.4902, 0.5083, 0.4952, 0.5083,
         0.4951, 0.5132],
        [0.4978, 0.5057, 0.4982, 0.5047, 0.5061, 0.4878, 0.4969, 0.5008, 0.5029,
         0.5019, 0.5100, 0.4996, 0.4959, 0.4873, 0.4907, 0.5081, 0.4951, 0.5079,
         0.4965, 0.5128],
        [0.4978, 0.5047, 0.4984, 0.5050, 0.5065, 0.4881, 0.4952, 0.5011, 0.5029,
         0.5018, 0.5098, 0.4997, 0.4963, 0.4889, 0.4919, 0.5078, 0.4932, 0.5063,
         0.4933, 0.5126],
        [0.4980, 0.5048, 0.4975, 0.5048, 0.5080, 0.4879, 0.4960, 0.5002, 0.5018,
         0.5008, 0.5067, 0.5012, 0.4974, 0.4887, 0.4923, 0.5083, 0.4929, 0.5085,
         0.4941, 0.5142],
        [0.4984, 0.5057, 0.4995, 0.5048, 0.5074, 0.4881, 0.4962, 0.5017, 0.5018,
         0.5040, 0.5076, 0.5002, 0.4943, 0.4908, 0.4928, 0.5095, 0.4941, 0.5048,
         0.4951, 0.5155]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4972, 0.5067, 0.4989, 0.5066, 0.5067, 0.4887, 0.4952, 0.5018, 0.4998,
        0.5025, 0.5101, 0.5006, 0.4960, 0.4887, 0.4923, 0.5076, 0.4924, 0.5098,
        0.4963, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5075, 0.4990, 0.5063, 0.5066, 0.4899, 0.4958, 0.5028, 0.5023,
        0.5030, 0.5084, 0.4990, 0.4961, 0.4882, 0.4902, 0.5077, 0.4917, 0.5089,
        0.4965, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5075, 0.4983, 0.5067, 0.5061, 0.4889, 0.4962, 0.5006, 0.5040,
        0.5015, 0.5069, 0.4987, 0.4963, 0.4882, 0.4900, 0.5095, 0.4933, 0.5091,
        0.4959, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5062, 0.4987, 0.5068, 0.5061, 0.4896, 0.4947, 0.5002, 0.5023,
        0.5036, 0.5067, 0.4985, 0.4969, 0.4880, 0.4897, 0.5077, 0.4922, 0.5102,
        0.4936, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5077, 0.4982, 0.5052, 0.5054, 0.4887, 0.4954, 0.4989, 0.5020,
        0.5027, 0.5088, 0.4991, 0.4965, 0.4882, 0.4915, 0.5062, 0.4945, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4974, 0.5048, 0.4977, 0.5060, 0.5051, 0.4883, 0.4946, 0.5000, 0.5009,
        0.5030, 0.5077, 0.4997, 0.4952, 0.4879, 0.4902, 0.5083, 0.4952, 0.5083,
        0.4951, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5057, 0.4982, 0.5047, 0.5061, 0.4878, 0.4969, 0.5008, 0.5029,
        0.5019, 0.5100, 0.4996, 0.4959, 0.4873, 0.4907, 0.5081, 0.4951, 0.5079,
        0.4965, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5047, 0.4984, 0.5050, 0.5065, 0.4881, 0.4952, 0.5011, 0.5029,
        0.5018, 0.5098, 0.4997, 0.4963, 0.4889, 0.4919, 0.5078, 0.4932, 0.5063,
        0.4933, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5048, 0.4975, 0.5048, 0.5080, 0.4879, 0.4960, 0.5002, 0.5018,
        0.5008, 0.5067, 0.5012, 0.4974, 0.4887, 0.4923, 0.5083, 0.4929, 0.5085,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5048, 0.5074, 0.4881, 0.4962, 0.5017, 0.5018,
        0.5040, 0.5076, 0.5002, 0.4943, 0.4908, 0.4928, 0.5095, 0.4941, 0.5048,
        0.4951, 0.5155], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5103, 0.4892, 0.5120, 0.4864, 0.4853, 0.4942, 0.4896, 0.4942, 0.4987,
         0.5074, 0.5028, 0.5003, 0.5026, 0.4919, 0.5046, 0.4913, 0.4858, 0.4984,
         0.5077, 0.4946],
        [0.5088, 0.4896, 0.5108, 0.4850, 0.4856, 0.4929, 0.4884, 0.4953, 0.4982,
         0.5052, 0.5038, 0.5000, 0.5034, 0.4917, 0.5030, 0.4888, 0.4862, 0.4982,
         0.5070, 0.4933],
        [0.5103, 0.4904, 0.5107, 0.4868, 0.4852, 0.4944, 0.4889, 0.4963, 0.5010,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5041, 0.4908, 0.4874, 0.4981,
         0.5069, 0.4925],
        [0.5079, 0.4894, 0.5116, 0.4860, 0.4871, 0.4934, 0.4899, 0.4951, 0.4983,
         0.5086, 0.5039, 0.5001, 0.5050, 0.4916, 0.5041, 0.4905, 0.4862, 0.4984,
         0.5057, 0.4924],
        [0.5095, 0.4912, 0.5131, 0.4860, 0.4872, 0.4929, 0.4910, 0.4922, 0.4956,
         0.5067, 0.5045, 0.4999, 0.5030, 0.4914, 0.5040, 0.4894, 0.4853, 0.4961,
         0.5054, 0.4936],
        [0.5110, 0.4917, 0.5102, 0.4866, 0.4862, 0.4946, 0.4908, 0.4939, 0.4970,
         0.5068, 0.5060, 0.5004, 0.5039, 0.4897, 0.5041, 0.4899, 0.4851, 0.4970,
         0.5061, 0.4930],
        [0.5096, 0.4915, 0.5125, 0.4880, 0.4859, 0.4937, 0.4889, 0.4942, 0.4976,
         0.5072, 0.5064, 0.4996, 0.5049, 0.4904, 0.5038, 0.4893, 0.4846, 0.4985,
         0.5072, 0.4933],
        [0.5102, 0.4914, 0.5114, 0.4853, 0.4868, 0.4927, 0.4892, 0.4947, 0.4998,
         0.5072, 0.5068, 0.5008, 0.5035, 0.4893, 0.5049, 0.4914, 0.4854, 0.4957,
         0.5075, 0.4933],
        [0.5111, 0.4927, 0.5110, 0.4851, 0.4887, 0.4928, 0.4894, 0.4941, 0.4982,
         0.5074, 0.5051, 0.4974, 0.5041, 0.4896, 0.5040, 0.4908, 0.4864, 0.4965,
         0.5082, 0.4937],
        [0.5113, 0.4922, 0.5119, 0.4870, 0.4882, 0.4928, 0.4894, 0.4932, 0.4998,
         0.5052, 0.5053, 0.4985, 0.5036, 0.4888, 0.5005, 0.4932, 0.4836, 0.4973,
         0.5058, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5103, 0.4892, 0.5120, 0.4864, 0.4853, 0.4942, 0.4896, 0.4942, 0.4987,
        0.5074, 0.5028, 0.5003, 0.5026, 0.4919, 0.5046, 0.4913, 0.4858, 0.4984,
        0.5077, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4896, 0.5108, 0.4850, 0.4856, 0.4929, 0.4884, 0.4953, 0.4982,
        0.5052, 0.5038, 0.5000, 0.5034, 0.4917, 0.5030, 0.4888, 0.4862, 0.4982,
        0.5070, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5103, 0.4904, 0.5107, 0.4868, 0.4852, 0.4944, 0.4889, 0.4963, 0.5010,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5041, 0.4908, 0.4874, 0.4981,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4894, 0.5116, 0.4860, 0.4871, 0.4934, 0.4899, 0.4951, 0.4983,
        0.5086, 0.5039, 0.5001, 0.5050, 0.4916, 0.5041, 0.4905, 0.4862, 0.4984,
        0.5057, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5131, 0.4860, 0.4872, 0.4929, 0.4910, 0.4922, 0.4956,
        0.5067, 0.5045, 0.4999, 0.5030, 0.4914, 0.5040, 0.4894, 0.4853, 0.4961,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4917, 0.5102, 0.4866, 0.4862, 0.4946, 0.4908, 0.4939, 0.4970,
        0.5068, 0.5060, 0.5004, 0.5039, 0.4897, 0.5041, 0.4899, 0.4851, 0.4970,
        0.5061, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4915, 0.5125, 0.4880, 0.4859, 0.4937, 0.4889, 0.4942, 0.4976,
        0.5072, 0.5064, 0.4996, 0.5049, 0.4904, 0.5038, 0.4893, 0.4846, 0.4985,
        0.5072, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5102, 0.4914, 0.5114, 0.4853, 0.4868, 0.4927, 0.4892, 0.4947, 0.4998,
        0.5072, 0.5068, 0.5008, 0.5035, 0.4893, 0.5049, 0.4914, 0.4854, 0.4957,
        0.5075, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5111, 0.4927, 0.5110, 0.4851, 0.4887, 0.4928, 0.4894, 0.4941, 0.4982,
        0.5074, 0.5051, 0.4974, 0.5041, 0.4896, 0.5040, 0.4908, 0.4864, 0.4965,
        0.5082, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4922, 0.5119, 0.4870, 0.4882, 0.4928, 0.4894, 0.4932, 0.4998,
        0.5052, 0.5053, 0.4985, 0.5036, 0.4888, 0.5005, 0.4932, 0.4836, 0.4973,
        0.5058, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5125, 0.4963, 0.4967, 0.5027, 0.5067, 0.4927, 0.5093, 0.4953, 0.4968,
         0.5027, 0.4855, 0.4978, 0.4838, 0.5021, 0.5023, 0.4983, 0.4991, 0.5144,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4957, 0.5045, 0.5036, 0.4931, 0.5103, 0.4965, 0.4959,
         0.5025, 0.4849, 0.4967, 0.4855, 0.5015, 0.5002, 0.4978, 0.4981, 0.5149,
         0.4945, 0.4970],
        [0.5131, 0.4972, 0.4976, 0.5026, 0.5046, 0.4921, 0.5105, 0.4978, 0.4961,
         0.5027, 0.4859, 0.4988, 0.4846, 0.5016, 0.4999, 0.4976, 0.4992, 0.5165,
         0.4946, 0.4967],
        [0.5126, 0.4976, 0.4971, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
         0.5027, 0.4841, 0.4967, 0.4845, 0.5022, 0.5008, 0.4975, 0.5011, 0.5144,
         0.4953, 0.4970],
        [0.5128, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5089, 0.4953, 0.4966,
         0.5030, 0.4869, 0.4978, 0.4825, 0.5028, 0.5003, 0.4994, 0.5001, 0.5152,
         0.4956, 0.4971],
        [0.5137, 0.4974, 0.4963, 0.5032, 0.5050, 0.4927, 0.5078, 0.4967, 0.4955,
         0.5025, 0.4856, 0.4962, 0.4851, 0.5034, 0.5017, 0.5004, 0.4996, 0.5147,
         0.4953, 0.4982],
        [0.5115, 0.4965, 0.4980, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4960,
         0.5025, 0.4877, 0.4971, 0.4850, 0.5028, 0.5009, 0.4990, 0.5000, 0.5139,
         0.4954, 0.4997],
        [0.5120, 0.4973, 0.4952, 0.5010, 0.5040, 0.4921, 0.5082, 0.4944, 0.4969,
         0.5022, 0.4859, 0.4984, 0.4864, 0.5013, 0.5009, 0.4987, 0.4999, 0.5153,
         0.4959, 0.4989],
        [0.5128, 0.4981, 0.4950, 0.5011, 0.5049, 0.4924, 0.5095, 0.4966, 0.4967,
         0.5019, 0.4884, 0.4951, 0.4853, 0.5015, 0.5012, 0.4972, 0.5007, 0.5142,
         0.4958, 0.4995],
        [0.5126, 0.4984, 0.4938, 0.5027, 0.5050, 0.4936, 0.5078, 0.4988, 0.4964,
         0.5032, 0.4885, 0.4978, 0.4857, 0.5001, 0.5011, 0.4982, 0.4996, 0.5154,
         0.4971, 0.5006]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5125, 0.4963, 0.4967, 0.5027, 0.5067, 0.4927, 0.5093, 0.4953, 0.4968,
        0.5027, 0.4855, 0.4978, 0.4838, 0.5021, 0.5023, 0.4983, 0.4991, 0.5144,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4957, 0.5045, 0.5036, 0.4931, 0.5103, 0.4965, 0.4959,
        0.5025, 0.4849, 0.4967, 0.4855, 0.5015, 0.5002, 0.4978, 0.4981, 0.5149,
        0.4945, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4972, 0.4976, 0.5026, 0.5046, 0.4921, 0.5105, 0.4978, 0.4961,
        0.5027, 0.4859, 0.4988, 0.4846, 0.5016, 0.4999, 0.4976, 0.4992, 0.5165,
        0.4946, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4976, 0.4971, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
        0.5027, 0.4841, 0.4967, 0.4845, 0.5022, 0.5008, 0.4975, 0.5011, 0.5144,
        0.4953, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5089, 0.4953, 0.4966,
        0.5030, 0.4869, 0.4978, 0.4825, 0.5028, 0.5003, 0.4994, 0.5001, 0.5152,
        0.4956, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4974, 0.4963, 0.5032, 0.5050, 0.4927, 0.5078, 0.4967, 0.4955,
        0.5025, 0.4856, 0.4962, 0.4851, 0.5034, 0.5017, 0.5004, 0.4996, 0.5147,
        0.4953, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4965, 0.4980, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4960,
        0.5025, 0.4877, 0.4971, 0.4850, 0.5028, 0.5009, 0.4990, 0.5000, 0.5139,
        0.4954, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4973, 0.4952, 0.5010, 0.5040, 0.4921, 0.5082, 0.4944, 0.4969,
        0.5022, 0.4859, 0.4984, 0.4864, 0.5013, 0.5009, 0.4987, 0.4999, 0.5153,
        0.4959, 0.4989], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4950, 0.5011, 0.5049, 0.4924, 0.5095, 0.4966, 0.4967,
        0.5019, 0.4884, 0.4951, 0.4853, 0.5015, 0.5012, 0.4972, 0.5007, 0.5142,
        0.4958, 0.4995], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4984, 0.4938, 0.5027, 0.5050, 0.4936, 0.5078, 0.4988, 0.4964,
        0.5032, 0.4885, 0.4978, 0.4857, 0.5001, 0.5011, 0.4982, 0.4996, 0.5154,
        0.4971, 0.5006], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [9, 1, 7, 6, 8, 4, 3, 2, 0, 5]
replay_buffer._size: [8700 8700 8700 8700 8700 8700 8700 8700 8700 8700]
snapshot at best
2023-08-12 10:41:09,341 MainThread INFO: EPOCH:56
2023-08-12 10:41:09,342 MainThread INFO: Time Consumed:1.074176549911499s
2023-08-12 10:41:09,342 MainThread INFO: Total Frames:85500s
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 57/80 [01:02<00:29,  1.29s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1472.98432
Train_Epoch_Reward                    24952.82970
Running_Training_Average_Rewards      1040.46125
Explore_Time                          0.09620
Train___Time                          0.40627
Eval____Time                          0.00299
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -51.27842
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.72554
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.46023
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.49687
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.85533
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.49194
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.55741
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 14980.88965
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.80476
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.37592
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.19663      0.64733    10.32502    8.59773
alpha_0                               0.91846      0.00039    0.91901     0.91791
alpha_1                               0.91846      0.00039    0.91901     0.91790
alpha_2                               0.91847      0.00039    0.91902     0.91792
alpha_3                               0.91845      0.00039    0.91900     0.91790
alpha_4                               0.91846      0.00039    0.91901     0.91791
alpha_5                               0.91845      0.00039    0.91900     0.91790
alpha_6                               0.91846      0.00039    0.91902     0.91791
alpha_7                               0.91845      0.00039    0.91900     0.91789
alpha_8                               0.91845      0.00039    0.91900     0.91790
alpha_9                               0.91847      0.00039    0.91902     0.91792
Alpha_loss                            -0.57102     0.00257    -0.56802    -0.57521
Training/policy_loss                  -2.82596     0.00614    -2.81730    -2.83451
Training/qf1_loss                     1666.31072   210.09790  2030.95959  1413.82629
Training/qf2_loss                     1663.78694   210.18596  2028.45142  1411.21851
Training/pf_norm                      0.12068      0.02016    0.15867     0.09898
Training/qf1_norm                     33.95439     1.74510    37.02004    32.27837
Training/qf2_norm                     41.55348     1.85250    44.49422    39.09391
log_std/mean                          -0.13334     0.00005    -0.13327    -0.13343
log_std/std                           0.00881      0.00005    0.00888     0.00874
log_std/max                           -0.11571     0.00026    -0.11531    -0.11600
log_std/min                           -0.15445     0.00030    -0.15402    -0.15489
log_probs/mean                        -2.73696     0.00765    -2.72727    -2.74946
log_probs/std                         0.23358      0.00776    0.24101     0.21871
log_probs/max                         -2.14123     0.01389    -2.12431    -2.16464
log_probs/min                         -4.49301     0.23545    -4.16090    -4.76952
mean/mean                             0.00206      0.00011    0.00222     0.00188
mean/std                              0.01263      0.00040    0.01314     0.01204
mean/max                              0.02043      0.00065    0.02115     0.01943
mean/min                              -0.02012     0.00065    -0.01918    -0.02093
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [4, 6, 2, 9, 0, 1, 8, 3, 7, 5]
replay_buffer._size: [8850 8850 8850 8850 8850 8850 8850 8850 8850 8850]
snapshot at best
2023-08-12 10:41:10,506 MainThread INFO: EPOCH:57
2023-08-12 10:41:10,507 MainThread INFO: Time Consumed:0.9997603893280029s
2023-08-12 10:41:10,507 MainThread INFO: Total Frames:87000s
 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 58/80 [01:04<00:27,  1.26s/it]------------------------------------  -----------  ----------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1517.47572
Train_Epoch_Reward                    10761.04030
Running_Training_Average_Rewards      1305.68275
Explore_Time                          0.00414
Train___Time                          0.49584
Eval____Time                          0.00334
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.51902
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.65575
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.75604
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.35799
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.09404
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.68372
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.74513
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 15422.46096
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.73850
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -24.15356
mean_success_rate                     0.00000

Name                                  Mean         Std         Max         Min
Reward_Mean                           10.50885     0.55680     11.38463    9.72095
alpha_0                               0.91708      0.00039     0.91763     0.91653
alpha_1                               0.91708      0.00039     0.91763     0.91652
alpha_2                               0.91709      0.00039     0.91764     0.91654
alpha_3                               0.91707      0.00039     0.91762     0.91652
alpha_4                               0.91708      0.00039     0.91763     0.91653
alpha_5                               0.91707      0.00039     0.91762     0.91652
alpha_6                               0.91709      0.00039     0.91764     0.91653
alpha_7                               0.91707      0.00039     0.91762     0.91652
alpha_8                               0.91707      0.00039     0.91762     0.91652
alpha_9                               0.91709      0.00039     0.91764     0.91654
Alpha_loss                            -0.58122     0.00269     -0.57765    -0.58500
Training/policy_loss                  -2.83391     0.00152     -2.83173    -2.83649
Training/qf1_loss                     3431.53743   1623.09606  5510.50977  1940.39392
Training/qf2_loss                     3428.50930   1623.05713  5507.22705  1937.70276
Training/pf_norm                      0.12314      0.02680     0.15824     0.09358
Training/qf1_norm                     37.83820     1.20071     39.38206    36.18053
Training/qf2_norm                     46.99505     2.01977     49.36602    44.19475
log_std/mean                          -0.13336     0.00010     -0.13321    -0.13347
log_std/std                           0.00878      0.00004     0.00886     0.00873
log_std/max                           -0.11536     0.00018     -0.11507    -0.11558
log_std/min                           -0.15487     0.00048     -0.15412    -0.15553
log_probs/mean                        -2.73787     0.00309     -2.73474    -2.74347
log_probs/std                         0.23475      0.00516     0.24121     0.22841
log_probs/max                         -2.15822     0.02917     -2.11142    -2.19110
log_probs/min                         -4.80859     0.42589     -4.26559    -5.47011
mean/mean                             0.00220      0.00003     0.00223     0.00216
mean/std                              0.01345      0.00013     0.01364     0.01326
mean/max                              0.02133      0.00013     0.02146     0.02108
mean/min                              -0.02154     0.00024     -0.02111    -0.02183
------------------------------------  -----------  ----------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4972, 0.5067, 0.4989, 0.5066, 0.5067, 0.4887, 0.4951, 0.5018, 0.4999,
         0.5025, 0.5101, 0.5005, 0.4961, 0.4887, 0.4923, 0.5076, 0.4924, 0.5098,
         0.4963, 0.5142],
        [0.4981, 0.5075, 0.4990, 0.5064, 0.5066, 0.4899, 0.4958, 0.5028, 0.5022,
         0.5030, 0.5084, 0.4990, 0.4961, 0.4882, 0.4902, 0.5077, 0.4917, 0.5089,
         0.4965, 0.5125],
        [0.4978, 0.5076, 0.4983, 0.5068, 0.5061, 0.4889, 0.4962, 0.5006, 0.5039,
         0.5016, 0.5070, 0.4987, 0.4962, 0.4882, 0.4899, 0.5095, 0.4933, 0.5092,
         0.4959, 0.5146],
        [0.4984, 0.5062, 0.4987, 0.5068, 0.5061, 0.4896, 0.4947, 0.5002, 0.5023,
         0.5037, 0.5068, 0.4985, 0.4969, 0.4880, 0.4897, 0.5077, 0.4922, 0.5102,
         0.4937, 0.5140],
        [0.4985, 0.5077, 0.4983, 0.5052, 0.5053, 0.4888, 0.4954, 0.4988, 0.5019,
         0.5028, 0.5088, 0.4991, 0.4964, 0.4882, 0.4916, 0.5061, 0.4945, 0.5098,
         0.4953, 0.5132],
        [0.4973, 0.5049, 0.4977, 0.5060, 0.5051, 0.4883, 0.4946, 0.5000, 0.5009,
         0.5030, 0.5077, 0.4997, 0.4952, 0.4879, 0.4902, 0.5083, 0.4952, 0.5083,
         0.4952, 0.5132],
        [0.4977, 0.5057, 0.4983, 0.5047, 0.5060, 0.4879, 0.4969, 0.5007, 0.5029,
         0.5019, 0.5100, 0.4996, 0.4959, 0.4873, 0.4907, 0.5082, 0.4951, 0.5079,
         0.4965, 0.5128],
        [0.4977, 0.5048, 0.4984, 0.5050, 0.5065, 0.4881, 0.4952, 0.5011, 0.5029,
         0.5018, 0.5098, 0.4997, 0.4962, 0.4889, 0.4919, 0.5078, 0.4932, 0.5064,
         0.4933, 0.5126],
        [0.4980, 0.5047, 0.4975, 0.5048, 0.5081, 0.4879, 0.4961, 0.5002, 0.5017,
         0.5008, 0.5067, 0.5012, 0.4974, 0.4887, 0.4923, 0.5083, 0.4929, 0.5084,
         0.4941, 0.5142],
        [0.4984, 0.5057, 0.4995, 0.5048, 0.5074, 0.4881, 0.4963, 0.5016, 0.5018,
         0.5040, 0.5076, 0.5003, 0.4942, 0.4908, 0.4928, 0.5095, 0.4940, 0.5048,
         0.4951, 0.5155]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4972, 0.5067, 0.4989, 0.5066, 0.5067, 0.4887, 0.4951, 0.5018, 0.4999,
        0.5025, 0.5101, 0.5005, 0.4961, 0.4887, 0.4923, 0.5076, 0.4924, 0.5098,
        0.4963, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5075, 0.4990, 0.5064, 0.5066, 0.4899, 0.4958, 0.5028, 0.5022,
        0.5030, 0.5084, 0.4990, 0.4961, 0.4882, 0.4902, 0.5077, 0.4917, 0.5089,
        0.4965, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5076, 0.4983, 0.5068, 0.5061, 0.4889, 0.4962, 0.5006, 0.5039,
        0.5016, 0.5070, 0.4987, 0.4962, 0.4882, 0.4899, 0.5095, 0.4933, 0.5092,
        0.4959, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5062, 0.4987, 0.5068, 0.5061, 0.4896, 0.4947, 0.5002, 0.5023,
        0.5037, 0.5068, 0.4985, 0.4969, 0.4880, 0.4897, 0.5077, 0.4922, 0.5102,
        0.4937, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5077, 0.4983, 0.5052, 0.5053, 0.4888, 0.4954, 0.4988, 0.5019,
        0.5028, 0.5088, 0.4991, 0.4964, 0.4882, 0.4916, 0.5061, 0.4945, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4973, 0.5049, 0.4977, 0.5060, 0.5051, 0.4883, 0.4946, 0.5000, 0.5009,
        0.5030, 0.5077, 0.4997, 0.4952, 0.4879, 0.4902, 0.5083, 0.4952, 0.5083,
        0.4952, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5057, 0.4983, 0.5047, 0.5060, 0.4879, 0.4969, 0.5007, 0.5029,
        0.5019, 0.5100, 0.4996, 0.4959, 0.4873, 0.4907, 0.5082, 0.4951, 0.5079,
        0.4965, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4984, 0.5050, 0.5065, 0.4881, 0.4952, 0.5011, 0.5029,
        0.5018, 0.5098, 0.4997, 0.4962, 0.4889, 0.4919, 0.5078, 0.4932, 0.5064,
        0.4933, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4975, 0.5048, 0.5081, 0.4879, 0.4961, 0.5002, 0.5017,
        0.5008, 0.5067, 0.5012, 0.4974, 0.4887, 0.4923, 0.5083, 0.4929, 0.5084,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5057, 0.4995, 0.5048, 0.5074, 0.4881, 0.4963, 0.5016, 0.5018,
        0.5040, 0.5076, 0.5003, 0.4942, 0.4908, 0.4928, 0.5095, 0.4940, 0.5048,
        0.4951, 0.5155], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5103, 0.4893, 0.5119, 0.4864, 0.4853, 0.4942, 0.4896, 0.4941, 0.4987,
         0.5074, 0.5028, 0.5003, 0.5026, 0.4919, 0.5046, 0.4912, 0.4858, 0.4984,
         0.5076, 0.4946],
        [0.5089, 0.4897, 0.5108, 0.4850, 0.4856, 0.4929, 0.4885, 0.4953, 0.4982,
         0.5052, 0.5038, 0.5000, 0.5034, 0.4917, 0.5031, 0.4889, 0.4862, 0.4982,
         0.5070, 0.4933],
        [0.5103, 0.4904, 0.5107, 0.4868, 0.4853, 0.4944, 0.4889, 0.4964, 0.5010,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4874, 0.4981,
         0.5069, 0.4925],
        [0.5080, 0.4895, 0.5116, 0.4860, 0.4871, 0.4934, 0.4900, 0.4951, 0.4982,
         0.5086, 0.5039, 0.5002, 0.5050, 0.4916, 0.5042, 0.4905, 0.4862, 0.4984,
         0.5057, 0.4923],
        [0.5095, 0.4912, 0.5130, 0.4860, 0.4872, 0.4929, 0.4910, 0.4923, 0.4955,
         0.5067, 0.5044, 0.5000, 0.5030, 0.4914, 0.5040, 0.4893, 0.4852, 0.4961,
         0.5053, 0.4936],
        [0.5110, 0.4916, 0.5102, 0.4866, 0.4861, 0.4946, 0.4908, 0.4939, 0.4970,
         0.5068, 0.5059, 0.5004, 0.5039, 0.4898, 0.5041, 0.4897, 0.4850, 0.4970,
         0.5060, 0.4930],
        [0.5097, 0.4914, 0.5125, 0.4880, 0.4859, 0.4938, 0.4889, 0.4942, 0.4976,
         0.5072, 0.5063, 0.4996, 0.5049, 0.4904, 0.5037, 0.4893, 0.4846, 0.4984,
         0.5071, 0.4933],
        [0.5102, 0.4914, 0.5114, 0.4852, 0.4869, 0.4926, 0.4892, 0.4947, 0.4998,
         0.5071, 0.5068, 0.5008, 0.5035, 0.4893, 0.5049, 0.4914, 0.4854, 0.4957,
         0.5075, 0.4933],
        [0.5112, 0.4927, 0.5110, 0.4851, 0.4887, 0.4929, 0.4895, 0.4941, 0.4981,
         0.5073, 0.5051, 0.4974, 0.5041, 0.4896, 0.5039, 0.4909, 0.4864, 0.4965,
         0.5082, 0.4937],
        [0.5114, 0.4922, 0.5119, 0.4870, 0.4883, 0.4928, 0.4894, 0.4933, 0.4998,
         0.5052, 0.5053, 0.4984, 0.5036, 0.4888, 0.5005, 0.4932, 0.4837, 0.4973,
         0.5058, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5103, 0.4893, 0.5119, 0.4864, 0.4853, 0.4942, 0.4896, 0.4941, 0.4987,
        0.5074, 0.5028, 0.5003, 0.5026, 0.4919, 0.5046, 0.4912, 0.4858, 0.4984,
        0.5076, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4897, 0.5108, 0.4850, 0.4856, 0.4929, 0.4885, 0.4953, 0.4982,
        0.5052, 0.5038, 0.5000, 0.5034, 0.4917, 0.5031, 0.4889, 0.4862, 0.4982,
        0.5070, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5103, 0.4904, 0.5107, 0.4868, 0.4853, 0.4944, 0.4889, 0.4964, 0.5010,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4909, 0.4874, 0.4981,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5080, 0.4895, 0.5116, 0.4860, 0.4871, 0.4934, 0.4900, 0.4951, 0.4982,
        0.5086, 0.5039, 0.5002, 0.5050, 0.4916, 0.5042, 0.4905, 0.4862, 0.4984,
        0.5057, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4860, 0.4872, 0.4929, 0.4910, 0.4923, 0.4955,
        0.5067, 0.5044, 0.5000, 0.5030, 0.4914, 0.5040, 0.4893, 0.4852, 0.4961,
        0.5053, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4916, 0.5102, 0.4866, 0.4861, 0.4946, 0.4908, 0.4939, 0.4970,
        0.5068, 0.5059, 0.5004, 0.5039, 0.4898, 0.5041, 0.4897, 0.4850, 0.4970,
        0.5060, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5097, 0.4914, 0.5125, 0.4880, 0.4859, 0.4938, 0.4889, 0.4942, 0.4976,
        0.5072, 0.5063, 0.4996, 0.5049, 0.4904, 0.5037, 0.4893, 0.4846, 0.4984,
        0.5071, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5102, 0.4914, 0.5114, 0.4852, 0.4869, 0.4926, 0.4892, 0.4947, 0.4998,
        0.5071, 0.5068, 0.5008, 0.5035, 0.4893, 0.5049, 0.4914, 0.4854, 0.4957,
        0.5075, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4927, 0.5110, 0.4851, 0.4887, 0.4929, 0.4895, 0.4941, 0.4981,
        0.5073, 0.5051, 0.4974, 0.5041, 0.4896, 0.5039, 0.4909, 0.4864, 0.4965,
        0.5082, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4922, 0.5119, 0.4870, 0.4883, 0.4928, 0.4894, 0.4933, 0.4998,
        0.5052, 0.5053, 0.4984, 0.5036, 0.4888, 0.5005, 0.4932, 0.4837, 0.4973,
        0.5058, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5125, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4953, 0.4967,
         0.5027, 0.4854, 0.4977, 0.4838, 0.5021, 0.5023, 0.4982, 0.4991, 0.5144,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4957, 0.5045, 0.5036, 0.4931, 0.5103, 0.4965, 0.4959,
         0.5025, 0.4849, 0.4967, 0.4855, 0.5015, 0.5001, 0.4978, 0.4981, 0.5149,
         0.4945, 0.4969],
        [0.5131, 0.4971, 0.4976, 0.5026, 0.5046, 0.4921, 0.5104, 0.4978, 0.4961,
         0.5027, 0.4859, 0.4987, 0.4845, 0.5017, 0.4998, 0.4976, 0.4992, 0.5165,
         0.4946, 0.4967],
        [0.5126, 0.4976, 0.4971, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
         0.5027, 0.4841, 0.4967, 0.4845, 0.5022, 0.5008, 0.4975, 0.5011, 0.5145,
         0.4954, 0.4970],
        [0.5128, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5089, 0.4953, 0.4966,
         0.5031, 0.4868, 0.4979, 0.4826, 0.5028, 0.5003, 0.4994, 0.5001, 0.5152,
         0.4956, 0.4971],
        [0.5137, 0.4974, 0.4963, 0.5031, 0.5049, 0.4927, 0.5079, 0.4967, 0.4955,
         0.5025, 0.4856, 0.4962, 0.4851, 0.5033, 0.5017, 0.5004, 0.4995, 0.5148,
         0.4953, 0.4983],
        [0.5115, 0.4965, 0.4980, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4960,
         0.5026, 0.4876, 0.4971, 0.4850, 0.5028, 0.5008, 0.4990, 0.5001, 0.5140,
         0.4955, 0.4997],
        [0.5120, 0.4973, 0.4952, 0.5010, 0.5040, 0.4921, 0.5082, 0.4944, 0.4969,
         0.5021, 0.4859, 0.4983, 0.4864, 0.5013, 0.5009, 0.4987, 0.4999, 0.5153,
         0.4959, 0.4989],
        [0.5128, 0.4981, 0.4950, 0.5011, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
         0.5018, 0.4884, 0.4951, 0.4853, 0.5015, 0.5011, 0.4973, 0.5007, 0.5143,
         0.4958, 0.4996],
        [0.5127, 0.4985, 0.4938, 0.5026, 0.5050, 0.4936, 0.5078, 0.4988, 0.4964,
         0.5032, 0.4885, 0.4978, 0.4858, 0.5001, 0.5011, 0.4982, 0.4996, 0.5154,
         0.4972, 0.5006]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5125, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4953, 0.4967,
        0.5027, 0.4854, 0.4977, 0.4838, 0.5021, 0.5023, 0.4982, 0.4991, 0.5144,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4957, 0.5045, 0.5036, 0.4931, 0.5103, 0.4965, 0.4959,
        0.5025, 0.4849, 0.4967, 0.4855, 0.5015, 0.5001, 0.4978, 0.4981, 0.5149,
        0.4945, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4971, 0.4976, 0.5026, 0.5046, 0.4921, 0.5104, 0.4978, 0.4961,
        0.5027, 0.4859, 0.4987, 0.4845, 0.5017, 0.4998, 0.4976, 0.4992, 0.5165,
        0.4946, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4976, 0.4971, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
        0.5027, 0.4841, 0.4967, 0.4845, 0.5022, 0.5008, 0.4975, 0.5011, 0.5145,
        0.4954, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5089, 0.4953, 0.4966,
        0.5031, 0.4868, 0.4979, 0.4826, 0.5028, 0.5003, 0.4994, 0.5001, 0.5152,
        0.4956, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4974, 0.4963, 0.5031, 0.5049, 0.4927, 0.5079, 0.4967, 0.4955,
        0.5025, 0.4856, 0.4962, 0.4851, 0.5033, 0.5017, 0.5004, 0.4995, 0.5148,
        0.4953, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4965, 0.4980, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4960,
        0.5026, 0.4876, 0.4971, 0.4850, 0.5028, 0.5008, 0.4990, 0.5001, 0.5140,
        0.4955, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4973, 0.4952, 0.5010, 0.5040, 0.4921, 0.5082, 0.4944, 0.4969,
        0.5021, 0.4859, 0.4983, 0.4864, 0.5013, 0.5009, 0.4987, 0.4999, 0.5153,
        0.4959, 0.4989], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4950, 0.5011, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
        0.5018, 0.4884, 0.4951, 0.4853, 0.5015, 0.5011, 0.4973, 0.5007, 0.5143,
        0.4958, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4938, 0.5026, 0.5050, 0.4936, 0.5078, 0.4988, 0.4964,
        0.5032, 0.4885, 0.4978, 0.4858, 0.5001, 0.5011, 0.4982, 0.4996, 0.5154,
        0.4972, 0.5006], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [1, 3, 9, 4, 5, 8, 7, 6, 0, 2]
replay_buffer._size: [9000 9000 9000 9000 9000 9000 9000 9000 9000 9000]
snapshot at best
2023-08-12 10:41:12,254 MainThread INFO: EPOCH:58
2023-08-12 10:41:12,267 MainThread INFO: Time Consumed:1.1729414463043213s
2023-08-12 10:41:12,267 MainThread INFO: Total Frames:88500s
 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 59/80 [01:05<00:29,  1.41s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1557.49593
Train_Epoch_Reward                    4276.06283
Running_Training_Average_Rewards      1332.99776
Explore_Time                          0.00405
Train___Time                          0.36528
Eval____Time                          0.00289
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -46.30321
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.69300
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.97492
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.26702
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.26292
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.81685
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.87449
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 15821.85560
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.78452
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.91937
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.88136      0.86675    11.09191    8.54384
alpha_0                               0.91570      0.00039    0.91625     0.91515
alpha_1                               0.91570      0.00039    0.91625     0.91515
alpha_2                               0.91572      0.00039    0.91627     0.91517
alpha_3                               0.91570      0.00039    0.91625     0.91515
alpha_4                               0.91570      0.00039    0.91626     0.91515
alpha_5                               0.91569      0.00039    0.91624     0.91514
alpha_6                               0.91571      0.00039    0.91626     0.91516
alpha_7                               0.91569      0.00039    0.91624     0.91514
alpha_8                               0.91569      0.00039    0.91624     0.91514
alpha_9                               0.91571      0.00039    0.91626     0.91516
Alpha_loss                            -0.59078     0.00214    -0.58797    -0.59383
Training/policy_loss                  -2.82970     0.00618    -2.82242    -2.84043
Training/qf1_loss                     2117.75938   420.09660  2635.78784  1441.77979
Training/qf2_loss                     2113.49387   419.71334  2631.11182  1438.14844
Training/pf_norm                      0.12028      0.01906    0.14101     0.09330
Training/qf1_norm                     32.39892     2.30679    35.70242    28.79279
Training/qf2_norm                     45.71811     3.53243    50.37429    40.01163
log_std/mean                          -0.13306     0.00007    -0.13297    -0.13316
log_std/std                           0.00869      0.00006    0.00876     0.00861
log_std/max                           -0.11564     0.00028    -0.11539    -0.11619
log_std/min                           -0.15446     0.00053    -0.15387    -0.15538
log_probs/mean                        -2.73142     0.00859    -2.72017    -2.74556
log_probs/std                         0.23421      0.01808    0.25736     0.21894
log_probs/max                         -2.11540     0.01977    -2.08914    -2.14554
log_probs/min                         -5.03692     0.64934    -4.24834    -5.74072
mean/mean                             0.00226      0.00003    0.00229     0.00221
mean/std                              0.01395      0.00013    0.01414     0.01376
mean/max                              0.02228      0.00043    0.02281     0.02164
mean/min                              -0.02210     0.00009    -0.02200    -0.02226
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [2, 1, 8, 9, 5, 6, 7, 4, 0, 3]
replay_buffer._size: [9150 9150 9150 9150 9150 9150 9150 9150 9150 9150]
snapshot at best
2023-08-12 10:41:13,773 MainThread INFO: EPOCH:59
2023-08-12 10:41:13,773 MainThread INFO: Time Consumed:1.264209508895874s
2023-08-12 10:41:13,773 MainThread INFO: Total Frames:90000s
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 60/80 [01:07<00:28,  1.43s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1572.66107
Train_Epoch_Reward                    26215.05775
Running_Training_Average_Rewards      1375.07203
Explore_Time                          0.00388
Train___Time                          0.62075
Eval____Time                          0.00257
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.33742
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.74316
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.08250
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.29194
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.41768
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.94305
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.99833
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 15980.10981
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.84168
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.84334
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.47706      0.48742    10.07762    8.79661
alpha_0                               0.91432      0.00039    0.91487     0.91377
alpha_1                               0.91432      0.00039    0.91487     0.91378
alpha_2                               0.91434      0.00039    0.91489     0.91379
alpha_3                               0.91432      0.00039    0.91487     0.91377
alpha_4                               0.91433      0.00039    0.91488     0.91378
alpha_5                               0.91432      0.00039    0.91487     0.91377
alpha_6                               0.91433      0.00039    0.91488     0.91379
alpha_7                               0.91431      0.00039    0.91486     0.91376
alpha_8                               0.91432      0.00039    0.91487     0.91377
alpha_9                               0.91434      0.00039    0.91489     0.91379
Alpha_loss                            -0.60139     0.00267    -0.59775    -0.60528
Training/policy_loss                  -2.84253     0.00270    -2.83738    -2.84467
Training/qf1_loss                     1983.65059   303.49770  2355.43994  1629.55115
Training/qf2_loss                     1979.14370   303.36091  2350.79297  1624.92651
Training/pf_norm                      0.13467      0.02663    0.17828     0.10114
Training/qf1_norm                     31.59770     1.25352    33.16419    29.77832
Training/qf2_norm                     45.61797     1.90453    47.47751    42.12507
log_std/mean                          -0.13299     0.00004    -0.13293    -0.13305
log_std/std                           0.00860      0.00003    0.00865     0.00856
log_std/max                           -0.11590     0.00040    -0.11541    -0.11641
log_std/min                           -0.15328     0.00059    -0.15246    -0.15388
log_probs/mean                        -2.73694     0.00317    -2.73252    -2.74153
log_probs/std                         0.25205      0.00770    0.26665     0.24384
log_probs/max                         -2.12079     0.01884    -2.09692    -2.15212
log_probs/min                         -5.54142     0.71988    -4.66884    -6.72095
mean/mean                             0.00221      0.00010    0.00234     0.00211
mean/std                              0.01414      0.00007    0.01423     0.01404
mean/max                              0.02318      0.00026    0.02345     0.02272
mean/min                              -0.02172     0.00041    -0.02110    -0.02220
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4972, 0.5067, 0.4990, 0.5067, 0.5067, 0.4887, 0.4951, 0.5018, 0.4999,
         0.5025, 0.5101, 0.5004, 0.4960, 0.4887, 0.4922, 0.5076, 0.4924, 0.5099,
         0.4963, 0.5142],
        [0.4981, 0.5075, 0.4991, 0.5064, 0.5066, 0.4900, 0.4958, 0.5028, 0.5022,
         0.5030, 0.5084, 0.4990, 0.4961, 0.4882, 0.4902, 0.5076, 0.4917, 0.5090,
         0.4965, 0.5125],
        [0.4978, 0.5076, 0.4983, 0.5068, 0.5060, 0.4890, 0.4961, 0.5006, 0.5039,
         0.5016, 0.5070, 0.4987, 0.4962, 0.4882, 0.4899, 0.5094, 0.4932, 0.5092,
         0.4959, 0.5146],
        [0.4984, 0.5063, 0.4988, 0.5068, 0.5061, 0.4895, 0.4947, 0.5003, 0.5023,
         0.5036, 0.5069, 0.4985, 0.4969, 0.4880, 0.4897, 0.5077, 0.4923, 0.5102,
         0.4937, 0.5140],
        [0.4985, 0.5077, 0.4983, 0.5052, 0.5053, 0.4887, 0.4953, 0.4989, 0.5019,
         0.5028, 0.5088, 0.4990, 0.4965, 0.4882, 0.4915, 0.5061, 0.4946, 0.5098,
         0.4953, 0.5132],
        [0.4974, 0.5049, 0.4977, 0.5060, 0.5051, 0.4883, 0.4946, 0.5001, 0.5008,
         0.5030, 0.5077, 0.4997, 0.4952, 0.4879, 0.4901, 0.5083, 0.4952, 0.5083,
         0.4953, 0.5132],
        [0.4978, 0.5057, 0.4982, 0.5047, 0.5061, 0.4879, 0.4968, 0.5008, 0.5028,
         0.5020, 0.5100, 0.4997, 0.4959, 0.4874, 0.4906, 0.5081, 0.4950, 0.5080,
         0.4964, 0.5128],
        [0.4977, 0.5048, 0.4984, 0.5050, 0.5065, 0.4882, 0.4952, 0.5011, 0.5029,
         0.5018, 0.5098, 0.4997, 0.4962, 0.4889, 0.4919, 0.5078, 0.4932, 0.5064,
         0.4934, 0.5126],
        [0.4980, 0.5047, 0.4976, 0.5047, 0.5081, 0.4879, 0.4961, 0.5002, 0.5017,
         0.5009, 0.5067, 0.5012, 0.4973, 0.4887, 0.4923, 0.5083, 0.4928, 0.5084,
         0.4941, 0.5142],
        [0.4984, 0.5056, 0.4995, 0.5048, 0.5074, 0.4881, 0.4962, 0.5016, 0.5018,
         0.5040, 0.5076, 0.5003, 0.4943, 0.4909, 0.4928, 0.5095, 0.4940, 0.5048,
         0.4951, 0.5155]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4972, 0.5067, 0.4990, 0.5067, 0.5067, 0.4887, 0.4951, 0.5018, 0.4999,
        0.5025, 0.5101, 0.5004, 0.4960, 0.4887, 0.4922, 0.5076, 0.4924, 0.5099,
        0.4963, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5075, 0.4991, 0.5064, 0.5066, 0.4900, 0.4958, 0.5028, 0.5022,
        0.5030, 0.5084, 0.4990, 0.4961, 0.4882, 0.4902, 0.5076, 0.4917, 0.5090,
        0.4965, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5076, 0.4983, 0.5068, 0.5060, 0.4890, 0.4961, 0.5006, 0.5039,
        0.5016, 0.5070, 0.4987, 0.4962, 0.4882, 0.4899, 0.5094, 0.4932, 0.5092,
        0.4959, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5063, 0.4988, 0.5068, 0.5061, 0.4895, 0.4947, 0.5003, 0.5023,
        0.5036, 0.5069, 0.4985, 0.4969, 0.4880, 0.4897, 0.5077, 0.4923, 0.5102,
        0.4937, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4985, 0.5077, 0.4983, 0.5052, 0.5053, 0.4887, 0.4953, 0.4989, 0.5019,
        0.5028, 0.5088, 0.4990, 0.4965, 0.4882, 0.4915, 0.5061, 0.4946, 0.5098,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4974, 0.5049, 0.4977, 0.5060, 0.5051, 0.4883, 0.4946, 0.5001, 0.5008,
        0.5030, 0.5077, 0.4997, 0.4952, 0.4879, 0.4901, 0.5083, 0.4952, 0.5083,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5057, 0.4982, 0.5047, 0.5061, 0.4879, 0.4968, 0.5008, 0.5028,
        0.5020, 0.5100, 0.4997, 0.4959, 0.4874, 0.4906, 0.5081, 0.4950, 0.5080,
        0.4964, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4984, 0.5050, 0.5065, 0.4882, 0.4952, 0.5011, 0.5029,
        0.5018, 0.5098, 0.4997, 0.4962, 0.4889, 0.4919, 0.5078, 0.4932, 0.5064,
        0.4934, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4976, 0.5047, 0.5081, 0.4879, 0.4961, 0.5002, 0.5017,
        0.5009, 0.5067, 0.5012, 0.4973, 0.4887, 0.4923, 0.5083, 0.4928, 0.5084,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5056, 0.4995, 0.5048, 0.5074, 0.4881, 0.4962, 0.5016, 0.5018,
        0.5040, 0.5076, 0.5003, 0.4943, 0.4909, 0.4928, 0.5095, 0.4940, 0.5048,
        0.4951, 0.5155], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4893, 0.5120, 0.4864, 0.4853, 0.4942, 0.4897, 0.4941, 0.4987,
         0.5074, 0.5028, 0.5003, 0.5026, 0.4919, 0.5046, 0.4912, 0.4858, 0.4984,
         0.5076, 0.4946],
        [0.5089, 0.4897, 0.5108, 0.4850, 0.4856, 0.4929, 0.4885, 0.4952, 0.4982,
         0.5052, 0.5038, 0.5000, 0.5034, 0.4918, 0.5031, 0.4888, 0.4862, 0.4982,
         0.5070, 0.4933],
        [0.5103, 0.4904, 0.5107, 0.4867, 0.4853, 0.4944, 0.4890, 0.4963, 0.5010,
         0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5041, 0.4908, 0.4874, 0.4981,
         0.5069, 0.4925],
        [0.5080, 0.4895, 0.5116, 0.4860, 0.4872, 0.4935, 0.4901, 0.4950, 0.4982,
         0.5086, 0.5040, 0.5002, 0.5049, 0.4915, 0.5042, 0.4906, 0.4863, 0.4983,
         0.5057, 0.4923],
        [0.5094, 0.4912, 0.5131, 0.4860, 0.4872, 0.4929, 0.4911, 0.4922, 0.4955,
         0.5067, 0.5046, 0.5000, 0.5030, 0.4914, 0.5041, 0.4894, 0.4852, 0.4960,
         0.5054, 0.4936],
        [0.5110, 0.4917, 0.5102, 0.4865, 0.4862, 0.4946, 0.4909, 0.4939, 0.4969,
         0.5068, 0.5061, 0.5004, 0.5039, 0.4898, 0.5041, 0.4899, 0.4851, 0.4970,
         0.5061, 0.4930],
        [0.5097, 0.4915, 0.5126, 0.4879, 0.4860, 0.4938, 0.4889, 0.4942, 0.4976,
         0.5072, 0.5064, 0.4997, 0.5049, 0.4904, 0.5038, 0.4894, 0.4845, 0.4984,
         0.5072, 0.4933],
        [0.5102, 0.4914, 0.5115, 0.4852, 0.4869, 0.4926, 0.4892, 0.4947, 0.4997,
         0.5072, 0.5068, 0.5007, 0.5035, 0.4893, 0.5049, 0.4914, 0.4854, 0.4957,
         0.5075, 0.4933],
        [0.5112, 0.4927, 0.5110, 0.4851, 0.4887, 0.4929, 0.4894, 0.4941, 0.4982,
         0.5073, 0.5050, 0.4974, 0.5042, 0.4896, 0.5039, 0.4909, 0.4863, 0.4965,
         0.5082, 0.4937],
        [0.5114, 0.4922, 0.5119, 0.4869, 0.4883, 0.4928, 0.4894, 0.4933, 0.4998,
         0.5052, 0.5053, 0.4984, 0.5037, 0.4888, 0.5005, 0.4932, 0.4837, 0.4973,
         0.5058, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4893, 0.5120, 0.4864, 0.4853, 0.4942, 0.4897, 0.4941, 0.4987,
        0.5074, 0.5028, 0.5003, 0.5026, 0.4919, 0.5046, 0.4912, 0.4858, 0.4984,
        0.5076, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4897, 0.5108, 0.4850, 0.4856, 0.4929, 0.4885, 0.4952, 0.4982,
        0.5052, 0.5038, 0.5000, 0.5034, 0.4918, 0.5031, 0.4888, 0.4862, 0.4982,
        0.5070, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5103, 0.4904, 0.5107, 0.4867, 0.4853, 0.4944, 0.4890, 0.4963, 0.5010,
        0.5069, 0.5047, 0.5003, 0.5060, 0.4920, 0.5041, 0.4908, 0.4874, 0.4981,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5080, 0.4895, 0.5116, 0.4860, 0.4872, 0.4935, 0.4901, 0.4950, 0.4982,
        0.5086, 0.5040, 0.5002, 0.5049, 0.4915, 0.5042, 0.4906, 0.4863, 0.4983,
        0.5057, 0.4923], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4912, 0.5131, 0.4860, 0.4872, 0.4929, 0.4911, 0.4922, 0.4955,
        0.5067, 0.5046, 0.5000, 0.5030, 0.4914, 0.5041, 0.4894, 0.4852, 0.4960,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4917, 0.5102, 0.4865, 0.4862, 0.4946, 0.4909, 0.4939, 0.4969,
        0.5068, 0.5061, 0.5004, 0.5039, 0.4898, 0.5041, 0.4899, 0.4851, 0.4970,
        0.5061, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5097, 0.4915, 0.5126, 0.4879, 0.4860, 0.4938, 0.4889, 0.4942, 0.4976,
        0.5072, 0.5064, 0.4997, 0.5049, 0.4904, 0.5038, 0.4894, 0.4845, 0.4984,
        0.5072, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5102, 0.4914, 0.5115, 0.4852, 0.4869, 0.4926, 0.4892, 0.4947, 0.4997,
        0.5072, 0.5068, 0.5007, 0.5035, 0.4893, 0.5049, 0.4914, 0.4854, 0.4957,
        0.5075, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4927, 0.5110, 0.4851, 0.4887, 0.4929, 0.4894, 0.4941, 0.4982,
        0.5073, 0.5050, 0.4974, 0.5042, 0.4896, 0.5039, 0.4909, 0.4863, 0.4965,
        0.5082, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4922, 0.5119, 0.4869, 0.4883, 0.4928, 0.4894, 0.4933, 0.4998,
        0.5052, 0.5053, 0.4984, 0.5037, 0.4888, 0.5005, 0.4932, 0.4837, 0.4973,
        0.5058, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
         0.5027, 0.4855, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4957, 0.5044, 0.5036, 0.4932, 0.5103, 0.4966, 0.4959,
         0.5025, 0.4848, 0.4967, 0.4855, 0.5015, 0.5001, 0.4978, 0.4981, 0.5149,
         0.4945, 0.4969],
        [0.5131, 0.4971, 0.4977, 0.5026, 0.5046, 0.4920, 0.5105, 0.4977, 0.4960,
         0.5027, 0.4858, 0.4987, 0.4845, 0.5018, 0.4999, 0.4976, 0.4992, 0.5165,
         0.4946, 0.4967],
        [0.5126, 0.4976, 0.4971, 0.5034, 0.5042, 0.4914, 0.5109, 0.4966, 0.4962,
         0.5027, 0.4841, 0.4967, 0.4844, 0.5023, 0.5008, 0.4976, 0.5011, 0.5145,
         0.4954, 0.4971],
        [0.5128, 0.4982, 0.4965, 0.5039, 0.5033, 0.4933, 0.5088, 0.4953, 0.4965,
         0.5030, 0.4868, 0.4978, 0.4825, 0.5029, 0.5003, 0.4994, 0.5001, 0.5152,
         0.4956, 0.4971],
        [0.5137, 0.4974, 0.4963, 0.5032, 0.5049, 0.4926, 0.5078, 0.4967, 0.4955,
         0.5025, 0.4856, 0.4962, 0.4851, 0.5034, 0.5016, 0.5005, 0.4996, 0.5147,
         0.4953, 0.4982],
        [0.5116, 0.4966, 0.4981, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4959,
         0.5025, 0.4876, 0.4971, 0.4850, 0.5028, 0.5008, 0.4990, 0.4999, 0.5140,
         0.4954, 0.4997],
        [0.5120, 0.4973, 0.4952, 0.5010, 0.5040, 0.4922, 0.5081, 0.4945, 0.4969,
         0.5021, 0.4860, 0.4983, 0.4865, 0.5013, 0.5008, 0.4988, 0.4999, 0.5153,
         0.4959, 0.4989],
        [0.5128, 0.4981, 0.4950, 0.5010, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
         0.5018, 0.4884, 0.4951, 0.4853, 0.5015, 0.5011, 0.4973, 0.5007, 0.5143,
         0.4958, 0.4996],
        [0.5127, 0.4985, 0.4938, 0.5026, 0.5051, 0.4936, 0.5078, 0.4988, 0.4964,
         0.5032, 0.4885, 0.4978, 0.4858, 0.5001, 0.5012, 0.4982, 0.4996, 0.5154,
         0.4972, 0.5006]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
        0.5027, 0.4855, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4957, 0.5044, 0.5036, 0.4932, 0.5103, 0.4966, 0.4959,
        0.5025, 0.4848, 0.4967, 0.4855, 0.5015, 0.5001, 0.4978, 0.4981, 0.5149,
        0.4945, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4971, 0.4977, 0.5026, 0.5046, 0.4920, 0.5105, 0.4977, 0.4960,
        0.5027, 0.4858, 0.4987, 0.4845, 0.5018, 0.4999, 0.4976, 0.4992, 0.5165,
        0.4946, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4976, 0.4971, 0.5034, 0.5042, 0.4914, 0.5109, 0.4966, 0.4962,
        0.5027, 0.4841, 0.4967, 0.4844, 0.5023, 0.5008, 0.4976, 0.5011, 0.5145,
        0.4954, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4982, 0.4965, 0.5039, 0.5033, 0.4933, 0.5088, 0.4953, 0.4965,
        0.5030, 0.4868, 0.4978, 0.4825, 0.5029, 0.5003, 0.4994, 0.5001, 0.5152,
        0.4956, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4974, 0.4963, 0.5032, 0.5049, 0.4926, 0.5078, 0.4967, 0.4955,
        0.5025, 0.4856, 0.4962, 0.4851, 0.5034, 0.5016, 0.5005, 0.4996, 0.5147,
        0.4953, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4966, 0.4981, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4959,
        0.5025, 0.4876, 0.4971, 0.4850, 0.5028, 0.5008, 0.4990, 0.4999, 0.5140,
        0.4954, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4973, 0.4952, 0.5010, 0.5040, 0.4922, 0.5081, 0.4945, 0.4969,
        0.5021, 0.4860, 0.4983, 0.4865, 0.5013, 0.5008, 0.4988, 0.4999, 0.5153,
        0.4959, 0.4989], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4950, 0.5010, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
        0.5018, 0.4884, 0.4951, 0.4853, 0.5015, 0.5011, 0.4973, 0.5007, 0.5143,
        0.4958, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4938, 0.5026, 0.5051, 0.4936, 0.5078, 0.4988, 0.4964,
        0.5032, 0.4885, 0.4978, 0.4858, 0.5001, 0.5012, 0.4982, 0.4996, 0.5154,
        0.4972, 0.5006], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [8, 2, 3, 9, 5, 4, 6, 0, 1, 7]
replay_buffer._size: [9300 9300 9300 9300 9300 9300 9300 9300 9300 9300]
snapshot at best
2023-08-12 10:41:15,383 MainThread INFO: EPOCH:60
2023-08-12 10:41:15,383 MainThread INFO: Time Consumed:1.0242998600006104s
2023-08-12 10:41:15,383 MainThread INFO: Total Frames:91500s
 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 61/80 [01:08<00:28,  1.49s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1594.44970
Train_Epoch_Reward                    11843.83214
Running_Training_Average_Rewards      1411.16509
Explore_Time                          0.00411
Train___Time                          0.40401
Eval____Time                          0.00307
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.72068
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.68014
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.21827
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.20401
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.50444
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -22.01276
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -22.06686
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16198.43257
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.77943
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.74896
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.79921      0.67899    10.97283    9.06417
alpha_0                               0.91295      0.00039    0.91350     0.91240
alpha_1                               0.91295      0.00039    0.91350     0.91240
alpha_2                               0.91297      0.00039    0.91352     0.91242
alpha_3                               0.91295      0.00039    0.91350     0.91240
alpha_4                               0.91296      0.00039    0.91351     0.91241
alpha_5                               0.91295      0.00039    0.91349     0.91240
alpha_6                               0.91296      0.00039    0.91351     0.91241
alpha_7                               0.91294      0.00039    0.91349     0.91239
alpha_8                               0.91294      0.00039    0.91349     0.91239
alpha_9                               0.91297      0.00039    0.91352     0.91242
Alpha_loss                            -0.61088     0.00290    -0.60630    -0.61419
Training/policy_loss                  -2.84271     0.00651    -2.83367    -2.85253
Training/qf1_loss                     1955.34690   260.01619  2285.27783  1633.99084
Training/qf2_loss                     1950.33650   259.65071  2279.23828  1629.37500
Training/pf_norm                      0.12306      0.02228    0.15717     0.09661
Training/qf1_norm                     32.83472     1.86808    35.99170    30.83459
Training/qf2_norm                     48.40106     3.43945    54.91539    45.17580
log_std/mean                          -0.13327     0.00015    -0.13303    -0.13340
log_std/std                           0.00854      0.00002    0.00857     0.00852
log_std/max                           -0.11577     0.00026    -0.11547    -0.11616
log_std/min                           -0.15295     0.00034    -0.15269    -0.15362
log_probs/mean                        -2.73000     0.00620    -2.72200    -2.73834
log_probs/std                         0.22767      0.01185    0.24741     0.21555
log_probs/max                         -2.12049     0.02697    -2.07638    -2.14902
log_probs/min                         -5.10493     1.14432    -3.83333    -7.01410
mean/mean                             0.00216      0.00002    0.00218     0.00213
mean/std                              0.01385      0.00007    0.01395     0.01377
mean/max                              0.02427      0.00031    0.02458     0.02381
mean/min                              -0.01979     0.00056    -0.01906    -0.02058
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [7, 0, 6, 2, 8, 9, 3, 1, 5, 4]
replay_buffer._size: [9450 9450 9450 9450 9450 9450 9450 9450 9450 9450]
snapshot at best
2023-08-12 10:41:16,535 MainThread INFO: EPOCH:61
2023-08-12 10:41:16,536 MainThread INFO: Time Consumed:0.9996442794799805s
2023-08-12 10:41:16,536 MainThread INFO: Total Frames:93000s
 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 62/80 [01:10<00:24,  1.38s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1595.89595
Train_Epoch_Reward                    9565.54551
Running_Training_Average_Rewards      1587.48118
Explore_Time                          0.00351
Train___Time                          0.44590
Eval____Time                          0.00270
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -45.12426
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.50527
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.22804
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -22.06298
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.38977
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.91742
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.97269
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16204.52928
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.59450
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.77490
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.50906      1.22630    11.56441    8.23603
alpha_0                               0.91158      0.00039    0.91213     0.91103
alpha_1                               0.91158      0.00039    0.91213     0.91103
alpha_2                               0.91160      0.00039    0.91215     0.91105
alpha_3                               0.91158      0.00039    0.91213     0.91103
alpha_4                               0.91159      0.00039    0.91214     0.91104
alpha_5                               0.91158      0.00039    0.91212     0.91103
alpha_6                               0.91159      0.00039    0.91214     0.91105
alpha_7                               0.91157      0.00039    0.91212     0.91103
alpha_8                               0.91157      0.00039    0.91212     0.91102
alpha_9                               0.91160      0.00039    0.91215     0.91105
Alpha_loss                            -0.62141     0.00279    -0.61804    -0.62551
Training/policy_loss                  -2.85503     0.00524    -2.84658    -2.86077
Training/qf1_loss                     1879.00698   664.16215  3105.28882  1351.12671
Training/qf2_loss                     1874.15623   663.25630  3098.87549  1347.01196
Training/pf_norm                      0.09657      0.01626    0.11958     0.08105
Training/qf1_norm                     32.22390     3.50919    38.07619    28.68154
Training/qf2_norm                     47.01673     6.54793    58.01124    41.02334
log_std/mean                          -0.13346     0.00005    -0.13340    -0.13352
log_std/std                           0.00849      0.00004    0.00853     0.00842
log_std/max                           -0.11633     0.00027    -0.11592    -0.11675
log_std/min                           -0.15284     0.00051    -0.15183    -0.15313
log_probs/mean                        -2.73455     0.00590    -2.72425    -2.74196
log_probs/std                         0.24024      0.01293    0.25716     0.22300
log_probs/max                         -2.16557     0.02424    -2.14708    -2.21221
log_probs/min                         -4.97820     0.37615    -4.45211    -5.46849
mean/mean                             0.00197      0.00012    0.00212     0.00178
mean/std                              0.01367      0.00010    0.01378     0.01350
mean/max                              0.02493      0.00016    0.02509     0.02463
mean/min                              -0.01843     0.00032    -0.01801    -0.01888
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4971, 0.5068, 0.4989, 0.5067, 0.5066, 0.4888, 0.4951, 0.5018, 0.4999,
         0.5025, 0.5101, 0.5004, 0.4960, 0.4886, 0.4922, 0.5076, 0.4923, 0.5099,
         0.4963, 0.5141],
        [0.4981, 0.5075, 0.4991, 0.5064, 0.5066, 0.4900, 0.4957, 0.5029, 0.5022,
         0.5030, 0.5085, 0.4990, 0.4961, 0.4882, 0.4901, 0.5076, 0.4916, 0.5090,
         0.4966, 0.5125],
        [0.4978, 0.5076, 0.4983, 0.5068, 0.5060, 0.4890, 0.4961, 0.5006, 0.5039,
         0.5016, 0.5071, 0.4987, 0.4961, 0.4882, 0.4899, 0.5093, 0.4932, 0.5093,
         0.4960, 0.5146],
        [0.4984, 0.5063, 0.4988, 0.5068, 0.5060, 0.4896, 0.4947, 0.5003, 0.5023,
         0.5037, 0.5070, 0.4985, 0.4968, 0.4880, 0.4897, 0.5077, 0.4923, 0.5102,
         0.4938, 0.5140],
        [0.4984, 0.5078, 0.4983, 0.5052, 0.5053, 0.4887, 0.4953, 0.4988, 0.5019,
         0.5028, 0.5088, 0.4990, 0.4964, 0.4882, 0.4915, 0.5061, 0.4946, 0.5098,
         0.4953, 0.5133],
        [0.4973, 0.5049, 0.4977, 0.5060, 0.5051, 0.4883, 0.4946, 0.5001, 0.5009,
         0.5031, 0.5078, 0.4997, 0.4952, 0.4879, 0.4901, 0.5083, 0.4952, 0.5083,
         0.4953, 0.5132],
        [0.4977, 0.5057, 0.4983, 0.5048, 0.5061, 0.4879, 0.4968, 0.5008, 0.5028,
         0.5021, 0.5100, 0.4996, 0.4958, 0.4874, 0.4907, 0.5081, 0.4950, 0.5080,
         0.4964, 0.5128],
        [0.4977, 0.5048, 0.4984, 0.5050, 0.5065, 0.4882, 0.4952, 0.5011, 0.5029,
         0.5019, 0.5098, 0.4997, 0.4962, 0.4889, 0.4920, 0.5078, 0.4931, 0.5064,
         0.4934, 0.5126],
        [0.4980, 0.5047, 0.4976, 0.5047, 0.5080, 0.4880, 0.4960, 0.5002, 0.5017,
         0.5009, 0.5067, 0.5012, 0.4973, 0.4887, 0.4924, 0.5083, 0.4928, 0.5084,
         0.4941, 0.5142],
        [0.4984, 0.5056, 0.4995, 0.5048, 0.5073, 0.4881, 0.4963, 0.5016, 0.5018,
         0.5040, 0.5076, 0.5003, 0.4942, 0.4908, 0.4929, 0.5095, 0.4941, 0.5049,
         0.4951, 0.5155]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4971, 0.5068, 0.4989, 0.5067, 0.5066, 0.4888, 0.4951, 0.5018, 0.4999,
        0.5025, 0.5101, 0.5004, 0.4960, 0.4886, 0.4922, 0.5076, 0.4923, 0.5099,
        0.4963, 0.5141], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5075, 0.4991, 0.5064, 0.5066, 0.4900, 0.4957, 0.5029, 0.5022,
        0.5030, 0.5085, 0.4990, 0.4961, 0.4882, 0.4901, 0.5076, 0.4916, 0.5090,
        0.4966, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5076, 0.4983, 0.5068, 0.5060, 0.4890, 0.4961, 0.5006, 0.5039,
        0.5016, 0.5071, 0.4987, 0.4961, 0.4882, 0.4899, 0.5093, 0.4932, 0.5093,
        0.4960, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5063, 0.4988, 0.5068, 0.5060, 0.4896, 0.4947, 0.5003, 0.5023,
        0.5037, 0.5070, 0.4985, 0.4968, 0.4880, 0.4897, 0.5077, 0.4923, 0.5102,
        0.4938, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5078, 0.4983, 0.5052, 0.5053, 0.4887, 0.4953, 0.4988, 0.5019,
        0.5028, 0.5088, 0.4990, 0.4964, 0.4882, 0.4915, 0.5061, 0.4946, 0.5098,
        0.4953, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4973, 0.5049, 0.4977, 0.5060, 0.5051, 0.4883, 0.4946, 0.5001, 0.5009,
        0.5031, 0.5078, 0.4997, 0.4952, 0.4879, 0.4901, 0.5083, 0.4952, 0.5083,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5057, 0.4983, 0.5048, 0.5061, 0.4879, 0.4968, 0.5008, 0.5028,
        0.5021, 0.5100, 0.4996, 0.4958, 0.4874, 0.4907, 0.5081, 0.4950, 0.5080,
        0.4964, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4984, 0.5050, 0.5065, 0.4882, 0.4952, 0.5011, 0.5029,
        0.5019, 0.5098, 0.4997, 0.4962, 0.4889, 0.4920, 0.5078, 0.4931, 0.5064,
        0.4934, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4976, 0.5047, 0.5080, 0.4880, 0.4960, 0.5002, 0.5017,
        0.5009, 0.5067, 0.5012, 0.4973, 0.4887, 0.4924, 0.5083, 0.4928, 0.5084,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5056, 0.4995, 0.5048, 0.5073, 0.4881, 0.4963, 0.5016, 0.5018,
        0.5040, 0.5076, 0.5003, 0.4942, 0.4908, 0.4929, 0.5095, 0.4941, 0.5049,
        0.4951, 0.5155], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4893, 0.5120, 0.4864, 0.4853, 0.4942, 0.4897, 0.4941, 0.4987,
         0.5074, 0.5028, 0.5004, 0.5026, 0.4919, 0.5046, 0.4912, 0.4858, 0.4983,
         0.5076, 0.4946],
        [0.5089, 0.4896, 0.5108, 0.4850, 0.4856, 0.4929, 0.4885, 0.4953, 0.4982,
         0.5052, 0.5038, 0.5000, 0.5034, 0.4918, 0.5031, 0.4888, 0.4863, 0.4982,
         0.5070, 0.4933],
        [0.5102, 0.4904, 0.5107, 0.4867, 0.4853, 0.4944, 0.4890, 0.4963, 0.5009,
         0.5070, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4908, 0.4873, 0.4981,
         0.5069, 0.4925],
        [0.5080, 0.4895, 0.5116, 0.4860, 0.4872, 0.4934, 0.4900, 0.4950, 0.4982,
         0.5086, 0.5039, 0.5002, 0.5050, 0.4916, 0.5042, 0.4905, 0.4862, 0.4983,
         0.5057, 0.4924],
        [0.5094, 0.4912, 0.5130, 0.4860, 0.4872, 0.4929, 0.4911, 0.4922, 0.4956,
         0.5068, 0.5046, 0.5000, 0.5030, 0.4914, 0.5041, 0.4893, 0.4852, 0.4961,
         0.5054, 0.4936],
        [0.5110, 0.4916, 0.5102, 0.4866, 0.4862, 0.4946, 0.4909, 0.4939, 0.4969,
         0.5068, 0.5060, 0.5005, 0.5039, 0.4898, 0.5042, 0.4898, 0.4851, 0.4970,
         0.5060, 0.4929],
        [0.5096, 0.4915, 0.5126, 0.4879, 0.4860, 0.4938, 0.4889, 0.4942, 0.4976,
         0.5072, 0.5064, 0.4996, 0.5049, 0.4904, 0.5038, 0.4893, 0.4845, 0.4984,
         0.5071, 0.4932],
        [0.5101, 0.4914, 0.5115, 0.4852, 0.4869, 0.4925, 0.4891, 0.4947, 0.4998,
         0.5072, 0.5067, 0.5008, 0.5035, 0.4894, 0.5049, 0.4913, 0.4853, 0.4957,
         0.5076, 0.4933],
        [0.5112, 0.4926, 0.5110, 0.4852, 0.4887, 0.4928, 0.4893, 0.4941, 0.4983,
         0.5073, 0.5049, 0.4974, 0.5042, 0.4896, 0.5038, 0.4908, 0.4862, 0.4965,
         0.5082, 0.4937],
        [0.5113, 0.4922, 0.5119, 0.4869, 0.4884, 0.4927, 0.4893, 0.4932, 0.4998,
         0.5053, 0.5053, 0.4985, 0.5037, 0.4889, 0.5005, 0.4931, 0.4836, 0.4973,
         0.5059, 0.4940]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4893, 0.5120, 0.4864, 0.4853, 0.4942, 0.4897, 0.4941, 0.4987,
        0.5074, 0.5028, 0.5004, 0.5026, 0.4919, 0.5046, 0.4912, 0.4858, 0.4983,
        0.5076, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4896, 0.5108, 0.4850, 0.4856, 0.4929, 0.4885, 0.4953, 0.4982,
        0.5052, 0.5038, 0.5000, 0.5034, 0.4918, 0.5031, 0.4888, 0.4863, 0.4982,
        0.5070, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5102, 0.4904, 0.5107, 0.4867, 0.4853, 0.4944, 0.4890, 0.4963, 0.5009,
        0.5070, 0.5047, 0.5003, 0.5060, 0.4920, 0.5040, 0.4908, 0.4873, 0.4981,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5080, 0.4895, 0.5116, 0.4860, 0.4872, 0.4934, 0.4900, 0.4950, 0.4982,
        0.5086, 0.5039, 0.5002, 0.5050, 0.4916, 0.5042, 0.4905, 0.4862, 0.4983,
        0.5057, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4912, 0.5130, 0.4860, 0.4872, 0.4929, 0.4911, 0.4922, 0.4956,
        0.5068, 0.5046, 0.5000, 0.5030, 0.4914, 0.5041, 0.4893, 0.4852, 0.4961,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4916, 0.5102, 0.4866, 0.4862, 0.4946, 0.4909, 0.4939, 0.4969,
        0.5068, 0.5060, 0.5005, 0.5039, 0.4898, 0.5042, 0.4898, 0.4851, 0.4970,
        0.5060, 0.4929], grad_fn=<UnbindBackward>), tensor([0.5096, 0.4915, 0.5126, 0.4879, 0.4860, 0.4938, 0.4889, 0.4942, 0.4976,
        0.5072, 0.5064, 0.4996, 0.5049, 0.4904, 0.5038, 0.4893, 0.4845, 0.4984,
        0.5071, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4914, 0.5115, 0.4852, 0.4869, 0.4925, 0.4891, 0.4947, 0.4998,
        0.5072, 0.5067, 0.5008, 0.5035, 0.4894, 0.5049, 0.4913, 0.4853, 0.4957,
        0.5076, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4926, 0.5110, 0.4852, 0.4887, 0.4928, 0.4893, 0.4941, 0.4983,
        0.5073, 0.5049, 0.4974, 0.5042, 0.4896, 0.5038, 0.4908, 0.4862, 0.4965,
        0.5082, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4922, 0.5119, 0.4869, 0.4884, 0.4927, 0.4893, 0.4932, 0.4998,
        0.5053, 0.5053, 0.4985, 0.5037, 0.4889, 0.5005, 0.4931, 0.4836, 0.4973,
        0.5059, 0.4940], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5125, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
         0.5027, 0.4854, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4957, 0.5044, 0.5036, 0.4932, 0.5103, 0.4966, 0.4959,
         0.5025, 0.4849, 0.4967, 0.4855, 0.5015, 0.5001, 0.4979, 0.4980, 0.5150,
         0.4945, 0.4969],
        [0.5131, 0.4971, 0.4977, 0.5026, 0.5045, 0.4921, 0.5104, 0.4978, 0.4960,
         0.5026, 0.4858, 0.4988, 0.4846, 0.5017, 0.4998, 0.4976, 0.4992, 0.5165,
         0.4946, 0.4966],
        [0.5126, 0.4976, 0.4971, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
         0.5027, 0.4841, 0.4968, 0.4844, 0.5022, 0.5008, 0.4976, 0.5011, 0.5145,
         0.4954, 0.4971],
        [0.5129, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5089, 0.4953, 0.4965,
         0.5030, 0.4868, 0.4978, 0.4825, 0.5028, 0.5003, 0.4994, 0.5001, 0.5152,
         0.4956, 0.4970],
        [0.5137, 0.4975, 0.4964, 0.5032, 0.5049, 0.4927, 0.5078, 0.4967, 0.4955,
         0.5025, 0.4856, 0.4962, 0.4851, 0.5034, 0.5016, 0.5005, 0.4996, 0.5148,
         0.4953, 0.4982],
        [0.5115, 0.4965, 0.4980, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4959,
         0.5025, 0.4876, 0.4971, 0.4851, 0.5027, 0.5008, 0.4990, 0.5000, 0.5140,
         0.4954, 0.4997],
        [0.5121, 0.4974, 0.4952, 0.5009, 0.5040, 0.4922, 0.5082, 0.4945, 0.4969,
         0.5021, 0.4860, 0.4984, 0.4865, 0.5012, 0.5009, 0.4988, 0.4999, 0.5153,
         0.4959, 0.4989],
        [0.5128, 0.4981, 0.4949, 0.5010, 0.5049, 0.4925, 0.5094, 0.4967, 0.4967,
         0.5019, 0.4883, 0.4952, 0.4853, 0.5014, 0.5011, 0.4972, 0.5007, 0.5142,
         0.4958, 0.4996],
        [0.5127, 0.4985, 0.4938, 0.5025, 0.5050, 0.4936, 0.5079, 0.4987, 0.4964,
         0.5032, 0.4884, 0.4978, 0.4858, 0.5000, 0.5011, 0.4982, 0.4996, 0.5154,
         0.4972, 0.5006]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5125, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
        0.5027, 0.4854, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4957, 0.5044, 0.5036, 0.4932, 0.5103, 0.4966, 0.4959,
        0.5025, 0.4849, 0.4967, 0.4855, 0.5015, 0.5001, 0.4979, 0.4980, 0.5150,
        0.4945, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4971, 0.4977, 0.5026, 0.5045, 0.4921, 0.5104, 0.4978, 0.4960,
        0.5026, 0.4858, 0.4988, 0.4846, 0.5017, 0.4998, 0.4976, 0.4992, 0.5165,
        0.4946, 0.4966], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4976, 0.4971, 0.5033, 0.5041, 0.4914, 0.5109, 0.4966, 0.4962,
        0.5027, 0.4841, 0.4968, 0.4844, 0.5022, 0.5008, 0.4976, 0.5011, 0.5145,
        0.4954, 0.4971], grad_fn=<UnbindBackward>), tensor([0.5129, 0.4982, 0.4964, 0.5039, 0.5033, 0.4934, 0.5089, 0.4953, 0.4965,
        0.5030, 0.4868, 0.4978, 0.4825, 0.5028, 0.5003, 0.4994, 0.5001, 0.5152,
        0.4956, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4975, 0.4964, 0.5032, 0.5049, 0.4927, 0.5078, 0.4967, 0.4955,
        0.5025, 0.4856, 0.4962, 0.4851, 0.5034, 0.5016, 0.5005, 0.4996, 0.5148,
        0.4953, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4965, 0.4980, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4959,
        0.5025, 0.4876, 0.4971, 0.4851, 0.5027, 0.5008, 0.4990, 0.5000, 0.5140,
        0.4954, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5121, 0.4974, 0.4952, 0.5009, 0.5040, 0.4922, 0.5082, 0.4945, 0.4969,
        0.5021, 0.4860, 0.4984, 0.4865, 0.5012, 0.5009, 0.4988, 0.4999, 0.5153,
        0.4959, 0.4989], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4949, 0.5010, 0.5049, 0.4925, 0.5094, 0.4967, 0.4967,
        0.5019, 0.4883, 0.4952, 0.4853, 0.5014, 0.5011, 0.4972, 0.5007, 0.5142,
        0.4958, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4938, 0.5025, 0.5050, 0.4936, 0.5079, 0.4987, 0.4964,
        0.5032, 0.4884, 0.4978, 0.4858, 0.5000, 0.5011, 0.4982, 0.4996, 0.5154,
        0.4972, 0.5006], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [6, 0, 4, 1, 8, 5, 9, 3, 7, 2]
replay_buffer._size: [9600 9600 9600 9600 9600 9600 9600 9600 9600 9600]
2023-08-12 10:41:17,716 MainThread INFO: EPOCH:62
2023-08-12 10:41:17,716 MainThread INFO: Time Consumed:0.6257987022399902s
2023-08-12 10:41:17,716 MainThread INFO: Total Frames:94500s
 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 63/80 [01:11<00:22,  1.32s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1595.35543
Train_Epoch_Reward                    16964.02118
Running_Training_Average_Rewards      1279.11329
Explore_Time                          0.00302
Train___Time                          0.61169
Eval____Time                          0.00807
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.12553
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.31390
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.24765
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.92362
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.29212
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.83606
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.89054
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16203.39140
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.39406
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.81358
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.10115      0.49860    9.65445     8.27126
alpha_0                               0.91021      0.00039    0.91076     0.90966
alpha_1                               0.91021      0.00039    0.91076     0.90966
alpha_2                               0.91023      0.00039    0.91078     0.90968
alpha_3                               0.91021      0.00039    0.91076     0.90967
alpha_4                               0.91022      0.00039    0.91077     0.90968
alpha_5                               0.91021      0.00039    0.91075     0.90966
alpha_6                               0.91023      0.00039    0.91077     0.90968
alpha_7                               0.91021      0.00039    0.91075     0.90966
alpha_8                               0.91020      0.00039    0.91075     0.90966
alpha_9                               0.91023      0.00039    0.91078     0.90968
Alpha_loss                            -0.63186     0.00284    -0.62848    -0.63580
Training/policy_loss                  -2.86607     0.00549    -2.85520    -2.86957
Training/qf1_loss                     1626.02285   184.49304  1886.17090  1421.14575
Training/qf2_loss                     1621.25850   184.27507  1881.06604  1416.48218
Training/pf_norm                      0.09841      0.02797    0.13891     0.06186
Training/qf1_norm                     31.56129     1.29668    33.13896    29.50841
Training/qf2_norm                     45.91167     2.01685    48.64623    43.42791
log_std/mean                          -0.13318     0.00010    -0.13302    -0.13329
log_std/std                           0.00836      0.00003    0.00842     0.00833
log_std/max                           -0.11688     0.00036    -0.11634    -0.11741
log_std/min                           -0.15239     0.00041    -0.15190    -0.15300
log_probs/mean                        -2.73814     0.00553    -2.72850    -2.74534
log_probs/std                         0.24905      0.01074    0.26263     0.23469
log_probs/max                         -2.12551     0.02919    -2.08731    -2.15997
log_probs/min                         -5.78220     0.84611    -4.68872    -6.91334
mean/mean                             0.00162      0.00010    0.00173     0.00145
mean/std                              0.01338      0.00003    0.01344     0.01334
mean/max                              0.02485      0.00009    0.02498     0.02474
mean/min                              -0.01753     0.00017    -0.01735    -0.01782
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [6, 8, 9, 4, 3, 5, 1, 2, 0, 7]
replay_buffer._size: [9750 9750 9750 9750 9750 9750 9750 9750 9750 9750]
2023-08-12 10:41:18,267 MainThread INFO: EPOCH:63
2023-08-12 10:41:18,267 MainThread INFO: Time Consumed:0.4248206615447998s
2023-08-12 10:41:18,267 MainThread INFO: Total Frames:96000s
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 64/80 [01:11<00:17,  1.11s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1583.18557
Train_Epoch_Reward                    23354.18953
Running_Training_Average_Rewards      1662.79187
Explore_Time                          0.00377
Train___Time                          0.41339
Eval____Time                          0.00328
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -43.03620
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.22803
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.14841
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.83422
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -21.07274
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.65737
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.71391
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16073.70907
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.29930
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.86315
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.63062      0.58238    10.58556    9.10307
alpha_0                               0.90884      0.00039    0.90939     0.90830
alpha_1                               0.90884      0.00039    0.90939     0.90830
alpha_2                               0.90886      0.00039    0.90941     0.90831
alpha_3                               0.90885      0.00039    0.90939     0.90830
alpha_4                               0.90886      0.00039    0.90940     0.90831
alpha_5                               0.90884      0.00039    0.90939     0.90830
alpha_6                               0.90886      0.00039    0.90941     0.90831
alpha_7                               0.90884      0.00039    0.90939     0.90830
alpha_8                               0.90883      0.00039    0.90938     0.90829
alpha_9                               0.90886      0.00039    0.90941     0.90831
Alpha_loss                            -0.64113     0.00304    -0.63783    -0.64587
Training/policy_loss                  -2.86467     0.00707    -2.85679    -2.87525
Training/qf1_loss                     1874.93213   287.94017  2293.93701  1555.41980
Training/qf2_loss                     1869.48640   287.67920  2288.08154  1550.28040
Training/pf_norm                      0.13664      0.02679    0.18697     0.11397
Training/qf1_norm                     33.17547     1.73963    35.92617    31.54689
Training/qf2_norm                     49.54792     2.54778    53.55452    46.93835
log_std/mean                          -0.13290     0.00006    -0.13282    -0.13299
log_std/std                           0.00831      0.00004    0.00836     0.00827
log_std/max                           -0.11682     0.00026    -0.11646    -0.11719
log_std/min                           -0.15235     0.00027    -0.15194    -0.15271
log_probs/mean                        -2.72921     0.00699    -2.72094    -2.73701
log_probs/std                         0.22384      0.01505    0.25252     0.21027
log_probs/max                         -2.14775     0.06003    -2.03671    -2.20201
log_probs/min                         -5.09236     1.10853    -3.98433    -6.89418
mean/mean                             0.00102      0.00021    0.00133     0.00078
mean/std                              0.01345      0.00014    0.01366     0.01327
mean/max                              0.02476      0.00017    0.02502     0.02457
mean/min                              -0.01759     0.00020    -0.01730    -0.01788
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4971, 0.5068, 0.4990, 0.5068, 0.5066, 0.4888, 0.4950, 0.5018, 0.4998,
         0.5025, 0.5101, 0.5004, 0.4960, 0.4886, 0.4922, 0.5076, 0.4923, 0.5100,
         0.4963, 0.5142],
        [0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4900, 0.4957, 0.5029, 0.5022,
         0.5031, 0.5085, 0.4990, 0.4961, 0.4882, 0.4901, 0.5076, 0.4916, 0.5090,
         0.4966, 0.5125],
        [0.4978, 0.5077, 0.4983, 0.5068, 0.5059, 0.4890, 0.4961, 0.5006, 0.5038,
         0.5017, 0.5072, 0.4987, 0.4961, 0.4882, 0.4898, 0.5093, 0.4932, 0.5093,
         0.4960, 0.5146],
        [0.4983, 0.5063, 0.4988, 0.5068, 0.5060, 0.4896, 0.4947, 0.5003, 0.5022,
         0.5037, 0.5071, 0.4985, 0.4967, 0.4880, 0.4897, 0.5076, 0.4923, 0.5102,
         0.4939, 0.5140],
        [0.4984, 0.5077, 0.4983, 0.5052, 0.5053, 0.4888, 0.4953, 0.4988, 0.5019,
         0.5029, 0.5088, 0.4991, 0.4963, 0.4882, 0.4915, 0.5061, 0.4946, 0.5099,
         0.4954, 0.5133],
        [0.4973, 0.5049, 0.4977, 0.5060, 0.5050, 0.4884, 0.4946, 0.5000, 0.5008,
         0.5030, 0.5078, 0.4997, 0.4952, 0.4879, 0.4901, 0.5083, 0.4952, 0.5084,
         0.4954, 0.5132],
        [0.4976, 0.5057, 0.4984, 0.5047, 0.5060, 0.4880, 0.4968, 0.5007, 0.5029,
         0.5020, 0.5100, 0.4996, 0.4959, 0.4873, 0.4906, 0.5083, 0.4950, 0.5080,
         0.4965, 0.5128],
        [0.4977, 0.5048, 0.4984, 0.5050, 0.5065, 0.4882, 0.4952, 0.5011, 0.5029,
         0.5019, 0.5098, 0.4996, 0.4962, 0.4889, 0.4919, 0.5078, 0.4931, 0.5064,
         0.4934, 0.5126],
        [0.4980, 0.5047, 0.4976, 0.5047, 0.5081, 0.4879, 0.4960, 0.5002, 0.5017,
         0.5009, 0.5068, 0.5012, 0.4972, 0.4887, 0.4924, 0.5083, 0.4928, 0.5083,
         0.4941, 0.5142],
        [0.4984, 0.5056, 0.4995, 0.5047, 0.5074, 0.4881, 0.4963, 0.5016, 0.5018,
         0.5040, 0.5076, 0.5003, 0.4943, 0.4909, 0.4929, 0.5095, 0.4940, 0.5048,
         0.4951, 0.5155]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4971, 0.5068, 0.4990, 0.5068, 0.5066, 0.4888, 0.4950, 0.5018, 0.4998,
        0.5025, 0.5101, 0.5004, 0.4960, 0.4886, 0.4922, 0.5076, 0.4923, 0.5100,
        0.4963, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4900, 0.4957, 0.5029, 0.5022,
        0.5031, 0.5085, 0.4990, 0.4961, 0.4882, 0.4901, 0.5076, 0.4916, 0.5090,
        0.4966, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5077, 0.4983, 0.5068, 0.5059, 0.4890, 0.4961, 0.5006, 0.5038,
        0.5017, 0.5072, 0.4987, 0.4961, 0.4882, 0.4898, 0.5093, 0.4932, 0.5093,
        0.4960, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5063, 0.4988, 0.5068, 0.5060, 0.4896, 0.4947, 0.5003, 0.5022,
        0.5037, 0.5071, 0.4985, 0.4967, 0.4880, 0.4897, 0.5076, 0.4923, 0.5102,
        0.4939, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5077, 0.4983, 0.5052, 0.5053, 0.4888, 0.4953, 0.4988, 0.5019,
        0.5029, 0.5088, 0.4991, 0.4963, 0.4882, 0.4915, 0.5061, 0.4946, 0.5099,
        0.4954, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4973, 0.5049, 0.4977, 0.5060, 0.5050, 0.4884, 0.4946, 0.5000, 0.5008,
        0.5030, 0.5078, 0.4997, 0.4952, 0.4879, 0.4901, 0.5083, 0.4952, 0.5084,
        0.4954, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5057, 0.4984, 0.5047, 0.5060, 0.4880, 0.4968, 0.5007, 0.5029,
        0.5020, 0.5100, 0.4996, 0.4959, 0.4873, 0.4906, 0.5083, 0.4950, 0.5080,
        0.4965, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4984, 0.5050, 0.5065, 0.4882, 0.4952, 0.5011, 0.5029,
        0.5019, 0.5098, 0.4996, 0.4962, 0.4889, 0.4919, 0.5078, 0.4931, 0.5064,
        0.4934, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4976, 0.5047, 0.5081, 0.4879, 0.4960, 0.5002, 0.5017,
        0.5009, 0.5068, 0.5012, 0.4972, 0.4887, 0.4924, 0.5083, 0.4928, 0.5083,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5056, 0.4995, 0.5047, 0.5074, 0.4881, 0.4963, 0.5016, 0.5018,
        0.5040, 0.5076, 0.5003, 0.4943, 0.4909, 0.4929, 0.5095, 0.4940, 0.5048,
        0.4951, 0.5155], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4892, 0.5120, 0.4863, 0.4852, 0.4942, 0.4898, 0.4941, 0.4987,
         0.5074, 0.5029, 0.5004, 0.5026, 0.4919, 0.5046, 0.4913, 0.4858, 0.4983,
         0.5076, 0.4946],
        [0.5089, 0.4897, 0.5108, 0.4850, 0.4856, 0.4929, 0.4885, 0.4953, 0.4982,
         0.5052, 0.5038, 0.5000, 0.5034, 0.4918, 0.5031, 0.4889, 0.4863, 0.4982,
         0.5070, 0.4932],
        [0.5103, 0.4903, 0.5107, 0.4867, 0.4854, 0.4944, 0.4890, 0.4963, 0.5009,
         0.5069, 0.5046, 0.5004, 0.5060, 0.4920, 0.5040, 0.4909, 0.4873, 0.4981,
         0.5068, 0.4925],
        [0.5080, 0.4895, 0.5117, 0.4860, 0.4872, 0.4935, 0.4901, 0.4950, 0.4982,
         0.5086, 0.5039, 0.5002, 0.5050, 0.4916, 0.5042, 0.4906, 0.4862, 0.4983,
         0.5056, 0.4924],
        [0.5095, 0.4912, 0.5130, 0.4860, 0.4872, 0.4930, 0.4910, 0.4923, 0.4956,
         0.5068, 0.5045, 0.5001, 0.5030, 0.4914, 0.5041, 0.4894, 0.4852, 0.4961,
         0.5054, 0.4936],
        [0.5110, 0.4916, 0.5101, 0.4866, 0.4861, 0.4947, 0.4909, 0.4939, 0.4969,
         0.5068, 0.5059, 0.5005, 0.5040, 0.4899, 0.5041, 0.4898, 0.4850, 0.4970,
         0.5060, 0.4930],
        [0.5097, 0.4915, 0.5125, 0.4879, 0.4860, 0.4938, 0.4890, 0.4942, 0.4977,
         0.5072, 0.5064, 0.4996, 0.5049, 0.4904, 0.5037, 0.4894, 0.4846, 0.4983,
         0.5071, 0.4933],
        [0.5101, 0.4915, 0.5115, 0.4852, 0.4869, 0.4925, 0.4891, 0.4946, 0.4998,
         0.5072, 0.5068, 0.5008, 0.5035, 0.4894, 0.5049, 0.4913, 0.4852, 0.4956,
         0.5076, 0.4934],
        [0.5112, 0.4927, 0.5111, 0.4851, 0.4888, 0.4929, 0.4894, 0.4941, 0.4983,
         0.5073, 0.5051, 0.4974, 0.5042, 0.4896, 0.5038, 0.4910, 0.4862, 0.4965,
         0.5082, 0.4937],
        [0.5114, 0.4923, 0.5119, 0.4869, 0.4884, 0.4928, 0.4894, 0.4932, 0.4998,
         0.5053, 0.5054, 0.4984, 0.5037, 0.4888, 0.5005, 0.4932, 0.4837, 0.4972,
         0.5059, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4892, 0.5120, 0.4863, 0.4852, 0.4942, 0.4898, 0.4941, 0.4987,
        0.5074, 0.5029, 0.5004, 0.5026, 0.4919, 0.5046, 0.4913, 0.4858, 0.4983,
        0.5076, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4897, 0.5108, 0.4850, 0.4856, 0.4929, 0.4885, 0.4953, 0.4982,
        0.5052, 0.5038, 0.5000, 0.5034, 0.4918, 0.5031, 0.4889, 0.4863, 0.4982,
        0.5070, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5103, 0.4903, 0.5107, 0.4867, 0.4854, 0.4944, 0.4890, 0.4963, 0.5009,
        0.5069, 0.5046, 0.5004, 0.5060, 0.4920, 0.5040, 0.4909, 0.4873, 0.4981,
        0.5068, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5080, 0.4895, 0.5117, 0.4860, 0.4872, 0.4935, 0.4901, 0.4950, 0.4982,
        0.5086, 0.5039, 0.5002, 0.5050, 0.4916, 0.5042, 0.4906, 0.4862, 0.4983,
        0.5056, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5130, 0.4860, 0.4872, 0.4930, 0.4910, 0.4923, 0.4956,
        0.5068, 0.5045, 0.5001, 0.5030, 0.4914, 0.5041, 0.4894, 0.4852, 0.4961,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4916, 0.5101, 0.4866, 0.4861, 0.4947, 0.4909, 0.4939, 0.4969,
        0.5068, 0.5059, 0.5005, 0.5040, 0.4899, 0.5041, 0.4898, 0.4850, 0.4970,
        0.5060, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5097, 0.4915, 0.5125, 0.4879, 0.4860, 0.4938, 0.4890, 0.4942, 0.4977,
        0.5072, 0.5064, 0.4996, 0.5049, 0.4904, 0.5037, 0.4894, 0.4846, 0.4983,
        0.5071, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4915, 0.5115, 0.4852, 0.4869, 0.4925, 0.4891, 0.4946, 0.4998,
        0.5072, 0.5068, 0.5008, 0.5035, 0.4894, 0.5049, 0.4913, 0.4852, 0.4956,
        0.5076, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4927, 0.5111, 0.4851, 0.4888, 0.4929, 0.4894, 0.4941, 0.4983,
        0.5073, 0.5051, 0.4974, 0.5042, 0.4896, 0.5038, 0.4910, 0.4862, 0.4965,
        0.5082, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4923, 0.5119, 0.4869, 0.4884, 0.4928, 0.4894, 0.4932, 0.4998,
        0.5053, 0.5054, 0.4984, 0.5037, 0.4888, 0.5005, 0.4932, 0.4837, 0.4972,
        0.5059, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
         0.5027, 0.4854, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
         0.4936, 0.4968],
        [0.5115, 0.4957, 0.4957, 0.5044, 0.5037, 0.4932, 0.5104, 0.4965, 0.4959,
         0.5025, 0.4849, 0.4967, 0.4855, 0.5015, 0.5001, 0.4979, 0.4981, 0.5150,
         0.4945, 0.4969],
        [0.5132, 0.4971, 0.4977, 0.5026, 0.5045, 0.4920, 0.5104, 0.4977, 0.4960,
         0.5026, 0.4858, 0.4987, 0.4846, 0.5018, 0.4998, 0.4977, 0.4992, 0.5166,
         0.4947, 0.4967],
        [0.5127, 0.4976, 0.4971, 0.5033, 0.5041, 0.4913, 0.5108, 0.4966, 0.4962,
         0.5027, 0.4841, 0.4968, 0.4844, 0.5022, 0.5008, 0.4976, 0.5012, 0.5146,
         0.4955, 0.4972],
        [0.5129, 0.4982, 0.4965, 0.5039, 0.5033, 0.4934, 0.5089, 0.4953, 0.4965,
         0.5030, 0.4867, 0.4978, 0.4825, 0.5029, 0.5003, 0.4995, 0.5001, 0.5152,
         0.4956, 0.4970],
        [0.5137, 0.4974, 0.4964, 0.5032, 0.5049, 0.4927, 0.5078, 0.4967, 0.4955,
         0.5025, 0.4856, 0.4961, 0.4850, 0.5034, 0.5016, 0.5005, 0.4996, 0.5148,
         0.4953, 0.4983],
        [0.5115, 0.4965, 0.4980, 0.5026, 0.5037, 0.4921, 0.5087, 0.4964, 0.4959,
         0.5026, 0.4874, 0.4971, 0.4851, 0.5028, 0.5009, 0.4991, 0.5002, 0.5141,
         0.4956, 0.4998],
        [0.5120, 0.4974, 0.4952, 0.5009, 0.5041, 0.4922, 0.5082, 0.4945, 0.4969,
         0.5021, 0.4860, 0.4983, 0.4865, 0.5013, 0.5009, 0.4988, 0.4999, 0.5153,
         0.4959, 0.4990],
        [0.5128, 0.4981, 0.4949, 0.5010, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
         0.5018, 0.4884, 0.4952, 0.4854, 0.5015, 0.5012, 0.4973, 0.5007, 0.5144,
         0.4958, 0.4996],
        [0.5127, 0.4985, 0.4938, 0.5026, 0.5051, 0.4935, 0.5078, 0.4987, 0.4964,
         0.5031, 0.4884, 0.4977, 0.4858, 0.5001, 0.5012, 0.4982, 0.4997, 0.5154,
         0.4972, 0.5006]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
        0.5027, 0.4854, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4957, 0.4957, 0.5044, 0.5037, 0.4932, 0.5104, 0.4965, 0.4959,
        0.5025, 0.4849, 0.4967, 0.4855, 0.5015, 0.5001, 0.4979, 0.4981, 0.5150,
        0.4945, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5132, 0.4971, 0.4977, 0.5026, 0.5045, 0.4920, 0.5104, 0.4977, 0.4960,
        0.5026, 0.4858, 0.4987, 0.4846, 0.5018, 0.4998, 0.4977, 0.4992, 0.5166,
        0.4947, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4976, 0.4971, 0.5033, 0.5041, 0.4913, 0.5108, 0.4966, 0.4962,
        0.5027, 0.4841, 0.4968, 0.4844, 0.5022, 0.5008, 0.4976, 0.5012, 0.5146,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5129, 0.4982, 0.4965, 0.5039, 0.5033, 0.4934, 0.5089, 0.4953, 0.4965,
        0.5030, 0.4867, 0.4978, 0.4825, 0.5029, 0.5003, 0.4995, 0.5001, 0.5152,
        0.4956, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4974, 0.4964, 0.5032, 0.5049, 0.4927, 0.5078, 0.4967, 0.4955,
        0.5025, 0.4856, 0.4961, 0.4850, 0.5034, 0.5016, 0.5005, 0.4996, 0.5148,
        0.4953, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4965, 0.4980, 0.5026, 0.5037, 0.4921, 0.5087, 0.4964, 0.4959,
        0.5026, 0.4874, 0.4971, 0.4851, 0.5028, 0.5009, 0.4991, 0.5002, 0.5141,
        0.4956, 0.4998], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4974, 0.4952, 0.5009, 0.5041, 0.4922, 0.5082, 0.4945, 0.4969,
        0.5021, 0.4860, 0.4983, 0.4865, 0.5013, 0.5009, 0.4988, 0.4999, 0.5153,
        0.4959, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4949, 0.5010, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
        0.5018, 0.4884, 0.4952, 0.4854, 0.5015, 0.5012, 0.4973, 0.5007, 0.5144,
        0.4958, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4938, 0.5026, 0.5051, 0.4935, 0.5078, 0.4987, 0.4964,
        0.5031, 0.4884, 0.4977, 0.4858, 0.5001, 0.5012, 0.4982, 0.4997, 0.5154,
        0.4972, 0.5006], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [9, 5, 3, 4, 8, 1, 7, 0, 6, 2]
replay_buffer._size: [9900 9900 9900 9900 9900 9900 9900 9900 9900 9900]
2023-08-12 10:41:19,405 MainThread INFO: EPOCH:64
2023-08-12 10:41:19,406 MainThread INFO: Time Consumed:0.38002991676330566s
2023-08-12 10:41:19,406 MainThread INFO: Total Frames:97500s
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 65/80 [01:12<00:16,  1.10s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1572.14072
Train_Epoch_Reward                    27151.65315
Running_Training_Average_Rewards      2248.99546
Explore_Time                          0.00405
Train___Time                          0.36822
Eval____Time                          0.00247
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -52.71207
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.20309
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.07361
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.80738
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.94524
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.55528
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.61346
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 15972.48823
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.27053
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.90038
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.11810     1.03605    11.85481    8.64043
alpha_0                               0.90748      0.00039    0.90802     0.90693
alpha_1                               0.90748      0.00039    0.90803     0.90693
alpha_2                               0.90749      0.00039    0.90804     0.90695
alpha_3                               0.90748      0.00039    0.90803     0.90694
alpha_4                               0.90749      0.00039    0.90804     0.90695
alpha_5                               0.90748      0.00039    0.90803     0.90694
alpha_6                               0.90750      0.00039    0.90804     0.90695
alpha_7                               0.90748      0.00039    0.90802     0.90693
alpha_8                               0.90747      0.00039    0.90802     0.90692
alpha_9                               0.90750      0.00039    0.90804     0.90695
Alpha_loss                            -0.65144     0.00322    -0.64589    -0.65554
Training/policy_loss                  -2.87596     0.01055    -2.85644    -2.88742
Training/qf1_loss                     2188.29692   477.12819  3104.48462  1754.65039
Training/qf2_loss                     2182.41580   476.41156  3097.28906  1749.88831
Training/pf_norm                      0.11751      0.01110    0.12942     0.09818
Training/qf1_norm                     34.71730     2.66183    39.09637    30.79242
Training/qf2_norm                     52.27214     5.26817    60.88223    44.64690
log_std/mean                          -0.13285     0.00005    -0.13278    -0.13291
log_std/std                           0.00822      0.00004    0.00827     0.00817
log_std/max                           -0.11679     0.00019    -0.11646    -0.11704
log_std/min                           -0.15184     0.00062    -0.15084    -0.15237
log_probs/mean                        -2.73132     0.00947    -2.71562    -2.74525
log_probs/std                         0.24081      0.00581    0.24711     0.23221
log_probs/max                         -2.11792     0.04406    -2.05506    -2.17256
log_probs/min                         -5.14204     0.45082    -4.58160    -5.71867
mean/mean                             0.00055      0.00010    0.00069     0.00041
mean/std                              0.01412      0.00019    0.01435     0.01381
mean/max                              0.02596      0.00050    0.02673     0.02520
mean/min                              -0.01817     0.00011    -0.01799    -0.01831
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [4, 1, 7, 6, 0, 8, 3, 9, 2, 5]
replay_buffer._size: [10050 10050 10050 10050 10050 10050 10050 10050 10050 10050]
2023-08-12 10:41:20,295 MainThread INFO: EPOCH:65
2023-08-12 10:41:20,296 MainThread INFO: Time Consumed:0.7510931491851807s
2023-08-12 10:41:20,296 MainThread INFO: Total Frames:99000s
 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 66/80 [01:13<00:14,  1.03s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1582.91148
Train_Epoch_Reward                    5091.03042
Running_Training_Average_Rewards      1853.22910
Explore_Time                          0.00580
Train___Time                          0.67822
Eval____Time                          0.00264
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.81687
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.21056
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.08541
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.71781
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.85341
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.47695
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.53598
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16074.89557
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.27990
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.80389
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.60068      0.55265    10.47065    8.81508
alpha_0                               0.90612      0.00038    0.90666     0.90557
alpha_1                               0.90612      0.00039    0.90666     0.90557
alpha_2                               0.90613      0.00038    0.90668     0.90559
alpha_3                               0.90612      0.00038    0.90667     0.90558
alpha_4                               0.90613      0.00038    0.90668     0.90559
alpha_5                               0.90612      0.00038    0.90666     0.90558
alpha_6                               0.90614      0.00038    0.90668     0.90559
alpha_7                               0.90612      0.00038    0.90666     0.90557
alpha_8                               0.90611      0.00038    0.90665     0.90556
alpha_9                               0.90613      0.00038    0.90668     0.90559
Alpha_loss                            -0.66158     0.00328    -0.65666    -0.66525
Training/policy_loss                  -2.88506     0.00844    -2.87382    -2.90001
Training/qf1_loss                     1946.47559   367.76111  2506.53467  1563.11597
Training/qf2_loss                     1940.54380   367.56380  2500.17407  1557.03381
Training/pf_norm                      0.11978      0.02738    0.14327     0.07062
Training/qf1_norm                     34.02976     1.73420    36.69735    31.48360
Training/qf2_norm                     51.67307     2.63003    55.53633    47.27877
log_std/mean                          -0.13300     0.00011    -0.13283    -0.13313
log_std/std                           0.00821      0.00005    0.00830     0.00817
log_std/max                           -0.11684     0.00030    -0.11634    -0.11720
log_std/min                           -0.15252     0.00038    -0.15208    -0.15315
log_probs/mean                        -2.73160     0.00768    -2.72263    -2.74570
log_probs/std                         0.22962      0.01707    0.26063     0.21120
log_probs/max                         -2.13219     0.04308    -2.05300    -2.17025
log_probs/min                         -4.75630     0.50241    -4.10355    -5.39646
mean/mean                             0.00030      0.00001    0.00033     0.00028
mean/std                              0.01449      0.00003    0.01453     0.01443
mean/max                              0.02675      0.00005    0.02683     0.02668
mean/min                              -0.01870     0.00015    -0.01851    -0.01889
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4971, 0.5068, 0.4990, 0.5068, 0.5066, 0.4888, 0.4950, 0.5018, 0.4999,
         0.5025, 0.5101, 0.5003, 0.4960, 0.4886, 0.4921, 0.5076, 0.4923, 0.5100,
         0.4963, 0.5142],
        [0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4900, 0.4957, 0.5029, 0.5022,
         0.5031, 0.5085, 0.4990, 0.4960, 0.4882, 0.4901, 0.5076, 0.4916, 0.5090,
         0.4966, 0.5125],
        [0.4978, 0.5077, 0.4983, 0.5068, 0.5059, 0.4891, 0.4961, 0.5007, 0.5038,
         0.5017, 0.5072, 0.4987, 0.4960, 0.4882, 0.4898, 0.5093, 0.4931, 0.5094,
         0.4961, 0.5146],
        [0.4983, 0.5063, 0.4989, 0.5068, 0.5060, 0.4896, 0.4947, 0.5004, 0.5022,
         0.5037, 0.5072, 0.4985, 0.4967, 0.4879, 0.4897, 0.5076, 0.4923, 0.5103,
         0.4939, 0.5140],
        [0.4984, 0.5078, 0.4983, 0.5052, 0.5052, 0.4888, 0.4952, 0.4988, 0.5019,
         0.5029, 0.5088, 0.4990, 0.4963, 0.4882, 0.4915, 0.5061, 0.4946, 0.5099,
         0.4953, 0.5133],
        [0.4973, 0.5050, 0.4977, 0.5060, 0.5050, 0.4884, 0.4946, 0.5000, 0.5009,
         0.5031, 0.5078, 0.4996, 0.4952, 0.4879, 0.4901, 0.5082, 0.4952, 0.5084,
         0.4953, 0.5132],
        [0.4976, 0.5057, 0.4983, 0.5047, 0.5060, 0.4881, 0.4968, 0.5007, 0.5029,
         0.5020, 0.5099, 0.4996, 0.4959, 0.4874, 0.4906, 0.5083, 0.4950, 0.5080,
         0.4964, 0.5128],
        [0.4977, 0.5048, 0.4984, 0.5050, 0.5064, 0.4882, 0.4951, 0.5011, 0.5029,
         0.5019, 0.5098, 0.4996, 0.4962, 0.4889, 0.4919, 0.5077, 0.4930, 0.5064,
         0.4933, 0.5126],
        [0.4980, 0.5047, 0.4977, 0.5047, 0.5081, 0.4879, 0.4960, 0.5003, 0.5017,
         0.5009, 0.5068, 0.5011, 0.4972, 0.4888, 0.4924, 0.5083, 0.4928, 0.5082,
         0.4941, 0.5142],
        [0.4984, 0.5056, 0.4995, 0.5048, 0.5074, 0.4880, 0.4963, 0.5016, 0.5018,
         0.5040, 0.5075, 0.5003, 0.4943, 0.4909, 0.4929, 0.5095, 0.4940, 0.5048,
         0.4950, 0.5155]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4971, 0.5068, 0.4990, 0.5068, 0.5066, 0.4888, 0.4950, 0.5018, 0.4999,
        0.5025, 0.5101, 0.5003, 0.4960, 0.4886, 0.4921, 0.5076, 0.4923, 0.5100,
        0.4963, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4900, 0.4957, 0.5029, 0.5022,
        0.5031, 0.5085, 0.4990, 0.4960, 0.4882, 0.4901, 0.5076, 0.4916, 0.5090,
        0.4966, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5077, 0.4983, 0.5068, 0.5059, 0.4891, 0.4961, 0.5007, 0.5038,
        0.5017, 0.5072, 0.4987, 0.4960, 0.4882, 0.4898, 0.5093, 0.4931, 0.5094,
        0.4961, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5063, 0.4989, 0.5068, 0.5060, 0.4896, 0.4947, 0.5004, 0.5022,
        0.5037, 0.5072, 0.4985, 0.4967, 0.4879, 0.4897, 0.5076, 0.4923, 0.5103,
        0.4939, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5078, 0.4983, 0.5052, 0.5052, 0.4888, 0.4952, 0.4988, 0.5019,
        0.5029, 0.5088, 0.4990, 0.4963, 0.4882, 0.4915, 0.5061, 0.4946, 0.5099,
        0.4953, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4973, 0.5050, 0.4977, 0.5060, 0.5050, 0.4884, 0.4946, 0.5000, 0.5009,
        0.5031, 0.5078, 0.4996, 0.4952, 0.4879, 0.4901, 0.5082, 0.4952, 0.5084,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5057, 0.4983, 0.5047, 0.5060, 0.4881, 0.4968, 0.5007, 0.5029,
        0.5020, 0.5099, 0.4996, 0.4959, 0.4874, 0.4906, 0.5083, 0.4950, 0.5080,
        0.4964, 0.5128], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4984, 0.5050, 0.5064, 0.4882, 0.4951, 0.5011, 0.5029,
        0.5019, 0.5098, 0.4996, 0.4962, 0.4889, 0.4919, 0.5077, 0.4930, 0.5064,
        0.4933, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4977, 0.5047, 0.5081, 0.4879, 0.4960, 0.5003, 0.5017,
        0.5009, 0.5068, 0.5011, 0.4972, 0.4888, 0.4924, 0.5083, 0.4928, 0.5082,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5056, 0.4995, 0.5048, 0.5074, 0.4880, 0.4963, 0.5016, 0.5018,
        0.5040, 0.5075, 0.5003, 0.4943, 0.4909, 0.4929, 0.5095, 0.4940, 0.5048,
        0.4950, 0.5155], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4893, 0.5120, 0.4863, 0.4852, 0.4943, 0.4897, 0.4940, 0.4987,
         0.5074, 0.5029, 0.5005, 0.5026, 0.4920, 0.5047, 0.4912, 0.4858, 0.4983,
         0.5076, 0.4946],
        [0.5089, 0.4897, 0.5108, 0.4849, 0.4856, 0.4929, 0.4884, 0.4953, 0.4982,
         0.5052, 0.5038, 0.5000, 0.5034, 0.4919, 0.5031, 0.4889, 0.4863, 0.4982,
         0.5071, 0.4932],
        [0.5102, 0.4904, 0.5107, 0.4867, 0.4854, 0.4944, 0.4890, 0.4963, 0.5008,
         0.5070, 0.5046, 0.5004, 0.5060, 0.4920, 0.5040, 0.4908, 0.4873, 0.4981,
         0.5069, 0.4925],
        [0.5080, 0.4895, 0.5117, 0.4860, 0.4872, 0.4935, 0.4901, 0.4950, 0.4981,
         0.5086, 0.5039, 0.5002, 0.5050, 0.4916, 0.5042, 0.4906, 0.4862, 0.4982,
         0.5056, 0.4924],
        [0.5095, 0.4912, 0.5129, 0.4860, 0.4871, 0.4930, 0.4910, 0.4924, 0.4957,
         0.5068, 0.5045, 0.5001, 0.5030, 0.4914, 0.5041, 0.4894, 0.4852, 0.4961,
         0.5054, 0.4936],
        [0.5110, 0.4916, 0.5101, 0.4866, 0.4861, 0.4948, 0.4909, 0.4939, 0.4970,
         0.5068, 0.5059, 0.5005, 0.5040, 0.4900, 0.5042, 0.4897, 0.4851, 0.4970,
         0.5060, 0.4930],
        [0.5097, 0.4915, 0.5125, 0.4880, 0.4861, 0.4938, 0.4889, 0.4941, 0.4977,
         0.5072, 0.5064, 0.4996, 0.5049, 0.4904, 0.5037, 0.4894, 0.4846, 0.4983,
         0.5071, 0.4933],
        [0.5101, 0.4915, 0.5115, 0.4853, 0.4870, 0.4926, 0.4891, 0.4946, 0.4998,
         0.5071, 0.5068, 0.5008, 0.5035, 0.4894, 0.5049, 0.4914, 0.4853, 0.4956,
         0.5076, 0.4934],
        [0.5112, 0.4927, 0.5111, 0.4852, 0.4888, 0.4928, 0.4893, 0.4941, 0.4984,
         0.5073, 0.5050, 0.4974, 0.5042, 0.4896, 0.5038, 0.4910, 0.4862, 0.4965,
         0.5082, 0.4937],
        [0.5114, 0.4923, 0.5119, 0.4869, 0.4885, 0.4927, 0.4893, 0.4932, 0.4999,
         0.5053, 0.5053, 0.4984, 0.5037, 0.4888, 0.5005, 0.4932, 0.4836, 0.4972,
         0.5059, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4893, 0.5120, 0.4863, 0.4852, 0.4943, 0.4897, 0.4940, 0.4987,
        0.5074, 0.5029, 0.5005, 0.5026, 0.4920, 0.5047, 0.4912, 0.4858, 0.4983,
        0.5076, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4897, 0.5108, 0.4849, 0.4856, 0.4929, 0.4884, 0.4953, 0.4982,
        0.5052, 0.5038, 0.5000, 0.5034, 0.4919, 0.5031, 0.4889, 0.4863, 0.4982,
        0.5071, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5102, 0.4904, 0.5107, 0.4867, 0.4854, 0.4944, 0.4890, 0.4963, 0.5008,
        0.5070, 0.5046, 0.5004, 0.5060, 0.4920, 0.5040, 0.4908, 0.4873, 0.4981,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5080, 0.4895, 0.5117, 0.4860, 0.4872, 0.4935, 0.4901, 0.4950, 0.4981,
        0.5086, 0.5039, 0.5002, 0.5050, 0.4916, 0.5042, 0.4906, 0.4862, 0.4982,
        0.5056, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5129, 0.4860, 0.4871, 0.4930, 0.4910, 0.4924, 0.4957,
        0.5068, 0.5045, 0.5001, 0.5030, 0.4914, 0.5041, 0.4894, 0.4852, 0.4961,
        0.5054, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4916, 0.5101, 0.4866, 0.4861, 0.4948, 0.4909, 0.4939, 0.4970,
        0.5068, 0.5059, 0.5005, 0.5040, 0.4900, 0.5042, 0.4897, 0.4851, 0.4970,
        0.5060, 0.4930], grad_fn=<UnbindBackward>), tensor([0.5097, 0.4915, 0.5125, 0.4880, 0.4861, 0.4938, 0.4889, 0.4941, 0.4977,
        0.5072, 0.5064, 0.4996, 0.5049, 0.4904, 0.5037, 0.4894, 0.4846, 0.4983,
        0.5071, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4915, 0.5115, 0.4853, 0.4870, 0.4926, 0.4891, 0.4946, 0.4998,
        0.5071, 0.5068, 0.5008, 0.5035, 0.4894, 0.5049, 0.4914, 0.4853, 0.4956,
        0.5076, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4927, 0.5111, 0.4852, 0.4888, 0.4928, 0.4893, 0.4941, 0.4984,
        0.5073, 0.5050, 0.4974, 0.5042, 0.4896, 0.5038, 0.4910, 0.4862, 0.4965,
        0.5082, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4923, 0.5119, 0.4869, 0.4885, 0.4927, 0.4893, 0.4932, 0.4999,
        0.5053, 0.5053, 0.4984, 0.5037, 0.4888, 0.5005, 0.4932, 0.4836, 0.4972,
        0.5059, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4953, 0.4967,
         0.5027, 0.4854, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4957, 0.5043, 0.5037, 0.4932, 0.5104, 0.4965, 0.4959,
         0.5025, 0.4849, 0.4967, 0.4854, 0.5015, 0.5001, 0.4979, 0.4981, 0.5150,
         0.4944, 0.4969],
        [0.5131, 0.4971, 0.4977, 0.5026, 0.5045, 0.4920, 0.5104, 0.4977, 0.4959,
         0.5026, 0.4858, 0.4987, 0.4845, 0.5019, 0.4998, 0.4977, 0.4992, 0.5166,
         0.4946, 0.4966],
        [0.5127, 0.4976, 0.4971, 0.5033, 0.5042, 0.4913, 0.5108, 0.4966, 0.4962,
         0.5027, 0.4841, 0.4968, 0.4844, 0.5022, 0.5008, 0.4976, 0.5012, 0.5146,
         0.4955, 0.4972],
        [0.5129, 0.4983, 0.4965, 0.5039, 0.5033, 0.4933, 0.5089, 0.4953, 0.4964,
         0.5030, 0.4866, 0.4977, 0.4825, 0.5029, 0.5003, 0.4995, 0.5001, 0.5152,
         0.4956, 0.4970],
        [0.5137, 0.4975, 0.4965, 0.5032, 0.5048, 0.4927, 0.5079, 0.4966, 0.4954,
         0.5025, 0.4856, 0.4962, 0.4850, 0.5034, 0.5017, 0.5005, 0.4997, 0.5148,
         0.4954, 0.4983],
        [0.5115, 0.4966, 0.4981, 0.5026, 0.5037, 0.4921, 0.5087, 0.4963, 0.4959,
         0.5026, 0.4874, 0.4971, 0.4851, 0.5028, 0.5009, 0.4991, 0.5002, 0.5141,
         0.4956, 0.4998],
        [0.5120, 0.4974, 0.4952, 0.5010, 0.5040, 0.4921, 0.5082, 0.4944, 0.4968,
         0.5021, 0.4860, 0.4983, 0.4864, 0.5013, 0.5010, 0.4988, 0.4999, 0.5153,
         0.4960, 0.4990],
        [0.5128, 0.4981, 0.4949, 0.5010, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
         0.5018, 0.4884, 0.4952, 0.4854, 0.5014, 0.5012, 0.4973, 0.5007, 0.5143,
         0.4958, 0.4996],
        [0.5127, 0.4985, 0.4938, 0.5025, 0.5051, 0.4935, 0.5078, 0.4987, 0.4964,
         0.5031, 0.4884, 0.4977, 0.4858, 0.5000, 0.5012, 0.4981, 0.4997, 0.5153,
         0.4972, 0.5006]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4953, 0.4967,
        0.5027, 0.4854, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4957, 0.5043, 0.5037, 0.4932, 0.5104, 0.4965, 0.4959,
        0.5025, 0.4849, 0.4967, 0.4854, 0.5015, 0.5001, 0.4979, 0.4981, 0.5150,
        0.4944, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4971, 0.4977, 0.5026, 0.5045, 0.4920, 0.5104, 0.4977, 0.4959,
        0.5026, 0.4858, 0.4987, 0.4845, 0.5019, 0.4998, 0.4977, 0.4992, 0.5166,
        0.4946, 0.4966], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4976, 0.4971, 0.5033, 0.5042, 0.4913, 0.5108, 0.4966, 0.4962,
        0.5027, 0.4841, 0.4968, 0.4844, 0.5022, 0.5008, 0.4976, 0.5012, 0.5146,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5129, 0.4983, 0.4965, 0.5039, 0.5033, 0.4933, 0.5089, 0.4953, 0.4964,
        0.5030, 0.4866, 0.4977, 0.4825, 0.5029, 0.5003, 0.4995, 0.5001, 0.5152,
        0.4956, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4975, 0.4965, 0.5032, 0.5048, 0.4927, 0.5079, 0.4966, 0.4954,
        0.5025, 0.4856, 0.4962, 0.4850, 0.5034, 0.5017, 0.5005, 0.4997, 0.5148,
        0.4954, 0.4983], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4966, 0.4981, 0.5026, 0.5037, 0.4921, 0.5087, 0.4963, 0.4959,
        0.5026, 0.4874, 0.4971, 0.4851, 0.5028, 0.5009, 0.4991, 0.5002, 0.5141,
        0.4956, 0.4998], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4974, 0.4952, 0.5010, 0.5040, 0.4921, 0.5082, 0.4944, 0.4968,
        0.5021, 0.4860, 0.4983, 0.4864, 0.5013, 0.5010, 0.4988, 0.4999, 0.5153,
        0.4960, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4949, 0.5010, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
        0.5018, 0.4884, 0.4952, 0.4854, 0.5014, 0.5012, 0.4973, 0.5007, 0.5143,
        0.4958, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4938, 0.5025, 0.5051, 0.4935, 0.5078, 0.4987, 0.4964,
        0.5031, 0.4884, 0.4977, 0.4858, 0.5000, 0.5012, 0.4981, 0.4997, 0.5153,
        0.4972, 0.5006], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [4, 2, 9, 0, 1, 7, 6, 8, 5, 3]
replay_buffer._size: [10200 10200 10200 10200 10200 10200 10200 10200 10200 10200]
snapshot at best
2023-08-12 10:41:21,855 MainThread INFO: EPOCH:66
2023-08-12 10:41:21,855 MainThread INFO: Time Consumed:1.0899453163146973s
2023-08-12 10:41:21,855 MainThread INFO: Total Frames:100500s
 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 67/80 [01:15<00:15,  1.19s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1598.38328
Train_Epoch_Reward                    4794.29609
Running_Training_Average_Rewards      1234.56599
Explore_Time                          0.00299
Train___Time                          0.56195
Eval____Time                          0.01044
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.76330
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.15524
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.10613
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.50845
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.64257
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.29613
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.35694
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16230.54562
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.22183
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.66225
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.50372      0.94110    10.84773    8.10239
alpha_0                               0.90476      0.00038    0.90530     0.90421
alpha_1                               0.90475      0.00038    0.90530     0.90421
alpha_2                               0.90477      0.00038    0.90532     0.90423
alpha_3                               0.90476      0.00038    0.90531     0.90422
alpha_4                               0.90477      0.00038    0.90532     0.90423
alpha_5                               0.90476      0.00038    0.90530     0.90422
alpha_6                               0.90478      0.00038    0.90532     0.90423
alpha_7                               0.90476      0.00038    0.90530     0.90421
alpha_8                               0.90475      0.00038    0.90529     0.90420
alpha_9                               0.90477      0.00038    0.90532     0.90423
Alpha_loss                            -0.67238     0.00244    -0.66916    -0.67602
Training/policy_loss                  -2.90031     0.00430    -2.89453    -2.90651
Training/qf1_loss                     1758.02339   456.38551  2535.84814  1230.94092
Training/qf2_loss                     1751.89390   455.98237  2528.86499  1224.68176
Training/pf_norm                      0.12721      0.03695    0.16192     0.05694
Training/qf1_norm                     33.89796     2.69743    37.98056    29.92908
Training/qf2_norm                     51.95198     4.38918    58.81977    45.11410
log_std/mean                          -0.13317     0.00004    -0.13311    -0.13324
log_std/std                           0.00814      0.00004    0.00821     0.00809
log_std/max                           -0.11758     0.00016    -0.11737    -0.11780
log_std/min                           -0.15302     0.00039    -0.15253    -0.15363
log_probs/mean                        -2.73859     0.00646    -2.72939    -2.74692
log_probs/std                         0.24382      0.02119    0.28180     0.21974
log_probs/max                         -2.10198     0.04166    -2.03423    -2.16279
log_probs/min                         -5.02180     0.58156    -4.54587    -6.16534
mean/mean                             0.00036      0.00006    0.00045     0.00030
mean/std                              0.01467      0.00009    0.01476     0.01452
mean/max                              0.02704      0.00008    0.02716     0.02692
mean/min                              -0.01950     0.00025    -0.01909    -0.01976
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [1, 5, 7, 3, 9, 8, 0, 2, 4, 6]
replay_buffer._size: [10350 10350 10350 10350 10350 10350 10350 10350 10350 10350]
snapshot at best
2023-08-12 10:41:22,820 MainThread INFO: EPOCH:67
2023-08-12 10:41:22,820 MainThread INFO: Time Consumed:0.8510425090789795s
2023-08-12 10:41:22,820 MainThread INFO: Total Frames:102000s
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 68/80 [01:16<00:13,  1.13s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1598.82344
Train_Epoch_Reward                    11130.22657
Running_Training_Average_Rewards      700.51844
Explore_Time                          0.00331
Train___Time                          0.30983
Eval____Time                          0.00253
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.53454
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.18000
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.05461
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.43331
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.47242
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -21.15272
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.21569
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16233.12738
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.24635
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.60336
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.87371      0.36036    10.37439    9.46947
alpha_0                               0.90340      0.00038    0.90394     0.90285
alpha_1                               0.90340      0.00038    0.90394     0.90285
alpha_2                               0.90341      0.00038    0.90396     0.90287
alpha_3                               0.90341      0.00038    0.90395     0.90286
alpha_4                               0.90341      0.00038    0.90396     0.90287
alpha_5                               0.90340      0.00038    0.90395     0.90286
alpha_6                               0.90342      0.00038    0.90396     0.90288
alpha_7                               0.90340      0.00038    0.90394     0.90286
alpha_8                               0.90339      0.00038    0.90393     0.90285
alpha_9                               0.90342      0.00038    0.90396     0.90287
Alpha_loss                            -0.68166     0.00294    -0.67771    -0.68607
Training/policy_loss                  -2.90141     0.00365    -2.89618    -2.90740
Training/qf1_loss                     2008.81973   209.90438  2357.18286  1724.29749
Training/qf2_loss                     2002.49133   209.75087  2350.49829  1718.18433
Training/pf_norm                      0.10519      0.02906    0.14209     0.07416
Training/qf1_norm                     35.39127     1.06625    36.68813    33.99407
Training/qf2_norm                     53.83317     1.80659    56.11546    51.93258
log_std/mean                          -0.13324     0.00002    -0.13321    -0.13327
log_std/std                           0.00809      0.00003    0.00813     0.00803
log_std/max                           -0.11754     0.00016    -0.11733    -0.11779
log_std/min                           -0.15278     0.00028    -0.15246    -0.15327
log_probs/mean                        -2.73028     0.00216    -2.72795    -2.73385
log_probs/std                         0.22783      0.00896    0.24113     0.21552
log_probs/max                         -2.13441     0.05202    -2.07572    -2.21426
log_probs/min                         -4.83976     0.27217    -4.38029    -5.16489
mean/mean                             0.00076      0.00012    0.00091     0.00057
mean/std                              0.01461      0.00004    0.01469     0.01456
mean/max                              0.02720      0.00020    0.02757     0.02698
mean/min                              -0.01910     0.00033    -0.01866    -0.01960
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4971, 0.5068, 0.4990, 0.5068, 0.5065, 0.4888, 0.4950, 0.5018, 0.4999,
         0.5026, 0.5102, 0.5003, 0.4960, 0.4886, 0.4921, 0.5076, 0.4923, 0.5101,
         0.4963, 0.5142],
        [0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4901, 0.4957, 0.5029, 0.5022,
         0.5031, 0.5085, 0.4989, 0.4960, 0.4882, 0.4901, 0.5076, 0.4916, 0.5090,
         0.4965, 0.5125],
        [0.4978, 0.5077, 0.4984, 0.5069, 0.5058, 0.4891, 0.4960, 0.5007, 0.5038,
         0.5017, 0.5072, 0.4986, 0.4960, 0.4882, 0.4898, 0.5093, 0.4931, 0.5094,
         0.4960, 0.5146],
        [0.4983, 0.5063, 0.4989, 0.5068, 0.5060, 0.4896, 0.4947, 0.5004, 0.5022,
         0.5037, 0.5073, 0.4985, 0.4967, 0.4879, 0.4897, 0.5076, 0.4923, 0.5103,
         0.4939, 0.5140],
        [0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4887, 0.4952, 0.4988, 0.5019,
         0.5030, 0.5088, 0.4990, 0.4963, 0.4882, 0.4915, 0.5061, 0.4947, 0.5098,
         0.4953, 0.5133],
        [0.4972, 0.5050, 0.4977, 0.5060, 0.5050, 0.4883, 0.4946, 0.5001, 0.5008,
         0.5031, 0.5079, 0.4996, 0.4952, 0.4879, 0.4901, 0.5082, 0.4952, 0.5084,
         0.4953, 0.5132],
        [0.4976, 0.5057, 0.4983, 0.5048, 0.5061, 0.4880, 0.4968, 0.5008, 0.5027,
         0.5021, 0.5100, 0.4995, 0.4958, 0.4873, 0.4906, 0.5083, 0.4949, 0.5080,
         0.4963, 0.5129],
        [0.4977, 0.5048, 0.4985, 0.5051, 0.5065, 0.4882, 0.4951, 0.5012, 0.5028,
         0.5019, 0.5099, 0.4996, 0.4962, 0.4889, 0.4919, 0.5078, 0.4931, 0.5064,
         0.4934, 0.5126],
        [0.4980, 0.5047, 0.4977, 0.5047, 0.5081, 0.4879, 0.4961, 0.5003, 0.5017,
         0.5010, 0.5068, 0.5011, 0.4972, 0.4888, 0.4924, 0.5083, 0.4929, 0.5082,
         0.4941, 0.5142],
        [0.4984, 0.5056, 0.4995, 0.5047, 0.5074, 0.4881, 0.4963, 0.5016, 0.5018,
         0.5040, 0.5076, 0.5003, 0.4943, 0.4909, 0.4929, 0.5094, 0.4940, 0.5048,
         0.4950, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4971, 0.5068, 0.4990, 0.5068, 0.5065, 0.4888, 0.4950, 0.5018, 0.4999,
        0.5026, 0.5102, 0.5003, 0.4960, 0.4886, 0.4921, 0.5076, 0.4923, 0.5101,
        0.4963, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4901, 0.4957, 0.5029, 0.5022,
        0.5031, 0.5085, 0.4989, 0.4960, 0.4882, 0.4901, 0.5076, 0.4916, 0.5090,
        0.4965, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5077, 0.4984, 0.5069, 0.5058, 0.4891, 0.4960, 0.5007, 0.5038,
        0.5017, 0.5072, 0.4986, 0.4960, 0.4882, 0.4898, 0.5093, 0.4931, 0.5094,
        0.4960, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5063, 0.4989, 0.5068, 0.5060, 0.4896, 0.4947, 0.5004, 0.5022,
        0.5037, 0.5073, 0.4985, 0.4967, 0.4879, 0.4897, 0.5076, 0.4923, 0.5103,
        0.4939, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4887, 0.4952, 0.4988, 0.5019,
        0.5030, 0.5088, 0.4990, 0.4963, 0.4882, 0.4915, 0.5061, 0.4947, 0.5098,
        0.4953, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4972, 0.5050, 0.4977, 0.5060, 0.5050, 0.4883, 0.4946, 0.5001, 0.5008,
        0.5031, 0.5079, 0.4996, 0.4952, 0.4879, 0.4901, 0.5082, 0.4952, 0.5084,
        0.4953, 0.5132], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5057, 0.4983, 0.5048, 0.5061, 0.4880, 0.4968, 0.5008, 0.5027,
        0.5021, 0.5100, 0.4995, 0.4958, 0.4873, 0.4906, 0.5083, 0.4949, 0.5080,
        0.4963, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4985, 0.5051, 0.5065, 0.4882, 0.4951, 0.5012, 0.5028,
        0.5019, 0.5099, 0.4996, 0.4962, 0.4889, 0.4919, 0.5078, 0.4931, 0.5064,
        0.4934, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4977, 0.5047, 0.5081, 0.4879, 0.4961, 0.5003, 0.5017,
        0.5010, 0.5068, 0.5011, 0.4972, 0.4888, 0.4924, 0.5083, 0.4929, 0.5082,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5056, 0.4995, 0.5047, 0.5074, 0.4881, 0.4963, 0.5016, 0.5018,
        0.5040, 0.5076, 0.5003, 0.4943, 0.4909, 0.4929, 0.5094, 0.4940, 0.5048,
        0.4950, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4893, 0.5119, 0.4863, 0.4852, 0.4943, 0.4897, 0.4940, 0.4987,
         0.5075, 0.5028, 0.5005, 0.5026, 0.4920, 0.5047, 0.4912, 0.4858, 0.4983,
         0.5077, 0.4946],
        [0.5089, 0.4897, 0.5108, 0.4849, 0.4857, 0.4930, 0.4884, 0.4953, 0.4983,
         0.5052, 0.5038, 0.5000, 0.5035, 0.4919, 0.5031, 0.4889, 0.4863, 0.4981,
         0.5071, 0.4933],
        [0.5101, 0.4904, 0.5107, 0.4867, 0.4853, 0.4945, 0.4890, 0.4961, 0.5007,
         0.5070, 0.5046, 0.5004, 0.5059, 0.4921, 0.5041, 0.4907, 0.4874, 0.4980,
         0.5069, 0.4925],
        [0.5079, 0.4896, 0.5117, 0.4860, 0.4872, 0.4935, 0.4902, 0.4949, 0.4981,
         0.5086, 0.5039, 0.5003, 0.5050, 0.4917, 0.5043, 0.4906, 0.4862, 0.4982,
         0.5056, 0.4924],
        [0.5095, 0.4912, 0.5129, 0.4860, 0.4871, 0.4931, 0.4909, 0.4924, 0.4958,
         0.5068, 0.5046, 0.5001, 0.5030, 0.4914, 0.5040, 0.4895, 0.4852, 0.4961,
         0.5055, 0.4936],
        [0.5110, 0.4917, 0.5101, 0.4866, 0.4861, 0.4947, 0.4909, 0.4939, 0.4970,
         0.5068, 0.5060, 0.5005, 0.5040, 0.4899, 0.5042, 0.4898, 0.4851, 0.4970,
         0.5060, 0.4929],
        [0.5097, 0.4917, 0.5125, 0.4879, 0.4860, 0.4938, 0.4890, 0.4941, 0.4978,
         0.5072, 0.5065, 0.4996, 0.5049, 0.4904, 0.5038, 0.4894, 0.4846, 0.4982,
         0.5072, 0.4933],
        [0.5101, 0.4916, 0.5117, 0.4852, 0.4869, 0.4925, 0.4892, 0.4945, 0.4998,
         0.5072, 0.5069, 0.5008, 0.5035, 0.4893, 0.5049, 0.4915, 0.4852, 0.4956,
         0.5075, 0.4934],
        [0.5112, 0.4927, 0.5111, 0.4852, 0.4888, 0.4928, 0.4893, 0.4942, 0.4985,
         0.5072, 0.5051, 0.4974, 0.5042, 0.4896, 0.5037, 0.4911, 0.4862, 0.4965,
         0.5082, 0.4937],
        [0.5114, 0.4923, 0.5119, 0.4869, 0.4885, 0.4928, 0.4893, 0.4932, 0.4998,
         0.5053, 0.5054, 0.4983, 0.5037, 0.4888, 0.5005, 0.4934, 0.4836, 0.4971,
         0.5059, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4893, 0.5119, 0.4863, 0.4852, 0.4943, 0.4897, 0.4940, 0.4987,
        0.5075, 0.5028, 0.5005, 0.5026, 0.4920, 0.5047, 0.4912, 0.4858, 0.4983,
        0.5077, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4897, 0.5108, 0.4849, 0.4857, 0.4930, 0.4884, 0.4953, 0.4983,
        0.5052, 0.5038, 0.5000, 0.5035, 0.4919, 0.5031, 0.4889, 0.4863, 0.4981,
        0.5071, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4904, 0.5107, 0.4867, 0.4853, 0.4945, 0.4890, 0.4961, 0.5007,
        0.5070, 0.5046, 0.5004, 0.5059, 0.4921, 0.5041, 0.4907, 0.4874, 0.4980,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4896, 0.5117, 0.4860, 0.4872, 0.4935, 0.4902, 0.4949, 0.4981,
        0.5086, 0.5039, 0.5003, 0.5050, 0.4917, 0.5043, 0.4906, 0.4862, 0.4982,
        0.5056, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5095, 0.4912, 0.5129, 0.4860, 0.4871, 0.4931, 0.4909, 0.4924, 0.4958,
        0.5068, 0.5046, 0.5001, 0.5030, 0.4914, 0.5040, 0.4895, 0.4852, 0.4961,
        0.5055, 0.4936], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4917, 0.5101, 0.4866, 0.4861, 0.4947, 0.4909, 0.4939, 0.4970,
        0.5068, 0.5060, 0.5005, 0.5040, 0.4899, 0.5042, 0.4898, 0.4851, 0.4970,
        0.5060, 0.4929], grad_fn=<UnbindBackward>), tensor([0.5097, 0.4917, 0.5125, 0.4879, 0.4860, 0.4938, 0.4890, 0.4941, 0.4978,
        0.5072, 0.5065, 0.4996, 0.5049, 0.4904, 0.5038, 0.4894, 0.4846, 0.4982,
        0.5072, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4916, 0.5117, 0.4852, 0.4869, 0.4925, 0.4892, 0.4945, 0.4998,
        0.5072, 0.5069, 0.5008, 0.5035, 0.4893, 0.5049, 0.4915, 0.4852, 0.4956,
        0.5075, 0.4934], grad_fn=<UnbindBackward>), tensor([0.5112, 0.4927, 0.5111, 0.4852, 0.4888, 0.4928, 0.4893, 0.4942, 0.4985,
        0.5072, 0.5051, 0.4974, 0.5042, 0.4896, 0.5037, 0.4911, 0.4862, 0.4965,
        0.5082, 0.4937], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4923, 0.5119, 0.4869, 0.4885, 0.4928, 0.4893, 0.4932, 0.4998,
        0.5053, 0.5054, 0.4983, 0.5037, 0.4888, 0.5005, 0.4934, 0.4836, 0.4971,
        0.5059, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
         0.5027, 0.4854, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
         0.4936, 0.4968],
        [0.5115, 0.4958, 0.4958, 0.5043, 0.5037, 0.4931, 0.5104, 0.4964, 0.4959,
         0.5024, 0.4849, 0.4967, 0.4854, 0.5016, 0.5001, 0.4979, 0.4981, 0.5150,
         0.4944, 0.4969],
        [0.5131, 0.4971, 0.4978, 0.5027, 0.5045, 0.4920, 0.5105, 0.4977, 0.4959,
         0.5026, 0.4858, 0.4986, 0.4845, 0.5019, 0.4999, 0.4977, 0.4992, 0.5166,
         0.4946, 0.4967],
        [0.5126, 0.4976, 0.4972, 0.5034, 0.5042, 0.4913, 0.5108, 0.4966, 0.4962,
         0.5027, 0.4841, 0.4968, 0.4844, 0.5022, 0.5008, 0.4976, 0.5012, 0.5146,
         0.4955, 0.4972],
        [0.5129, 0.4982, 0.4965, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4964,
         0.5030, 0.4866, 0.4976, 0.4825, 0.5029, 0.5004, 0.4995, 0.5002, 0.5152,
         0.4956, 0.4970],
        [0.5136, 0.4975, 0.4966, 0.5032, 0.5048, 0.4926, 0.5079, 0.4966, 0.4954,
         0.5024, 0.4856, 0.4962, 0.4850, 0.5034, 0.5017, 0.5006, 0.4997, 0.5148,
         0.4953, 0.4982],
        [0.5115, 0.4965, 0.4981, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4959,
         0.5025, 0.4874, 0.4971, 0.4851, 0.5028, 0.5009, 0.4991, 0.5001, 0.5141,
         0.4955, 0.4997],
        [0.5120, 0.4974, 0.4952, 0.5009, 0.5041, 0.4922, 0.5082, 0.4945, 0.4968,
         0.5021, 0.4860, 0.4982, 0.4865, 0.5013, 0.5009, 0.4988, 0.5000, 0.5153,
         0.4959, 0.4990],
        [0.5128, 0.4980, 0.4949, 0.5010, 0.5050, 0.4924, 0.5094, 0.4966, 0.4967,
         0.5018, 0.4884, 0.4952, 0.4854, 0.5014, 0.5011, 0.4973, 0.5007, 0.5144,
         0.4958, 0.4996],
        [0.5127, 0.4984, 0.4938, 0.5025, 0.5051, 0.4935, 0.5078, 0.4987, 0.4964,
         0.5031, 0.4884, 0.4977, 0.4858, 0.5001, 0.5012, 0.4982, 0.4997, 0.5154,
         0.4972, 0.5006]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
        0.5027, 0.4854, 0.4978, 0.4838, 0.5020, 0.5023, 0.4982, 0.4991, 0.5144,
        0.4936, 0.4968], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4958, 0.4958, 0.5043, 0.5037, 0.4931, 0.5104, 0.4964, 0.4959,
        0.5024, 0.4849, 0.4967, 0.4854, 0.5016, 0.5001, 0.4979, 0.4981, 0.5150,
        0.4944, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4971, 0.4978, 0.5027, 0.5045, 0.4920, 0.5105, 0.4977, 0.4959,
        0.5026, 0.4858, 0.4986, 0.4845, 0.5019, 0.4999, 0.4977, 0.4992, 0.5166,
        0.4946, 0.4967], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4976, 0.4972, 0.5034, 0.5042, 0.4913, 0.5108, 0.4966, 0.4962,
        0.5027, 0.4841, 0.4968, 0.4844, 0.5022, 0.5008, 0.4976, 0.5012, 0.5146,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5129, 0.4982, 0.4965, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4964,
        0.5030, 0.4866, 0.4976, 0.4825, 0.5029, 0.5004, 0.4995, 0.5002, 0.5152,
        0.4956, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5136, 0.4975, 0.4966, 0.5032, 0.5048, 0.4926, 0.5079, 0.4966, 0.4954,
        0.5024, 0.4856, 0.4962, 0.4850, 0.5034, 0.5017, 0.5006, 0.4997, 0.5148,
        0.4953, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4965, 0.4981, 0.5026, 0.5038, 0.4921, 0.5087, 0.4963, 0.4959,
        0.5025, 0.4874, 0.4971, 0.4851, 0.5028, 0.5009, 0.4991, 0.5001, 0.5141,
        0.4955, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4974, 0.4952, 0.5009, 0.5041, 0.4922, 0.5082, 0.4945, 0.4968,
        0.5021, 0.4860, 0.4982, 0.4865, 0.5013, 0.5009, 0.4988, 0.5000, 0.5153,
        0.4959, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4980, 0.4949, 0.5010, 0.5050, 0.4924, 0.5094, 0.4966, 0.4967,
        0.5018, 0.4884, 0.4952, 0.4854, 0.5014, 0.5011, 0.4973, 0.5007, 0.5144,
        0.4958, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4984, 0.4938, 0.5025, 0.5051, 0.4935, 0.5078, 0.4987, 0.4964,
        0.5031, 0.4884, 0.4977, 0.4858, 0.5001, 0.5012, 0.4982, 0.4997, 0.5154,
        0.4972, 0.5006], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [5, 9, 1, 8, 3, 2, 4, 6, 7, 0]
replay_buffer._size: [10500 10500 10500 10500 10500 10500 10500 10500 10500 10500]
snapshot at best
2023-08-12 10:41:24,321 MainThread INFO: EPOCH:68
2023-08-12 10:41:24,321 MainThread INFO: Time Consumed:0.9353008270263672s
2023-08-12 10:41:24,321 MainThread INFO: Total Frames:103500s
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 69/80 [01:17<00:13,  1.24s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1599.12138
Train_Epoch_Reward                    6195.76291
Running_Training_Average_Rewards      737.34285
Explore_Time                          0.00292
Train___Time                          0.30158
Eval____Time                          0.00250
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -45.52386
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.20693
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.99966
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.35568
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.28956
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.99766
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -21.06232
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16232.46640
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.27404
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.54288
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.68698      0.93412    10.34223    7.86803
alpha_0                               0.90204      0.00038    0.90258     0.90150
alpha_1                               0.90204      0.00038    0.90258     0.90150
alpha_2                               0.90206      0.00038    0.90260     0.90152
alpha_3                               0.90205      0.00038    0.90259     0.90151
alpha_4                               0.90206      0.00038    0.90260     0.90151
alpha_5                               0.90205      0.00038    0.90259     0.90151
alpha_6                               0.90207      0.00038    0.90261     0.90152
alpha_7                               0.90204      0.00038    0.90259     0.90150
alpha_8                               0.90204      0.00038    0.90258     0.90149
alpha_9                               0.90206      0.00038    0.90260     0.90152
Alpha_loss                            -0.69168     0.00322    -0.68716    -0.69603
Training/policy_loss                  -2.91158     0.00639    -2.90253    -2.91929
Training/qf1_loss                     1878.08015   426.59933  2191.14941  1087.68823
Training/qf2_loss                     1871.38140   425.96730  2183.61499  1082.17432
Training/pf_norm                      0.10897      0.02002    0.12989     0.07565
Training/qf1_norm                     35.17347     2.71939    36.96832    29.82089
Training/qf2_norm                     54.60311     4.81164    58.92752    45.57174
log_std/mean                          -0.13352     0.00020    -0.13328    -0.13384
log_std/std                           0.00798      0.00002    0.00801     0.00794
log_std/max                           -0.11806     0.00052    -0.11736    -0.11892
log_std/min                           -0.15336     0.00055    -0.15240    -0.15389
log_probs/mean                        -2.72937     0.00414    -2.72473    -2.73590
log_probs/std                         0.23097      0.00408    0.23585     0.22350
log_probs/max                         -2.11852     0.02514    -2.09408    -2.16154
log_probs/min                         -4.71978     0.15081    -4.51404    -4.92523
mean/mean                             0.00093      0.00002    0.00096     0.00090
mean/std                              0.01477      0.00006    0.01483     0.01466
mean/max                              0.02752      0.00018    0.02773     0.02729
mean/min                              -0.01896     0.00013    -0.01874    -0.01909
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [1, 4, 5, 7, 0, 6, 8, 9, 2, 3]
replay_buffer._size: [10650 10650 10650 10650 10650 10650 10650 10650 10650 10650]
2023-08-12 10:41:24,880 MainThread INFO: EPOCH:69
2023-08-12 10:41:24,881 MainThread INFO: Time Consumed:0.4210855960845947s
2023-08-12 10:41:24,881 MainThread INFO: Total Frames:105000s
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 70/80 [01:18<00:10,  1.04s/it]------------------------------------  -----------  ----------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1592.42360
Train_Epoch_Reward                    16237.89583
Running_Training_Average_Rewards      1118.79618
Explore_Time                          0.00417
Train___Time                          0.40964
Eval____Time                          0.00252
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.64680
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.12265
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.95824
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.26436
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.13291
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.86605
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.93230
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16167.91278
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.18490
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.56863
mean_success_rate                     0.00000

Name                                  Mean         Std         Max         Min
Reward_Mean                           10.18479     0.50959     10.89647    9.39797
alpha_0                               0.90069      0.00038     0.90123     0.90015
alpha_1                               0.90068      0.00038     0.90123     0.90014
alpha_2                               0.90070      0.00038     0.90125     0.90016
alpha_3                               0.90070      0.00038     0.90124     0.90016
alpha_4                               0.90070      0.00038     0.90124     0.90016
alpha_5                               0.90070      0.00038     0.90124     0.90016
alpha_6                               0.90071      0.00038     0.90125     0.90017
alpha_7                               0.90069      0.00038     0.90123     0.90015
alpha_8                               0.90068      0.00038     0.90122     0.90014
alpha_9                               0.90071      0.00038     0.90125     0.90017
Alpha_loss                            -0.70276     0.00303     -0.69809    -0.70663
Training/policy_loss                  -2.93040     0.00674     -2.92166    -2.94228
Training/qf1_loss                     2921.01431   1356.18039  5613.99023  1975.69458
Training/qf2_loss                     2914.11526   1356.35596  5607.46875  1968.81677
Training/pf_norm                      0.12713      0.01957     0.15677     0.09668
Training/qf1_norm                     37.45738     1.37841     39.67882    35.39402
Training/qf2_norm                     57.24979     1.10832     58.20127    55.25912
log_std/mean                          -0.13422     0.00020     -0.13393    -0.13449
log_std/std                           0.00795      0.00004     0.00803     0.00791
log_std/max                           -0.11848     0.00025     -0.11809    -0.11876
log_std/min                           -0.15334     0.00053     -0.15278    -0.15403
log_probs/mean                        -2.73876     0.00703     -2.73272    -2.75238
log_probs/std                         0.23787      0.01553     0.25859     0.22046
log_probs/max                         -2.12532     0.04253     -2.06404    -2.18575
log_probs/min                         -4.78998     0.47618     -4.26494    -5.45171
mean/mean                             0.00092      0.00006     0.00099     0.00083
mean/std                              0.01477      0.00005     0.01482     0.01469
mean/max                              0.02744      0.00025     0.02775     0.02703
mean/min                              -0.01881     0.00006     -0.01871    -0.01890
------------------------------------  -----------  ----------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4971, 0.5068, 0.4990, 0.5069, 0.5065, 0.4889, 0.4950, 0.5019, 0.4998,
         0.5026, 0.5102, 0.5004, 0.4959, 0.4886, 0.4920, 0.5075, 0.4922, 0.5101,
         0.4964, 0.5142],
        [0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4900, 0.4957, 0.5030, 0.5022,
         0.5031, 0.5085, 0.4990, 0.4960, 0.4882, 0.4900, 0.5076, 0.4916, 0.5090,
         0.4966, 0.5124],
        [0.4978, 0.5078, 0.4984, 0.5069, 0.5059, 0.4891, 0.4961, 0.5008, 0.5037,
         0.5018, 0.5073, 0.4986, 0.4960, 0.4882, 0.4897, 0.5093, 0.4931, 0.5095,
         0.4961, 0.5146],
        [0.4982, 0.5063, 0.4989, 0.5069, 0.5060, 0.4896, 0.4948, 0.5005, 0.5022,
         0.5037, 0.5074, 0.4985, 0.4966, 0.4879, 0.4897, 0.5076, 0.4923, 0.5103,
         0.4940, 0.5139],
        [0.4984, 0.5078, 0.4984, 0.5052, 0.5052, 0.4889, 0.4952, 0.4987, 0.5018,
         0.5030, 0.5088, 0.4990, 0.4961, 0.4882, 0.4915, 0.5061, 0.4947, 0.5098,
         0.4954, 0.5133],
        [0.4972, 0.5050, 0.4977, 0.5060, 0.5050, 0.4884, 0.4946, 0.5000, 0.5008,
         0.5031, 0.5079, 0.4996, 0.4951, 0.4879, 0.4901, 0.5082, 0.4952, 0.5085,
         0.4954, 0.5131],
        [0.4976, 0.5056, 0.4984, 0.5048, 0.5060, 0.4880, 0.4967, 0.5008, 0.5028,
         0.5021, 0.5099, 0.4996, 0.4958, 0.4874, 0.4906, 0.5083, 0.4949, 0.5080,
         0.4964, 0.5129],
        [0.4977, 0.5048, 0.4984, 0.5051, 0.5064, 0.4882, 0.4951, 0.5012, 0.5028,
         0.5019, 0.5099, 0.4996, 0.4963, 0.4889, 0.4919, 0.5077, 0.4930, 0.5064,
         0.4933, 0.5126],
        [0.4980, 0.5047, 0.4977, 0.5047, 0.5081, 0.4879, 0.4960, 0.5004, 0.5017,
         0.5010, 0.5068, 0.5011, 0.4971, 0.4888, 0.4924, 0.5083, 0.4928, 0.5081,
         0.4941, 0.5142],
        [0.4984, 0.5056, 0.4994, 0.5047, 0.5074, 0.4880, 0.4963, 0.5016, 0.5018,
         0.5040, 0.5075, 0.5003, 0.4943, 0.4909, 0.4929, 0.5094, 0.4940, 0.5048,
         0.4950, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4971, 0.5068, 0.4990, 0.5069, 0.5065, 0.4889, 0.4950, 0.5019, 0.4998,
        0.5026, 0.5102, 0.5004, 0.4959, 0.4886, 0.4920, 0.5075, 0.4922, 0.5101,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4900, 0.4957, 0.5030, 0.5022,
        0.5031, 0.5085, 0.4990, 0.4960, 0.4882, 0.4900, 0.5076, 0.4916, 0.5090,
        0.4966, 0.5124], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5078, 0.4984, 0.5069, 0.5059, 0.4891, 0.4961, 0.5008, 0.5037,
        0.5018, 0.5073, 0.4986, 0.4960, 0.4882, 0.4897, 0.5093, 0.4931, 0.5095,
        0.4961, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5063, 0.4989, 0.5069, 0.5060, 0.4896, 0.4948, 0.5005, 0.5022,
        0.5037, 0.5074, 0.4985, 0.4966, 0.4879, 0.4897, 0.5076, 0.4923, 0.5103,
        0.4940, 0.5139], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5078, 0.4984, 0.5052, 0.5052, 0.4889, 0.4952, 0.4987, 0.5018,
        0.5030, 0.5088, 0.4990, 0.4961, 0.4882, 0.4915, 0.5061, 0.4947, 0.5098,
        0.4954, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4972, 0.5050, 0.4977, 0.5060, 0.5050, 0.4884, 0.4946, 0.5000, 0.5008,
        0.5031, 0.5079, 0.4996, 0.4951, 0.4879, 0.4901, 0.5082, 0.4952, 0.5085,
        0.4954, 0.5131], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5056, 0.4984, 0.5048, 0.5060, 0.4880, 0.4967, 0.5008, 0.5028,
        0.5021, 0.5099, 0.4996, 0.4958, 0.4874, 0.4906, 0.5083, 0.4949, 0.5080,
        0.4964, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4984, 0.5051, 0.5064, 0.4882, 0.4951, 0.5012, 0.5028,
        0.5019, 0.5099, 0.4996, 0.4963, 0.4889, 0.4919, 0.5077, 0.4930, 0.5064,
        0.4933, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4977, 0.5047, 0.5081, 0.4879, 0.4960, 0.5004, 0.5017,
        0.5010, 0.5068, 0.5011, 0.4971, 0.4888, 0.4924, 0.5083, 0.4928, 0.5081,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5056, 0.4994, 0.5047, 0.5074, 0.4880, 0.4963, 0.5016, 0.5018,
        0.5040, 0.5075, 0.5003, 0.4943, 0.4909, 0.4929, 0.5094, 0.4940, 0.5048,
        0.4950, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4892, 0.5120, 0.4863, 0.4852, 0.4943, 0.4897, 0.4941, 0.4987,
         0.5075, 0.5029, 0.5006, 0.5026, 0.4920, 0.5047, 0.4914, 0.4858, 0.4983,
         0.5077, 0.4946],
        [0.5089, 0.4897, 0.5108, 0.4849, 0.4855, 0.4930, 0.4884, 0.4953, 0.4983,
         0.5052, 0.5038, 0.5000, 0.5035, 0.4919, 0.5031, 0.4889, 0.4864, 0.4981,
         0.5071, 0.4932],
        [0.5100, 0.4905, 0.5108, 0.4866, 0.4853, 0.4944, 0.4891, 0.4961, 0.5005,
         0.5070, 0.5047, 0.5004, 0.5059, 0.4921, 0.5042, 0.4907, 0.4873, 0.4980,
         0.5069, 0.4925],
        [0.5079, 0.4895, 0.5118, 0.4860, 0.4872, 0.4935, 0.4902, 0.4950, 0.4981,
         0.5086, 0.5039, 0.5003, 0.5050, 0.4917, 0.5043, 0.4906, 0.4862, 0.4982,
         0.5056, 0.4924],
        [0.5094, 0.4911, 0.5128, 0.4860, 0.4871, 0.4931, 0.4908, 0.4925, 0.4958,
         0.5068, 0.5046, 0.5001, 0.5031, 0.4915, 0.5040, 0.4895, 0.4853, 0.4961,
         0.5055, 0.4935],
        [0.5110, 0.4916, 0.5101, 0.4866, 0.4861, 0.4948, 0.4909, 0.4939, 0.4970,
         0.5068, 0.5060, 0.5006, 0.5040, 0.4900, 0.5041, 0.4898, 0.4851, 0.4970,
         0.5060, 0.4929],
        [0.5097, 0.4917, 0.5125, 0.4879, 0.4860, 0.4938, 0.4889, 0.4941, 0.4978,
         0.5073, 0.5064, 0.4996, 0.5049, 0.4904, 0.5038, 0.4893, 0.4846, 0.4982,
         0.5072, 0.4933],
        [0.5100, 0.4916, 0.5117, 0.4853, 0.4869, 0.4925, 0.4891, 0.4945, 0.4998,
         0.5072, 0.5069, 0.5009, 0.5035, 0.4894, 0.5049, 0.4915, 0.4852, 0.4955,
         0.5076, 0.4935],
        [0.5113, 0.4927, 0.5111, 0.4852, 0.4888, 0.4928, 0.4892, 0.4942, 0.4986,
         0.5072, 0.5050, 0.4974, 0.5043, 0.4896, 0.5037, 0.4911, 0.4861, 0.4965,
         0.5082, 0.4938],
        [0.5114, 0.4924, 0.5119, 0.4869, 0.4886, 0.4927, 0.4893, 0.4931, 0.4999,
         0.5053, 0.5053, 0.4983, 0.5037, 0.4888, 0.5004, 0.4933, 0.4836, 0.4971,
         0.5059, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4892, 0.5120, 0.4863, 0.4852, 0.4943, 0.4897, 0.4941, 0.4987,
        0.5075, 0.5029, 0.5006, 0.5026, 0.4920, 0.5047, 0.4914, 0.4858, 0.4983,
        0.5077, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4897, 0.5108, 0.4849, 0.4855, 0.4930, 0.4884, 0.4953, 0.4983,
        0.5052, 0.5038, 0.5000, 0.5035, 0.4919, 0.5031, 0.4889, 0.4864, 0.4981,
        0.5071, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5100, 0.4905, 0.5108, 0.4866, 0.4853, 0.4944, 0.4891, 0.4961, 0.5005,
        0.5070, 0.5047, 0.5004, 0.5059, 0.4921, 0.5042, 0.4907, 0.4873, 0.4980,
        0.5069, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4895, 0.5118, 0.4860, 0.4872, 0.4935, 0.4902, 0.4950, 0.4981,
        0.5086, 0.5039, 0.5003, 0.5050, 0.4917, 0.5043, 0.4906, 0.4862, 0.4982,
        0.5056, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4911, 0.5128, 0.4860, 0.4871, 0.4931, 0.4908, 0.4925, 0.4958,
        0.5068, 0.5046, 0.5001, 0.5031, 0.4915, 0.5040, 0.4895, 0.4853, 0.4961,
        0.5055, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4916, 0.5101, 0.4866, 0.4861, 0.4948, 0.4909, 0.4939, 0.4970,
        0.5068, 0.5060, 0.5006, 0.5040, 0.4900, 0.5041, 0.4898, 0.4851, 0.4970,
        0.5060, 0.4929], grad_fn=<UnbindBackward>), tensor([0.5097, 0.4917, 0.5125, 0.4879, 0.4860, 0.4938, 0.4889, 0.4941, 0.4978,
        0.5073, 0.5064, 0.4996, 0.5049, 0.4904, 0.5038, 0.4893, 0.4846, 0.4982,
        0.5072, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5100, 0.4916, 0.5117, 0.4853, 0.4869, 0.4925, 0.4891, 0.4945, 0.4998,
        0.5072, 0.5069, 0.5009, 0.5035, 0.4894, 0.5049, 0.4915, 0.4852, 0.4955,
        0.5076, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4927, 0.5111, 0.4852, 0.4888, 0.4928, 0.4892, 0.4942, 0.4986,
        0.5072, 0.5050, 0.4974, 0.5043, 0.4896, 0.5037, 0.4911, 0.4861, 0.4965,
        0.5082, 0.4938], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4924, 0.5119, 0.4869, 0.4886, 0.4927, 0.4893, 0.4931, 0.4999,
        0.5053, 0.5053, 0.4983, 0.5037, 0.4888, 0.5004, 0.4933, 0.4836, 0.4971,
        0.5059, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5124, 0.4962, 0.4968, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
         0.5027, 0.4854, 0.4978, 0.4838, 0.5019, 0.5023, 0.4982, 0.4991, 0.5145,
         0.4936, 0.4969],
        [0.5115, 0.4957, 0.4958, 0.5043, 0.5037, 0.4932, 0.5104, 0.4965, 0.4959,
         0.5024, 0.4850, 0.4967, 0.4854, 0.5015, 0.5000, 0.4979, 0.4981, 0.5150,
         0.4944, 0.4969],
        [0.5131, 0.4970, 0.4977, 0.5026, 0.5045, 0.4920, 0.5104, 0.4977, 0.4959,
         0.5026, 0.4857, 0.4986, 0.4846, 0.5019, 0.4998, 0.4978, 0.4992, 0.5165,
         0.4946, 0.4966],
        [0.5127, 0.4976, 0.4972, 0.5033, 0.5042, 0.4913, 0.5108, 0.4966, 0.4961,
         0.5027, 0.4840, 0.4968, 0.4844, 0.5022, 0.5007, 0.4976, 0.5012, 0.5146,
         0.4955, 0.4972],
        [0.5130, 0.4983, 0.4965, 0.5040, 0.5033, 0.4933, 0.5088, 0.4953, 0.4964,
         0.5030, 0.4866, 0.4977, 0.4825, 0.5030, 0.5003, 0.4996, 0.5002, 0.5153,
         0.4957, 0.4970],
        [0.5137, 0.4975, 0.4966, 0.5032, 0.5048, 0.4927, 0.5079, 0.4966, 0.4954,
         0.5024, 0.4856, 0.4962, 0.4850, 0.5035, 0.5016, 0.5006, 0.4997, 0.5149,
         0.4954, 0.4982],
        [0.5116, 0.4965, 0.4981, 0.5026, 0.5037, 0.4921, 0.5087, 0.4964, 0.4959,
         0.5026, 0.4874, 0.4971, 0.4851, 0.5028, 0.5009, 0.4991, 0.5001, 0.5141,
         0.4955, 0.4997],
        [0.5120, 0.4974, 0.4953, 0.5010, 0.5040, 0.4921, 0.5082, 0.4944, 0.4968,
         0.5021, 0.4860, 0.4982, 0.4864, 0.5013, 0.5010, 0.4988, 0.5000, 0.5153,
         0.4959, 0.4990],
        [0.5128, 0.4981, 0.4949, 0.5010, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
         0.5018, 0.4883, 0.4953, 0.4854, 0.5014, 0.5012, 0.4973, 0.5006, 0.5144,
         0.4958, 0.4996],
        [0.5127, 0.4985, 0.4938, 0.5025, 0.5051, 0.4935, 0.5078, 0.4987, 0.4964,
         0.5031, 0.4884, 0.4976, 0.4858, 0.5001, 0.5013, 0.4982, 0.4997, 0.5154,
         0.4972, 0.5007]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5124, 0.4962, 0.4968, 0.5027, 0.5068, 0.4927, 0.5094, 0.4954, 0.4967,
        0.5027, 0.4854, 0.4978, 0.4838, 0.5019, 0.5023, 0.4982, 0.4991, 0.5145,
        0.4936, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4957, 0.4958, 0.5043, 0.5037, 0.4932, 0.5104, 0.4965, 0.4959,
        0.5024, 0.4850, 0.4967, 0.4854, 0.5015, 0.5000, 0.4979, 0.4981, 0.5150,
        0.4944, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4970, 0.4977, 0.5026, 0.5045, 0.4920, 0.5104, 0.4977, 0.4959,
        0.5026, 0.4857, 0.4986, 0.4846, 0.5019, 0.4998, 0.4978, 0.4992, 0.5165,
        0.4946, 0.4966], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4976, 0.4972, 0.5033, 0.5042, 0.4913, 0.5108, 0.4966, 0.4961,
        0.5027, 0.4840, 0.4968, 0.4844, 0.5022, 0.5007, 0.4976, 0.5012, 0.5146,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5130, 0.4983, 0.4965, 0.5040, 0.5033, 0.4933, 0.5088, 0.4953, 0.4964,
        0.5030, 0.4866, 0.4977, 0.4825, 0.5030, 0.5003, 0.4996, 0.5002, 0.5153,
        0.4957, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4975, 0.4966, 0.5032, 0.5048, 0.4927, 0.5079, 0.4966, 0.4954,
        0.5024, 0.4856, 0.4962, 0.4850, 0.5035, 0.5016, 0.5006, 0.4997, 0.5149,
        0.4954, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4965, 0.4981, 0.5026, 0.5037, 0.4921, 0.5087, 0.4964, 0.4959,
        0.5026, 0.4874, 0.4971, 0.4851, 0.5028, 0.5009, 0.4991, 0.5001, 0.5141,
        0.4955, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4974, 0.4953, 0.5010, 0.5040, 0.4921, 0.5082, 0.4944, 0.4968,
        0.5021, 0.4860, 0.4982, 0.4864, 0.5013, 0.5010, 0.4988, 0.5000, 0.5153,
        0.4959, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4981, 0.4949, 0.5010, 0.5049, 0.4924, 0.5094, 0.4966, 0.4967,
        0.5018, 0.4883, 0.4953, 0.4854, 0.5014, 0.5012, 0.4973, 0.5006, 0.5144,
        0.4958, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4938, 0.5025, 0.5051, 0.4935, 0.5078, 0.4987, 0.4964,
        0.5031, 0.4884, 0.4976, 0.4858, 0.5001, 0.5013, 0.4982, 0.4997, 0.5154,
        0.4972, 0.5007], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [2, 7, 4, 8, 1, 9, 6, 0, 5, 3]
replay_buffer._size: [10800 10800 10800 10800 10800 10800 10800 10800 10800 10800]
2023-08-12 10:41:25,743 MainThread INFO: EPOCH:70
2023-08-12 10:41:25,743 MainThread INFO: Time Consumed:0.3269164562225342s
2023-08-12 10:41:25,743 MainThread INFO: Total Frames:106500s
 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 71/80 [01:19<00:09,  1.01s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1591.47255
Train_Epoch_Reward                    27249.04013
Running_Training_Average_Rewards      1656.08996
Explore_Time                          0.00306
Train___Time                          0.32002
Eval____Time                          0.00316
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -49.25880
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.12547
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.93030
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.22784
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.04562
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.79259
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.86151
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16158.70214
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.18774
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.54674
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           10.09421     0.73822    11.42035    9.25882
alpha_0                               0.89934      0.00038    0.89988     0.89879
alpha_1                               0.89933      0.00038    0.89987     0.89879
alpha_2                               0.89935      0.00038    0.89989     0.89881
alpha_3                               0.89935      0.00038    0.89989     0.89881
alpha_4                               0.89935      0.00038    0.89989     0.89881
alpha_5                               0.89934      0.00038    0.89989     0.89880
alpha_6                               0.89936      0.00038    0.89990     0.89882
alpha_7                               0.89934      0.00038    0.89988     0.89880
alpha_8                               0.89933      0.00038    0.89987     0.89879
alpha_9                               0.89936      0.00038    0.89990     0.89882
Alpha_loss                            -0.71222     0.00274    -0.70800    -0.71590
Training/policy_loss                  -2.93694     0.00336    -2.93068    -2.94011
Training/qf1_loss                     2192.26194   381.80221  2857.39917  1710.97925
Training/qf2_loss                     2185.32307   381.19721  2849.23901  1704.70740
Training/pf_norm                      0.12497      0.03484    0.17364     0.06598
Training/qf1_norm                     37.28924     2.31013    41.52544    34.86654
Training/qf2_norm                     57.02067     4.27145    64.95248    52.41607
log_std/mean                          -0.13472     0.00012    -0.13453    -0.13484
log_std/std                           0.00799      0.00004    0.00805     0.00792
log_std/max                           -0.11888     0.00018    -0.11867    -0.11916
log_std/min                           -0.15410     0.00037    -0.15349    -0.15452
log_probs/mean                        -2.73248     0.00265    -2.72909    -2.73675
log_probs/std                         0.23230      0.00977    0.25017     0.22181
log_probs/max                         -2.13573     0.03453    -2.07807    -2.17932
log_probs/min                         -5.02261     0.68071    -4.33075    -6.13245
mean/mean                             0.00129      0.00020    0.00159     0.00105
mean/std                              0.01464      0.00008    0.01473     0.01451
mean/max                              0.02729      0.00018    0.02758     0.02714
mean/min                              -0.01838     0.00035    -0.01782    -0.01877
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [0, 7, 9, 1, 2, 8, 4, 3, 5, 6]
replay_buffer._size: [10950 10950 10950 10950 10950 10950 10950 10950 10950 10950]
2023-08-12 10:41:26,494 MainThread INFO: EPOCH:71
2023-08-12 10:41:26,495 MainThread INFO: Time Consumed:0.48108363151550293s
2023-08-12 10:41:26,495 MainThread INFO: Total Frames:108000s
 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 72/80 [01:20<00:07,  1.11it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1582.53739
Train_Epoch_Reward                    4440.41632
Running_Training_Average_Rewards      1597.57841
Explore_Time                          0.02678
Train___Time                          0.41830
Eval____Time                          0.00288
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.81858
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.13831
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.86757
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.24673
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.97651
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.73922
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.81026
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16067.75590
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.20232
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.58251
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.76182      0.68974    10.80021    8.92776
alpha_0                               0.89798      0.00038    0.89852     0.89744
alpha_1                               0.89798      0.00038    0.89852     0.89744
alpha_2                               0.89800      0.00038    0.89854     0.89746
alpha_3                               0.89800      0.00038    0.89854     0.89746
alpha_4                               0.89800      0.00038    0.89854     0.89746
alpha_5                               0.89799      0.00038    0.89853     0.89745
alpha_6                               0.89801      0.00038    0.89855     0.89748
alpha_7                               0.89799      0.00038    0.89853     0.89745
alpha_8                               0.89798      0.00038    0.89852     0.89744
alpha_9                               0.89801      0.00038    0.89855     0.89747
Alpha_loss                            -0.72266     0.00241    -0.71900    -0.72540
Training/policy_loss                  -2.95020     0.00312    -2.94567    -2.95464
Training/qf1_loss                     1877.33174   388.88219  2439.52393  1352.45667
Training/qf2_loss                     1870.04336   388.44906  2431.34790  1345.36841
Training/pf_norm                      0.09706      0.01447    0.11737     0.07373
Training/qf1_norm                     37.16105     1.92393    39.95663    34.75441
Training/qf2_norm                     57.78126     3.46037    63.36633    53.19688
log_std/mean                          -0.13492     0.00003    -0.13488    -0.13495
log_std/std                           0.00805      0.00006    0.00812     0.00796
log_std/max                           -0.11839     0.00018    -0.11807    -0.11860
log_std/min                           -0.15474     0.00052    -0.15383    -0.15540
log_probs/mean                        -2.73560     0.00647    -2.72344    -2.74102
log_probs/std                         0.23282      0.00814    0.24537     0.22255
log_probs/max                         -2.12718     0.04116    -2.06527    -2.19307
log_probs/min                         -4.67285     0.26521    -4.22661    -5.05158
mean/mean                             0.00200      0.00014    0.00215     0.00175
mean/std                              0.01439      0.00005    0.01447     0.01435
mean/max                              0.02754      0.00013    0.02777     0.02738
mean/min                              -0.01702     0.00020    -0.01681    -0.01740
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4970, 0.5069, 0.4990, 0.5069, 0.5065, 0.4889, 0.4949, 0.5019, 0.4999,
         0.5026, 0.5102, 0.5003, 0.4959, 0.4886, 0.4920, 0.5076, 0.4922, 0.5101,
         0.4963, 0.5142],
        [0.4981, 0.5076, 0.4990, 0.5064, 0.5066, 0.4901, 0.4956, 0.5029, 0.5022,
         0.5032, 0.5085, 0.4989, 0.4960, 0.4882, 0.4900, 0.5075, 0.4915, 0.5091,
         0.4965, 0.5125],
        [0.4977, 0.5078, 0.4984, 0.5069, 0.5058, 0.4891, 0.4960, 0.5008, 0.5037,
         0.5018, 0.5073, 0.4985, 0.4960, 0.4882, 0.4897, 0.5092, 0.4931, 0.5095,
         0.4961, 0.5146],
        [0.4983, 0.5064, 0.4989, 0.5068, 0.5060, 0.4896, 0.4947, 0.5005, 0.5022,
         0.5037, 0.5073, 0.4984, 0.4967, 0.4880, 0.4897, 0.5076, 0.4923, 0.5103,
         0.4939, 0.5140],
        [0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4888, 0.4951, 0.4987, 0.5019,
         0.5030, 0.5088, 0.4989, 0.4961, 0.4883, 0.4914, 0.5061, 0.4946, 0.5098,
         0.4953, 0.5133],
        [0.4973, 0.5050, 0.4977, 0.5059, 0.5050, 0.4884, 0.4946, 0.5001, 0.5008,
         0.5031, 0.5080, 0.4996, 0.4951, 0.4879, 0.4901, 0.5081, 0.4952, 0.5085,
         0.4954, 0.5131],
        [0.4977, 0.5056, 0.4983, 0.5048, 0.5061, 0.4881, 0.4967, 0.5008, 0.5027,
         0.5022, 0.5099, 0.4996, 0.4958, 0.4874, 0.4906, 0.5083, 0.4948, 0.5080,
         0.4962, 0.5129],
        [0.4977, 0.5048, 0.4985, 0.5051, 0.5064, 0.4882, 0.4951, 0.5012, 0.5028,
         0.5019, 0.5099, 0.4996, 0.4962, 0.4889, 0.4919, 0.5077, 0.4930, 0.5065,
         0.4934, 0.5126],
        [0.4980, 0.5047, 0.4978, 0.5046, 0.5081, 0.4879, 0.4960, 0.5004, 0.5017,
         0.5011, 0.5069, 0.5010, 0.4971, 0.4889, 0.4924, 0.5084, 0.4929, 0.5081,
         0.4941, 0.5142],
        [0.4984, 0.5056, 0.4994, 0.5047, 0.5074, 0.4880, 0.4963, 0.5016, 0.5018,
         0.5039, 0.5075, 0.5004, 0.4943, 0.4909, 0.4929, 0.5094, 0.4940, 0.5048,
         0.4949, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4970, 0.5069, 0.4990, 0.5069, 0.5065, 0.4889, 0.4949, 0.5019, 0.4999,
        0.5026, 0.5102, 0.5003, 0.4959, 0.4886, 0.4920, 0.5076, 0.4922, 0.5101,
        0.4963, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5076, 0.4990, 0.5064, 0.5066, 0.4901, 0.4956, 0.5029, 0.5022,
        0.5032, 0.5085, 0.4989, 0.4960, 0.4882, 0.4900, 0.5075, 0.4915, 0.5091,
        0.4965, 0.5125], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5078, 0.4984, 0.5069, 0.5058, 0.4891, 0.4960, 0.5008, 0.5037,
        0.5018, 0.5073, 0.4985, 0.4960, 0.4882, 0.4897, 0.5092, 0.4931, 0.5095,
        0.4961, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5064, 0.4989, 0.5068, 0.5060, 0.4896, 0.4947, 0.5005, 0.5022,
        0.5037, 0.5073, 0.4984, 0.4967, 0.4880, 0.4897, 0.5076, 0.4923, 0.5103,
        0.4939, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4888, 0.4951, 0.4987, 0.5019,
        0.5030, 0.5088, 0.4989, 0.4961, 0.4883, 0.4914, 0.5061, 0.4946, 0.5098,
        0.4953, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4973, 0.5050, 0.4977, 0.5059, 0.5050, 0.4884, 0.4946, 0.5001, 0.5008,
        0.5031, 0.5080, 0.4996, 0.4951, 0.4879, 0.4901, 0.5081, 0.4952, 0.5085,
        0.4954, 0.5131], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5056, 0.4983, 0.5048, 0.5061, 0.4881, 0.4967, 0.5008, 0.5027,
        0.5022, 0.5099, 0.4996, 0.4958, 0.4874, 0.4906, 0.5083, 0.4948, 0.5080,
        0.4962, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4985, 0.5051, 0.5064, 0.4882, 0.4951, 0.5012, 0.5028,
        0.5019, 0.5099, 0.4996, 0.4962, 0.4889, 0.4919, 0.5077, 0.4930, 0.5065,
        0.4934, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4978, 0.5046, 0.5081, 0.4879, 0.4960, 0.5004, 0.5017,
        0.5011, 0.5069, 0.5010, 0.4971, 0.4889, 0.4924, 0.5084, 0.4929, 0.5081,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5056, 0.4994, 0.5047, 0.5074, 0.4880, 0.4963, 0.5016, 0.5018,
        0.5039, 0.5075, 0.5004, 0.4943, 0.4909, 0.4929, 0.5094, 0.4940, 0.5048,
        0.4949, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4893, 0.5120, 0.4863, 0.4852, 0.4943, 0.4897, 0.4939, 0.4987,
         0.5075, 0.5028, 0.5006, 0.5026, 0.4921, 0.5047, 0.4913, 0.4858, 0.4982,
         0.5077, 0.4946],
        [0.5088, 0.4897, 0.5107, 0.4849, 0.4856, 0.4930, 0.4883, 0.4953, 0.4984,
         0.5052, 0.5037, 0.5000, 0.5035, 0.4920, 0.5032, 0.4888, 0.4864, 0.4981,
         0.5071, 0.4932],
        [0.5099, 0.4905, 0.5108, 0.4866, 0.4854, 0.4944, 0.4891, 0.4959, 0.5005,
         0.5070, 0.5046, 0.5005, 0.5059, 0.4921, 0.5042, 0.4906, 0.4873, 0.4979,
         0.5068, 0.4926],
        [0.5079, 0.4896, 0.5116, 0.4861, 0.4872, 0.4935, 0.4902, 0.4948, 0.4981,
         0.5086, 0.5039, 0.5003, 0.5050, 0.4917, 0.5043, 0.4906, 0.4862, 0.4982,
         0.5056, 0.4924],
        [0.5094, 0.4911, 0.5127, 0.4860, 0.4870, 0.4932, 0.4908, 0.4926, 0.4960,
         0.5068, 0.5045, 0.5002, 0.5032, 0.4915, 0.5040, 0.4894, 0.4853, 0.4962,
         0.5056, 0.4935],
        [0.5110, 0.4916, 0.5101, 0.4866, 0.4862, 0.4948, 0.4909, 0.4939, 0.4970,
         0.5068, 0.5060, 0.5006, 0.5040, 0.4900, 0.5042, 0.4898, 0.4851, 0.4970,
         0.5061, 0.4929],
        [0.5097, 0.4917, 0.5126, 0.4879, 0.4861, 0.4938, 0.4889, 0.4941, 0.4978,
         0.5073, 0.5066, 0.4997, 0.5050, 0.4904, 0.5038, 0.4895, 0.4845, 0.4982,
         0.5073, 0.4933],
        [0.5100, 0.4917, 0.5117, 0.4852, 0.4870, 0.4925, 0.4891, 0.4945, 0.4998,
         0.5072, 0.5069, 0.5009, 0.5035, 0.4893, 0.5049, 0.4916, 0.4852, 0.4955,
         0.5076, 0.4935],
        [0.5113, 0.4928, 0.5111, 0.4852, 0.4889, 0.4928, 0.4893, 0.4942, 0.4986,
         0.5072, 0.5051, 0.4973, 0.5043, 0.4895, 0.5036, 0.4912, 0.4861, 0.4965,
         0.5082, 0.4938],
        [0.5114, 0.4924, 0.5119, 0.4868, 0.4886, 0.4927, 0.4893, 0.4931, 0.4999,
         0.5053, 0.5053, 0.4983, 0.5037, 0.4888, 0.5005, 0.4934, 0.4836, 0.4971,
         0.5060, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4893, 0.5120, 0.4863, 0.4852, 0.4943, 0.4897, 0.4939, 0.4987,
        0.5075, 0.5028, 0.5006, 0.5026, 0.4921, 0.5047, 0.4913, 0.4858, 0.4982,
        0.5077, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4897, 0.5107, 0.4849, 0.4856, 0.4930, 0.4883, 0.4953, 0.4984,
        0.5052, 0.5037, 0.5000, 0.5035, 0.4920, 0.5032, 0.4888, 0.4864, 0.4981,
        0.5071, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5099, 0.4905, 0.5108, 0.4866, 0.4854, 0.4944, 0.4891, 0.4959, 0.5005,
        0.5070, 0.5046, 0.5005, 0.5059, 0.4921, 0.5042, 0.4906, 0.4873, 0.4979,
        0.5068, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4896, 0.5116, 0.4861, 0.4872, 0.4935, 0.4902, 0.4948, 0.4981,
        0.5086, 0.5039, 0.5003, 0.5050, 0.4917, 0.5043, 0.4906, 0.4862, 0.4982,
        0.5056, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4911, 0.5127, 0.4860, 0.4870, 0.4932, 0.4908, 0.4926, 0.4960,
        0.5068, 0.5045, 0.5002, 0.5032, 0.4915, 0.5040, 0.4894, 0.4853, 0.4962,
        0.5056, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5110, 0.4916, 0.5101, 0.4866, 0.4862, 0.4948, 0.4909, 0.4939, 0.4970,
        0.5068, 0.5060, 0.5006, 0.5040, 0.4900, 0.5042, 0.4898, 0.4851, 0.4970,
        0.5061, 0.4929], grad_fn=<UnbindBackward>), tensor([0.5097, 0.4917, 0.5126, 0.4879, 0.4861, 0.4938, 0.4889, 0.4941, 0.4978,
        0.5073, 0.5066, 0.4997, 0.5050, 0.4904, 0.5038, 0.4895, 0.4845, 0.4982,
        0.5073, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5100, 0.4917, 0.5117, 0.4852, 0.4870, 0.4925, 0.4891, 0.4945, 0.4998,
        0.5072, 0.5069, 0.5009, 0.5035, 0.4893, 0.5049, 0.4916, 0.4852, 0.4955,
        0.5076, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4928, 0.5111, 0.4852, 0.4889, 0.4928, 0.4893, 0.4942, 0.4986,
        0.5072, 0.5051, 0.4973, 0.5043, 0.4895, 0.5036, 0.4912, 0.4861, 0.4965,
        0.5082, 0.4938], grad_fn=<UnbindBackward>), tensor([0.5114, 0.4924, 0.5119, 0.4868, 0.4886, 0.4927, 0.4893, 0.4931, 0.4999,
        0.5053, 0.5053, 0.4983, 0.5037, 0.4888, 0.5005, 0.4934, 0.4836, 0.4971,
        0.5060, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5123, 0.4963, 0.4967, 0.5028, 0.5068, 0.4927, 0.5095, 0.4953, 0.4967,
         0.5027, 0.4855, 0.4978, 0.4838, 0.5019, 0.5024, 0.4981, 0.4991, 0.5144,
         0.4936, 0.4969],
        [0.5115, 0.4957, 0.4958, 0.5043, 0.5037, 0.4930, 0.5105, 0.4964, 0.4959,
         0.5024, 0.4850, 0.4966, 0.4854, 0.5016, 0.5002, 0.4978, 0.4981, 0.5150,
         0.4944, 0.4969],
        [0.5131, 0.4971, 0.4978, 0.5027, 0.5045, 0.4920, 0.5106, 0.4977, 0.4959,
         0.5026, 0.4858, 0.4985, 0.4845, 0.5019, 0.4999, 0.4978, 0.4993, 0.5165,
         0.4946, 0.4966],
        [0.5126, 0.4977, 0.4972, 0.5034, 0.5042, 0.4913, 0.5109, 0.4966, 0.4961,
         0.5027, 0.4840, 0.4968, 0.4844, 0.5022, 0.5009, 0.4977, 0.5013, 0.5145,
         0.4955, 0.4972],
        [0.5130, 0.4983, 0.4965, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4963,
         0.5030, 0.4866, 0.4976, 0.4825, 0.5030, 0.5004, 0.4996, 0.5002, 0.5153,
         0.4957, 0.4970],
        [0.5137, 0.4975, 0.4967, 0.5033, 0.5047, 0.4926, 0.5079, 0.4966, 0.4954,
         0.5024, 0.4856, 0.4962, 0.4849, 0.5035, 0.5016, 0.5006, 0.4997, 0.5149,
         0.4954, 0.4982],
        [0.5116, 0.4966, 0.4982, 0.5027, 0.5038, 0.4920, 0.5087, 0.4963, 0.4959,
         0.5025, 0.4874, 0.4971, 0.4851, 0.5028, 0.5010, 0.4992, 0.5001, 0.5141,
         0.4955, 0.4998],
        [0.5120, 0.4974, 0.4953, 0.5010, 0.5041, 0.4921, 0.5082, 0.4945, 0.4968,
         0.5020, 0.4860, 0.4982, 0.4864, 0.5014, 0.5010, 0.4988, 0.5000, 0.5153,
         0.4959, 0.4990],
        [0.5127, 0.4979, 0.4949, 0.5011, 0.5050, 0.4924, 0.5094, 0.4965, 0.4966,
         0.5019, 0.4884, 0.4953, 0.4854, 0.5014, 0.5012, 0.4973, 0.5007, 0.5144,
         0.4958, 0.4997],
        [0.5127, 0.4985, 0.4937, 0.5025, 0.5052, 0.4935, 0.5078, 0.4987, 0.4964,
         0.5031, 0.4884, 0.4976, 0.4858, 0.5000, 0.5013, 0.4982, 0.4997, 0.5153,
         0.4972, 0.5007]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5123, 0.4963, 0.4967, 0.5028, 0.5068, 0.4927, 0.5095, 0.4953, 0.4967,
        0.5027, 0.4855, 0.4978, 0.4838, 0.5019, 0.5024, 0.4981, 0.4991, 0.5144,
        0.4936, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4957, 0.4958, 0.5043, 0.5037, 0.4930, 0.5105, 0.4964, 0.4959,
        0.5024, 0.4850, 0.4966, 0.4854, 0.5016, 0.5002, 0.4978, 0.4981, 0.5150,
        0.4944, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4971, 0.4978, 0.5027, 0.5045, 0.4920, 0.5106, 0.4977, 0.4959,
        0.5026, 0.4858, 0.4985, 0.4845, 0.5019, 0.4999, 0.4978, 0.4993, 0.5165,
        0.4946, 0.4966], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4977, 0.4972, 0.5034, 0.5042, 0.4913, 0.5109, 0.4966, 0.4961,
        0.5027, 0.4840, 0.4968, 0.4844, 0.5022, 0.5009, 0.4977, 0.5013, 0.5145,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5130, 0.4983, 0.4965, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4963,
        0.5030, 0.4866, 0.4976, 0.4825, 0.5030, 0.5004, 0.4996, 0.5002, 0.5153,
        0.4957, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4975, 0.4967, 0.5033, 0.5047, 0.4926, 0.5079, 0.4966, 0.4954,
        0.5024, 0.4856, 0.4962, 0.4849, 0.5035, 0.5016, 0.5006, 0.4997, 0.5149,
        0.4954, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4966, 0.4982, 0.5027, 0.5038, 0.4920, 0.5087, 0.4963, 0.4959,
        0.5025, 0.4874, 0.4971, 0.4851, 0.5028, 0.5010, 0.4992, 0.5001, 0.5141,
        0.4955, 0.4998], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4974, 0.4953, 0.5010, 0.5041, 0.4921, 0.5082, 0.4945, 0.4968,
        0.5020, 0.4860, 0.4982, 0.4864, 0.5014, 0.5010, 0.4988, 0.5000, 0.5153,
        0.4959, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4979, 0.4949, 0.5011, 0.5050, 0.4924, 0.5094, 0.4965, 0.4966,
        0.5019, 0.4884, 0.4953, 0.4854, 0.5014, 0.5012, 0.4973, 0.5007, 0.5144,
        0.4958, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4937, 0.5025, 0.5052, 0.4935, 0.5078, 0.4987, 0.4964,
        0.5031, 0.4884, 0.4976, 0.4858, 0.5000, 0.5013, 0.4982, 0.4997, 0.5153,
        0.4972, 0.5007], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [3, 9, 8, 6, 7, 1, 2, 4, 0, 5]
replay_buffer._size: [11100 11100 11100 11100 11100 11100 11100 11100 11100 11100]
2023-08-12 10:41:28,541 MainThread INFO: EPOCH:72
2023-08-12 10:41:28,541 MainThread INFO: Time Consumed:0.3320012092590332s
2023-08-12 10:41:28,541 MainThread INFO: Total Frames:109500s
 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 73/80 [01:22<00:08,  1.24s/it]------------------------------------  -----------  ---------  ----------  ---------
Name                                  Value
Running_Average_Rewards               1579.16962
Train_Epoch_Reward                    13139.34911
Running_Training_Average_Rewards      1494.29352
Explore_Time                          0.00478
Train___Time                          0.31871
Eval____Time                          0.00255
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.69891
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.02755
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.89501
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.22343
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.01446
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.77502
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.84663
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16036.91232
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.08812
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.64701
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.42882      1.11795    10.69653    7.72359
alpha_0                               0.89664      0.00038    0.89717     0.89610
alpha_1                               0.89663      0.00038    0.89717     0.89610
alpha_2                               0.89666      0.00038    0.89719     0.89612
alpha_3                               0.89665      0.00038    0.89719     0.89611
alpha_4                               0.89666      0.00038    0.89719     0.89612
alpha_5                               0.89665      0.00038    0.89719     0.89611
alpha_6                               0.89667      0.00038    0.89721     0.89613
alpha_7                               0.89664      0.00038    0.89718     0.89610
alpha_8                               0.89663      0.00038    0.89717     0.89609
alpha_9                               0.89666      0.00038    0.89720     0.89612
Alpha_loss                            -0.73229     0.00308    -0.72730    -0.73574
Training/policy_loss                  -2.95755     0.00719    -2.94461    -2.96642
Training/qf1_loss                     1766.00060   558.63084  2605.93750  999.73273
Training/qf2_loss                     1758.80439   557.84362  2598.26611  994.17871
Training/pf_norm                      0.12082      0.03509    0.17914     0.08526
Training/qf1_norm                     36.57038     3.64676    40.58411    30.80940
Training/qf2_norm                     56.72143     6.48175    64.35471    45.92604
log_std/mean                          -0.13514     0.00017    -0.13491    -0.13536
log_std/std                           0.00808      0.00002    0.00811     0.00806
log_std/max                           -0.11829     0.00030    -0.11793    -0.11870
log_std/min                           -0.15596     0.00053    -0.15504    -0.15652
log_probs/mean                        -2.73107     0.00699    -2.72234    -2.74019
log_probs/std                         0.24129      0.01871    0.27319     0.22443
log_probs/max                         -2.17369     0.02135    -2.13852    -2.19606
log_probs/min                         -5.04996     0.67710    -4.09755    -6.19269
mean/mean                             0.00196      0.00014    0.00215     0.00175
mean/std                              0.01429      0.00003    0.01434     0.01426
mean/max                              0.02744      0.00019    0.02765     0.02721
mean/min                              -0.01717     0.00019    -0.01691    -0.01744
------------------------------------  -----------  ---------  ----------  ---------
self.update_end_epoch 3750
sample: [7, 8, 2, 4, 0, 9, 1, 5, 6, 3]
replay_buffer._size: [11250 11250 11250 11250 11250 11250 11250 11250 11250 11250]
2023-08-12 10:41:28,937 MainThread INFO: EPOCH:73
2023-08-12 10:41:28,937 MainThread INFO: Time Consumed:0.2955613136291504s
2023-08-12 10:41:28,937 MainThread INFO: Total Frames:111000s
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 74/80 [01:22<00:05,  1.01it/s]------------------------------------  -----------  ----------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1579.60743
Train_Epoch_Reward                    7215.87636
Running_Training_Average_Rewards      826.52139
Explore_Time                          0.00219
Train___Time                          0.29089
Eval____Time                          0.00202
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.05544
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.97199
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.92381
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.22335
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.07431
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.82846
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.90088
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16038.76246
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.03247
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.67745
mean_success_rate                     0.00000

Name                                  Mean         Std         Max         Min
Reward_Mean                           9.66142      0.50694     10.42884    8.88291
alpha_0                               0.89529      0.00038     0.89583     0.89475
alpha_1                               0.89529      0.00038     0.89583     0.89475
alpha_2                               0.89531      0.00038     0.89585     0.89477
alpha_3                               0.89530      0.00038     0.89584     0.89477
alpha_4                               0.89531      0.00038     0.89585     0.89477
alpha_5                               0.89530      0.00038     0.89584     0.89476
alpha_6                               0.89533      0.00038     0.89586     0.89479
alpha_7                               0.89530      0.00038     0.89584     0.89476
alpha_8                               0.89528      0.00038     0.89582     0.89475
alpha_9                               0.89531      0.00038     0.89585     0.89477
Alpha_loss                            -0.74268     0.00259     -0.73891    -0.74629
Training/policy_loss                  -2.97209     0.00446     -2.96716    -2.97917
Training/qf1_loss                     2463.88433   1409.47075  5267.32471  1571.91504
Training/qf2_loss                     2456.18147   1408.95392  5258.59326  1563.93652
Training/pf_norm                      0.12574      0.02920     0.17006     0.08313
Training/qf1_norm                     37.78486     1.43815     40.14132    35.65238
Training/qf2_norm                     59.16717     3.34858     64.45024    53.90379
log_std/mean                          -0.13558     0.00003     -0.13553    -0.13562
log_std/std                           0.00805      0.00004     0.00811     0.00799
log_std/max                           -0.11912     0.00028     -0.11869    -0.11952
log_std/min                           -0.15728     0.00048     -0.15668    -0.15809
log_probs/mean                        -2.73365     0.00351     -2.72934    -2.73814
log_probs/std                         0.24868      0.01312     0.26758     0.22741
log_probs/max                         -2.15439     0.05524     -2.06831    -2.22753
log_probs/min                         -5.91749     0.74456     -4.94254    -7.07831
mean/mean                             0.00138      0.00015     0.00160     0.00116
mean/std                              0.01462      0.00019     0.01492     0.01441
mean/max                              0.02768      0.00033     0.02830     0.02741
mean/min                              -0.01789     0.00025     -0.01761    -0.01824
------------------------------------  -----------  ----------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4971, 0.5069, 0.4990, 0.5069, 0.5065, 0.4890, 0.4949, 0.5019, 0.4998,
         0.5026, 0.5102, 0.5003, 0.4958, 0.4886, 0.4920, 0.5075, 0.4922, 0.5102,
         0.4964, 0.5142],
        [0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4901, 0.4957, 0.5030, 0.5022,
         0.5031, 0.5085, 0.4989, 0.4960, 0.4882, 0.4900, 0.5075, 0.4915, 0.5091,
         0.4965, 0.5124],
        [0.4977, 0.5078, 0.4984, 0.5069, 0.5058, 0.4892, 0.4960, 0.5008, 0.5037,
         0.5018, 0.5074, 0.4986, 0.4959, 0.4883, 0.4897, 0.5091, 0.4930, 0.5096,
         0.4961, 0.5145],
        [0.4982, 0.5063, 0.4990, 0.5068, 0.5060, 0.4896, 0.4947, 0.5005, 0.5022,
         0.5038, 0.5074, 0.4984, 0.4966, 0.4880, 0.4897, 0.5076, 0.4923, 0.5104,
         0.4939, 0.5140],
        [0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4888, 0.4951, 0.4987, 0.5018,
         0.5030, 0.5088, 0.4990, 0.4961, 0.4883, 0.4915, 0.5061, 0.4947, 0.5098,
         0.4953, 0.5133],
        [0.4973, 0.5050, 0.4977, 0.5059, 0.5049, 0.4884, 0.4945, 0.5001, 0.5008,
         0.5031, 0.5079, 0.4996, 0.4951, 0.4879, 0.4901, 0.5081, 0.4952, 0.5085,
         0.4954, 0.5131],
        [0.4978, 0.5055, 0.4983, 0.5048, 0.5061, 0.4880, 0.4967, 0.5008, 0.5026,
         0.5022, 0.5099, 0.4996, 0.4958, 0.4875, 0.4906, 0.5082, 0.4948, 0.5081,
         0.4962, 0.5129],
        [0.4977, 0.5048, 0.4985, 0.5051, 0.5064, 0.4883, 0.4951, 0.5012, 0.5027,
         0.5019, 0.5100, 0.4996, 0.4961, 0.4889, 0.4919, 0.5077, 0.4930, 0.5065,
         0.4934, 0.5126],
        [0.4980, 0.5047, 0.4978, 0.5046, 0.5081, 0.4879, 0.4961, 0.5005, 0.5017,
         0.5011, 0.5069, 0.5011, 0.4970, 0.4889, 0.4924, 0.5084, 0.4929, 0.5080,
         0.4941, 0.5142],
        [0.4984, 0.5056, 0.4994, 0.5047, 0.5074, 0.4880, 0.4963, 0.5015, 0.5018,
         0.5039, 0.5076, 0.5004, 0.4943, 0.4909, 0.4929, 0.5094, 0.4940, 0.5049,
         0.4949, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4971, 0.5069, 0.4990, 0.5069, 0.5065, 0.4890, 0.4949, 0.5019, 0.4998,
        0.5026, 0.5102, 0.5003, 0.4958, 0.4886, 0.4920, 0.5075, 0.4922, 0.5102,
        0.4964, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4901, 0.4957, 0.5030, 0.5022,
        0.5031, 0.5085, 0.4989, 0.4960, 0.4882, 0.4900, 0.5075, 0.4915, 0.5091,
        0.4965, 0.5124], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5078, 0.4984, 0.5069, 0.5058, 0.4892, 0.4960, 0.5008, 0.5037,
        0.5018, 0.5074, 0.4986, 0.4959, 0.4883, 0.4897, 0.5091, 0.4930, 0.5096,
        0.4961, 0.5145], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5063, 0.4990, 0.5068, 0.5060, 0.4896, 0.4947, 0.5005, 0.5022,
        0.5038, 0.5074, 0.4984, 0.4966, 0.4880, 0.4897, 0.5076, 0.4923, 0.5104,
        0.4939, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4888, 0.4951, 0.4987, 0.5018,
        0.5030, 0.5088, 0.4990, 0.4961, 0.4883, 0.4915, 0.5061, 0.4947, 0.5098,
        0.4953, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4973, 0.5050, 0.4977, 0.5059, 0.5049, 0.4884, 0.4945, 0.5001, 0.5008,
        0.5031, 0.5079, 0.4996, 0.4951, 0.4879, 0.4901, 0.5081, 0.4952, 0.5085,
        0.4954, 0.5131], grad_fn=<UnbindBackward>), tensor([0.4978, 0.5055, 0.4983, 0.5048, 0.5061, 0.4880, 0.4967, 0.5008, 0.5026,
        0.5022, 0.5099, 0.4996, 0.4958, 0.4875, 0.4906, 0.5082, 0.4948, 0.5081,
        0.4962, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4985, 0.5051, 0.5064, 0.4883, 0.4951, 0.5012, 0.5027,
        0.5019, 0.5100, 0.4996, 0.4961, 0.4889, 0.4919, 0.5077, 0.4930, 0.5065,
        0.4934, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4978, 0.5046, 0.5081, 0.4879, 0.4961, 0.5005, 0.5017,
        0.5011, 0.5069, 0.5011, 0.4970, 0.4889, 0.4924, 0.5084, 0.4929, 0.5080,
        0.4941, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5056, 0.4994, 0.5047, 0.5074, 0.4880, 0.4963, 0.5015, 0.5018,
        0.5039, 0.5076, 0.5004, 0.4943, 0.4909, 0.4929, 0.5094, 0.4940, 0.5049,
        0.4949, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5102, 0.4893, 0.5120, 0.4862, 0.4852, 0.4943, 0.4896, 0.4940, 0.4987,
         0.5075, 0.5027, 0.5007, 0.5026, 0.4921, 0.5047, 0.4913, 0.4858, 0.4983,
         0.5078, 0.4946],
        [0.5089, 0.4897, 0.5107, 0.4849, 0.4856, 0.4930, 0.4883, 0.4954, 0.4984,
         0.5052, 0.5037, 0.5000, 0.5035, 0.4920, 0.5031, 0.4889, 0.4864, 0.4981,
         0.5072, 0.4932],
        [0.5099, 0.4905, 0.5108, 0.4866, 0.4854, 0.4944, 0.4891, 0.4959, 0.5004,
         0.5070, 0.5045, 0.5005, 0.5059, 0.4921, 0.5041, 0.4907, 0.4873, 0.4980,
         0.5068, 0.4925],
        [0.5079, 0.4896, 0.5117, 0.4860, 0.4873, 0.4935, 0.4902, 0.4948, 0.4981,
         0.5086, 0.5038, 0.5003, 0.5050, 0.4917, 0.5042, 0.4907, 0.4862, 0.4982,
         0.5055, 0.4924],
        [0.5094, 0.4911, 0.5127, 0.4860, 0.4870, 0.4932, 0.4907, 0.4926, 0.4960,
         0.5068, 0.5045, 0.5002, 0.5032, 0.4916, 0.5040, 0.4895, 0.4854, 0.4962,
         0.5056, 0.4935],
        [0.5109, 0.4917, 0.5101, 0.4866, 0.4861, 0.4949, 0.4909, 0.4939, 0.4970,
         0.5068, 0.5060, 0.5006, 0.5041, 0.4900, 0.5042, 0.4898, 0.4851, 0.4970,
         0.5061, 0.4929],
        [0.5098, 0.4917, 0.5126, 0.4879, 0.4862, 0.4938, 0.4888, 0.4941, 0.4978,
         0.5073, 0.5066, 0.4997, 0.5050, 0.4903, 0.5038, 0.4896, 0.4845, 0.4982,
         0.5073, 0.4933],
        [0.5101, 0.4916, 0.5117, 0.4852, 0.4870, 0.4925, 0.4891, 0.4945, 0.4997,
         0.5072, 0.5068, 0.5008, 0.5035, 0.4893, 0.5049, 0.4917, 0.4852, 0.4955,
         0.5076, 0.4935],
        [0.5113, 0.4928, 0.5111, 0.4852, 0.4889, 0.4928, 0.4892, 0.4942, 0.4986,
         0.5072, 0.5051, 0.4973, 0.5043, 0.4895, 0.5036, 0.4913, 0.4861, 0.4965,
         0.5083, 0.4938],
        [0.5113, 0.4924, 0.5119, 0.4868, 0.4886, 0.4927, 0.4893, 0.4931, 0.4999,
         0.5053, 0.5053, 0.4983, 0.5037, 0.4888, 0.5005, 0.4934, 0.4836, 0.4971,
         0.5060, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5102, 0.4893, 0.5120, 0.4862, 0.4852, 0.4943, 0.4896, 0.4940, 0.4987,
        0.5075, 0.5027, 0.5007, 0.5026, 0.4921, 0.5047, 0.4913, 0.4858, 0.4983,
        0.5078, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5089, 0.4897, 0.5107, 0.4849, 0.4856, 0.4930, 0.4883, 0.4954, 0.4984,
        0.5052, 0.5037, 0.5000, 0.5035, 0.4920, 0.5031, 0.4889, 0.4864, 0.4981,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5099, 0.4905, 0.5108, 0.4866, 0.4854, 0.4944, 0.4891, 0.4959, 0.5004,
        0.5070, 0.5045, 0.5005, 0.5059, 0.4921, 0.5041, 0.4907, 0.4873, 0.4980,
        0.5068, 0.4925], grad_fn=<UnbindBackward>), tensor([0.5079, 0.4896, 0.5117, 0.4860, 0.4873, 0.4935, 0.4902, 0.4948, 0.4981,
        0.5086, 0.5038, 0.5003, 0.5050, 0.4917, 0.5042, 0.4907, 0.4862, 0.4982,
        0.5055, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4911, 0.5127, 0.4860, 0.4870, 0.4932, 0.4907, 0.4926, 0.4960,
        0.5068, 0.5045, 0.5002, 0.5032, 0.4916, 0.5040, 0.4895, 0.4854, 0.4962,
        0.5056, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4917, 0.5101, 0.4866, 0.4861, 0.4949, 0.4909, 0.4939, 0.4970,
        0.5068, 0.5060, 0.5006, 0.5041, 0.4900, 0.5042, 0.4898, 0.4851, 0.4970,
        0.5061, 0.4929], grad_fn=<UnbindBackward>), tensor([0.5098, 0.4917, 0.5126, 0.4879, 0.4862, 0.4938, 0.4888, 0.4941, 0.4978,
        0.5073, 0.5066, 0.4997, 0.5050, 0.4903, 0.5038, 0.4896, 0.4845, 0.4982,
        0.5073, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5101, 0.4916, 0.5117, 0.4852, 0.4870, 0.4925, 0.4891, 0.4945, 0.4997,
        0.5072, 0.5068, 0.5008, 0.5035, 0.4893, 0.5049, 0.4917, 0.4852, 0.4955,
        0.5076, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4928, 0.5111, 0.4852, 0.4889, 0.4928, 0.4892, 0.4942, 0.4986,
        0.5072, 0.5051, 0.4973, 0.5043, 0.4895, 0.5036, 0.4913, 0.4861, 0.4965,
        0.5083, 0.4938], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4924, 0.5119, 0.4868, 0.4886, 0.4927, 0.4893, 0.4931, 0.4999,
        0.5053, 0.5053, 0.4983, 0.5037, 0.4888, 0.5005, 0.4934, 0.4836, 0.4971,
        0.5060, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4928, 0.5095, 0.4955, 0.4967,
         0.5027, 0.4854, 0.4978, 0.4838, 0.5018, 0.5023, 0.4982, 0.4991, 0.5145,
         0.4936, 0.4969],
        [0.5115, 0.4957, 0.4958, 0.5042, 0.5037, 0.4931, 0.5105, 0.4964, 0.4959,
         0.5024, 0.4850, 0.4966, 0.4854, 0.5016, 0.5001, 0.4979, 0.4981, 0.5151,
         0.4944, 0.4969],
        [0.5132, 0.4971, 0.4978, 0.5027, 0.5045, 0.4920, 0.5105, 0.4977, 0.4958,
         0.5025, 0.4857, 0.4986, 0.4845, 0.5019, 0.4998, 0.4978, 0.4992, 0.5166,
         0.4946, 0.4966],
        [0.5127, 0.4977, 0.4972, 0.5034, 0.5041, 0.4913, 0.5109, 0.4966, 0.4961,
         0.5027, 0.4840, 0.4968, 0.4844, 0.5022, 0.5009, 0.4977, 0.5013, 0.5146,
         0.4955, 0.4972],
        [0.5131, 0.4983, 0.4965, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4964,
         0.5030, 0.4865, 0.4976, 0.4825, 0.5030, 0.5003, 0.4996, 0.5001, 0.5153,
         0.4957, 0.4970],
        [0.5137, 0.4976, 0.4967, 0.5032, 0.5047, 0.4926, 0.5079, 0.4966, 0.4954,
         0.5024, 0.4856, 0.4962, 0.4849, 0.5036, 0.5016, 0.5007, 0.4997, 0.5149,
         0.4954, 0.4982],
        [0.5116, 0.4966, 0.4982, 0.5027, 0.5038, 0.4920, 0.5087, 0.4963, 0.4959,
         0.5024, 0.4873, 0.4971, 0.4851, 0.5029, 0.5009, 0.4992, 0.5000, 0.5141,
         0.4955, 0.4997],
        [0.5120, 0.4974, 0.4953, 0.5009, 0.5040, 0.4921, 0.5081, 0.4945, 0.4968,
         0.5020, 0.4860, 0.4982, 0.4864, 0.5014, 0.5009, 0.4989, 0.5000, 0.5153,
         0.4959, 0.4990],
        [0.5128, 0.4979, 0.4949, 0.5010, 0.5050, 0.4924, 0.5094, 0.4966, 0.4967,
         0.5018, 0.4883, 0.4953, 0.4855, 0.5013, 0.5011, 0.4974, 0.5007, 0.5144,
         0.4958, 0.4997],
        [0.5127, 0.4984, 0.4937, 0.5025, 0.5052, 0.4935, 0.5078, 0.4987, 0.4964,
         0.5030, 0.4884, 0.4976, 0.4858, 0.5000, 0.5012, 0.4982, 0.4997, 0.5153,
         0.4972, 0.5006]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5124, 0.4963, 0.4967, 0.5027, 0.5068, 0.4928, 0.5095, 0.4955, 0.4967,
        0.5027, 0.4854, 0.4978, 0.4838, 0.5018, 0.5023, 0.4982, 0.4991, 0.5145,
        0.4936, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4957, 0.4958, 0.5042, 0.5037, 0.4931, 0.5105, 0.4964, 0.4959,
        0.5024, 0.4850, 0.4966, 0.4854, 0.5016, 0.5001, 0.4979, 0.4981, 0.5151,
        0.4944, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5132, 0.4971, 0.4978, 0.5027, 0.5045, 0.4920, 0.5105, 0.4977, 0.4958,
        0.5025, 0.4857, 0.4986, 0.4845, 0.5019, 0.4998, 0.4978, 0.4992, 0.5166,
        0.4946, 0.4966], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4977, 0.4972, 0.5034, 0.5041, 0.4913, 0.5109, 0.4966, 0.4961,
        0.5027, 0.4840, 0.4968, 0.4844, 0.5022, 0.5009, 0.4977, 0.5013, 0.5146,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4983, 0.4965, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4964,
        0.5030, 0.4865, 0.4976, 0.4825, 0.5030, 0.5003, 0.4996, 0.5001, 0.5153,
        0.4957, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4976, 0.4967, 0.5032, 0.5047, 0.4926, 0.5079, 0.4966, 0.4954,
        0.5024, 0.4856, 0.4962, 0.4849, 0.5036, 0.5016, 0.5007, 0.4997, 0.5149,
        0.4954, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4966, 0.4982, 0.5027, 0.5038, 0.4920, 0.5087, 0.4963, 0.4959,
        0.5024, 0.4873, 0.4971, 0.4851, 0.5029, 0.5009, 0.4992, 0.5000, 0.5141,
        0.4955, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4974, 0.4953, 0.5009, 0.5040, 0.4921, 0.5081, 0.4945, 0.4968,
        0.5020, 0.4860, 0.4982, 0.4864, 0.5014, 0.5009, 0.4989, 0.5000, 0.5153,
        0.4959, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4979, 0.4949, 0.5010, 0.5050, 0.4924, 0.5094, 0.4966, 0.4967,
        0.5018, 0.4883, 0.4953, 0.4855, 0.5013, 0.5011, 0.4974, 0.5007, 0.5144,
        0.4958, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4984, 0.4937, 0.5025, 0.5052, 0.4935, 0.5078, 0.4987, 0.4964,
        0.5030, 0.4884, 0.4976, 0.4858, 0.5000, 0.5012, 0.4982, 0.4997, 0.5153,
        0.4972, 0.5006], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [8, 4, 1, 3, 7, 6, 9, 2, 5, 0]
replay_buffer._size: [11400 11400 11400 11400 11400 11400 11400 11400 11400 11400]
2023-08-12 10:41:29,950 MainThread INFO: EPOCH:74
2023-08-12 10:41:29,950 MainThread INFO: Time Consumed:0.615443229675293s
2023-08-12 10:41:29,950 MainThread INFO: Total Frames:112500s
 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 75/80 [01:23<00:04,  1.01it/s]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1581.57219
Train_Epoch_Reward                    6136.28608
Running_Training_Average_Rewards      883.05039
Explore_Time                          0.00318
Train___Time                          0.60811
Eval____Time                          0.00323
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -47.10419
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.04072
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.90704
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.25034
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -20.07364
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.82940
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.90340
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16057.57679
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.10806
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.63808
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.80152      0.70440    11.10771    9.08435
alpha_0                               0.89395      0.00038    0.89448     0.89341
alpha_1                               0.89395      0.00038    0.89449     0.89341
alpha_2                               0.89396      0.00038    0.89450     0.89343
alpha_3                               0.89396      0.00038    0.89450     0.89342
alpha_4                               0.89397      0.00038    0.89450     0.89343
alpha_5                               0.89396      0.00038    0.89449     0.89342
alpha_6                               0.89398      0.00038    0.89452     0.89345
alpha_7                               0.89396      0.00038    0.89449     0.89342
alpha_8                               0.89394      0.00038    0.89448     0.89340
alpha_9                               0.89397      0.00038    0.89450     0.89343
Alpha_loss                            -0.75254     0.00319    -0.74797    -0.75684
Training/policy_loss                  -2.98267     0.00723    -2.97260    -2.99079
Training/qf1_loss                     1846.13965   323.30724  2398.95557  1496.40466
Training/qf2_loss                     1838.26750   323.07569  2390.68066  1488.46545
Training/pf_norm                      0.10681      0.02147    0.13345     0.07300
Training/qf1_norm                     39.01750     2.51796    43.52303    35.77684
Training/qf2_norm                     60.73637     3.40969    66.47289    55.80416
log_std/mean                          -0.13543     0.00004    -0.13538    -0.13549
log_std/std                           0.00806      0.00002    0.00808     0.00802
log_std/max                           -0.11924     0.00040    -0.11879    -0.11972
log_std/min                           -0.15745     0.00044    -0.15663    -0.15794
log_probs/mean                        -2.73141     0.00436    -2.72635    -2.73781
log_probs/std                         0.23862      0.01434    0.25660     0.21899
log_probs/max                         -2.14054     0.03243    -2.08762    -2.17826
log_probs/min                         -5.09905     0.56758    -4.21807    -5.97406
mean/mean                             0.00103      0.00003    0.00108     0.00099
mean/std                              0.01546      0.00029    0.01590     0.01509
mean/max                              0.02941      0.00084    0.03062     0.02834
mean/min                              -0.01850     0.00010    -0.01841    -0.01869
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [9, 3, 0, 5, 2, 8, 4, 7, 1, 6]
replay_buffer._size: [11550 11550 11550 11550 11550 11550 11550 11550 11550 11550]
2023-08-12 10:41:31,457 MainThread INFO: EPOCH:75
2023-08-12 10:41:31,457 MainThread INFO: Time Consumed:1.4088749885559082s
2023-08-12 10:41:31,457 MainThread INFO: Total Frames:114000s
 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 76/80 [01:24<00:04,  1.15s/it]------------------------------------  -----------  ----------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1597.45089
Train_Epoch_Reward                    19229.14699
Running_Training_Average_Rewards      1086.04365
Explore_Time                          0.00232
Train___Time                          1.39904
Eval____Time                          0.00296
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.38645
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.02694
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -19.95612
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -21.11842
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.98007
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.74572
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.82113
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16217.15473
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.09931
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.51163
mean_success_rate                     0.00000

Name                                  Mean         Std         Max         Min
Reward_Mean                           9.99838      1.35268     12.45346    8.71330
alpha_0                               0.89260      0.00038     0.89314     0.89207
alpha_1                               0.89261      0.00038     0.89314     0.89207
alpha_2                               0.89262      0.00038     0.89316     0.89209
alpha_3                               0.89262      0.00038     0.89315     0.89208
alpha_4                               0.89263      0.00038     0.89316     0.89209
alpha_5                               0.89261      0.00038     0.89315     0.89208
alpha_6                               0.89264      0.00038     0.89318     0.89211
alpha_7                               0.89261      0.00038     0.89315     0.89208
alpha_8                               0.89260      0.00038     0.89314     0.89206
alpha_9                               0.89263      0.00038     0.89316     0.89209
Alpha_loss                            -0.76268     0.00256     -0.75909    -0.76572
Training/policy_loss                  -2.99766     0.00314     -2.99447    -3.00234
Training/qf1_loss                     2769.45654   1962.63099  6659.02051  1492.33167
Training/qf2_loss                     2761.14910   1961.81756  6649.18359  1484.70959
Training/pf_norm                      0.11596      0.02358     0.15088     0.08629
Training/qf1_norm                     40.24913     4.03033     47.49097    36.29231
Training/qf2_norm                     63.01980     6.50059     74.62098    56.67667
log_std/mean                          -0.13507     0.00023     -0.13472    -0.13535
log_std/std                           0.00795      0.00003     0.00798     0.00790
log_std/max                           -0.11908     0.00027     -0.11863    -0.11945
log_std/min                           -0.15710     0.00062     -0.15597    -0.15778
log_probs/mean                        -2.73172     0.00542     -2.72290    -2.73826
log_probs/std                         0.22993      0.00972     0.24585     0.21725
log_probs/max                         -2.14851     0.04280     -2.06518    -2.18665
log_probs/min                         -5.15298     0.61815     -4.32619    -5.94197
mean/mean                             0.00077      0.00011     0.00092     0.00059
mean/std                              0.01649      0.00027     0.01689     0.01611
mean/max                              0.03140      0.00040     0.03186     0.03068
mean/min                              -0.01938     0.00035     -0.01891    -0.01989
------------------------------------  -----------  ----------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4971, 0.5069, 0.4991, 0.5070, 0.5065, 0.4889, 0.4949, 0.5019, 0.4999,
         0.5027, 0.5102, 0.5003, 0.4959, 0.4886, 0.4919, 0.5076, 0.4922, 0.5102,
         0.4963, 0.5143],
        [0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4901, 0.4957, 0.5031, 0.5021,
         0.5031, 0.5086, 0.4990, 0.4960, 0.4882, 0.4900, 0.5075, 0.4915, 0.5091,
         0.4966, 0.5124],
        [0.4977, 0.5078, 0.4984, 0.5070, 0.5057, 0.4892, 0.4959, 0.5008, 0.5036,
         0.5018, 0.5074, 0.4985, 0.4959, 0.4882, 0.4897, 0.5092, 0.4930, 0.5096,
         0.4961, 0.5146],
        [0.4983, 0.5063, 0.4990, 0.5068, 0.5059, 0.4896, 0.4947, 0.5005, 0.5022,
         0.5038, 0.5074, 0.4984, 0.4966, 0.4880, 0.4897, 0.5076, 0.4923, 0.5104,
         0.4939, 0.5140],
        [0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4889, 0.4951, 0.4987, 0.5018,
         0.5031, 0.5088, 0.4990, 0.4960, 0.4883, 0.4915, 0.5061, 0.4947, 0.5098,
         0.4953, 0.5133],
        [0.4973, 0.5050, 0.4978, 0.5060, 0.5049, 0.4883, 0.4945, 0.5001, 0.5007,
         0.5031, 0.5080, 0.4996, 0.4951, 0.4879, 0.4901, 0.5081, 0.4952, 0.5085,
         0.4954, 0.5131],
        [0.4977, 0.5055, 0.4983, 0.5048, 0.5061, 0.4880, 0.4967, 0.5008, 0.5026,
         0.5022, 0.5100, 0.4997, 0.4958, 0.4874, 0.4905, 0.5083, 0.4948, 0.5081,
         0.4963, 0.5129],
        [0.4977, 0.5048, 0.4985, 0.5051, 0.5064, 0.4882, 0.4951, 0.5013, 0.5028,
         0.5019, 0.5099, 0.4996, 0.4962, 0.4889, 0.4919, 0.5077, 0.4929, 0.5065,
         0.4934, 0.5126],
        [0.4980, 0.5047, 0.4978, 0.5046, 0.5081, 0.4878, 0.4960, 0.5005, 0.5017,
         0.5012, 0.5069, 0.5010, 0.4970, 0.4889, 0.4924, 0.5084, 0.4929, 0.5080,
         0.4941, 0.5143],
        [0.4984, 0.5055, 0.4994, 0.5047, 0.5075, 0.4880, 0.4963, 0.5015, 0.5018,
         0.5039, 0.5075, 0.5004, 0.4944, 0.4910, 0.4930, 0.5094, 0.4940, 0.5049,
         0.4949, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4971, 0.5069, 0.4991, 0.5070, 0.5065, 0.4889, 0.4949, 0.5019, 0.4999,
        0.5027, 0.5102, 0.5003, 0.4959, 0.4886, 0.4919, 0.5076, 0.4922, 0.5102,
        0.4963, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4901, 0.4957, 0.5031, 0.5021,
        0.5031, 0.5086, 0.4990, 0.4960, 0.4882, 0.4900, 0.5075, 0.4915, 0.5091,
        0.4966, 0.5124], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5078, 0.4984, 0.5070, 0.5057, 0.4892, 0.4959, 0.5008, 0.5036,
        0.5018, 0.5074, 0.4985, 0.4959, 0.4882, 0.4897, 0.5092, 0.4930, 0.5096,
        0.4961, 0.5146], grad_fn=<UnbindBackward>), tensor([0.4983, 0.5063, 0.4990, 0.5068, 0.5059, 0.4896, 0.4947, 0.5005, 0.5022,
        0.5038, 0.5074, 0.4984, 0.4966, 0.4880, 0.4897, 0.5076, 0.4923, 0.5104,
        0.4939, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4889, 0.4951, 0.4987, 0.5018,
        0.5031, 0.5088, 0.4990, 0.4960, 0.4883, 0.4915, 0.5061, 0.4947, 0.5098,
        0.4953, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4973, 0.5050, 0.4978, 0.5060, 0.5049, 0.4883, 0.4945, 0.5001, 0.5007,
        0.5031, 0.5080, 0.4996, 0.4951, 0.4879, 0.4901, 0.5081, 0.4952, 0.5085,
        0.4954, 0.5131], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5055, 0.4983, 0.5048, 0.5061, 0.4880, 0.4967, 0.5008, 0.5026,
        0.5022, 0.5100, 0.4997, 0.4958, 0.4874, 0.4905, 0.5083, 0.4948, 0.5081,
        0.4963, 0.5129], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4985, 0.5051, 0.5064, 0.4882, 0.4951, 0.5013, 0.5028,
        0.5019, 0.5099, 0.4996, 0.4962, 0.4889, 0.4919, 0.5077, 0.4929, 0.5065,
        0.4934, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5047, 0.4978, 0.5046, 0.5081, 0.4878, 0.4960, 0.5005, 0.5017,
        0.5012, 0.5069, 0.5010, 0.4970, 0.4889, 0.4924, 0.5084, 0.4929, 0.5080,
        0.4941, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5055, 0.4994, 0.5047, 0.5075, 0.4880, 0.4963, 0.5015, 0.5018,
        0.5039, 0.5075, 0.5004, 0.4944, 0.4910, 0.4930, 0.5094, 0.4940, 0.5049,
        0.4949, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5101, 0.4894, 0.5120, 0.4862, 0.4851, 0.4943, 0.4896, 0.4939, 0.4987,
         0.5075, 0.5027, 0.5007, 0.5025, 0.4921, 0.5048, 0.4913, 0.4858, 0.4983,
         0.5078, 0.4946],
        [0.5088, 0.4897, 0.5107, 0.4848, 0.4855, 0.4930, 0.4883, 0.4953, 0.4984,
         0.5052, 0.5037, 0.5000, 0.5035, 0.4921, 0.5032, 0.4888, 0.4865, 0.4981,
         0.5072, 0.4931],
        [0.5098, 0.4906, 0.5109, 0.4866, 0.4855, 0.4944, 0.4892, 0.4957, 0.5003,
         0.5070, 0.5045, 0.5005, 0.5058, 0.4921, 0.5041, 0.4906, 0.4872, 0.4979,
         0.5068, 0.4926],
        [0.5078, 0.4896, 0.5117, 0.4860, 0.4873, 0.4935, 0.4902, 0.4948, 0.4981,
         0.5086, 0.5038, 0.5003, 0.5050, 0.4917, 0.5042, 0.4907, 0.4862, 0.4982,
         0.5056, 0.4924],
        [0.5094, 0.4910, 0.5126, 0.4860, 0.4870, 0.4932, 0.4907, 0.4927, 0.4960,
         0.5068, 0.5045, 0.5003, 0.5033, 0.4917, 0.5040, 0.4895, 0.4854, 0.4962,
         0.5056, 0.4935],
        [0.5109, 0.4917, 0.5101, 0.4866, 0.4862, 0.4949, 0.4909, 0.4939, 0.4970,
         0.5068, 0.5060, 0.5006, 0.5041, 0.4900, 0.5042, 0.4899, 0.4851, 0.4970,
         0.5061, 0.4929],
        [0.5097, 0.4918, 0.5126, 0.4878, 0.4861, 0.4938, 0.4888, 0.4941, 0.4978,
         0.5073, 0.5065, 0.4997, 0.5050, 0.4904, 0.5038, 0.4895, 0.4845, 0.4982,
         0.5074, 0.4933],
        [0.5100, 0.4917, 0.5117, 0.4853, 0.4870, 0.4925, 0.4890, 0.4944, 0.4998,
         0.5072, 0.5068, 0.5009, 0.5035, 0.4894, 0.5049, 0.4916, 0.4851, 0.4955,
         0.5076, 0.4935],
        [0.5113, 0.4929, 0.5111, 0.4852, 0.4889, 0.4928, 0.4892, 0.4942, 0.4988,
         0.5072, 0.5051, 0.4973, 0.5043, 0.4895, 0.5036, 0.4913, 0.4860, 0.4965,
         0.5082, 0.4938],
        [0.5113, 0.4925, 0.5119, 0.4868, 0.4886, 0.4927, 0.4892, 0.4931, 0.5000,
         0.5054, 0.5053, 0.4983, 0.5038, 0.4889, 0.5005, 0.4933, 0.4836, 0.4971,
         0.5060, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5101, 0.4894, 0.5120, 0.4862, 0.4851, 0.4943, 0.4896, 0.4939, 0.4987,
        0.5075, 0.5027, 0.5007, 0.5025, 0.4921, 0.5048, 0.4913, 0.4858, 0.4983,
        0.5078, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4897, 0.5107, 0.4848, 0.4855, 0.4930, 0.4883, 0.4953, 0.4984,
        0.5052, 0.5037, 0.5000, 0.5035, 0.4921, 0.5032, 0.4888, 0.4865, 0.4981,
        0.5072, 0.4931], grad_fn=<UnbindBackward>), tensor([0.5098, 0.4906, 0.5109, 0.4866, 0.4855, 0.4944, 0.4892, 0.4957, 0.5003,
        0.5070, 0.5045, 0.5005, 0.5058, 0.4921, 0.5041, 0.4906, 0.4872, 0.4979,
        0.5068, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4896, 0.5117, 0.4860, 0.4873, 0.4935, 0.4902, 0.4948, 0.4981,
        0.5086, 0.5038, 0.5003, 0.5050, 0.4917, 0.5042, 0.4907, 0.4862, 0.4982,
        0.5056, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4910, 0.5126, 0.4860, 0.4870, 0.4932, 0.4907, 0.4927, 0.4960,
        0.5068, 0.5045, 0.5003, 0.5033, 0.4917, 0.5040, 0.4895, 0.4854, 0.4962,
        0.5056, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4917, 0.5101, 0.4866, 0.4862, 0.4949, 0.4909, 0.4939, 0.4970,
        0.5068, 0.5060, 0.5006, 0.5041, 0.4900, 0.5042, 0.4899, 0.4851, 0.4970,
        0.5061, 0.4929], grad_fn=<UnbindBackward>), tensor([0.5097, 0.4918, 0.5126, 0.4878, 0.4861, 0.4938, 0.4888, 0.4941, 0.4978,
        0.5073, 0.5065, 0.4997, 0.5050, 0.4904, 0.5038, 0.4895, 0.4845, 0.4982,
        0.5074, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5100, 0.4917, 0.5117, 0.4853, 0.4870, 0.4925, 0.4890, 0.4944, 0.4998,
        0.5072, 0.5068, 0.5009, 0.5035, 0.4894, 0.5049, 0.4916, 0.4851, 0.4955,
        0.5076, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4929, 0.5111, 0.4852, 0.4889, 0.4928, 0.4892, 0.4942, 0.4988,
        0.5072, 0.5051, 0.4973, 0.5043, 0.4895, 0.5036, 0.4913, 0.4860, 0.4965,
        0.5082, 0.4938], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4925, 0.5119, 0.4868, 0.4886, 0.4927, 0.4892, 0.4931, 0.5000,
        0.5054, 0.5053, 0.4983, 0.5038, 0.4889, 0.5005, 0.4933, 0.4836, 0.4971,
        0.5060, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5123, 0.4962, 0.4967, 0.5027, 0.5068, 0.4928, 0.5095, 0.4954, 0.4967,
         0.5027, 0.4854, 0.4978, 0.4839, 0.5018, 0.5023, 0.4981, 0.4991, 0.5144,
         0.4936, 0.4969],
        [0.5115, 0.4957, 0.4958, 0.5042, 0.5038, 0.4932, 0.5105, 0.4964, 0.4959,
         0.5024, 0.4851, 0.4966, 0.4854, 0.5015, 0.5001, 0.4978, 0.4981, 0.5150,
         0.4943, 0.4969],
        [0.5132, 0.4971, 0.4978, 0.5027, 0.5045, 0.4920, 0.5106, 0.4977, 0.4958,
         0.5025, 0.4857, 0.4985, 0.4845, 0.5019, 0.4998, 0.4978, 0.4992, 0.5165,
         0.4946, 0.4966],
        [0.5126, 0.4977, 0.4972, 0.5034, 0.5041, 0.4913, 0.5109, 0.4966, 0.4961,
         0.5027, 0.4840, 0.4969, 0.4844, 0.5022, 0.5009, 0.4977, 0.5013, 0.5146,
         0.4955, 0.4972],
        [0.5131, 0.4983, 0.4965, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4963,
         0.5029, 0.4865, 0.4976, 0.4825, 0.5031, 0.5003, 0.4997, 0.5002, 0.5153,
         0.4957, 0.4970],
        [0.5137, 0.4976, 0.4967, 0.5032, 0.5047, 0.4926, 0.5079, 0.4966, 0.4954,
         0.5024, 0.4856, 0.4962, 0.4849, 0.5036, 0.5016, 0.5007, 0.4997, 0.5149,
         0.4954, 0.4982],
        [0.5117, 0.4966, 0.4981, 0.5026, 0.5038, 0.4920, 0.5086, 0.4963, 0.4959,
         0.5025, 0.4873, 0.4972, 0.4851, 0.5027, 0.5009, 0.4992, 0.5000, 0.5141,
         0.4955, 0.4997],
        [0.5120, 0.4974, 0.4953, 0.5010, 0.5040, 0.4921, 0.5082, 0.4945, 0.4968,
         0.5020, 0.4860, 0.4982, 0.4864, 0.5014, 0.5010, 0.4989, 0.5000, 0.5153,
         0.4959, 0.4991],
        [0.5127, 0.4979, 0.4949, 0.5011, 0.5050, 0.4924, 0.5095, 0.4965, 0.4967,
         0.5018, 0.4884, 0.4952, 0.4855, 0.5013, 0.5012, 0.4974, 0.5007, 0.5145,
         0.4958, 0.4997],
        [0.5127, 0.4985, 0.4937, 0.5025, 0.5052, 0.4935, 0.5079, 0.4987, 0.4964,
         0.5030, 0.4884, 0.4976, 0.4858, 0.5000, 0.5013, 0.4981, 0.4998, 0.5153,
         0.4972, 0.5007]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5123, 0.4962, 0.4967, 0.5027, 0.5068, 0.4928, 0.5095, 0.4954, 0.4967,
        0.5027, 0.4854, 0.4978, 0.4839, 0.5018, 0.5023, 0.4981, 0.4991, 0.5144,
        0.4936, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4957, 0.4958, 0.5042, 0.5038, 0.4932, 0.5105, 0.4964, 0.4959,
        0.5024, 0.4851, 0.4966, 0.4854, 0.5015, 0.5001, 0.4978, 0.4981, 0.5150,
        0.4943, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5132, 0.4971, 0.4978, 0.5027, 0.5045, 0.4920, 0.5106, 0.4977, 0.4958,
        0.5025, 0.4857, 0.4985, 0.4845, 0.5019, 0.4998, 0.4978, 0.4992, 0.5165,
        0.4946, 0.4966], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4977, 0.4972, 0.5034, 0.5041, 0.4913, 0.5109, 0.4966, 0.4961,
        0.5027, 0.4840, 0.4969, 0.4844, 0.5022, 0.5009, 0.4977, 0.5013, 0.5146,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4983, 0.4965, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4963,
        0.5029, 0.4865, 0.4976, 0.4825, 0.5031, 0.5003, 0.4997, 0.5002, 0.5153,
        0.4957, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4976, 0.4967, 0.5032, 0.5047, 0.4926, 0.5079, 0.4966, 0.4954,
        0.5024, 0.4856, 0.4962, 0.4849, 0.5036, 0.5016, 0.5007, 0.4997, 0.5149,
        0.4954, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5117, 0.4966, 0.4981, 0.5026, 0.5038, 0.4920, 0.5086, 0.4963, 0.4959,
        0.5025, 0.4873, 0.4972, 0.4851, 0.5027, 0.5009, 0.4992, 0.5000, 0.5141,
        0.4955, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4974, 0.4953, 0.5010, 0.5040, 0.4921, 0.5082, 0.4945, 0.4968,
        0.5020, 0.4860, 0.4982, 0.4864, 0.5014, 0.5010, 0.4989, 0.5000, 0.5153,
        0.4959, 0.4991], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4979, 0.4949, 0.5011, 0.5050, 0.4924, 0.5095, 0.4965, 0.4967,
        0.5018, 0.4884, 0.4952, 0.4855, 0.5013, 0.5012, 0.4974, 0.5007, 0.5145,
        0.4958, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4937, 0.5025, 0.5052, 0.4935, 0.5079, 0.4987, 0.4964,
        0.5030, 0.4884, 0.4976, 0.4858, 0.5000, 0.5013, 0.4981, 0.4998, 0.5153,
        0.4972, 0.5007], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [7, 5, 1, 8, 0, 9, 2, 6, 4, 3]
replay_buffer._size: [11700 11700 11700 11700 11700 11700 11700 11700 11700 11700]
snapshot at best
2023-08-12 10:41:33,240 MainThread INFO: EPOCH:76
2023-08-12 10:41:33,240 MainThread INFO: Time Consumed:1.2927570343017578s
2023-08-12 10:41:33,240 MainThread INFO: Total Frames:115500s
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 77/80 [01:26<00:04,  1.36s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1633.60382
Train_Epoch_Reward                    7080.12915
Running_Training_Average_Rewards      1081.51874
Explore_Time                          0.00309
Train___Time                          0.75274
Eval____Time                          0.00909
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -50.31847
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.92526
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.14698
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -20.89734
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.86098
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.75475
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.82981
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16580.08957
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.00305
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.31478
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.22321      0.45719    9.83957     8.60143
alpha_0                               0.89127      0.00038    0.89180     0.89073
alpha_1                               0.89127      0.00038    0.89180     0.89073
alpha_2                               0.89128      0.00038    0.89182     0.89075
alpha_3                               0.89128      0.00038    0.89181     0.89074
alpha_4                               0.89129      0.00038    0.89182     0.89075
alpha_5                               0.89127      0.00038    0.89181     0.89074
alpha_6                               0.89131      0.00038    0.89184     0.89077
alpha_7                               0.89128      0.00038    0.89181     0.89074
alpha_8                               0.89126      0.00038    0.89179     0.89072
alpha_9                               0.89129      0.00038    0.89182     0.89075
Alpha_loss                            -0.77352     0.00288    -0.76957    -0.77765
Training/policy_loss                  -3.01786     0.00588    -3.00838    -3.02362
Training/qf1_loss                     1840.46409   243.82199  2078.61084  1398.02417
Training/qf2_loss                     1832.71365   243.64051  2070.22974  1390.57361
Training/pf_norm                      0.09962      0.02190    0.12308     0.06396
Training/qf1_norm                     38.47824     1.63180    41.08773    36.19299
Training/qf2_norm                     59.45821     2.46787    63.95357    56.42243
log_std/mean                          -0.13430     0.00016    -0.13407    -0.13452
log_std/std                           0.00787      0.00005    0.00795     0.00779
log_std/max                           -0.11855     0.00018    -0.11840    -0.11890
log_std/min                           -0.15635     0.00048    -0.15574    -0.15690
log_probs/mean                        -2.73808     0.00508    -2.72843    -2.74322
log_probs/std                         0.24864      0.01707    0.27463     0.22162
log_probs/max                         -2.13467     0.04311    -2.06966    -2.18141
log_probs/min                         -5.51365     0.93641    -4.34935    -6.80915
mean/mean                             0.00028      0.00011    0.00044     0.00012
mean/std                              0.01739      0.00020    0.01768     0.01708
mean/max                              0.03262      0.00019    0.03282     0.03228
mean/min                              -0.02089     0.00035    -0.02038    -0.02141
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [7, 5, 2, 0, 1, 9, 8, 6, 3, 4]
replay_buffer._size: [11850 11850 11850 11850 11850 11850 11850 11850 11850 11850]
snapshot at best
2023-08-12 10:41:35,178 MainThread INFO: EPOCH:77
2023-08-12 10:41:35,178 MainThread INFO: Time Consumed:1.7140374183654785s
2023-08-12 10:41:35,179 MainThread INFO: Total Frames:117000s
 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 78/80 [01:28<00:03,  1.53s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1660.30886
Train_Epoch_Reward                    11655.97838
Running_Training_Average_Rewards      1265.50848
Explore_Time                          0.02048
Train___Time                          0.88523
Eval____Time                          0.00363
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -45.30794
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.94892
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.23979
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -20.75768
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.60299
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.70618
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.78254
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 16841.59722
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.03741
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -23.12519
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           8.92937      0.45680    9.43226     8.16766
alpha_0                               0.88993      0.00038    0.89046     0.88939
alpha_1                               0.88993      0.00038    0.89047     0.88940
alpha_2                               0.88994      0.00038    0.89048     0.88941
alpha_3                               0.88994      0.00038    0.89048     0.88941
alpha_4                               0.88995      0.00038    0.89048     0.88941
alpha_5                               0.88994      0.00038    0.89047     0.88940
alpha_6                               0.88997      0.00038    0.89050     0.88943
alpha_7                               0.88994      0.00038    0.89047     0.88940
alpha_8                               0.88992      0.00038    0.89046     0.88939
alpha_9                               0.88995      0.00038    0.89049     0.88942
Alpha_loss                            -0.78261     0.00210    -0.77978    -0.78549
Training/policy_loss                  -3.02435     0.00314    -3.02052    -3.02978
Training/qf1_loss                     1451.29124   263.53436  1853.53540  1066.25281
Training/qf2_loss                     1443.47434   263.39601  1845.66772  1058.95605
Training/pf_norm                      0.11555      0.01327    0.13013     0.09356
Training/qf1_norm                     38.08198     1.45703    39.71329    35.73923
Training/qf2_norm                     59.07761     2.38562    61.22305    55.12701
log_std/mean                          -0.13361     0.00023    -0.13327    -0.13397
log_std/std                           0.00772      0.00006    0.00780     0.00764
log_std/max                           -0.11794     0.00024    -0.11769    -0.11836
log_std/min                           -0.15499     0.00078    -0.15366    -0.15590
log_probs/mean                        -2.72926     0.00717    -2.71934    -2.73968
log_probs/std                         0.22951      0.00984    0.24720     0.21796
log_probs/max                         -2.12198     0.02136    -2.09561    -2.14856
log_probs/min                         -4.90715     1.07550    -4.01415    -7.00395
mean/mean                             -0.00010     0.00011    0.00007     -0.00020
mean/std                              0.01823      0.00032    0.01871     0.01785
mean/max                              0.03371      0.00047    0.03456     0.03333
mean/min                              -0.02261     0.00063    -0.02181    -0.02358
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
start to update mask
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.],
        [0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]]), tensor([[1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.],
        [1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]])]
normalized_mask tensor([[0.4970, 0.5069, 0.4991, 0.5070, 0.5064, 0.4890, 0.4949, 0.5019, 0.4999,
         0.5027, 0.5102, 0.5002, 0.4959, 0.4886, 0.4919, 0.5076, 0.4922, 0.5102,
         0.4963, 0.5143],
        [0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4901, 0.4957, 0.5031, 0.5021,
         0.5032, 0.5086, 0.4989, 0.4959, 0.4882, 0.4900, 0.5075, 0.4915, 0.5091,
         0.4966, 0.5124],
        [0.4977, 0.5078, 0.4984, 0.5070, 0.5057, 0.4892, 0.4959, 0.5008, 0.5036,
         0.5018, 0.5074, 0.4985, 0.4959, 0.4883, 0.4897, 0.5091, 0.4930, 0.5097,
         0.4961, 0.5145],
        [0.4982, 0.5063, 0.4990, 0.5068, 0.5059, 0.4896, 0.4947, 0.5006, 0.5022,
         0.5038, 0.5075, 0.4984, 0.4966, 0.4879, 0.4897, 0.5076, 0.4923, 0.5104,
         0.4939, 0.5140],
        [0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4888, 0.4950, 0.4987, 0.5018,
         0.5031, 0.5088, 0.4989, 0.4960, 0.4883, 0.4915, 0.5061, 0.4947, 0.5098,
         0.4953, 0.5133],
        [0.4972, 0.5050, 0.4978, 0.5060, 0.5049, 0.4883, 0.4946, 0.5001, 0.5007,
         0.5032, 0.5080, 0.4996, 0.4951, 0.4879, 0.4901, 0.5081, 0.4952, 0.5085,
         0.4955, 0.5131],
        [0.4976, 0.5055, 0.4984, 0.5048, 0.5061, 0.4880, 0.4967, 0.5008, 0.5026,
         0.5022, 0.5100, 0.4995, 0.4958, 0.4873, 0.4906, 0.5084, 0.4949, 0.5080,
         0.4962, 0.5130],
        [0.4977, 0.5048, 0.4985, 0.5051, 0.5064, 0.4883, 0.4951, 0.5013, 0.5027,
         0.5019, 0.5100, 0.4996, 0.4961, 0.4889, 0.4919, 0.5077, 0.4929, 0.5066,
         0.4934, 0.5126],
        [0.4980, 0.5046, 0.4979, 0.5046, 0.5081, 0.4879, 0.4961, 0.5005, 0.5017,
         0.5012, 0.5070, 0.5011, 0.4969, 0.4889, 0.4924, 0.5084, 0.4929, 0.5079,
         0.4942, 0.5142],
        [0.4984, 0.5055, 0.4994, 0.5047, 0.5075, 0.4880, 0.4963, 0.5015, 0.5018,
         0.5039, 0.5075, 0.5004, 0.4944, 0.4909, 0.4930, 0.5094, 0.4940, 0.5049,
         0.4948, 0.5154]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])], [tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 1.]), tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.4970, 0.5069, 0.4991, 0.5070, 0.5064, 0.4890, 0.4949, 0.5019, 0.4999,
        0.5027, 0.5102, 0.5002, 0.4959, 0.4886, 0.4919, 0.5076, 0.4922, 0.5102,
        0.4963, 0.5143], grad_fn=<UnbindBackward>), tensor([0.4981, 0.5076, 0.4991, 0.5064, 0.5066, 0.4901, 0.4957, 0.5031, 0.5021,
        0.5032, 0.5086, 0.4989, 0.4959, 0.4882, 0.4900, 0.5075, 0.4915, 0.5091,
        0.4966, 0.5124], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5078, 0.4984, 0.5070, 0.5057, 0.4892, 0.4959, 0.5008, 0.5036,
        0.5018, 0.5074, 0.4985, 0.4959, 0.4883, 0.4897, 0.5091, 0.4930, 0.5097,
        0.4961, 0.5145], grad_fn=<UnbindBackward>), tensor([0.4982, 0.5063, 0.4990, 0.5068, 0.5059, 0.4896, 0.4947, 0.5006, 0.5022,
        0.5038, 0.5075, 0.4984, 0.4966, 0.4879, 0.4897, 0.5076, 0.4923, 0.5104,
        0.4939, 0.5140], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5078, 0.4984, 0.5053, 0.5052, 0.4888, 0.4950, 0.4987, 0.5018,
        0.5031, 0.5088, 0.4989, 0.4960, 0.4883, 0.4915, 0.5061, 0.4947, 0.5098,
        0.4953, 0.5133], grad_fn=<UnbindBackward>), tensor([0.4972, 0.5050, 0.4978, 0.5060, 0.5049, 0.4883, 0.4946, 0.5001, 0.5007,
        0.5032, 0.5080, 0.4996, 0.4951, 0.4879, 0.4901, 0.5081, 0.4952, 0.5085,
        0.4955, 0.5131], grad_fn=<UnbindBackward>), tensor([0.4976, 0.5055, 0.4984, 0.5048, 0.5061, 0.4880, 0.4967, 0.5008, 0.5026,
        0.5022, 0.5100, 0.4995, 0.4958, 0.4873, 0.4906, 0.5084, 0.4949, 0.5080,
        0.4962, 0.5130], grad_fn=<UnbindBackward>), tensor([0.4977, 0.5048, 0.4985, 0.5051, 0.5064, 0.4883, 0.4951, 0.5013, 0.5027,
        0.5019, 0.5100, 0.4996, 0.4961, 0.4889, 0.4919, 0.5077, 0.4929, 0.5066,
        0.4934, 0.5126], grad_fn=<UnbindBackward>), tensor([0.4980, 0.5046, 0.4979, 0.5046, 0.5081, 0.4879, 0.4961, 0.5005, 0.5017,
        0.5012, 0.5070, 0.5011, 0.4969, 0.4889, 0.4924, 0.5084, 0.4929, 0.5079,
        0.4942, 0.5142], grad_fn=<UnbindBackward>), tensor([0.4984, 0.5055, 0.4994, 0.5047, 0.5075, 0.4880, 0.4963, 0.5015, 0.5018,
        0.5039, 0.5075, 0.5004, 0.4944, 0.4909, 0.4930, 0.5094, 0.4940, 0.5049,
        0.4948, 0.5154], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 1., 0., 0., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.],
        [1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]]), tensor([[1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.],
        [1., 1., 1., 0., 1., 0., 0., 0., 1., 0.]])]
normalized_mask tensor([[0.5101, 0.4894, 0.5120, 0.4862, 0.4851, 0.4943, 0.4896, 0.4939, 0.4987,
         0.5075, 0.5027, 0.5007, 0.5026, 0.4921, 0.5048, 0.4914, 0.4858, 0.4983,
         0.5078, 0.4946],
        [0.5088, 0.4897, 0.5107, 0.4848, 0.4856, 0.4930, 0.4883, 0.4953, 0.4985,
         0.5052, 0.5037, 0.5000, 0.5035, 0.4921, 0.5032, 0.4889, 0.4865, 0.4981,
         0.5072, 0.4932],
        [0.5098, 0.4906, 0.5109, 0.4865, 0.4855, 0.4944, 0.4893, 0.4957, 0.5002,
         0.5070, 0.5045, 0.5005, 0.5058, 0.4921, 0.5041, 0.4907, 0.4872, 0.4979,
         0.5067, 0.4926],
        [0.5078, 0.4896, 0.5117, 0.4860, 0.4873, 0.4934, 0.4902, 0.4947, 0.4981,
         0.5086, 0.5037, 0.5004, 0.5050, 0.4918, 0.5042, 0.4907, 0.4862, 0.4982,
         0.5055, 0.4924],
        [0.5094, 0.4911, 0.5126, 0.4860, 0.4869, 0.4933, 0.4907, 0.4927, 0.4960,
         0.5068, 0.5045, 0.5003, 0.5033, 0.4917, 0.5040, 0.4895, 0.4855, 0.4962,
         0.5056, 0.4935],
        [0.5109, 0.4917, 0.5100, 0.4866, 0.4861, 0.4949, 0.4910, 0.4939, 0.4970,
         0.5068, 0.5060, 0.5006, 0.5041, 0.4900, 0.5042, 0.4899, 0.4851, 0.4970,
         0.5061, 0.4929],
        [0.5098, 0.4918, 0.5126, 0.4878, 0.4861, 0.4937, 0.4888, 0.4941, 0.4979,
         0.5073, 0.5065, 0.4996, 0.5050, 0.4903, 0.5038, 0.4896, 0.4845, 0.4982,
         0.5073, 0.4933],
        [0.5100, 0.4917, 0.5118, 0.4852, 0.4870, 0.4925, 0.4891, 0.4944, 0.4998,
         0.5073, 0.5069, 0.5009, 0.5035, 0.4893, 0.5049, 0.4917, 0.4851, 0.4956,
         0.5076, 0.4935],
        [0.5113, 0.4928, 0.5111, 0.4852, 0.4889, 0.4927, 0.4891, 0.4943, 0.4987,
         0.5072, 0.5050, 0.4973, 0.5043, 0.4896, 0.5035, 0.4914, 0.4860, 0.4965,
         0.5083, 0.4938],
        [0.5113, 0.4925, 0.5119, 0.4868, 0.4887, 0.4927, 0.4892, 0.4931, 0.4999,
         0.5054, 0.5053, 0.4983, 0.5038, 0.4889, 0.5005, 0.4934, 0.4837, 0.4971,
         0.5060, 0.4941]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])], [tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 1.]), tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0.])]]
batch_task_probs_masks [tensor([0.5101, 0.4894, 0.5120, 0.4862, 0.4851, 0.4943, 0.4896, 0.4939, 0.4987,
        0.5075, 0.5027, 0.5007, 0.5026, 0.4921, 0.5048, 0.4914, 0.4858, 0.4983,
        0.5078, 0.4946], grad_fn=<UnbindBackward>), tensor([0.5088, 0.4897, 0.5107, 0.4848, 0.4856, 0.4930, 0.4883, 0.4953, 0.4985,
        0.5052, 0.5037, 0.5000, 0.5035, 0.4921, 0.5032, 0.4889, 0.4865, 0.4981,
        0.5072, 0.4932], grad_fn=<UnbindBackward>), tensor([0.5098, 0.4906, 0.5109, 0.4865, 0.4855, 0.4944, 0.4893, 0.4957, 0.5002,
        0.5070, 0.5045, 0.5005, 0.5058, 0.4921, 0.5041, 0.4907, 0.4872, 0.4979,
        0.5067, 0.4926], grad_fn=<UnbindBackward>), tensor([0.5078, 0.4896, 0.5117, 0.4860, 0.4873, 0.4934, 0.4902, 0.4947, 0.4981,
        0.5086, 0.5037, 0.5004, 0.5050, 0.4918, 0.5042, 0.4907, 0.4862, 0.4982,
        0.5055, 0.4924], grad_fn=<UnbindBackward>), tensor([0.5094, 0.4911, 0.5126, 0.4860, 0.4869, 0.4933, 0.4907, 0.4927, 0.4960,
        0.5068, 0.5045, 0.5003, 0.5033, 0.4917, 0.5040, 0.4895, 0.4855, 0.4962,
        0.5056, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5109, 0.4917, 0.5100, 0.4866, 0.4861, 0.4949, 0.4910, 0.4939, 0.4970,
        0.5068, 0.5060, 0.5006, 0.5041, 0.4900, 0.5042, 0.4899, 0.4851, 0.4970,
        0.5061, 0.4929], grad_fn=<UnbindBackward>), tensor([0.5098, 0.4918, 0.5126, 0.4878, 0.4861, 0.4937, 0.4888, 0.4941, 0.4979,
        0.5073, 0.5065, 0.4996, 0.5050, 0.4903, 0.5038, 0.4896, 0.4845, 0.4982,
        0.5073, 0.4933], grad_fn=<UnbindBackward>), tensor([0.5100, 0.4917, 0.5118, 0.4852, 0.4870, 0.4925, 0.4891, 0.4944, 0.4998,
        0.5073, 0.5069, 0.5009, 0.5035, 0.4893, 0.5049, 0.4917, 0.4851, 0.4956,
        0.5076, 0.4935], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4928, 0.5111, 0.4852, 0.4889, 0.4927, 0.4891, 0.4943, 0.4987,
        0.5072, 0.5050, 0.4973, 0.5043, 0.4896, 0.5035, 0.4914, 0.4860, 0.4965,
        0.5083, 0.4938], grad_fn=<UnbindBackward>), tensor([0.5113, 0.4925, 0.5119, 0.4868, 0.4887, 0.4927, 0.4892, 0.4931, 0.4999,
        0.5054, 0.5053, 0.4983, 0.5038, 0.4889, 0.5005, 0.4934, 0.4837, 0.4971,
        0.5060, 0.4941], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
task_onehot_batch torch.Size([10, 10])
tmp.shape torch.Size([10, 151, 19])
mask_vector torch.Size([10, 20])
task_binary_masks [tensor([[1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.],
        [1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]]), tensor([[0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 1., 0., 1., 1., 0., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.],
        [0., 0., 0., 1., 1., 0., 1., 1., 0., 1.]])]
normalized_mask tensor([[0.5123, 0.4962, 0.4967, 0.5028, 0.5068, 0.4928, 0.5095, 0.4954, 0.4967,
         0.5027, 0.4854, 0.4978, 0.4838, 0.5018, 0.5023, 0.4981, 0.4991, 0.5144,
         0.4936, 0.4969],
        [0.5115, 0.4957, 0.4958, 0.5042, 0.5038, 0.4931, 0.5105, 0.4964, 0.4959,
         0.5024, 0.4851, 0.4966, 0.4854, 0.5016, 0.5001, 0.4979, 0.4981, 0.5150,
         0.4943, 0.4969],
        [0.5132, 0.4970, 0.4978, 0.5027, 0.5045, 0.4920, 0.5105, 0.4976, 0.4958,
         0.5025, 0.4857, 0.4985, 0.4845, 0.5020, 0.4998, 0.4979, 0.4992, 0.5166,
         0.4946, 0.4966],
        [0.5126, 0.4977, 0.4973, 0.5034, 0.5042, 0.4913, 0.5109, 0.4966, 0.4961,
         0.5028, 0.4839, 0.4969, 0.4843, 0.5022, 0.5009, 0.4977, 0.5013, 0.5146,
         0.4955, 0.4972],
        [0.5131, 0.4983, 0.4966, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4963,
         0.5029, 0.4865, 0.4975, 0.4825, 0.5031, 0.5004, 0.4997, 0.5002, 0.5153,
         0.4957, 0.4970],
        [0.5137, 0.4975, 0.4968, 0.5032, 0.5047, 0.4926, 0.5079, 0.4966, 0.4954,
         0.5023, 0.4857, 0.4961, 0.4849, 0.5035, 0.5015, 0.5007, 0.4997, 0.5149,
         0.4954, 0.4982],
        [0.5116, 0.4965, 0.4981, 0.5026, 0.5038, 0.4921, 0.5087, 0.4964, 0.4959,
         0.5025, 0.4873, 0.4971, 0.4852, 0.5028, 0.5009, 0.4992, 0.5002, 0.5141,
         0.4956, 0.4997],
        [0.5120, 0.4974, 0.4953, 0.5009, 0.5041, 0.4921, 0.5082, 0.4945, 0.4968,
         0.5020, 0.4861, 0.4982, 0.4864, 0.5014, 0.5009, 0.4989, 0.5000, 0.5153,
         0.4959, 0.4990],
        [0.5128, 0.4980, 0.4949, 0.5010, 0.5050, 0.4924, 0.5094, 0.4966, 0.4966,
         0.5019, 0.4883, 0.4953, 0.4855, 0.5013, 0.5011, 0.4974, 0.5006, 0.5144,
         0.4958, 0.4996],
        [0.5127, 0.4985, 0.4937, 0.5025, 0.5052, 0.4935, 0.5078, 0.4987, 0.4964,
         0.5030, 0.4884, 0.4976, 0.4858, 0.5000, 0.5013, 0.4982, 0.4998, 0.5153,
         0.4972, 0.5007]], grad_fn=<SigmoidBackward>)
converted_list [[tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])], [tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 1.])]]
batch_task_probs_masks [tensor([0.5123, 0.4962, 0.4967, 0.5028, 0.5068, 0.4928, 0.5095, 0.4954, 0.4967,
        0.5027, 0.4854, 0.4978, 0.4838, 0.5018, 0.5023, 0.4981, 0.4991, 0.5144,
        0.4936, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5115, 0.4957, 0.4958, 0.5042, 0.5038, 0.4931, 0.5105, 0.4964, 0.4959,
        0.5024, 0.4851, 0.4966, 0.4854, 0.5016, 0.5001, 0.4979, 0.4981, 0.5150,
        0.4943, 0.4969], grad_fn=<UnbindBackward>), tensor([0.5132, 0.4970, 0.4978, 0.5027, 0.5045, 0.4920, 0.5105, 0.4976, 0.4958,
        0.5025, 0.4857, 0.4985, 0.4845, 0.5020, 0.4998, 0.4979, 0.4992, 0.5166,
        0.4946, 0.4966], grad_fn=<UnbindBackward>), tensor([0.5126, 0.4977, 0.4973, 0.5034, 0.5042, 0.4913, 0.5109, 0.4966, 0.4961,
        0.5028, 0.4839, 0.4969, 0.4843, 0.5022, 0.5009, 0.4977, 0.5013, 0.5146,
        0.4955, 0.4972], grad_fn=<UnbindBackward>), tensor([0.5131, 0.4983, 0.4966, 0.5040, 0.5033, 0.4933, 0.5089, 0.4953, 0.4963,
        0.5029, 0.4865, 0.4975, 0.4825, 0.5031, 0.5004, 0.4997, 0.5002, 0.5153,
        0.4957, 0.4970], grad_fn=<UnbindBackward>), tensor([0.5137, 0.4975, 0.4968, 0.5032, 0.5047, 0.4926, 0.5079, 0.4966, 0.4954,
        0.5023, 0.4857, 0.4961, 0.4849, 0.5035, 0.5015, 0.5007, 0.4997, 0.5149,
        0.4954, 0.4982], grad_fn=<UnbindBackward>), tensor([0.5116, 0.4965, 0.4981, 0.5026, 0.5038, 0.4921, 0.5087, 0.4964, 0.4959,
        0.5025, 0.4873, 0.4971, 0.4852, 0.5028, 0.5009, 0.4992, 0.5002, 0.5141,
        0.4956, 0.4997], grad_fn=<UnbindBackward>), tensor([0.5120, 0.4974, 0.4953, 0.5009, 0.5041, 0.4921, 0.5082, 0.4945, 0.4968,
        0.5020, 0.4861, 0.4982, 0.4864, 0.5014, 0.5009, 0.4989, 0.5000, 0.5153,
        0.4959, 0.4990], grad_fn=<UnbindBackward>), tensor([0.5128, 0.4980, 0.4949, 0.5010, 0.5050, 0.4924, 0.5094, 0.4966, 0.4966,
        0.5019, 0.4883, 0.4953, 0.4855, 0.5013, 0.5011, 0.4974, 0.5006, 0.5144,
        0.4958, 0.4996], grad_fn=<UnbindBackward>), tensor([0.5127, 0.4985, 0.4937, 0.5025, 0.5052, 0.4935, 0.5078, 0.4987, 0.4964,
        0.5030, 0.4884, 0.4976, 0.4858, 0.5000, 0.5013, 0.4982, 0.4998, 0.5153,
        0.4972, 0.5007], grad_fn=<UnbindBackward>)]
tmp.shape torch.Size([10, 151, 19])
torch.Size([10, 64])
encoding torch.Size([10, 64])
sample: [3, 2, 5, 1, 7, 8, 9, 6, 0, 4]
replay_buffer._size: [12000 12000 12000 12000 12000 12000 12000 12000 12000 12000]
snapshot at best
2023-08-12 10:41:37,796 MainThread INFO: EPOCH:78
2023-08-12 10:41:37,797 MainThread INFO: Time Consumed:1.8997230529785156s
2023-08-12 10:41:37,797 MainThread INFO: Total Frames:118500s
 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 79/80 [01:31<00:01,  1.85s/it]------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1686.63437
Train_Epoch_Reward                    6004.38770
Running_Training_Average_Rewards      824.68317
Explore_Time                          0.00334
Train___Time                          1.27454
Eval____Time                          0.00341
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -42.12923
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -41.99582
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.34560
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -20.68430
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.46631
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.74255
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.81964
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 17101.57857
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.09592
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -22.95552
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.00874      0.21958    9.39237     8.78005
alpha_0                               0.88859      0.00038    0.88913     0.88806
alpha_1                               0.88859      0.00038    0.88913     0.88806
alpha_2                               0.88861      0.00038    0.88914     0.88807
alpha_3                               0.88861      0.00038    0.88914     0.88807
alpha_4                               0.88861      0.00038    0.88915     0.88808
alpha_5                               0.88860      0.00038    0.88914     0.88807
alpha_6                               0.88863      0.00038    0.88917     0.88810
alpha_7                               0.88860      0.00038    0.88914     0.88807
alpha_8                               0.88859      0.00038    0.88912     0.88805
alpha_9                               0.88862      0.00038    0.88915     0.88808
Alpha_loss                            -0.79295     0.00302    -0.78901    -0.79753
Training/policy_loss                  -3.04164     0.00736    -3.03396    -3.05216
Training/qf1_loss                     1600.65796   127.99260  1785.16956  1440.17249
Training/qf2_loss                     1592.96687   127.51819  1776.58203  1432.96875
Training/pf_norm                      0.10764      0.03039    0.14964     0.06233
Training/qf1_norm                     38.83458     0.74968    40.17370    38.00209
Training/qf2_norm                     59.24745     2.30059    62.01738    56.74160
log_std/mean                          -0.13311     0.00009    -0.13302    -0.13324
log_std/std                           0.00757      0.00004    0.00764     0.00752
log_std/max                           -0.11776     0.00031    -0.11728    -0.11822
log_std/min                           -0.15349     0.00060    -0.15276    -0.15448
log_probs/mean                        -2.73125     0.00449    -2.72596    -2.73626
log_probs/std                         0.23619      0.01324    0.25131     0.21803
log_probs/max                         -2.10931     0.06369    -2.03152    -2.18524
log_probs/min                         -4.68790     0.47910    -4.07098    -5.19928
mean/mean                             -0.00004     0.00010    0.00007     -0.00019
mean/std                              0.01931      0.00029    0.01970     0.01890
mean/max                              0.03547      0.00058    0.03623     0.03481
mean/min                              -0.02442     0.00039    -0.02387    -0.02501
------------------------------------  -----------  ---------  ----------  ----------
self.update_end_epoch 3750
sample: [8, 3, 1, 4, 2, 6, 9, 5, 7, 0]
replay_buffer._size: [12150 12150 12150 12150 12150 12150 12150 12150 12150 12150]
snapshot at best
2023-08-12 10:41:41,101 MainThread INFO: EPOCH:79
2023-08-12 10:41:41,102 MainThread INFO: Time Consumed:3.1362149715423584s
2023-08-12 10:41:41,102 MainThread INFO: Total Frames:120000s
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [01:34<00:00,  2.28s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [01:34<00:00,  1.18s/it]
------------------------------------  -----------  ---------  ----------  ----------
Name                                  Value
Running_Average_Rewards               1721.69078
Train_Epoch_Reward                    13749.45935
Running_Training_Average_Rewards      1046.99418
Explore_Time                          0.00318
Train___Time                          2.46508
Eval____Time                          0.02129
button-press-topdown-v1_success_rate  0.00000
button-press-topdown-v1_eval_rewards  -48.90375
door-v1_success_rate                  0.00000
door-v1_eval_rewards                  -42.06829
drawer-close-v1_success_rate          0.00000
drawer-close-v1_eval_rewards          -20.48045
drawer-open-v1_success_rate           0.00000
drawer-open-v1_eval_rewards           -20.58164
ped-insert-side-v1_success_rate       0.00000
ped-insert-side-v1_eval_rewards       -19.31939
pick-place-v1_success_rate            0.00000
pick-place-v1_eval_rewards            -20.77188
push-v1_success_rate                  0.00000
push-v1_eval_rewards                  -20.84910
reach-v1_success_rate                 0.00000
reach-v1_eval_rewards                 17458.78754
window-close-v1_success_rate          0.00000
window-close-v1_eval_rewards          -26.18363
window-open-v1_success_rate           0.00000
window-open-v1_eval_rewards           -22.72162
mean_success_rate                     0.00000

Name                                  Mean         Std        Max         Min
Reward_Mean                           9.39895      0.49931    10.17518    8.77979
alpha_0                               0.88726      0.00038    0.88779     0.88673
alpha_1                               0.88726      0.00038    0.88779     0.88673
alpha_2                               0.88727      0.00038    0.88780     0.88674
alpha_3                               0.88727      0.00038    0.88781     0.88674
alpha_4                               0.88728      0.00038    0.88781     0.88675
alpha_5                               0.88727      0.00038    0.88780     0.88674
alpha_6                               0.88730      0.00038    0.88783     0.88677
alpha_7                               0.88727      0.00038    0.88780     0.88674
alpha_8                               0.88726      0.00038    0.88779     0.88672
alpha_9                               0.88728      0.00038    0.88782     0.88675
Alpha_loss                            -0.80341     0.00323    -0.79904    -0.80837
Training/policy_loss                  -3.05876     0.00824    -3.04839    -3.07213
Training/qf1_loss                     1753.29026   277.01192  2261.92969  1502.68335
Training/qf2_loss                     1744.60391   276.52234  2252.53052  1494.82800
Training/pf_norm                      0.13102      0.03132    0.17208     0.07957
Training/qf1_norm                     41.39309     1.85822    44.38971    39.23143
Training/qf2_norm                     64.47562     3.28562    69.56228    60.83106
log_std/mean                          -0.13330     0.00013    -0.13318    -0.13353
log_std/std                           0.00762      0.00005    0.00771     0.00757
log_std/max                           -0.11795     0.00020    -0.11775    -0.11821
log_std/min                           -0.15425     0.00038    -0.15368    -0.15474
log_probs/mean                        -2.73420     0.00451    -2.72826    -2.74183
log_probs/std                         0.23311      0.00924    0.24183     0.21786
log_probs/max                         -2.13759     0.06021    -2.06491    -2.22957
log_probs/min                         -4.89771     0.27475    -4.47848    -5.25728
mean/mean                             0.00045      0.00021    0.00074     0.00016
mean/std                              0.02050      0.00046    0.02119     0.01991
mean/max                              0.03809      0.00118    0.03982     0.03661
mean/min                              -0.02532     0.00026    -0.02493    -0.02568
------------------------------------  -----------  ---------  ----------  ----------
snapshot at finish
Process Process-10:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-7:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-3:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-2:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-8:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-5:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-11:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-4:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-6:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-9:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 283, in train_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-18:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-15:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-17:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-21:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-16:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-20:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-12:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-14:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-13:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Process Process-19:
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/async_mt.py", line 390, in eval_worker_process
    shared_que.put({
  File "<string>", line 2, in put
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/spawn.py", line 129, in _main
    return self._bootstrap(parent_sentinel)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 331, in _bootstrap
    traceback.print_exc()
KeyboardInterrupt
Traceback (most recent call last):
  File "starter/mt_must_sac.py", line 303, in <module>
    experiment(args)
  File "starter/mt_must_sac.py", line 298, in experiment
    agent.train(env.num_tasks,params)
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/algo/rl_algo.py", line 448, in train
  File "/scratch/qianxi/t3s/t3s_code/./torchrl/collector/para/base.py", line 266, in terminate
    p.join()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/wandb/sdk/wandb_manager.py", line 186, in _teardown
    result = self._service.join()
  File "/scratch/qianxi/t3s/venv/lib/python3.8/site-packages/wandb/sdk/service/service.py", line 235, in join
    ret = self._internal_proc.wait()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/subprocess.py", line 1083, in wait
    return self._wait(timeout=timeout)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/subprocess.py", line 1806, in _wait
    (pid, sts) = self._try_wait(0)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/subprocess.py", line 1764, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
